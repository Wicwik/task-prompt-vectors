{
  "best_metric": 1.8082343339920044,
  "best_model_checkpoint": "llama31_test/checkpoint-1250",
  "epoch": 5.0,
  "eval_steps": 250,
  "global_step": 1250,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004,
      "grad_norm": 0.4784339964389801,
      "learning_rate": 5.263157894736842e-06,
      "loss": 6.214,
      "step": 1
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.4392923414707184,
      "learning_rate": 1.0526315789473684e-05,
      "loss": 5.934,
      "step": 2
    },
    {
      "epoch": 0.012,
      "grad_norm": 0.5393584370613098,
      "learning_rate": 1.5789473684210526e-05,
      "loss": 6.0088,
      "step": 3
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.4156465530395508,
      "learning_rate": 2.105263157894737e-05,
      "loss": 6.2645,
      "step": 4
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.4890686273574829,
      "learning_rate": 2.6315789473684212e-05,
      "loss": 6.0765,
      "step": 5
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.3873763382434845,
      "learning_rate": 3.157894736842105e-05,
      "loss": 6.366,
      "step": 6
    },
    {
      "epoch": 0.028,
      "grad_norm": 0.39986780285835266,
      "learning_rate": 3.6842105263157895e-05,
      "loss": 6.3909,
      "step": 7
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.4439467787742615,
      "learning_rate": 4.210526315789474e-05,
      "loss": 6.1008,
      "step": 8
    },
    {
      "epoch": 0.036,
      "grad_norm": 0.4216740131378174,
      "learning_rate": 4.736842105263158e-05,
      "loss": 6.1863,
      "step": 9
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.4025573432445526,
      "learning_rate": 5.2631578947368424e-05,
      "loss": 6.3604,
      "step": 10
    },
    {
      "epoch": 0.044,
      "grad_norm": 0.45535969734191895,
      "learning_rate": 5.789473684210527e-05,
      "loss": 5.7532,
      "step": 11
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.4248896837234497,
      "learning_rate": 6.31578947368421e-05,
      "loss": 5.9956,
      "step": 12
    },
    {
      "epoch": 0.052,
      "grad_norm": 0.4309670627117157,
      "learning_rate": 6.842105263157895e-05,
      "loss": 6.2867,
      "step": 13
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.4236027002334595,
      "learning_rate": 7.368421052631579e-05,
      "loss": 6.1302,
      "step": 14
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.4374712407588959,
      "learning_rate": 7.894736842105263e-05,
      "loss": 6.2312,
      "step": 15
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.4149826467037201,
      "learning_rate": 8.421052631578948e-05,
      "loss": 5.8842,
      "step": 16
    },
    {
      "epoch": 0.068,
      "grad_norm": 0.445383220911026,
      "learning_rate": 8.947368421052632e-05,
      "loss": 5.615,
      "step": 17
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.4713725447654724,
      "learning_rate": 9.473684210526316e-05,
      "loss": 5.802,
      "step": 18
    },
    {
      "epoch": 0.076,
      "grad_norm": 0.4527994394302368,
      "learning_rate": 0.0001,
      "loss": 6.1326,
      "step": 19
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.5209766030311584,
      "learning_rate": 0.00010526315789473685,
      "loss": 5.9146,
      "step": 20
    },
    {
      "epoch": 0.084,
      "grad_norm": 0.4161616265773773,
      "learning_rate": 0.0001105263157894737,
      "loss": 5.4767,
      "step": 21
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.4446420967578888,
      "learning_rate": 0.00011578947368421053,
      "loss": 5.9121,
      "step": 22
    },
    {
      "epoch": 0.092,
      "grad_norm": 0.4683161973953247,
      "learning_rate": 0.00012105263157894738,
      "loss": 5.9461,
      "step": 23
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.5022540092468262,
      "learning_rate": 0.0001263157894736842,
      "loss": 5.988,
      "step": 24
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.5101831555366516,
      "learning_rate": 0.00013157894736842108,
      "loss": 6.0137,
      "step": 25
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.46853986382484436,
      "learning_rate": 0.0001368421052631579,
      "loss": 6.1885,
      "step": 26
    },
    {
      "epoch": 0.108,
      "grad_norm": 0.513238787651062,
      "learning_rate": 0.00014210526315789474,
      "loss": 5.6406,
      "step": 27
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.5133530497550964,
      "learning_rate": 0.00014736842105263158,
      "loss": 5.5967,
      "step": 28
    },
    {
      "epoch": 0.116,
      "grad_norm": 0.5018254518508911,
      "learning_rate": 0.00015263157894736845,
      "loss": 5.8969,
      "step": 29
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.4603661298751831,
      "learning_rate": 0.00015789473684210527,
      "loss": 5.9683,
      "step": 30
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.5139949321746826,
      "learning_rate": 0.0001631578947368421,
      "loss": 5.7497,
      "step": 31
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.5443980693817139,
      "learning_rate": 0.00016842105263157895,
      "loss": 5.4531,
      "step": 32
    },
    {
      "epoch": 0.132,
      "grad_norm": 0.44543200731277466,
      "learning_rate": 0.0001736842105263158,
      "loss": 6.1245,
      "step": 33
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.5164801478385925,
      "learning_rate": 0.00017894736842105264,
      "loss": 5.5738,
      "step": 34
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.5367088913917542,
      "learning_rate": 0.00018421052631578948,
      "loss": 5.3527,
      "step": 35
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.5045109391212463,
      "learning_rate": 0.00018947368421052632,
      "loss": 5.4463,
      "step": 36
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.4642709791660309,
      "learning_rate": 0.00019473684210526317,
      "loss": 5.492,
      "step": 37
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.46966925263404846,
      "learning_rate": 0.0002,
      "loss": 5.7257,
      "step": 38
    },
    {
      "epoch": 0.156,
      "grad_norm": 0.4432699382305145,
      "learning_rate": 0.00019999966405802826,
      "loss": 5.3454,
      "step": 39
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.45373111963272095,
      "learning_rate": 0.00019999865623437013,
      "loss": 5.4597,
      "step": 40
    },
    {
      "epoch": 0.164,
      "grad_norm": 0.45346054434776306,
      "learning_rate": 0.00019999697653579705,
      "loss": 5.6333,
      "step": 41
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.4783134162425995,
      "learning_rate": 0.00019999462497359466,
      "loss": 5.0227,
      "step": 42
    },
    {
      "epoch": 0.172,
      "grad_norm": 0.4400278627872467,
      "learning_rate": 0.0001999916015635627,
      "loss": 5.4072,
      "step": 43
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.393835186958313,
      "learning_rate": 0.00019998790632601496,
      "loss": 4.9995,
      "step": 44
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.4605180621147156,
      "learning_rate": 0.00019998353928577919,
      "loss": 5.1187,
      "step": 45
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.38661497831344604,
      "learning_rate": 0.0001999785004721968,
      "loss": 5.214,
      "step": 46
    },
    {
      "epoch": 0.188,
      "grad_norm": 0.5314667224884033,
      "learning_rate": 0.0001999727899191228,
      "loss": 5.4626,
      "step": 47
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.3709624707698822,
      "learning_rate": 0.00019996640766492543,
      "loss": 5.0064,
      "step": 48
    },
    {
      "epoch": 0.196,
      "grad_norm": 0.448943555355072,
      "learning_rate": 0.00019995935375248606,
      "loss": 4.8794,
      "step": 49
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.386583149433136,
      "learning_rate": 0.00019995162822919883,
      "loss": 4.8934,
      "step": 50
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.41233915090560913,
      "learning_rate": 0.00019994323114697022,
      "loss": 4.9477,
      "step": 51
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.35428494215011597,
      "learning_rate": 0.00019993416256221895,
      "loss": 4.9791,
      "step": 52
    },
    {
      "epoch": 0.212,
      "grad_norm": 0.3947752118110657,
      "learning_rate": 0.0001999244225358753,
      "loss": 5.1636,
      "step": 53
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.3787893056869507,
      "learning_rate": 0.00019991401113338104,
      "loss": 5.2237,
      "step": 54
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.35882118344306946,
      "learning_rate": 0.00019990292842468868,
      "loss": 4.8061,
      "step": 55
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.4556887149810791,
      "learning_rate": 0.00019989117448426108,
      "loss": 4.8841,
      "step": 56
    },
    {
      "epoch": 0.228,
      "grad_norm": 0.3702075481414795,
      "learning_rate": 0.0001998787493910712,
      "loss": 4.9646,
      "step": 57
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.35284265875816345,
      "learning_rate": 0.00019986565322860115,
      "loss": 4.9417,
      "step": 58
    },
    {
      "epoch": 0.236,
      "grad_norm": 0.340786874294281,
      "learning_rate": 0.000199851886084842,
      "loss": 4.3131,
      "step": 59
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.473077654838562,
      "learning_rate": 0.00019983744805229296,
      "loss": 4.6141,
      "step": 60
    },
    {
      "epoch": 0.244,
      "grad_norm": 0.33983147144317627,
      "learning_rate": 0.00019982233922796085,
      "loss": 4.7423,
      "step": 61
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.37472233176231384,
      "learning_rate": 0.00019980655971335945,
      "loss": 4.9183,
      "step": 62
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.44570332765579224,
      "learning_rate": 0.00019979010961450878,
      "loss": 4.7789,
      "step": 63
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.356198787689209,
      "learning_rate": 0.00019977298904193437,
      "loss": 4.6349,
      "step": 64
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.35470113158226013,
      "learning_rate": 0.00019975519811066663,
      "loss": 4.5819,
      "step": 65
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.3320401906967163,
      "learning_rate": 0.00019973673694024,
      "loss": 4.606,
      "step": 66
    },
    {
      "epoch": 0.268,
      "grad_norm": 0.3716651499271393,
      "learning_rate": 0.0001997176056546921,
      "loss": 4.6923,
      "step": 67
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.3037758767604828,
      "learning_rate": 0.00019969780438256293,
      "loss": 4.4587,
      "step": 68
    },
    {
      "epoch": 0.276,
      "grad_norm": 0.2729921340942383,
      "learning_rate": 0.0001996773332568941,
      "loss": 4.695,
      "step": 69
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.3160737454891205,
      "learning_rate": 0.0001996561924152278,
      "loss": 4.8538,
      "step": 70
    },
    {
      "epoch": 0.284,
      "grad_norm": 0.3136674165725708,
      "learning_rate": 0.00019963438199960599,
      "loss": 4.3914,
      "step": 71
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.27826938033103943,
      "learning_rate": 0.0001996119021565693,
      "loss": 4.3679,
      "step": 72
    },
    {
      "epoch": 0.292,
      "grad_norm": 0.27338913083076477,
      "learning_rate": 0.00019958875303715615,
      "loss": 4.5426,
      "step": 73
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.2611103057861328,
      "learning_rate": 0.0001995649347969019,
      "loss": 4.1769,
      "step": 74
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.2666304111480713,
      "learning_rate": 0.0001995404475958373,
      "loss": 4.3617,
      "step": 75
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.2848054766654968,
      "learning_rate": 0.00019951529159848805,
      "loss": 4.3225,
      "step": 76
    },
    {
      "epoch": 0.308,
      "grad_norm": 0.25759223103523254,
      "learning_rate": 0.0001994894669738732,
      "loss": 4.0179,
      "step": 77
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.24610958993434906,
      "learning_rate": 0.00019946297389550433,
      "loss": 4.2192,
      "step": 78
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.27180609107017517,
      "learning_rate": 0.0001994358125413841,
      "loss": 4.3505,
      "step": 79
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.2435264140367508,
      "learning_rate": 0.00019940798309400526,
      "loss": 4.0923,
      "step": 80
    },
    {
      "epoch": 0.324,
      "grad_norm": 0.26358747482299805,
      "learning_rate": 0.0001993794857403495,
      "loss": 4.0708,
      "step": 81
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.22146102786064148,
      "learning_rate": 0.0001993503206718859,
      "loss": 3.9416,
      "step": 82
    },
    {
      "epoch": 0.332,
      "grad_norm": 0.2412416785955429,
      "learning_rate": 0.0001993204880845699,
      "loss": 4.3656,
      "step": 83
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.24067983031272888,
      "learning_rate": 0.00019928998817884182,
      "loss": 4.4357,
      "step": 84
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.20288822054862976,
      "learning_rate": 0.00019925882115962568,
      "loss": 4.2211,
      "step": 85
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.2635680139064789,
      "learning_rate": 0.00019922698723632767,
      "loss": 4.636,
      "step": 86
    },
    {
      "epoch": 0.348,
      "grad_norm": 0.2533942461013794,
      "learning_rate": 0.00019919448662283478,
      "loss": 4.218,
      "step": 87
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.2142430692911148,
      "learning_rate": 0.00019916131953751342,
      "loss": 4.2783,
      "step": 88
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.22049568593502045,
      "learning_rate": 0.00019912748620320794,
      "loss": 3.9361,
      "step": 89
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.228661447763443,
      "learning_rate": 0.00019909298684723904,
      "loss": 4.3481,
      "step": 90
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.23301751911640167,
      "learning_rate": 0.00019905782170140238,
      "loss": 4.1397,
      "step": 91
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.23621246218681335,
      "learning_rate": 0.00019902199100196697,
      "loss": 4.0657,
      "step": 92
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.23004989326000214,
      "learning_rate": 0.00019898549498967343,
      "loss": 4.3948,
      "step": 93
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.2750054597854614,
      "learning_rate": 0.00019894833390973266,
      "loss": 4.5767,
      "step": 94
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.23061150312423706,
      "learning_rate": 0.000198910508011824,
      "loss": 4.2543,
      "step": 95
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.21883933246135712,
      "learning_rate": 0.00019887201755009357,
      "loss": 4.0949,
      "step": 96
    },
    {
      "epoch": 0.388,
      "grad_norm": 0.22329269349575043,
      "learning_rate": 0.00019883286278315262,
      "loss": 4.0383,
      "step": 97
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.18738536536693573,
      "learning_rate": 0.0001987930439740757,
      "loss": 3.9865,
      "step": 98
    },
    {
      "epoch": 0.396,
      "grad_norm": 0.22126416862010956,
      "learning_rate": 0.00019875256139039902,
      "loss": 4.1082,
      "step": 99
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.18853801488876343,
      "learning_rate": 0.00019871141530411853,
      "loss": 3.9139,
      "step": 100
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.19810861349105835,
      "learning_rate": 0.00019866960599168826,
      "loss": 3.9028,
      "step": 101
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.21376661956310272,
      "learning_rate": 0.0001986271337340182,
      "loss": 3.996,
      "step": 102
    },
    {
      "epoch": 0.412,
      "grad_norm": 0.22600379586219788,
      "learning_rate": 0.0001985839988164726,
      "loss": 3.7888,
      "step": 103
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.22871556878089905,
      "learning_rate": 0.00019854020152886814,
      "loss": 3.6307,
      "step": 104
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.19921822845935822,
      "learning_rate": 0.00019849574216547171,
      "loss": 3.8658,
      "step": 105
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.21603383123874664,
      "learning_rate": 0.0001984506210249986,
      "loss": 4.023,
      "step": 106
    },
    {
      "epoch": 0.428,
      "grad_norm": 0.2065233290195465,
      "learning_rate": 0.00019840483841061058,
      "loss": 3.9083,
      "step": 107
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.204634889960289,
      "learning_rate": 0.00019835839462991361,
      "loss": 3.8998,
      "step": 108
    },
    {
      "epoch": 0.436,
      "grad_norm": 0.1894771307706833,
      "learning_rate": 0.00019831128999495606,
      "loss": 3.8346,
      "step": 109
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.21026335656642914,
      "learning_rate": 0.00019826352482222638,
      "loss": 3.9877,
      "step": 110
    },
    {
      "epoch": 0.444,
      "grad_norm": 0.2095920741558075,
      "learning_rate": 0.0001982150994326511,
      "loss": 3.8244,
      "step": 111
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.21727772057056427,
      "learning_rate": 0.00019816601415159263,
      "loss": 3.8133,
      "step": 112
    },
    {
      "epoch": 0.452,
      "grad_norm": 0.26117390394210815,
      "learning_rate": 0.0001981162693088471,
      "loss": 4.0344,
      "step": 113
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.20109862089157104,
      "learning_rate": 0.0001980658652386421,
      "loss": 3.8446,
      "step": 114
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.20960460603237152,
      "learning_rate": 0.0001980148022796345,
      "loss": 4.0537,
      "step": 115
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.19522885978221893,
      "learning_rate": 0.00019796308077490817,
      "loss": 3.6823,
      "step": 116
    },
    {
      "epoch": 0.468,
      "grad_norm": 0.24077893793582916,
      "learning_rate": 0.00019791070107197153,
      "loss": 3.9274,
      "step": 117
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.25236910581588745,
      "learning_rate": 0.00019785766352275542,
      "loss": 4.0123,
      "step": 118
    },
    {
      "epoch": 0.476,
      "grad_norm": 0.22876451909542084,
      "learning_rate": 0.0001978039684836106,
      "loss": 3.9554,
      "step": 119
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.1948583424091339,
      "learning_rate": 0.00019774961631530545,
      "loss": 3.6754,
      "step": 120
    },
    {
      "epoch": 0.484,
      "grad_norm": 0.1958794891834259,
      "learning_rate": 0.0001976946073830234,
      "loss": 3.9746,
      "step": 121
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.19073235988616943,
      "learning_rate": 0.00019763894205636072,
      "loss": 3.6453,
      "step": 122
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.18706361949443817,
      "learning_rate": 0.00019758262070932375,
      "loss": 3.9003,
      "step": 123
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.28321877121925354,
      "learning_rate": 0.00019752564372032657,
      "loss": 3.9541,
      "step": 124
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.19286976754665375,
      "learning_rate": 0.00019746801147218842,
      "loss": 3.6473,
      "step": 125
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.20077985525131226,
      "learning_rate": 0.00019740972435213115,
      "loss": 3.6844,
      "step": 126
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.18777069449424744,
      "learning_rate": 0.00019735078275177654,
      "loss": 3.5302,
      "step": 127
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.21749374270439148,
      "learning_rate": 0.00019729118706714375,
      "loss": 3.8206,
      "step": 128
    },
    {
      "epoch": 0.516,
      "grad_norm": 0.22461704909801483,
      "learning_rate": 0.00019723093769864663,
      "loss": 3.6288,
      "step": 129
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.207794189453125,
      "learning_rate": 0.00019717003505109095,
      "loss": 3.8557,
      "step": 130
    },
    {
      "epoch": 0.524,
      "grad_norm": 0.1765696108341217,
      "learning_rate": 0.0001971084795336719,
      "loss": 3.8331,
      "step": 131
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.18416836857795715,
      "learning_rate": 0.00019704627155997108,
      "loss": 3.5759,
      "step": 132
    },
    {
      "epoch": 0.532,
      "grad_norm": 0.17642898857593536,
      "learning_rate": 0.00019698341154795389,
      "loss": 3.8103,
      "step": 133
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.1897030770778656,
      "learning_rate": 0.00019691989991996663,
      "loss": 3.4696,
      "step": 134
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.17490051686763763,
      "learning_rate": 0.00019685573710273376,
      "loss": 3.7594,
      "step": 135
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.19629855453968048,
      "learning_rate": 0.0001967909235273549,
      "loss": 3.6005,
      "step": 136
    },
    {
      "epoch": 0.548,
      "grad_norm": 0.1772780567407608,
      "learning_rate": 0.00019672545962930215,
      "loss": 3.3996,
      "step": 137
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.21250469982624054,
      "learning_rate": 0.00019665934584841682,
      "loss": 3.533,
      "step": 138
    },
    {
      "epoch": 0.556,
      "grad_norm": 0.1839977204799652,
      "learning_rate": 0.00019659258262890683,
      "loss": 3.7061,
      "step": 139
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.20183798670768738,
      "learning_rate": 0.00019652517041934356,
      "loss": 3.7575,
      "step": 140
    },
    {
      "epoch": 0.564,
      "grad_norm": 0.20151212811470032,
      "learning_rate": 0.00019645710967265882,
      "loss": 3.6179,
      "step": 141
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.17049460113048553,
      "learning_rate": 0.00019638840084614182,
      "loss": 3.798,
      "step": 142
    },
    {
      "epoch": 0.572,
      "grad_norm": 0.20975027978420258,
      "learning_rate": 0.00019631904440143612,
      "loss": 3.7226,
      "step": 143
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.18792618811130524,
      "learning_rate": 0.00019624904080453655,
      "loss": 3.641,
      "step": 144
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.19618527591228485,
      "learning_rate": 0.00019617839052578603,
      "loss": 3.6207,
      "step": 145
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.19512119889259338,
      "learning_rate": 0.00019610709403987246,
      "loss": 3.5474,
      "step": 146
    },
    {
      "epoch": 0.588,
      "grad_norm": 0.16474847495555878,
      "learning_rate": 0.0001960351518258255,
      "loss": 3.5127,
      "step": 147
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.16645503044128418,
      "learning_rate": 0.00019596256436701324,
      "loss": 3.4305,
      "step": 148
    },
    {
      "epoch": 0.596,
      "grad_norm": 0.1710980087518692,
      "learning_rate": 0.00019588933215113926,
      "loss": 3.503,
      "step": 149
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.18621136248111725,
      "learning_rate": 0.000195815455670239,
      "loss": 3.9021,
      "step": 150
    },
    {
      "epoch": 0.604,
      "grad_norm": 0.19290271401405334,
      "learning_rate": 0.00019574093542067673,
      "loss": 3.781,
      "step": 151
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.17418935894966125,
      "learning_rate": 0.00019566577190314197,
      "loss": 3.4184,
      "step": 152
    },
    {
      "epoch": 0.612,
      "grad_norm": 0.1979985237121582,
      "learning_rate": 0.0001955899656226464,
      "loss": 3.8018,
      "step": 153
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.19483564794063568,
      "learning_rate": 0.0001955135170885202,
      "loss": 3.3272,
      "step": 154
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.17463375627994537,
      "learning_rate": 0.0001954364268144088,
      "loss": 3.4532,
      "step": 155
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.17619574069976807,
      "learning_rate": 0.00019535869531826937,
      "loss": 3.5688,
      "step": 156
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.1940525621175766,
      "learning_rate": 0.00019528032312236736,
      "loss": 3.5482,
      "step": 157
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.16512367129325867,
      "learning_rate": 0.00019520131075327298,
      "loss": 3.475,
      "step": 158
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.18556439876556396,
      "learning_rate": 0.00019512165874185767,
      "loss": 3.4638,
      "step": 159
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.1898597776889801,
      "learning_rate": 0.00019504136762329047,
      "loss": 3.6337,
      "step": 160
    },
    {
      "epoch": 0.644,
      "grad_norm": 0.21816740930080414,
      "learning_rate": 0.0001949604379370345,
      "loss": 3.628,
      "step": 161
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.2109871804714203,
      "learning_rate": 0.00019487887022684336,
      "loss": 3.8156,
      "step": 162
    },
    {
      "epoch": 0.652,
      "grad_norm": 0.2020304650068283,
      "learning_rate": 0.00019479666504075736,
      "loss": 3.5084,
      "step": 163
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.20162932574748993,
      "learning_rate": 0.00019471382293110003,
      "loss": 3.6731,
      "step": 164
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.19267459213733673,
      "learning_rate": 0.0001946303444544741,
      "loss": 3.4903,
      "step": 165
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.16757938265800476,
      "learning_rate": 0.00019454623017175812,
      "loss": 3.3496,
      "step": 166
    },
    {
      "epoch": 0.668,
      "grad_norm": 0.17372755706310272,
      "learning_rate": 0.00019446148064810242,
      "loss": 3.6256,
      "step": 167
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.18136070668697357,
      "learning_rate": 0.00019437609645292546,
      "loss": 3.4332,
      "step": 168
    },
    {
      "epoch": 0.676,
      "grad_norm": 0.23116272687911987,
      "learning_rate": 0.00019429007815990993,
      "loss": 3.396,
      "step": 169
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.2031964510679245,
      "learning_rate": 0.0001942034263469989,
      "loss": 3.3292,
      "step": 170
    },
    {
      "epoch": 0.684,
      "grad_norm": 0.1872299164533615,
      "learning_rate": 0.00019411614159639204,
      "loss": 3.4754,
      "step": 171
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.18025097250938416,
      "learning_rate": 0.00019402822449454153,
      "loss": 3.0741,
      "step": 172
    },
    {
      "epoch": 0.692,
      "grad_norm": 0.180389404296875,
      "learning_rate": 0.00019393967563214833,
      "loss": 3.612,
      "step": 173
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.17443440854549408,
      "learning_rate": 0.00019385049560415794,
      "loss": 3.5024,
      "step": 174
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.2160956710577011,
      "learning_rate": 0.00019376068500975667,
      "loss": 3.5082,
      "step": 175
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.2168867141008377,
      "learning_rate": 0.00019367024445236754,
      "loss": 3.3685,
      "step": 176
    },
    {
      "epoch": 0.708,
      "grad_norm": 0.20543432235717773,
      "learning_rate": 0.000193579174539646,
      "loss": 3.7719,
      "step": 177
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.19736941158771515,
      "learning_rate": 0.00019348747588347637,
      "loss": 3.5088,
      "step": 178
    },
    {
      "epoch": 0.716,
      "grad_norm": 0.17034073173999786,
      "learning_rate": 0.00019339514909996706,
      "loss": 3.3853,
      "step": 179
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.18376456201076508,
      "learning_rate": 0.00019330219480944694,
      "loss": 3.4838,
      "step": 180
    },
    {
      "epoch": 0.724,
      "grad_norm": 0.1802004873752594,
      "learning_rate": 0.00019320861363646095,
      "loss": 3.6067,
      "step": 181
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.26620882749557495,
      "learning_rate": 0.00019311440620976597,
      "loss": 3.4619,
      "step": 182
    },
    {
      "epoch": 0.732,
      "grad_norm": 0.173911914229393,
      "learning_rate": 0.00019301957316232658,
      "loss": 3.259,
      "step": 183
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.1645464152097702,
      "learning_rate": 0.0001929241151313108,
      "loss": 3.2834,
      "step": 184
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.15920527279376984,
      "learning_rate": 0.0001928280327580858,
      "loss": 3.2604,
      "step": 185
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.18966422975063324,
      "learning_rate": 0.00019273132668821364,
      "loss": 3.3534,
      "step": 186
    },
    {
      "epoch": 0.748,
      "grad_norm": 0.17744332551956177,
      "learning_rate": 0.00019263399757144683,
      "loss": 3.3653,
      "step": 187
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.21870556473731995,
      "learning_rate": 0.00019253604606172417,
      "loss": 3.3405,
      "step": 188
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.1836492419242859,
      "learning_rate": 0.000192437472817166,
      "loss": 3.222,
      "step": 189
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.15214946866035461,
      "learning_rate": 0.00019233827850007027,
      "loss": 3.3366,
      "step": 190
    },
    {
      "epoch": 0.764,
      "grad_norm": 0.16666916012763977,
      "learning_rate": 0.00019223846377690754,
      "loss": 3.1794,
      "step": 191
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.16685420274734497,
      "learning_rate": 0.00019213802931831696,
      "loss": 3.3915,
      "step": 192
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.19861777126789093,
      "learning_rate": 0.00019203697579910154,
      "loss": 3.5565,
      "step": 193
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.1829143464565277,
      "learning_rate": 0.00019193530389822363,
      "loss": 3.2966,
      "step": 194
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.160686194896698,
      "learning_rate": 0.00019183301429880043,
      "loss": 3.4499,
      "step": 195
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.1680517941713333,
      "learning_rate": 0.00019173010768809933,
      "loss": 3.197,
      "step": 196
    },
    {
      "epoch": 0.788,
      "grad_norm": 0.16934868693351746,
      "learning_rate": 0.00019162658475753327,
      "loss": 3.5043,
      "step": 197
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.18683680891990662,
      "learning_rate": 0.0001915224462026563,
      "loss": 3.2846,
      "step": 198
    },
    {
      "epoch": 0.796,
      "grad_norm": 0.20635023713111877,
      "learning_rate": 0.00019141769272315858,
      "loss": 3.3011,
      "step": 199
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.18586567044258118,
      "learning_rate": 0.00019131232502286188,
      "loss": 3.3198,
      "step": 200
    },
    {
      "epoch": 0.804,
      "grad_norm": 0.162764310836792,
      "learning_rate": 0.00019120634380971496,
      "loss": 2.9819,
      "step": 201
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.19849567115306854,
      "learning_rate": 0.0001910997497957885,
      "loss": 3.1487,
      "step": 202
    },
    {
      "epoch": 0.812,
      "grad_norm": 0.14675414562225342,
      "learning_rate": 0.0001909925436972706,
      "loss": 3.1662,
      "step": 203
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.17686086893081665,
      "learning_rate": 0.00019088472623446183,
      "loss": 3.2521,
      "step": 204
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.15433034300804138,
      "learning_rate": 0.00019077629813177036,
      "loss": 2.9268,
      "step": 205
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.1729031652212143,
      "learning_rate": 0.00019066726011770726,
      "loss": 3.0227,
      "step": 206
    },
    {
      "epoch": 0.828,
      "grad_norm": 0.1801833063364029,
      "learning_rate": 0.00019055761292488142,
      "loss": 3.0575,
      "step": 207
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.23211678862571716,
      "learning_rate": 0.0001904473572899947,
      "loss": 3.0818,
      "step": 208
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.1703633815050125,
      "learning_rate": 0.00019033649395383702,
      "loss": 3.0053,
      "step": 209
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.18966303765773773,
      "learning_rate": 0.00019022502366128135,
      "loss": 3.5013,
      "step": 210
    },
    {
      "epoch": 0.844,
      "grad_norm": 0.20013146102428436,
      "learning_rate": 0.00019011294716127867,
      "loss": 3.1926,
      "step": 211
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.21084697544574738,
      "learning_rate": 0.00019000026520685302,
      "loss": 3.3481,
      "step": 212
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.1806865632534027,
      "learning_rate": 0.0001898869785550963,
      "loss": 3.0052,
      "step": 213
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.18296949565410614,
      "learning_rate": 0.0001897730879671634,
      "loss": 3.2331,
      "step": 214
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.15883435308933258,
      "learning_rate": 0.00018965859420826684,
      "loss": 3.0514,
      "step": 215
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.25622278451919556,
      "learning_rate": 0.00018954349804767184,
      "loss": 3.4411,
      "step": 216
    },
    {
      "epoch": 0.868,
      "grad_norm": 0.15008386969566345,
      "learning_rate": 0.00018942780025869098,
      "loss": 3.09,
      "step": 217
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.2581194341182709,
      "learning_rate": 0.00018931150161867916,
      "loss": 3.5092,
      "step": 218
    },
    {
      "epoch": 0.876,
      "grad_norm": 0.19812160730361938,
      "learning_rate": 0.00018919460290902826,
      "loss": 3.2435,
      "step": 219
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.23643657565116882,
      "learning_rate": 0.00018907710491516199,
      "loss": 3.2776,
      "step": 220
    },
    {
      "epoch": 0.884,
      "grad_norm": 0.1677044928073883,
      "learning_rate": 0.0001889590084265304,
      "loss": 3.1014,
      "step": 221
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.16789324581623077,
      "learning_rate": 0.0001888403142366049,
      "loss": 3.1991,
      "step": 222
    },
    {
      "epoch": 0.892,
      "grad_norm": 0.17056231200695038,
      "learning_rate": 0.0001887210231428727,
      "loss": 2.9674,
      "step": 223
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.2036849409341812,
      "learning_rate": 0.00018860113594683148,
      "loss": 2.9393,
      "step": 224
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.14891906082630157,
      "learning_rate": 0.0001884806534539841,
      "loss": 3.2221,
      "step": 225
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.17370694875717163,
      "learning_rate": 0.00018835957647383303,
      "loss": 3.2765,
      "step": 226
    },
    {
      "epoch": 0.908,
      "grad_norm": 0.18648014962673187,
      "learning_rate": 0.0001882379058198751,
      "loss": 2.9754,
      "step": 227
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.17965064942836761,
      "learning_rate": 0.00018811564230959588,
      "loss": 3.1177,
      "step": 228
    },
    {
      "epoch": 0.916,
      "grad_norm": 0.19484280049800873,
      "learning_rate": 0.00018799278676446423,
      "loss": 3.2657,
      "step": 229
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.174091175198555,
      "learning_rate": 0.00018786934000992688,
      "loss": 3.0531,
      "step": 230
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.1678793728351593,
      "learning_rate": 0.00018774530287540278,
      "loss": 2.8636,
      "step": 231
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.1563955694437027,
      "learning_rate": 0.00018762067619427746,
      "loss": 3.03,
      "step": 232
    },
    {
      "epoch": 0.932,
      "grad_norm": 0.21710093319416046,
      "learning_rate": 0.00018749546080389757,
      "loss": 3.3249,
      "step": 233
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.173653706908226,
      "learning_rate": 0.00018736965754556528,
      "loss": 3.1334,
      "step": 234
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.1981709599494934,
      "learning_rate": 0.00018724326726453244,
      "loss": 2.7557,
      "step": 235
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.1846802681684494,
      "learning_rate": 0.00018711629080999504,
      "loss": 3.2447,
      "step": 236
    },
    {
      "epoch": 0.948,
      "grad_norm": 0.19806210696697235,
      "learning_rate": 0.00018698872903508755,
      "loss": 3.2318,
      "step": 237
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.19059094786643982,
      "learning_rate": 0.00018686058279687698,
      "loss": 3.1543,
      "step": 238
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.18573921918869019,
      "learning_rate": 0.0001867318529563574,
      "loss": 3.1641,
      "step": 239
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.14330364763736725,
      "learning_rate": 0.00018660254037844388,
      "loss": 2.8336,
      "step": 240
    },
    {
      "epoch": 0.964,
      "grad_norm": 0.21990638971328735,
      "learning_rate": 0.00018647264593196688,
      "loss": 3.1004,
      "step": 241
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.1869647055864334,
      "learning_rate": 0.00018634217048966637,
      "loss": 3.3031,
      "step": 242
    },
    {
      "epoch": 0.972,
      "grad_norm": 0.19950971007347107,
      "learning_rate": 0.00018621111492818585,
      "loss": 2.9891,
      "step": 243
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.1722448319196701,
      "learning_rate": 0.0001860794801280666,
      "loss": 3.001,
      "step": 244
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.15594443678855896,
      "learning_rate": 0.00018594726697374175,
      "loss": 2.8377,
      "step": 245
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.1793275624513626,
      "learning_rate": 0.0001858144763535302,
      "loss": 3.1687,
      "step": 246
    },
    {
      "epoch": 0.988,
      "grad_norm": 0.15000538527965546,
      "learning_rate": 0.0001856811091596308,
      "loss": 3.0487,
      "step": 247
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.1787269115447998,
      "learning_rate": 0.0001855471662881164,
      "loss": 2.7906,
      "step": 248
    },
    {
      "epoch": 0.996,
      "grad_norm": 0.17408689856529236,
      "learning_rate": 0.00018541264863892754,
      "loss": 3.2872,
      "step": 249
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.17045819759368896,
      "learning_rate": 0.00018527755711586678,
      "loss": 3.1007,
      "step": 250
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.990558624267578,
      "eval_runtime": 24.051,
      "eval_samples_per_second": 20.789,
      "eval_steps_per_second": 2.619,
      "step": 250
    },
    {
      "epoch": 1.004,
      "grad_norm": 0.2082027792930603,
      "learning_rate": 0.00018514189262659235,
      "loss": 2.8898,
      "step": 251
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.18725883960723877,
      "learning_rate": 0.00018500565608261214,
      "loss": 3.0481,
      "step": 252
    },
    {
      "epoch": 1.012,
      "grad_norm": 0.1778808832168579,
      "learning_rate": 0.00018486884839927768,
      "loss": 3.0432,
      "step": 253
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.15298837423324585,
      "learning_rate": 0.00018473147049577774,
      "loss": 3.0926,
      "step": 254
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.22449415922164917,
      "learning_rate": 0.0001845935232951325,
      "loss": 2.9081,
      "step": 255
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.15181326866149902,
      "learning_rate": 0.00018445500772418697,
      "loss": 2.882,
      "step": 256
    },
    {
      "epoch": 1.028,
      "grad_norm": 0.18487092852592468,
      "learning_rate": 0.00018431592471360503,
      "loss": 3.2031,
      "step": 257
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.21008017659187317,
      "learning_rate": 0.00018417627519786315,
      "loss": 2.908,
      "step": 258
    },
    {
      "epoch": 1.036,
      "grad_norm": 0.1959112584590912,
      "learning_rate": 0.000184036060115244,
      "loss": 2.8672,
      "step": 259
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.15446992218494415,
      "learning_rate": 0.00018389528040783012,
      "loss": 2.8602,
      "step": 260
    },
    {
      "epoch": 1.044,
      "grad_norm": 0.15976910293102264,
      "learning_rate": 0.00018375393702149787,
      "loss": 2.9407,
      "step": 261
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.1932137906551361,
      "learning_rate": 0.00018361203090591071,
      "loss": 3.0609,
      "step": 262
    },
    {
      "epoch": 1.052,
      "grad_norm": 0.19297915697097778,
      "learning_rate": 0.00018346956301451304,
      "loss": 3.1188,
      "step": 263
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.17886877059936523,
      "learning_rate": 0.00018332653430452376,
      "loss": 2.9767,
      "step": 264
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.22046789526939392,
      "learning_rate": 0.00018318294573692985,
      "loss": 3.0687,
      "step": 265
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.14293304085731506,
      "learning_rate": 0.00018303879827647975,
      "loss": 3.1285,
      "step": 266
    },
    {
      "epoch": 1.068,
      "grad_norm": 0.21329905092716217,
      "learning_rate": 0.0001828940928916772,
      "loss": 2.9014,
      "step": 267
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.18550504744052887,
      "learning_rate": 0.00018274883055477436,
      "loss": 2.6986,
      "step": 268
    },
    {
      "epoch": 1.076,
      "grad_norm": 0.1602412313222885,
      "learning_rate": 0.00018260301224176558,
      "loss": 3.1524,
      "step": 269
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.19258257746696472,
      "learning_rate": 0.00018245663893238075,
      "loss": 2.7578,
      "step": 270
    },
    {
      "epoch": 1.084,
      "grad_norm": 0.15281230211257935,
      "learning_rate": 0.00018230971161007853,
      "loss": 2.7409,
      "step": 271
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.15957097709178925,
      "learning_rate": 0.00018216223126204007,
      "loss": 2.8374,
      "step": 272
    },
    {
      "epoch": 1.092,
      "grad_norm": 0.1723804622888565,
      "learning_rate": 0.00018201419887916214,
      "loss": 2.7459,
      "step": 273
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.18181458115577698,
      "learning_rate": 0.00018186561545605054,
      "loss": 2.801,
      "step": 274
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.213564932346344,
      "learning_rate": 0.00018171648199101346,
      "loss": 2.779,
      "step": 275
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.1618279367685318,
      "learning_rate": 0.00018156679948605467,
      "loss": 2.7939,
      "step": 276
    },
    {
      "epoch": 1.108,
      "grad_norm": 0.17854879796504974,
      "learning_rate": 0.00018141656894686689,
      "loss": 2.9885,
      "step": 277
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.16151148080825806,
      "learning_rate": 0.00018126579138282503,
      "loss": 3.0043,
      "step": 278
    },
    {
      "epoch": 1.116,
      "grad_norm": 0.16941747069358826,
      "learning_rate": 0.00018111446780697929,
      "loss": 2.6011,
      "step": 279
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.14842823147773743,
      "learning_rate": 0.0001809625992360485,
      "loss": 2.7062,
      "step": 280
    },
    {
      "epoch": 1.124,
      "grad_norm": 0.1542978286743164,
      "learning_rate": 0.00018081018669041324,
      "loss": 2.6876,
      "step": 281
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.2459702044725418,
      "learning_rate": 0.00018065723119410884,
      "loss": 3.2285,
      "step": 282
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 0.16819116473197937,
      "learning_rate": 0.00018050373377481878,
      "loss": 2.9791,
      "step": 283
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.17059993743896484,
      "learning_rate": 0.00018034969546386757,
      "loss": 2.6799,
      "step": 284
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.18292608857154846,
      "learning_rate": 0.0001801951172962139,
      "loss": 2.7351,
      "step": 285
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.18434052169322968,
      "learning_rate": 0.0001800400003104436,
      "loss": 3.1496,
      "step": 286
    },
    {
      "epoch": 1.148,
      "grad_norm": 0.18487919867038727,
      "learning_rate": 0.0001798843455487629,
      "loss": 2.7495,
      "step": 287
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.15079112350940704,
      "learning_rate": 0.00017972815405699103,
      "loss": 2.8065,
      "step": 288
    },
    {
      "epoch": 1.156,
      "grad_norm": 0.15222522616386414,
      "learning_rate": 0.00017957142688455362,
      "loss": 2.6825,
      "step": 289
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.16851770877838135,
      "learning_rate": 0.00017941416508447536,
      "loss": 2.9736,
      "step": 290
    },
    {
      "epoch": 1.164,
      "grad_norm": 0.20419618487358093,
      "learning_rate": 0.00017925636971337304,
      "loss": 2.837,
      "step": 291
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.2203681468963623,
      "learning_rate": 0.0001790980418314484,
      "loss": 2.5853,
      "step": 292
    },
    {
      "epoch": 1.172,
      "grad_norm": 0.1571640968322754,
      "learning_rate": 0.00017893918250248104,
      "loss": 3.032,
      "step": 293
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.17670083045959473,
      "learning_rate": 0.00017877979279382135,
      "loss": 2.9062,
      "step": 294
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.1997249722480774,
      "learning_rate": 0.00017861987377638312,
      "loss": 2.8049,
      "step": 295
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.21550047397613525,
      "learning_rate": 0.0001784594265246366,
      "loss": 2.8209,
      "step": 296
    },
    {
      "epoch": 1.188,
      "grad_norm": 0.15742239356040955,
      "learning_rate": 0.0001782984521166011,
      "loss": 2.6473,
      "step": 297
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.16464199125766754,
      "learning_rate": 0.0001781369516338378,
      "loss": 2.9639,
      "step": 298
    },
    {
      "epoch": 1.196,
      "grad_norm": 0.16636008024215698,
      "learning_rate": 0.00017797492616144256,
      "loss": 2.8825,
      "step": 299
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.27598729729652405,
      "learning_rate": 0.00017781237678803847,
      "loss": 2.5044,
      "step": 300
    },
    {
      "epoch": 1.204,
      "grad_norm": 0.27128639817237854,
      "learning_rate": 0.00017764930460576866,
      "loss": 3.0665,
      "step": 301
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.18155336380004883,
      "learning_rate": 0.000177485710710289,
      "loss": 2.8386,
      "step": 302
    },
    {
      "epoch": 1.212,
      "grad_norm": 0.20413954555988312,
      "learning_rate": 0.00017732159620076053,
      "loss": 2.9846,
      "step": 303
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.1970483809709549,
      "learning_rate": 0.00017715696217984235,
      "loss": 2.7463,
      "step": 304
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.16914483904838562,
      "learning_rate": 0.00017699180975368396,
      "loss": 2.692,
      "step": 305
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.22873033583164215,
      "learning_rate": 0.00017682614003191807,
      "loss": 2.6617,
      "step": 306
    },
    {
      "epoch": 1.228,
      "grad_norm": 0.1926119029521942,
      "learning_rate": 0.00017665995412765285,
      "loss": 2.7055,
      "step": 307
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.21092726290225983,
      "learning_rate": 0.00017649325315746478,
      "loss": 2.9376,
      "step": 308
    },
    {
      "epoch": 1.236,
      "grad_norm": 0.18494576215744019,
      "learning_rate": 0.00017632603824139085,
      "loss": 2.6358,
      "step": 309
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.20392940938472748,
      "learning_rate": 0.0001761583105029213,
      "loss": 2.6406,
      "step": 310
    },
    {
      "epoch": 1.244,
      "grad_norm": 0.15542443096637726,
      "learning_rate": 0.0001759900710689918,
      "loss": 2.5291,
      "step": 311
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.15850897133350372,
      "learning_rate": 0.00017582132106997616,
      "loss": 2.9654,
      "step": 312
    },
    {
      "epoch": 1.252,
      "grad_norm": 0.20047743618488312,
      "learning_rate": 0.00017565206163967846,
      "loss": 2.7453,
      "step": 313
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.15150131285190582,
      "learning_rate": 0.00017548229391532572,
      "loss": 2.652,
      "step": 314
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.23210768401622772,
      "learning_rate": 0.00017531201903755994,
      "loss": 2.8997,
      "step": 315
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.1742788553237915,
      "learning_rate": 0.00017514123815043074,
      "loss": 2.6056,
      "step": 316
    },
    {
      "epoch": 1.268,
      "grad_norm": 0.18891814351081848,
      "learning_rate": 0.00017496995240138744,
      "loss": 3.0309,
      "step": 317
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.16621069610118866,
      "learning_rate": 0.00017479816294127152,
      "loss": 2.7102,
      "step": 318
    },
    {
      "epoch": 1.276,
      "grad_norm": 0.1687590479850769,
      "learning_rate": 0.00017462587092430875,
      "loss": 2.645,
      "step": 319
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.17968977987766266,
      "learning_rate": 0.0001744530775081015,
      "loss": 2.7527,
      "step": 320
    },
    {
      "epoch": 1.284,
      "grad_norm": 0.19607365131378174,
      "learning_rate": 0.00017427978385362112,
      "loss": 2.7763,
      "step": 321
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.1578807681798935,
      "learning_rate": 0.0001741059911251997,
      "loss": 2.8599,
      "step": 322
    },
    {
      "epoch": 1.292,
      "grad_norm": 0.18493254482746124,
      "learning_rate": 0.0001739317004905227,
      "loss": 2.7425,
      "step": 323
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.18266692757606506,
      "learning_rate": 0.000173756913120621,
      "loss": 2.6171,
      "step": 324
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.2345346212387085,
      "learning_rate": 0.00017358163018986282,
      "loss": 2.6997,
      "step": 325
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.26040685176849365,
      "learning_rate": 0.00017340585287594604,
      "loss": 2.8777,
      "step": 326
    },
    {
      "epoch": 1.308,
      "grad_norm": 0.17406710982322693,
      "learning_rate": 0.00017322958235989016,
      "loss": 2.7128,
      "step": 327
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.17198453843593597,
      "learning_rate": 0.0001730528198260285,
      "loss": 2.498,
      "step": 328
    },
    {
      "epoch": 1.316,
      "grad_norm": 0.18165162205696106,
      "learning_rate": 0.00017287556646200018,
      "loss": 2.6286,
      "step": 329
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.19203831255435944,
      "learning_rate": 0.00017269782345874203,
      "loss": 2.725,
      "step": 330
    },
    {
      "epoch": 1.324,
      "grad_norm": 0.1644928753376007,
      "learning_rate": 0.00017251959201048083,
      "loss": 2.7227,
      "step": 331
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.18849171698093414,
      "learning_rate": 0.00017234087331472497,
      "loss": 2.8887,
      "step": 332
    },
    {
      "epoch": 1.332,
      "grad_norm": 0.1754441112279892,
      "learning_rate": 0.00017216166857225674,
      "loss": 2.6456,
      "step": 333
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.20156200230121613,
      "learning_rate": 0.00017198197898712404,
      "loss": 2.7547,
      "step": 334
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.22001998126506805,
      "learning_rate": 0.00017180180576663228,
      "loss": 2.5826,
      "step": 335
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.18154068291187286,
      "learning_rate": 0.00017162115012133643,
      "loss": 2.8103,
      "step": 336
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 0.1705700010061264,
      "learning_rate": 0.00017144001326503273,
      "loss": 2.6606,
      "step": 337
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.16810184717178345,
      "learning_rate": 0.00017125839641475072,
      "loss": 2.6679,
      "step": 338
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 0.17922335863113403,
      "learning_rate": 0.00017107630079074478,
      "loss": 2.7809,
      "step": 339
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.16561754047870636,
      "learning_rate": 0.00017089372761648616,
      "loss": 2.5435,
      "step": 340
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 0.19870486855506897,
      "learning_rate": 0.00017071067811865476,
      "loss": 2.8348,
      "step": 341
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.14896561205387115,
      "learning_rate": 0.00017052715352713075,
      "loss": 2.5294,
      "step": 342
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 0.2236500382423401,
      "learning_rate": 0.00017034315507498635,
      "loss": 2.6732,
      "step": 343
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.16938893496990204,
      "learning_rate": 0.00017015868399847768,
      "loss": 2.6032,
      "step": 344
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.16113606095314026,
      "learning_rate": 0.00016997374153703625,
      "loss": 2.6475,
      "step": 345
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.18429893255233765,
      "learning_rate": 0.00016978832893326074,
      "loss": 2.8936,
      "step": 346
    },
    {
      "epoch": 1.388,
      "grad_norm": 0.18665137887001038,
      "learning_rate": 0.00016960244743290868,
      "loss": 2.6118,
      "step": 347
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.17777474224567413,
      "learning_rate": 0.00016941609828488807,
      "loss": 2.7429,
      "step": 348
    },
    {
      "epoch": 1.396,
      "grad_norm": 0.1939142942428589,
      "learning_rate": 0.00016922928274124886,
      "loss": 2.5654,
      "step": 349
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.195732980966568,
      "learning_rate": 0.0001690420020571747,
      "loss": 2.5581,
      "step": 350
    },
    {
      "epoch": 1.404,
      "grad_norm": 0.1694985181093216,
      "learning_rate": 0.00016885425749097444,
      "loss": 2.5014,
      "step": 351
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.16092626750469208,
      "learning_rate": 0.0001686660503040737,
      "loss": 2.4344,
      "step": 352
    },
    {
      "epoch": 1.412,
      "grad_norm": 0.21173854172229767,
      "learning_rate": 0.00016847738176100632,
      "loss": 2.4334,
      "step": 353
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.14105521142482758,
      "learning_rate": 0.00016828825312940592,
      "loss": 2.6748,
      "step": 354
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.18185226619243622,
      "learning_rate": 0.0001680986656799975,
      "loss": 2.5518,
      "step": 355
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.23500433564186096,
      "learning_rate": 0.0001679086206865886,
      "loss": 2.5854,
      "step": 356
    },
    {
      "epoch": 1.428,
      "grad_norm": 0.1710791438817978,
      "learning_rate": 0.00016771811942606108,
      "loss": 2.565,
      "step": 357
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.15893039107322693,
      "learning_rate": 0.00016752716317836229,
      "loss": 2.4846,
      "step": 358
    },
    {
      "epoch": 1.436,
      "grad_norm": 0.24022246897220612,
      "learning_rate": 0.00016733575322649657,
      "loss": 2.5496,
      "step": 359
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.17566943168640137,
      "learning_rate": 0.0001671438908565167,
      "loss": 2.6377,
      "step": 360
    },
    {
      "epoch": 1.444,
      "grad_norm": 0.17920133471488953,
      "learning_rate": 0.00016695157735751513,
      "loss": 2.7653,
      "step": 361
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.24256287515163422,
      "learning_rate": 0.00016675881402161536,
      "loss": 2.6774,
      "step": 362
    },
    {
      "epoch": 1.452,
      "grad_norm": 0.1592651754617691,
      "learning_rate": 0.0001665656021439633,
      "loss": 2.3627,
      "step": 363
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.18511445820331573,
      "learning_rate": 0.0001663719430227186,
      "loss": 2.6889,
      "step": 364
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.19486095011234283,
      "learning_rate": 0.00016617783795904565,
      "loss": 2.4212,
      "step": 365
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.22086837887763977,
      "learning_rate": 0.00016598328825710533,
      "loss": 2.5615,
      "step": 366
    },
    {
      "epoch": 1.468,
      "grad_norm": 0.14962030947208405,
      "learning_rate": 0.00016578829522404583,
      "loss": 2.7334,
      "step": 367
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.20832780003547668,
      "learning_rate": 0.000165592860169994,
      "loss": 2.2951,
      "step": 368
    },
    {
      "epoch": 1.476,
      "grad_norm": 0.19668641686439514,
      "learning_rate": 0.00016539698440804661,
      "loss": 2.6852,
      "step": 369
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.1707521378993988,
      "learning_rate": 0.00016520066925426144,
      "loss": 2.5572,
      "step": 370
    },
    {
      "epoch": 1.484,
      "grad_norm": 0.20181632041931152,
      "learning_rate": 0.0001650039160276485,
      "loss": 2.5316,
      "step": 371
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.1736031174659729,
      "learning_rate": 0.0001648067260501611,
      "loss": 2.3791,
      "step": 372
    },
    {
      "epoch": 1.492,
      "grad_norm": 0.18900729715824127,
      "learning_rate": 0.0001646091006466871,
      "loss": 2.6888,
      "step": 373
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.24509203433990479,
      "learning_rate": 0.0001644110411450398,
      "loss": 2.4835,
      "step": 374
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.19061371684074402,
      "learning_rate": 0.00016421254887594917,
      "loss": 2.5431,
      "step": 375
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.1850917488336563,
      "learning_rate": 0.00016401362517305296,
      "loss": 2.6409,
      "step": 376
    },
    {
      "epoch": 1.508,
      "grad_norm": 0.1680152267217636,
      "learning_rate": 0.00016381427137288754,
      "loss": 2.7768,
      "step": 377
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.19064804911613464,
      "learning_rate": 0.00016361448881487914,
      "loss": 2.6862,
      "step": 378
    },
    {
      "epoch": 1.516,
      "grad_norm": 0.1788070648908615,
      "learning_rate": 0.0001634142788413346,
      "loss": 2.7704,
      "step": 379
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.1952916383743286,
      "learning_rate": 0.00016321364279743266,
      "loss": 2.5225,
      "step": 380
    },
    {
      "epoch": 1.524,
      "grad_norm": 0.15985770523548126,
      "learning_rate": 0.00016301258203121462,
      "loss": 2.456,
      "step": 381
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.16746337711811066,
      "learning_rate": 0.0001628110978935756,
      "loss": 2.5038,
      "step": 382
    },
    {
      "epoch": 1.532,
      "grad_norm": 0.20668505132198334,
      "learning_rate": 0.00016260919173825508,
      "loss": 2.7181,
      "step": 383
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.18437303602695465,
      "learning_rate": 0.00016240686492182804,
      "loss": 2.4956,
      "step": 384
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.2472464144229889,
      "learning_rate": 0.00016220411880369601,
      "loss": 2.4499,
      "step": 385
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.20210042595863342,
      "learning_rate": 0.00016200095474607753,
      "loss": 2.2603,
      "step": 386
    },
    {
      "epoch": 1.548,
      "grad_norm": 0.1679307371377945,
      "learning_rate": 0.00016179737411399926,
      "loss": 2.4872,
      "step": 387
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.20563571155071259,
      "learning_rate": 0.00016159337827528685,
      "loss": 2.5692,
      "step": 388
    },
    {
      "epoch": 1.556,
      "grad_norm": 0.19437505304813385,
      "learning_rate": 0.00016138896860055555,
      "loss": 2.3431,
      "step": 389
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.16625766456127167,
      "learning_rate": 0.0001611841464632011,
      "loss": 2.2359,
      "step": 390
    },
    {
      "epoch": 1.564,
      "grad_norm": 0.18835023045539856,
      "learning_rate": 0.00016097891323939062,
      "loss": 2.4588,
      "step": 391
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.16678737103939056,
      "learning_rate": 0.0001607732703080532,
      "loss": 2.4295,
      "step": 392
    },
    {
      "epoch": 1.572,
      "grad_norm": 0.15912556648254395,
      "learning_rate": 0.00016056721905087056,
      "loss": 2.294,
      "step": 393
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.22998102009296417,
      "learning_rate": 0.00016036076085226814,
      "loss": 2.4911,
      "step": 394
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.17910678684711456,
      "learning_rate": 0.00016015389709940538,
      "loss": 2.5071,
      "step": 395
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.22010616958141327,
      "learning_rate": 0.0001599466291821666,
      "loss": 2.2842,
      "step": 396
    },
    {
      "epoch": 1.588,
      "grad_norm": 0.17226716876029968,
      "learning_rate": 0.0001597389584931517,
      "loss": 2.5852,
      "step": 397
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.22156870365142822,
      "learning_rate": 0.0001595308864276666,
      "loss": 2.8931,
      "step": 398
    },
    {
      "epoch": 1.596,
      "grad_norm": 0.34022679924964905,
      "learning_rate": 0.0001593224143837142,
      "loss": 2.5516,
      "step": 399
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.18445925414562225,
      "learning_rate": 0.0001591135437619847,
      "loss": 2.4852,
      "step": 400
    },
    {
      "epoch": 1.604,
      "grad_norm": 0.16197091341018677,
      "learning_rate": 0.00015890427596584617,
      "loss": 2.3297,
      "step": 401
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.1524742841720581,
      "learning_rate": 0.0001586946124013354,
      "loss": 2.3842,
      "step": 402
    },
    {
      "epoch": 1.612,
      "grad_norm": 0.17333006858825684,
      "learning_rate": 0.00015848455447714822,
      "loss": 2.2001,
      "step": 403
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.18723492324352264,
      "learning_rate": 0.0001582741036046301,
      "loss": 2.566,
      "step": 404
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.18579038977622986,
      "learning_rate": 0.00015806326119776663,
      "loss": 2.4629,
      "step": 405
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.17910632491111755,
      "learning_rate": 0.00015785202867317407,
      "loss": 2.5345,
      "step": 406
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 0.17997631430625916,
      "learning_rate": 0.00015764040745008988,
      "loss": 2.4699,
      "step": 407
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.24215340614318848,
      "learning_rate": 0.00015742839895036305,
      "loss": 2.4891,
      "step": 408
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 0.19676744937896729,
      "learning_rate": 0.00015721600459844468,
      "loss": 2.3032,
      "step": 409
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.17996621131896973,
      "learning_rate": 0.00015700322582137827,
      "loss": 2.2155,
      "step": 410
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 0.24135266244411469,
      "learning_rate": 0.00015679006404879033,
      "loss": 2.4179,
      "step": 411
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.1824471354484558,
      "learning_rate": 0.0001565765207128805,
      "loss": 2.2686,
      "step": 412
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 0.1967494785785675,
      "learning_rate": 0.00015636259724841222,
      "loss": 2.3984,
      "step": 413
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.188935324549675,
      "learning_rate": 0.0001561482950927029,
      "loss": 2.4666,
      "step": 414
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.19695217907428741,
      "learning_rate": 0.00015593361568561428,
      "loss": 2.2996,
      "step": 415
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.18019133806228638,
      "learning_rate": 0.00015571856046954285,
      "loss": 2.2186,
      "step": 416
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 0.22328829765319824,
      "learning_rate": 0.0001555031308894101,
      "loss": 2.4031,
      "step": 417
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.23357196152210236,
      "learning_rate": 0.00015528732839265272,
      "loss": 2.4954,
      "step": 418
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 0.19573460519313812,
      "learning_rate": 0.0001550711544292131,
      "loss": 2.3325,
      "step": 419
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.16497112810611725,
      "learning_rate": 0.0001548546104515294,
      "loss": 2.3749,
      "step": 420
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 0.2917661964893341,
      "learning_rate": 0.00015463769791452574,
      "loss": 2.6517,
      "step": 421
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.24156291782855988,
      "learning_rate": 0.00015442041827560274,
      "loss": 2.2875,
      "step": 422
    },
    {
      "epoch": 1.692,
      "grad_norm": 0.18187840282917023,
      "learning_rate": 0.00015420277299462736,
      "loss": 2.5685,
      "step": 423
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.2046596109867096,
      "learning_rate": 0.00015398476353392323,
      "loss": 2.22,
      "step": 424
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.1914888322353363,
      "learning_rate": 0.00015376639135826107,
      "loss": 2.27,
      "step": 425
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.18382759392261505,
      "learning_rate": 0.00015354765793484834,
      "loss": 2.4968,
      "step": 426
    },
    {
      "epoch": 1.708,
      "grad_norm": 0.23019248247146606,
      "learning_rate": 0.00015332856473331978,
      "loss": 2.2832,
      "step": 427
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.19204913079738617,
      "learning_rate": 0.00015310911322572753,
      "loss": 2.241,
      "step": 428
    },
    {
      "epoch": 1.716,
      "grad_norm": 0.17317421734333038,
      "learning_rate": 0.00015288930488653094,
      "loss": 2.4007,
      "step": 429
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.2256227284669876,
      "learning_rate": 0.000152669141192587,
      "loss": 2.2302,
      "step": 430
    },
    {
      "epoch": 1.724,
      "grad_norm": 0.21414019167423248,
      "learning_rate": 0.0001524486236231402,
      "loss": 2.3774,
      "step": 431
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.22046643495559692,
      "learning_rate": 0.00015222775365981273,
      "loss": 2.4043,
      "step": 432
    },
    {
      "epoch": 1.732,
      "grad_norm": 0.17292536795139313,
      "learning_rate": 0.00015200653278659432,
      "loss": 2.3182,
      "step": 433
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.16933369636535645,
      "learning_rate": 0.00015178496248983254,
      "loss": 2.3434,
      "step": 434
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.19948233664035797,
      "learning_rate": 0.00015156304425822267,
      "loss": 2.2258,
      "step": 435
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.17962366342544556,
      "learning_rate": 0.00015134077958279765,
      "loss": 2.3616,
      "step": 436
    },
    {
      "epoch": 1.748,
      "grad_norm": 0.21868257224559784,
      "learning_rate": 0.00015111816995691809,
      "loss": 2.2991,
      "step": 437
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.20290447771549225,
      "learning_rate": 0.00015089521687626243,
      "loss": 2.441,
      "step": 438
    },
    {
      "epoch": 1.756,
      "grad_norm": 0.22686225175857544,
      "learning_rate": 0.00015067192183881658,
      "loss": 2.5948,
      "step": 439
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.27408501505851746,
      "learning_rate": 0.000150448286344864,
      "loss": 2.1497,
      "step": 440
    },
    {
      "epoch": 1.764,
      "grad_norm": 0.2716353237628937,
      "learning_rate": 0.00015022431189697568,
      "loss": 2.4596,
      "step": 441
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.257307231426239,
      "learning_rate": 0.00015000000000000001,
      "loss": 2.2735,
      "step": 442
    },
    {
      "epoch": 1.772,
      "grad_norm": 0.2330036163330078,
      "learning_rate": 0.0001497753521610526,
      "loss": 2.2313,
      "step": 443
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.2573404312133789,
      "learning_rate": 0.00014955036988950618,
      "loss": 2.369,
      "step": 444
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.20655156672000885,
      "learning_rate": 0.00014932505469698052,
      "loss": 2.5703,
      "step": 445
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.1829787790775299,
      "learning_rate": 0.00014909940809733222,
      "loss": 2.1102,
      "step": 446
    },
    {
      "epoch": 1.788,
      "grad_norm": 0.18688388168811798,
      "learning_rate": 0.0001488734316066446,
      "loss": 2.3978,
      "step": 447
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.26589077711105347,
      "learning_rate": 0.00014864712674321734,
      "loss": 2.171,
      "step": 448
    },
    {
      "epoch": 1.796,
      "grad_norm": 0.23303888738155365,
      "learning_rate": 0.0001484204950275565,
      "loss": 2.2617,
      "step": 449
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.23106397688388824,
      "learning_rate": 0.00014819353798236427,
      "loss": 2.0797,
      "step": 450
    },
    {
      "epoch": 1.804,
      "grad_norm": 0.20299752056598663,
      "learning_rate": 0.00014796625713252848,
      "loss": 2.1213,
      "step": 451
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.16940052807331085,
      "learning_rate": 0.00014773865400511272,
      "loss": 1.9216,
      "step": 452
    },
    {
      "epoch": 1.812,
      "grad_norm": 0.25728839635849,
      "learning_rate": 0.00014751073012934587,
      "loss": 2.285,
      "step": 453
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.1868535280227661,
      "learning_rate": 0.00014728248703661182,
      "loss": 2.4596,
      "step": 454
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.29915738105773926,
      "learning_rate": 0.0001470539262604393,
      "loss": 2.405,
      "step": 455
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.1725856214761734,
      "learning_rate": 0.00014682504933649144,
      "loss": 2.0576,
      "step": 456
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 0.19717518985271454,
      "learning_rate": 0.00014659585780255556,
      "loss": 1.8678,
      "step": 457
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.17078836262226105,
      "learning_rate": 0.00014636635319853275,
      "loss": 2.1845,
      "step": 458
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 0.17009608447551727,
      "learning_rate": 0.0001461365370664276,
      "loss": 2.3326,
      "step": 459
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.29109078645706177,
      "learning_rate": 0.00014590641095033787,
      "loss": 2.3823,
      "step": 460
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 0.18609720468521118,
      "learning_rate": 0.00014567597639644387,
      "loss": 2.0575,
      "step": 461
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.2412686049938202,
      "learning_rate": 0.00014544523495299842,
      "loss": 2.0678,
      "step": 462
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 0.20197857916355133,
      "learning_rate": 0.00014521418817031628,
      "loss": 2.3095,
      "step": 463
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.17876283824443817,
      "learning_rate": 0.0001449828376007636,
      "loss": 2.3318,
      "step": 464
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.17931701242923737,
      "learning_rate": 0.00014475118479874774,
      "loss": 2.2074,
      "step": 465
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.16827575862407684,
      "learning_rate": 0.0001445192313207067,
      "loss": 2.5512,
      "step": 466
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 0.20803618431091309,
      "learning_rate": 0.0001442869787250987,
      "loss": 2.3044,
      "step": 467
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.20197102427482605,
      "learning_rate": 0.0001440544285723915,
      "loss": 2.1675,
      "step": 468
    },
    {
      "epoch": 1.876,
      "grad_norm": 0.2958400249481201,
      "learning_rate": 0.00014382158242505234,
      "loss": 2.272,
      "step": 469
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.20997442305088043,
      "learning_rate": 0.00014358844184753712,
      "loss": 2.0162,
      "step": 470
    },
    {
      "epoch": 1.884,
      "grad_norm": 0.1865428239107132,
      "learning_rate": 0.00014335500840627986,
      "loss": 2.1981,
      "step": 471
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.20231419801712036,
      "learning_rate": 0.00014312128366968243,
      "loss": 2.3383,
      "step": 472
    },
    {
      "epoch": 1.892,
      "grad_norm": 0.17440253496170044,
      "learning_rate": 0.0001428872692081038,
      "loss": 2.5005,
      "step": 473
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.2900272607803345,
      "learning_rate": 0.00014265296659384956,
      "loss": 2.4718,
      "step": 474
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.23251210153102875,
      "learning_rate": 0.00014241837740116132,
      "loss": 2.0861,
      "step": 475
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.3362973630428314,
      "learning_rate": 0.00014218350320620624,
      "loss": 2.6548,
      "step": 476
    },
    {
      "epoch": 1.908,
      "grad_norm": 0.2431890219449997,
      "learning_rate": 0.00014194834558706632,
      "loss": 2.685,
      "step": 477
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.1677597165107727,
      "learning_rate": 0.0001417129061237278,
      "loss": 2.0594,
      "step": 478
    },
    {
      "epoch": 1.916,
      "grad_norm": 0.2346009463071823,
      "learning_rate": 0.0001414771863980707,
      "loss": 2.1055,
      "step": 479
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.2606126368045807,
      "learning_rate": 0.00014124118799385796,
      "loss": 2.4047,
      "step": 480
    },
    {
      "epoch": 1.924,
      "grad_norm": 0.21545985341072083,
      "learning_rate": 0.00014100491249672498,
      "loss": 2.3048,
      "step": 481
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.2643473744392395,
      "learning_rate": 0.00014076836149416887,
      "loss": 2.2152,
      "step": 482
    },
    {
      "epoch": 1.932,
      "grad_norm": 0.23589491844177246,
      "learning_rate": 0.0001405315365755379,
      "loss": 2.3446,
      "step": 483
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.17589299380779266,
      "learning_rate": 0.0001402944393320206,
      "loss": 2.0756,
      "step": 484
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.2525233328342438,
      "learning_rate": 0.00014005707135663527,
      "loss": 2.6994,
      "step": 485
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.25443917512893677,
      "learning_rate": 0.00013981943424421932,
      "loss": 2.4758,
      "step": 486
    },
    {
      "epoch": 1.948,
      "grad_norm": 0.2590820789337158,
      "learning_rate": 0.00013958152959141825,
      "loss": 2.0539,
      "step": 487
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.17872028052806854,
      "learning_rate": 0.00013934335899667527,
      "loss": 2.2332,
      "step": 488
    },
    {
      "epoch": 1.956,
      "grad_norm": 0.24020805954933167,
      "learning_rate": 0.00013910492406022033,
      "loss": 2.1271,
      "step": 489
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.21120630204677582,
      "learning_rate": 0.00013886622638405952,
      "loss": 2.2215,
      "step": 490
    },
    {
      "epoch": 1.964,
      "grad_norm": 0.2111382931470871,
      "learning_rate": 0.0001386272675719642,
      "loss": 2.3864,
      "step": 491
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.17840349674224854,
      "learning_rate": 0.00013838804922946027,
      "loss": 2.1905,
      "step": 492
    },
    {
      "epoch": 1.972,
      "grad_norm": 0.18323981761932373,
      "learning_rate": 0.00013814857296381728,
      "loss": 2.2331,
      "step": 493
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.21629758179187775,
      "learning_rate": 0.00013790884038403795,
      "loss": 2.0413,
      "step": 494
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.16951102018356323,
      "learning_rate": 0.00013766885310084688,
      "loss": 2.1128,
      "step": 495
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.20552338659763336,
      "learning_rate": 0.00013742861272668012,
      "loss": 2.3394,
      "step": 496
    },
    {
      "epoch": 1.988,
      "grad_norm": 0.17408131062984467,
      "learning_rate": 0.00013718812087567414,
      "loss": 2.1852,
      "step": 497
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.23815959692001343,
      "learning_rate": 0.00013694737916365517,
      "loss": 2.2799,
      "step": 498
    },
    {
      "epoch": 1.996,
      "grad_norm": 0.19214096665382385,
      "learning_rate": 0.000136706389208128,
      "loss": 2.2231,
      "step": 499
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.24942433834075928,
      "learning_rate": 0.00013646515262826552,
      "loss": 2.1883,
      "step": 500
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.2349820137023926,
      "eval_runtime": 23.761,
      "eval_samples_per_second": 21.043,
      "eval_steps_per_second": 2.651,
      "step": 500
    },
    {
      "epoch": 2.004,
      "grad_norm": 0.2045227736234665,
      "learning_rate": 0.00013622367104489756,
      "loss": 2.067,
      "step": 501
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.25126564502716064,
      "learning_rate": 0.0001359819460805001,
      "loss": 2.1963,
      "step": 502
    },
    {
      "epoch": 2.012,
      "grad_norm": 0.15164078772068024,
      "learning_rate": 0.0001357399793591844,
      "loss": 2.1282,
      "step": 503
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.163617342710495,
      "learning_rate": 0.0001354977725066859,
      "loss": 2.1365,
      "step": 504
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.18248052895069122,
      "learning_rate": 0.00013525532715035366,
      "loss": 1.9952,
      "step": 505
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.1918688416481018,
      "learning_rate": 0.00013501264491913906,
      "loss": 2.307,
      "step": 506
    },
    {
      "epoch": 2.028,
      "grad_norm": 0.18775244057178497,
      "learning_rate": 0.00013476972744358507,
      "loss": 2.1416,
      "step": 507
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.23231719434261322,
      "learning_rate": 0.0001345265763558152,
      "loss": 2.2077,
      "step": 508
    },
    {
      "epoch": 2.036,
      "grad_norm": 0.21832697093486786,
      "learning_rate": 0.00013428319328952253,
      "loss": 2.0469,
      "step": 509
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.16841080784797668,
      "learning_rate": 0.00013403957987995882,
      "loss": 2.0261,
      "step": 510
    },
    {
      "epoch": 2.044,
      "grad_norm": 0.29230329394340515,
      "learning_rate": 0.0001337957377639235,
      "loss": 2.2589,
      "step": 511
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.18653948605060577,
      "learning_rate": 0.0001335516685797525,
      "loss": 2.262,
      "step": 512
    },
    {
      "epoch": 2.052,
      "grad_norm": 0.3481246829032898,
      "learning_rate": 0.0001333073739673076,
      "loss": 2.3131,
      "step": 513
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.2878885269165039,
      "learning_rate": 0.00013306285556796495,
      "loss": 2.7005,
      "step": 514
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.24504698812961578,
      "learning_rate": 0.0001328181150246045,
      "loss": 2.1877,
      "step": 515
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.18866559863090515,
      "learning_rate": 0.00013257315398159864,
      "loss": 2.0319,
      "step": 516
    },
    {
      "epoch": 2.068,
      "grad_norm": 0.19440270960330963,
      "learning_rate": 0.00013232797408480127,
      "loss": 2.0927,
      "step": 517
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.210863396525383,
      "learning_rate": 0.00013208257698153677,
      "loss": 2.2488,
      "step": 518
    },
    {
      "epoch": 2.076,
      "grad_norm": 0.1637328714132309,
      "learning_rate": 0.00013183696432058888,
      "loss": 2.3139,
      "step": 519
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.19948065280914307,
      "learning_rate": 0.00013159113775218964,
      "loss": 2.3835,
      "step": 520
    },
    {
      "epoch": 2.084,
      "grad_norm": 0.21904189884662628,
      "learning_rate": 0.00013134509892800822,
      "loss": 2.4376,
      "step": 521
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.19126202166080475,
      "learning_rate": 0.00013109884950114007,
      "loss": 2.1826,
      "step": 522
    },
    {
      "epoch": 2.092,
      "grad_norm": 0.2532016932964325,
      "learning_rate": 0.00013085239112609547,
      "loss": 2.2474,
      "step": 523
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.2045210748910904,
      "learning_rate": 0.00013060572545878875,
      "loss": 2.0526,
      "step": 524
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.19045324623584747,
      "learning_rate": 0.00013035885415652685,
      "loss": 2.1227,
      "step": 525
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.2151007056236267,
      "learning_rate": 0.00013011177887799845,
      "loss": 2.2571,
      "step": 526
    },
    {
      "epoch": 2.108,
      "grad_norm": 0.1654035449028015,
      "learning_rate": 0.00012986450128326266,
      "loss": 2.2681,
      "step": 527
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.2270631045103073,
      "learning_rate": 0.00012961702303373795,
      "loss": 1.9358,
      "step": 528
    },
    {
      "epoch": 2.116,
      "grad_norm": 0.22204148769378662,
      "learning_rate": 0.00012936934579219094,
      "loss": 2.2369,
      "step": 529
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.15701240301132202,
      "learning_rate": 0.00012912147122272523,
      "loss": 2.0941,
      "step": 530
    },
    {
      "epoch": 2.124,
      "grad_norm": 0.20666079223155975,
      "learning_rate": 0.00012887340099077024,
      "loss": 2.1482,
      "step": 531
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.18200227618217468,
      "learning_rate": 0.00012862513676307008,
      "loss": 2.1604,
      "step": 532
    },
    {
      "epoch": 2.132,
      "grad_norm": 0.17886865139007568,
      "learning_rate": 0.0001283766802076722,
      "loss": 2.2975,
      "step": 533
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.1786285936832428,
      "learning_rate": 0.00012812803299391628,
      "loss": 2.036,
      "step": 534
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.21830686926841736,
      "learning_rate": 0.00012787919679242306,
      "loss": 2.0183,
      "step": 535
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.2340046465396881,
      "learning_rate": 0.00012763017327508305,
      "loss": 1.991,
      "step": 536
    },
    {
      "epoch": 2.148,
      "grad_norm": 0.22991491854190826,
      "learning_rate": 0.00012738096411504522,
      "loss": 2.0861,
      "step": 537
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.20468463003635406,
      "learning_rate": 0.0001271315709867059,
      "loss": 2.0776,
      "step": 538
    },
    {
      "epoch": 2.156,
      "grad_norm": 0.2529546320438385,
      "learning_rate": 0.00012688199556569753,
      "loss": 2.4214,
      "step": 539
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.22978870570659637,
      "learning_rate": 0.00012663223952887723,
      "loss": 2.019,
      "step": 540
    },
    {
      "epoch": 2.164,
      "grad_norm": 0.18388356268405914,
      "learning_rate": 0.0001263823045543158,
      "loss": 2.3398,
      "step": 541
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.2467704564332962,
      "learning_rate": 0.00012613219232128608,
      "loss": 1.9649,
      "step": 542
    },
    {
      "epoch": 2.172,
      "grad_norm": 0.17378175258636475,
      "learning_rate": 0.00012588190451025207,
      "loss": 2.2152,
      "step": 543
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.17413683235645294,
      "learning_rate": 0.00012563144280285741,
      "loss": 2.3107,
      "step": 544
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.29551419615745544,
      "learning_rate": 0.00012538080888191408,
      "loss": 1.8734,
      "step": 545
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.19811345636844635,
      "learning_rate": 0.00012513000443139112,
      "loss": 2.052,
      "step": 546
    },
    {
      "epoch": 2.188,
      "grad_norm": 0.176791712641716,
      "learning_rate": 0.00012487903113640337,
      "loss": 1.9852,
      "step": 547
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.15029460191726685,
      "learning_rate": 0.00012462789068320017,
      "loss": 2.0253,
      "step": 548
    },
    {
      "epoch": 2.196,
      "grad_norm": 0.1961965709924698,
      "learning_rate": 0.00012437658475915377,
      "loss": 2.373,
      "step": 549
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.27635782957077026,
      "learning_rate": 0.00012412511505274844,
      "loss": 2.418,
      "step": 550
    },
    {
      "epoch": 2.204,
      "grad_norm": 0.1972801685333252,
      "learning_rate": 0.00012387348325356874,
      "loss": 2.0946,
      "step": 551
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.1895841509103775,
      "learning_rate": 0.00012362169105228826,
      "loss": 2.0603,
      "step": 552
    },
    {
      "epoch": 2.212,
      "grad_norm": 0.1802162230014801,
      "learning_rate": 0.00012336974014065844,
      "loss": 2.1058,
      "step": 553
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.1406419724225998,
      "learning_rate": 0.000123117632211497,
      "loss": 1.952,
      "step": 554
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.2951201796531677,
      "learning_rate": 0.00012286536895867654,
      "loss": 2.2788,
      "step": 555
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.2049516886472702,
      "learning_rate": 0.00012261295207711346,
      "loss": 1.9317,
      "step": 556
    },
    {
      "epoch": 2.228,
      "grad_norm": 0.22501994669437408,
      "learning_rate": 0.00012236038326275626,
      "loss": 2.2184,
      "step": 557
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.22014661133289337,
      "learning_rate": 0.0001221076642125742,
      "loss": 2.5066,
      "step": 558
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 0.1810944676399231,
      "learning_rate": 0.00012185479662454595,
      "loss": 2.0995,
      "step": 559
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.25683358311653137,
      "learning_rate": 0.00012160178219764837,
      "loss": 2.1225,
      "step": 560
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 0.25220510363578796,
      "learning_rate": 0.00012134862263184467,
      "loss": 2.0077,
      "step": 561
    },
    {
      "epoch": 2.248,
      "grad_norm": 0.16244342923164368,
      "learning_rate": 0.00012109531962807332,
      "loss": 2.2287,
      "step": 562
    },
    {
      "epoch": 2.252,
      "grad_norm": 0.21872717142105103,
      "learning_rate": 0.00012084187488823657,
      "loss": 2.141,
      "step": 563
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.20502915978431702,
      "learning_rate": 0.00012058829011518896,
      "loss": 2.2432,
      "step": 564
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.21433058381080627,
      "learning_rate": 0.00012033456701272576,
      "loss": 2.2096,
      "step": 565
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.17969895899295807,
      "learning_rate": 0.00012008070728557186,
      "loss": 1.9935,
      "step": 566
    },
    {
      "epoch": 2.268,
      "grad_norm": 0.2115897387266159,
      "learning_rate": 0.00011982671263936995,
      "loss": 2.1159,
      "step": 567
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.18867698311805725,
      "learning_rate": 0.00011957258478066931,
      "loss": 1.9283,
      "step": 568
    },
    {
      "epoch": 2.276,
      "grad_norm": 0.3277583718299866,
      "learning_rate": 0.00011931832541691418,
      "loss": 2.4427,
      "step": 569
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.16848647594451904,
      "learning_rate": 0.00011906393625643244,
      "loss": 2.0919,
      "step": 570
    },
    {
      "epoch": 2.284,
      "grad_norm": 0.19793321192264557,
      "learning_rate": 0.00011880941900842397,
      "loss": 2.0405,
      "step": 571
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.22856798768043518,
      "learning_rate": 0.00011855477538294935,
      "loss": 2.2992,
      "step": 572
    },
    {
      "epoch": 2.292,
      "grad_norm": 0.16188204288482666,
      "learning_rate": 0.00011830000709091815,
      "loss": 1.8037,
      "step": 573
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.23483048379421234,
      "learning_rate": 0.00011804511584407763,
      "loss": 2.0626,
      "step": 574
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.1890602707862854,
      "learning_rate": 0.0001177901033550012,
      "loss": 2.1222,
      "step": 575
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.183222234249115,
      "learning_rate": 0.00011753497133707679,
      "loss": 2.4173,
      "step": 576
    },
    {
      "epoch": 2.308,
      "grad_norm": 0.139686718583107,
      "learning_rate": 0.00011727972150449544,
      "loss": 1.9295,
      "step": 577
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.17446854710578918,
      "learning_rate": 0.00011702435557223987,
      "loss": 2.0084,
      "step": 578
    },
    {
      "epoch": 2.316,
      "grad_norm": 0.16803738474845886,
      "learning_rate": 0.00011676887525607271,
      "loss": 2.0993,
      "step": 579
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.19748535752296448,
      "learning_rate": 0.00011651328227252517,
      "loss": 1.9961,
      "step": 580
    },
    {
      "epoch": 2.324,
      "grad_norm": 0.1528916358947754,
      "learning_rate": 0.00011625757833888551,
      "loss": 2.0652,
      "step": 581
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.2572670876979828,
      "learning_rate": 0.00011600176517318741,
      "loss": 2.1398,
      "step": 582
    },
    {
      "epoch": 2.332,
      "grad_norm": 0.28067469596862793,
      "learning_rate": 0.0001157458444941984,
      "loss": 2.1746,
      "step": 583
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.19753672182559967,
      "learning_rate": 0.00011548981802140848,
      "loss": 2.2,
      "step": 584
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.1476505845785141,
      "learning_rate": 0.00011523368747501839,
      "loss": 2.1189,
      "step": 585
    },
    {
      "epoch": 2.344,
      "grad_norm": 0.18562138080596924,
      "learning_rate": 0.00011497745457592816,
      "loss": 2.1452,
      "step": 586
    },
    {
      "epoch": 2.348,
      "grad_norm": 0.21142861247062683,
      "learning_rate": 0.00011472112104572547,
      "loss": 1.938,
      "step": 587
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.1823478490114212,
      "learning_rate": 0.00011446468860667421,
      "loss": 2.0482,
      "step": 588
    },
    {
      "epoch": 2.356,
      "grad_norm": 0.18604394793510437,
      "learning_rate": 0.0001142081589817027,
      "loss": 2.1004,
      "step": 589
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.19793963432312012,
      "learning_rate": 0.00011395153389439233,
      "loss": 2.028,
      "step": 590
    },
    {
      "epoch": 2.364,
      "grad_norm": 0.1618792712688446,
      "learning_rate": 0.00011369481506896582,
      "loss": 1.8852,
      "step": 591
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.17950376868247986,
      "learning_rate": 0.00011343800423027582,
      "loss": 1.9679,
      "step": 592
    },
    {
      "epoch": 2.372,
      "grad_norm": 0.18492065370082855,
      "learning_rate": 0.00011318110310379301,
      "loss": 2.1147,
      "step": 593
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.19769540429115295,
      "learning_rate": 0.0001129241134155949,
      "loss": 2.3209,
      "step": 594
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.18148808181285858,
      "learning_rate": 0.00011266703689235394,
      "loss": 1.9597,
      "step": 595
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.27210623025894165,
      "learning_rate": 0.00011240987526132594,
      "loss": 2.1206,
      "step": 596
    },
    {
      "epoch": 2.388,
      "grad_norm": 0.14450953900814056,
      "learning_rate": 0.00011215263025033869,
      "loss": 1.7278,
      "step": 597
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.29406508803367615,
      "learning_rate": 0.00011189530358778005,
      "loss": 1.8148,
      "step": 598
    },
    {
      "epoch": 2.396,
      "grad_norm": 0.22594888508319855,
      "learning_rate": 0.00011163789700258655,
      "loss": 2.1626,
      "step": 599
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.18433333933353424,
      "learning_rate": 0.00011138041222423177,
      "loss": 2.3308,
      "step": 600
    },
    {
      "epoch": 2.404,
      "grad_norm": 0.1620372235774994,
      "learning_rate": 0.00011112285098271451,
      "loss": 2.0092,
      "step": 601
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.16127929091453552,
      "learning_rate": 0.00011086521500854745,
      "loss": 1.977,
      "step": 602
    },
    {
      "epoch": 2.412,
      "grad_norm": 0.22987830638885498,
      "learning_rate": 0.00011060750603274535,
      "loss": 2.2146,
      "step": 603
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.18060705065727234,
      "learning_rate": 0.00011034972578681338,
      "loss": 2.036,
      "step": 604
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.16269712150096893,
      "learning_rate": 0.00011009187600273566,
      "loss": 1.9233,
      "step": 605
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.15734319388866425,
      "learning_rate": 0.00010983395841296348,
      "loss": 2.5637,
      "step": 606
    },
    {
      "epoch": 2.428,
      "grad_norm": 0.2742651402950287,
      "learning_rate": 0.00010957597475040373,
      "loss": 2.1306,
      "step": 607
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.1357511579990387,
      "learning_rate": 0.00010931792674840718,
      "loss": 2.2603,
      "step": 608
    },
    {
      "epoch": 2.436,
      "grad_norm": 0.17030450701713562,
      "learning_rate": 0.00010905981614075693,
      "loss": 1.97,
      "step": 609
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.24511027336120605,
      "learning_rate": 0.00010880164466165674,
      "loss": 1.9373,
      "step": 610
    },
    {
      "epoch": 2.444,
      "grad_norm": 0.2153739482164383,
      "learning_rate": 0.00010854341404571928,
      "loss": 2.1208,
      "step": 611
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.15565064549446106,
      "learning_rate": 0.00010828512602795462,
      "loss": 2.0333,
      "step": 612
    },
    {
      "epoch": 2.452,
      "grad_norm": 0.2044702172279358,
      "learning_rate": 0.00010802678234375851,
      "loss": 1.8064,
      "step": 613
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.1499081552028656,
      "learning_rate": 0.00010776838472890065,
      "loss": 1.8572,
      "step": 614
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.19229094684123993,
      "learning_rate": 0.0001075099349195131,
      "loss": 2.4403,
      "step": 615
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.21295735239982605,
      "learning_rate": 0.00010725143465207867,
      "loss": 2.0545,
      "step": 616
    },
    {
      "epoch": 2.468,
      "grad_norm": 0.1537831872701645,
      "learning_rate": 0.00010699288566341914,
      "loss": 1.9869,
      "step": 617
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.15025602281093597,
      "learning_rate": 0.00010673428969068364,
      "loss": 1.8364,
      "step": 618
    },
    {
      "epoch": 2.476,
      "grad_norm": 0.16458475589752197,
      "learning_rate": 0.000106475648471337,
      "loss": 2.123,
      "step": 619
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.20051711797714233,
      "learning_rate": 0.00010621696374314807,
      "loss": 1.9288,
      "step": 620
    },
    {
      "epoch": 2.484,
      "grad_norm": 0.19618502259254456,
      "learning_rate": 0.00010595823724417795,
      "loss": 2.0872,
      "step": 621
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.15890859067440033,
      "learning_rate": 0.00010569947071276847,
      "loss": 1.8292,
      "step": 622
    },
    {
      "epoch": 2.492,
      "grad_norm": 0.1967117339372635,
      "learning_rate": 0.00010544066588753044,
      "loss": 2.0165,
      "step": 623
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.1616954505443573,
      "learning_rate": 0.00010518182450733186,
      "loss": 1.9291,
      "step": 624
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.17152057588100433,
      "learning_rate": 0.00010492294831128641,
      "loss": 2.0695,
      "step": 625
    },
    {
      "epoch": 2.504,
      "grad_norm": 0.14590948820114136,
      "learning_rate": 0.00010466403903874176,
      "loss": 1.8343,
      "step": 626
    },
    {
      "epoch": 2.508,
      "grad_norm": 0.16265377402305603,
      "learning_rate": 0.00010440509842926767,
      "loss": 1.9904,
      "step": 627
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.12479811161756516,
      "learning_rate": 0.00010414612822264455,
      "loss": 2.1444,
      "step": 628
    },
    {
      "epoch": 2.516,
      "grad_norm": 0.2951153516769409,
      "learning_rate": 0.00010388713015885161,
      "loss": 2.4103,
      "step": 629
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.14703083038330078,
      "learning_rate": 0.00010362810597805526,
      "loss": 1.9599,
      "step": 630
    },
    {
      "epoch": 2.524,
      "grad_norm": 0.22389915585517883,
      "learning_rate": 0.00010336905742059742,
      "loss": 2.2641,
      "step": 631
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.2583003044128418,
      "learning_rate": 0.0001031099862269837,
      "loss": 2.1202,
      "step": 632
    },
    {
      "epoch": 2.532,
      "grad_norm": 0.18293912708759308,
      "learning_rate": 0.0001028508941378719,
      "loss": 2.0505,
      "step": 633
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.2322653830051422,
      "learning_rate": 0.00010259178289406011,
      "loss": 2.1892,
      "step": 634
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.18160414695739746,
      "learning_rate": 0.00010233265423647523,
      "loss": 2.1207,
      "step": 635
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.1556388884782791,
      "learning_rate": 0.00010207350990616107,
      "loss": 1.8495,
      "step": 636
    },
    {
      "epoch": 2.548,
      "grad_norm": 0.17538346350193024,
      "learning_rate": 0.00010181435164426676,
      "loss": 2.0246,
      "step": 637
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.19769003987312317,
      "learning_rate": 0.0001015551811920351,
      "loss": 2.1966,
      "step": 638
    },
    {
      "epoch": 2.556,
      "grad_norm": 0.41468051075935364,
      "learning_rate": 0.00010129600029079072,
      "loss": 1.8968,
      "step": 639
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.244182750582695,
      "learning_rate": 0.00010103681068192845,
      "loss": 2.0539,
      "step": 640
    },
    {
      "epoch": 2.564,
      "grad_norm": 0.18310318887233734,
      "learning_rate": 0.00010077761410690172,
      "loss": 1.9814,
      "step": 641
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.1874096840620041,
      "learning_rate": 0.00010051841230721065,
      "loss": 2.0184,
      "step": 642
    },
    {
      "epoch": 2.572,
      "grad_norm": 0.1842314600944519,
      "learning_rate": 0.00010025920702439051,
      "loss": 2.1099,
      "step": 643
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.23102633655071259,
      "learning_rate": 0.0001,
      "loss": 1.8497,
      "step": 644
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.22248736023902893,
      "learning_rate": 9.97407929756095e-05,
      "loss": 1.684,
      "step": 645
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.17979896068572998,
      "learning_rate": 9.948158769278939e-05,
      "loss": 1.996,
      "step": 646
    },
    {
      "epoch": 2.588,
      "grad_norm": 0.15631046891212463,
      "learning_rate": 9.92223858930983e-05,
      "loss": 1.8105,
      "step": 647
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.2246106117963791,
      "learning_rate": 9.896318931807155e-05,
      "loss": 1.9914,
      "step": 648
    },
    {
      "epoch": 2.596,
      "grad_norm": 0.15404613316059113,
      "learning_rate": 9.870399970920932e-05,
      "loss": 2.0027,
      "step": 649
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.20971478521823883,
      "learning_rate": 9.844481880796491e-05,
      "loss": 2.062,
      "step": 650
    },
    {
      "epoch": 2.604,
      "grad_norm": 0.18851901590824127,
      "learning_rate": 9.818564835573323e-05,
      "loss": 1.7959,
      "step": 651
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.16200150549411774,
      "learning_rate": 9.792649009383899e-05,
      "loss": 1.7786,
      "step": 652
    },
    {
      "epoch": 2.612,
      "grad_norm": 0.21786220371723175,
      "learning_rate": 9.766734576352478e-05,
      "loss": 1.8845,
      "step": 653
    },
    {
      "epoch": 2.616,
      "grad_norm": 0.1756245195865631,
      "learning_rate": 9.740821710593989e-05,
      "loss": 1.9905,
      "step": 654
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.15581092238426208,
      "learning_rate": 9.714910586212816e-05,
      "loss": 2.0474,
      "step": 655
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.15878917276859283,
      "learning_rate": 9.689001377301633e-05,
      "loss": 2.2744,
      "step": 656
    },
    {
      "epoch": 2.628,
      "grad_norm": 0.17826415598392487,
      "learning_rate": 9.663094257940258e-05,
      "loss": 1.8469,
      "step": 657
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.1503579169511795,
      "learning_rate": 9.637189402194476e-05,
      "loss": 1.86,
      "step": 658
    },
    {
      "epoch": 2.636,
      "grad_norm": 0.14635200798511505,
      "learning_rate": 9.611286984114841e-05,
      "loss": 1.92,
      "step": 659
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.13543826341629028,
      "learning_rate": 9.585387177735547e-05,
      "loss": 1.8554,
      "step": 660
    },
    {
      "epoch": 2.644,
      "grad_norm": 0.15403532981872559,
      "learning_rate": 9.559490157073236e-05,
      "loss": 1.9836,
      "step": 661
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.1389348953962326,
      "learning_rate": 9.533596096125825e-05,
      "loss": 1.8247,
      "step": 662
    },
    {
      "epoch": 2.652,
      "grad_norm": 0.16044658422470093,
      "learning_rate": 9.507705168871358e-05,
      "loss": 2.1065,
      "step": 663
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.2710728049278259,
      "learning_rate": 9.481817549266817e-05,
      "loss": 1.7826,
      "step": 664
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.16373810172080994,
      "learning_rate": 9.455933411246958e-05,
      "loss": 1.8716,
      "step": 665
    },
    {
      "epoch": 2.664,
      "grad_norm": 0.2470797598361969,
      "learning_rate": 9.430052928723153e-05,
      "loss": 1.9077,
      "step": 666
    },
    {
      "epoch": 2.668,
      "grad_norm": 0.13708074390888214,
      "learning_rate": 9.404176275582208e-05,
      "loss": 2.0555,
      "step": 667
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.14753413200378418,
      "learning_rate": 9.378303625685195e-05,
      "loss": 1.9522,
      "step": 668
    },
    {
      "epoch": 2.676,
      "grad_norm": 0.18609081208705902,
      "learning_rate": 9.352435152866298e-05,
      "loss": 1.7716,
      "step": 669
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.16802561283111572,
      "learning_rate": 9.326571030931637e-05,
      "loss": 2.2276,
      "step": 670
    },
    {
      "epoch": 2.684,
      "grad_norm": 0.17084893584251404,
      "learning_rate": 9.300711433658087e-05,
      "loss": 1.8985,
      "step": 671
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.1510791778564453,
      "learning_rate": 9.274856534792138e-05,
      "loss": 1.9951,
      "step": 672
    },
    {
      "epoch": 2.692,
      "grad_norm": 0.13875995576381683,
      "learning_rate": 9.249006508048694e-05,
      "loss": 1.7061,
      "step": 673
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.1901439130306244,
      "learning_rate": 9.223161527109937e-05,
      "loss": 1.7938,
      "step": 674
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.17847593128681183,
      "learning_rate": 9.197321765624152e-05,
      "loss": 1.8443,
      "step": 675
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.16448496282100677,
      "learning_rate": 9.171487397204539e-05,
      "loss": 1.6034,
      "step": 676
    },
    {
      "epoch": 2.708,
      "grad_norm": 0.1370270550251007,
      "learning_rate": 9.145658595428074e-05,
      "loss": 1.9547,
      "step": 677
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.1491595208644867,
      "learning_rate": 9.119835533834331e-05,
      "loss": 1.817,
      "step": 678
    },
    {
      "epoch": 2.716,
      "grad_norm": 0.2423771321773529,
      "learning_rate": 9.09401838592431e-05,
      "loss": 1.9272,
      "step": 679
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.15361756086349487,
      "learning_rate": 9.068207325159284e-05,
      "loss": 1.8609,
      "step": 680
    },
    {
      "epoch": 2.724,
      "grad_norm": 0.16426873207092285,
      "learning_rate": 9.04240252495963e-05,
      "loss": 1.5616,
      "step": 681
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.1415938138961792,
      "learning_rate": 9.016604158703654e-05,
      "loss": 2.1733,
      "step": 682
    },
    {
      "epoch": 2.732,
      "grad_norm": 0.2685019075870514,
      "learning_rate": 8.990812399726435e-05,
      "loss": 1.884,
      "step": 683
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.15310733020305634,
      "learning_rate": 8.965027421318665e-05,
      "loss": 1.8362,
      "step": 684
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.15076956152915955,
      "learning_rate": 8.939249396725467e-05,
      "loss": 1.6272,
      "step": 685
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.1618519127368927,
      "learning_rate": 8.913478499145254e-05,
      "loss": 1.9735,
      "step": 686
    },
    {
      "epoch": 2.748,
      "grad_norm": 0.19540993869304657,
      "learning_rate": 8.887714901728551e-05,
      "loss": 1.8568,
      "step": 687
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.17696402966976166,
      "learning_rate": 8.861958777576827e-05,
      "loss": 1.7977,
      "step": 688
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 0.17330212891101837,
      "learning_rate": 8.836210299741346e-05,
      "loss": 1.8128,
      "step": 689
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.2995413541793823,
      "learning_rate": 8.810469641222001e-05,
      "loss": 2.0634,
      "step": 690
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 0.22277355194091797,
      "learning_rate": 8.784736974966135e-05,
      "loss": 1.7826,
      "step": 691
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.18263229727745056,
      "learning_rate": 8.759012473867407e-05,
      "loss": 2.3785,
      "step": 692
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 0.2195274829864502,
      "learning_rate": 8.733296310764611e-05,
      "loss": 2.1618,
      "step": 693
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.16705581545829773,
      "learning_rate": 8.707588658440511e-05,
      "loss": 2.0607,
      "step": 694
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 0.14317256212234497,
      "learning_rate": 8.6818896896207e-05,
      "loss": 1.9644,
      "step": 695
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.25592413544654846,
      "learning_rate": 8.656199576972423e-05,
      "loss": 2.3467,
      "step": 696
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 0.18913403153419495,
      "learning_rate": 8.63051849310342e-05,
      "loss": 2.0022,
      "step": 697
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.1497451215982437,
      "learning_rate": 8.604846610560771e-05,
      "loss": 1.9212,
      "step": 698
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 0.1749730259180069,
      "learning_rate": 8.579184101829734e-05,
      "loss": 1.8596,
      "step": 699
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.15654607117176056,
      "learning_rate": 8.553531139332582e-05,
      "loss": 1.8289,
      "step": 700
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 0.18694479763507843,
      "learning_rate": 8.527887895427454e-05,
      "loss": 1.6319,
      "step": 701
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.13817040622234344,
      "learning_rate": 8.502254542407186e-05,
      "loss": 1.8937,
      "step": 702
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 0.15409019589424133,
      "learning_rate": 8.476631252498162e-05,
      "loss": 2.1491,
      "step": 703
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.14858001470565796,
      "learning_rate": 8.451018197859153e-05,
      "loss": 1.8035,
      "step": 704
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.18732406198978424,
      "learning_rate": 8.425415550580162e-05,
      "loss": 1.9305,
      "step": 705
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.14656221866607666,
      "learning_rate": 8.399823482681262e-05,
      "loss": 1.8639,
      "step": 706
    },
    {
      "epoch": 2.828,
      "grad_norm": 0.20689214766025543,
      "learning_rate": 8.374242166111448e-05,
      "loss": 1.8736,
      "step": 707
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.26563897728919983,
      "learning_rate": 8.348671772747487e-05,
      "loss": 2.3962,
      "step": 708
    },
    {
      "epoch": 2.836,
      "grad_norm": 0.13878609240055084,
      "learning_rate": 8.323112474392731e-05,
      "loss": 1.7364,
      "step": 709
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.21830011904239655,
      "learning_rate": 8.297564442776014e-05,
      "loss": 1.8907,
      "step": 710
    },
    {
      "epoch": 2.844,
      "grad_norm": 0.1388748735189438,
      "learning_rate": 8.272027849550457e-05,
      "loss": 1.8351,
      "step": 711
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.18855561316013336,
      "learning_rate": 8.246502866292324e-05,
      "loss": 2.1214,
      "step": 712
    },
    {
      "epoch": 2.852,
      "grad_norm": 0.20257987082004547,
      "learning_rate": 8.220989664499878e-05,
      "loss": 1.8997,
      "step": 713
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.23046156764030457,
      "learning_rate": 8.195488415592238e-05,
      "loss": 1.771,
      "step": 714
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.15141865611076355,
      "learning_rate": 8.169999290908188e-05,
      "loss": 1.6185,
      "step": 715
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.1794278472661972,
      "learning_rate": 8.144522461705067e-05,
      "loss": 1.9861,
      "step": 716
    },
    {
      "epoch": 2.868,
      "grad_norm": 0.15025009214878082,
      "learning_rate": 8.119058099157604e-05,
      "loss": 1.9571,
      "step": 717
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.16221442818641663,
      "learning_rate": 8.093606374356759e-05,
      "loss": 1.9609,
      "step": 718
    },
    {
      "epoch": 2.876,
      "grad_norm": 0.14964789152145386,
      "learning_rate": 8.068167458308582e-05,
      "loss": 2.0662,
      "step": 719
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.2767201364040375,
      "learning_rate": 8.042741521933071e-05,
      "loss": 1.9643,
      "step": 720
    },
    {
      "epoch": 2.884,
      "grad_norm": 0.23399552702903748,
      "learning_rate": 8.017328736063006e-05,
      "loss": 2.1019,
      "step": 721
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.15427672863006592,
      "learning_rate": 7.991929271442817e-05,
      "loss": 1.7895,
      "step": 722
    },
    {
      "epoch": 2.892,
      "grad_norm": 0.1621965616941452,
      "learning_rate": 7.966543298727425e-05,
      "loss": 1.9684,
      "step": 723
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.3451474606990814,
      "learning_rate": 7.941170988481108e-05,
      "loss": 2.0822,
      "step": 724
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.1831127107143402,
      "learning_rate": 7.915812511176347e-05,
      "loss": 1.9204,
      "step": 725
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.1613176465034485,
      "learning_rate": 7.89046803719267e-05,
      "loss": 1.7971,
      "step": 726
    },
    {
      "epoch": 2.908,
      "grad_norm": 0.1715327948331833,
      "learning_rate": 7.865137736815535e-05,
      "loss": 2.1245,
      "step": 727
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.17193591594696045,
      "learning_rate": 7.839821780235168e-05,
      "loss": 1.824,
      "step": 728
    },
    {
      "epoch": 2.916,
      "grad_norm": 0.1586778461933136,
      "learning_rate": 7.814520337545406e-05,
      "loss": 1.7614,
      "step": 729
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.2161073088645935,
      "learning_rate": 7.789233578742582e-05,
      "loss": 1.8626,
      "step": 730
    },
    {
      "epoch": 2.924,
      "grad_norm": 0.2184544801712036,
      "learning_rate": 7.763961673724379e-05,
      "loss": 1.7929,
      "step": 731
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.22426220774650574,
      "learning_rate": 7.738704792288655e-05,
      "loss": 1.8551,
      "step": 732
    },
    {
      "epoch": 2.932,
      "grad_norm": 0.20324860513210297,
      "learning_rate": 7.713463104132345e-05,
      "loss": 1.8551,
      "step": 733
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.1811738908290863,
      "learning_rate": 7.688236778850306e-05,
      "loss": 1.7324,
      "step": 734
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.19647830724716187,
      "learning_rate": 7.663025985934158e-05,
      "loss": 2.0676,
      "step": 735
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.21182474493980408,
      "learning_rate": 7.637830894771175e-05,
      "loss": 1.7351,
      "step": 736
    },
    {
      "epoch": 2.948,
      "grad_norm": 0.14565347135066986,
      "learning_rate": 7.61265167464313e-05,
      "loss": 1.901,
      "step": 737
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.1601180136203766,
      "learning_rate": 7.587488494725157e-05,
      "loss": 1.9537,
      "step": 738
    },
    {
      "epoch": 2.956,
      "grad_norm": 0.27412593364715576,
      "learning_rate": 7.562341524084623e-05,
      "loss": 1.9204,
      "step": 739
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.15422390401363373,
      "learning_rate": 7.537210931679987e-05,
      "loss": 2.2244,
      "step": 740
    },
    {
      "epoch": 2.964,
      "grad_norm": 0.13262850046157837,
      "learning_rate": 7.512096886359664e-05,
      "loss": 1.9955,
      "step": 741
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.3681770861148834,
      "learning_rate": 7.48699955686089e-05,
      "loss": 2.0325,
      "step": 742
    },
    {
      "epoch": 2.972,
      "grad_norm": 0.15433210134506226,
      "learning_rate": 7.461919111808595e-05,
      "loss": 1.9011,
      "step": 743
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.15942245721817017,
      "learning_rate": 7.43685571971426e-05,
      "loss": 1.7252,
      "step": 744
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.20460253953933716,
      "learning_rate": 7.411809548974792e-05,
      "loss": 1.8014,
      "step": 745
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.19481869041919708,
      "learning_rate": 7.386780767871397e-05,
      "loss": 1.8996,
      "step": 746
    },
    {
      "epoch": 2.988,
      "grad_norm": 0.17041510343551636,
      "learning_rate": 7.361769544568425e-05,
      "loss": 2.063,
      "step": 747
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.1859513521194458,
      "learning_rate": 7.336776047112276e-05,
      "loss": 1.6568,
      "step": 748
    },
    {
      "epoch": 2.996,
      "grad_norm": 0.14082273840904236,
      "learning_rate": 7.311800443430251e-05,
      "loss": 1.6851,
      "step": 749
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.16044463217258453,
      "learning_rate": 7.286842901329412e-05,
      "loss": 2.1589,
      "step": 750
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.9331860542297363,
      "eval_runtime": 23.8118,
      "eval_samples_per_second": 20.998,
      "eval_steps_per_second": 2.646,
      "step": 750
    },
    {
      "epoch": 3.004,
      "grad_norm": 0.14655764400959015,
      "learning_rate": 7.26190358849548e-05,
      "loss": 1.7313,
      "step": 751
    },
    {
      "epoch": 3.008,
      "grad_norm": 0.14153249561786652,
      "learning_rate": 7.236982672491698e-05,
      "loss": 1.8013,
      "step": 752
    },
    {
      "epoch": 3.012,
      "grad_norm": 0.14553223550319672,
      "learning_rate": 7.212080320757695e-05,
      "loss": 2.0243,
      "step": 753
    },
    {
      "epoch": 3.016,
      "grad_norm": 0.1511400043964386,
      "learning_rate": 7.187196700608373e-05,
      "loss": 1.9527,
      "step": 754
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.1293843388557434,
      "learning_rate": 7.162331979232783e-05,
      "loss": 1.688,
      "step": 755
    },
    {
      "epoch": 3.024,
      "grad_norm": 0.21717089414596558,
      "learning_rate": 7.137486323692995e-05,
      "loss": 1.681,
      "step": 756
    },
    {
      "epoch": 3.028,
      "grad_norm": 0.1538146734237671,
      "learning_rate": 7.112659900922976e-05,
      "loss": 1.8357,
      "step": 757
    },
    {
      "epoch": 3.032,
      "grad_norm": 0.18631549179553986,
      "learning_rate": 7.087852877727481e-05,
      "loss": 2.0506,
      "step": 758
    },
    {
      "epoch": 3.036,
      "grad_norm": 0.17060211300849915,
      "learning_rate": 7.06306542078091e-05,
      "loss": 1.9486,
      "step": 759
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.16513684391975403,
      "learning_rate": 7.038297696626206e-05,
      "loss": 1.8927,
      "step": 760
    },
    {
      "epoch": 3.044,
      "grad_norm": 0.14050766825675964,
      "learning_rate": 7.013549871673736e-05,
      "loss": 1.9739,
      "step": 761
    },
    {
      "epoch": 3.048,
      "grad_norm": 0.18849031627178192,
      "learning_rate": 6.988822112200156e-05,
      "loss": 1.8203,
      "step": 762
    },
    {
      "epoch": 3.052,
      "grad_norm": 0.1695445477962494,
      "learning_rate": 6.964114584347316e-05,
      "loss": 1.8976,
      "step": 763
    },
    {
      "epoch": 3.056,
      "grad_norm": 0.32886698842048645,
      "learning_rate": 6.939427454121128e-05,
      "loss": 2.0769,
      "step": 764
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.18496815860271454,
      "learning_rate": 6.914760887390452e-05,
      "loss": 1.91,
      "step": 765
    },
    {
      "epoch": 3.064,
      "grad_norm": 0.11893784254789352,
      "learning_rate": 6.890115049885994e-05,
      "loss": 1.8031,
      "step": 766
    },
    {
      "epoch": 3.068,
      "grad_norm": 0.14966514706611633,
      "learning_rate": 6.865490107199181e-05,
      "loss": 1.5976,
      "step": 767
    },
    {
      "epoch": 3.072,
      "grad_norm": 0.15529659390449524,
      "learning_rate": 6.84088622478104e-05,
      "loss": 1.9209,
      "step": 768
    },
    {
      "epoch": 3.076,
      "grad_norm": 0.2724829912185669,
      "learning_rate": 6.816303567941112e-05,
      "loss": 2.0821,
      "step": 769
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.1402284950017929,
      "learning_rate": 6.791742301846326e-05,
      "loss": 2.003,
      "step": 770
    },
    {
      "epoch": 3.084,
      "grad_norm": 0.17507174611091614,
      "learning_rate": 6.767202591519875e-05,
      "loss": 1.5617,
      "step": 771
    },
    {
      "epoch": 3.088,
      "grad_norm": 0.18104027211666107,
      "learning_rate": 6.742684601840141e-05,
      "loss": 1.9601,
      "step": 772
    },
    {
      "epoch": 3.092,
      "grad_norm": 0.22878123819828033,
      "learning_rate": 6.718188497539554e-05,
      "loss": 2.1454,
      "step": 773
    },
    {
      "epoch": 3.096,
      "grad_norm": 0.2671351134777069,
      "learning_rate": 6.693714443203507e-05,
      "loss": 2.1883,
      "step": 774
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.29806968569755554,
      "learning_rate": 6.669262603269246e-05,
      "loss": 2.1951,
      "step": 775
    },
    {
      "epoch": 3.104,
      "grad_norm": 0.1867784559726715,
      "learning_rate": 6.644833142024751e-05,
      "loss": 1.9118,
      "step": 776
    },
    {
      "epoch": 3.108,
      "grad_norm": 0.1499013751745224,
      "learning_rate": 6.620426223607654e-05,
      "loss": 1.8087,
      "step": 777
    },
    {
      "epoch": 3.112,
      "grad_norm": 0.14317987859249115,
      "learning_rate": 6.59604201200412e-05,
      "loss": 1.6615,
      "step": 778
    },
    {
      "epoch": 3.116,
      "grad_norm": 0.1532500982284546,
      "learning_rate": 6.571680671047749e-05,
      "loss": 1.923,
      "step": 779
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.14258083701133728,
      "learning_rate": 6.547342364418481e-05,
      "loss": 2.0129,
      "step": 780
    },
    {
      "epoch": 3.124,
      "grad_norm": 0.1661711037158966,
      "learning_rate": 6.523027255641493e-05,
      "loss": 1.7717,
      "step": 781
    },
    {
      "epoch": 3.128,
      "grad_norm": 0.13541385531425476,
      "learning_rate": 6.498735508086093e-05,
      "loss": 1.8253,
      "step": 782
    },
    {
      "epoch": 3.132,
      "grad_norm": 0.13708704710006714,
      "learning_rate": 6.474467284964634e-05,
      "loss": 1.9908,
      "step": 783
    },
    {
      "epoch": 3.136,
      "grad_norm": 0.1709585040807724,
      "learning_rate": 6.450222749331414e-05,
      "loss": 1.7767,
      "step": 784
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.17042765021324158,
      "learning_rate": 6.426002064081565e-05,
      "loss": 1.8552,
      "step": 785
    },
    {
      "epoch": 3.144,
      "grad_norm": 0.1955793797969818,
      "learning_rate": 6.40180539194999e-05,
      "loss": 2.024,
      "step": 786
    },
    {
      "epoch": 3.148,
      "grad_norm": 0.17830407619476318,
      "learning_rate": 6.377632895510248e-05,
      "loss": 2.1665,
      "step": 787
    },
    {
      "epoch": 3.152,
      "grad_norm": 0.15156123042106628,
      "learning_rate": 6.35348473717345e-05,
      "loss": 1.6929,
      "step": 788
    },
    {
      "epoch": 3.156,
      "grad_norm": 0.190584197640419,
      "learning_rate": 6.329361079187199e-05,
      "loss": 1.6335,
      "step": 789
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.1928376704454422,
      "learning_rate": 6.305262083634488e-05,
      "loss": 2.1148,
      "step": 790
    },
    {
      "epoch": 3.164,
      "grad_norm": 0.15712465345859528,
      "learning_rate": 6.281187912432587e-05,
      "loss": 1.6548,
      "step": 791
    },
    {
      "epoch": 3.168,
      "grad_norm": 0.25352925062179565,
      "learning_rate": 6.25713872733199e-05,
      "loss": 2.3843,
      "step": 792
    },
    {
      "epoch": 3.172,
      "grad_norm": 0.14709199965000153,
      "learning_rate": 6.233114689915316e-05,
      "loss": 1.9981,
      "step": 793
    },
    {
      "epoch": 3.176,
      "grad_norm": 0.15159213542938232,
      "learning_rate": 6.209115961596208e-05,
      "loss": 2.1985,
      "step": 794
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.2086704671382904,
      "learning_rate": 6.18514270361827e-05,
      "loss": 1.9992,
      "step": 795
    },
    {
      "epoch": 3.184,
      "grad_norm": 0.16376937925815582,
      "learning_rate": 6.161195077053976e-05,
      "loss": 1.8868,
      "step": 796
    },
    {
      "epoch": 3.188,
      "grad_norm": 0.14401736855506897,
      "learning_rate": 6.13727324280358e-05,
      "loss": 1.8456,
      "step": 797
    },
    {
      "epoch": 3.192,
      "grad_norm": 0.1257525235414505,
      "learning_rate": 6.113377361594049e-05,
      "loss": 1.7509,
      "step": 798
    },
    {
      "epoch": 3.196,
      "grad_norm": 0.14073200523853302,
      "learning_rate": 6.08950759397797e-05,
      "loss": 1.9561,
      "step": 799
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.14204742014408112,
      "learning_rate": 6.065664100332478e-05,
      "loss": 1.9433,
      "step": 800
    },
    {
      "epoch": 3.204,
      "grad_norm": 0.3065812587738037,
      "learning_rate": 6.0418470408581774e-05,
      "loss": 2.1299,
      "step": 801
    },
    {
      "epoch": 3.208,
      "grad_norm": 0.16953125596046448,
      "learning_rate": 6.018056575578075e-05,
      "loss": 2.0335,
      "step": 802
    },
    {
      "epoch": 3.212,
      "grad_norm": 0.1600950062274933,
      "learning_rate": 5.9942928643364724e-05,
      "loss": 1.9828,
      "step": 803
    },
    {
      "epoch": 3.216,
      "grad_norm": 0.14240026473999023,
      "learning_rate": 5.970556066797941e-05,
      "loss": 1.8836,
      "step": 804
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.14592640101909637,
      "learning_rate": 5.946846342446214e-05,
      "loss": 1.9436,
      "step": 805
    },
    {
      "epoch": 3.224,
      "grad_norm": 0.1734544336795807,
      "learning_rate": 5.923163850583113e-05,
      "loss": 1.7323,
      "step": 806
    },
    {
      "epoch": 3.228,
      "grad_norm": 0.15547755360603333,
      "learning_rate": 5.899508750327501e-05,
      "loss": 1.808,
      "step": 807
    },
    {
      "epoch": 3.232,
      "grad_norm": 0.14127393066883087,
      "learning_rate": 5.875881200614207e-05,
      "loss": 1.8146,
      "step": 808
    },
    {
      "epoch": 3.2359999999999998,
      "grad_norm": 0.11992346495389938,
      "learning_rate": 5.8522813601929324e-05,
      "loss": 1.8377,
      "step": 809
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.13415876030921936,
      "learning_rate": 5.828709387627218e-05,
      "loss": 1.9035,
      "step": 810
    },
    {
      "epoch": 3.2439999999999998,
      "grad_norm": 0.14985086023807526,
      "learning_rate": 5.80516544129337e-05,
      "loss": 1.6779,
      "step": 811
    },
    {
      "epoch": 3.248,
      "grad_norm": 0.21421697735786438,
      "learning_rate": 5.781649679379378e-05,
      "loss": 1.7615,
      "step": 812
    },
    {
      "epoch": 3.252,
      "grad_norm": 0.1621718555688858,
      "learning_rate": 5.758162259883867e-05,
      "loss": 1.6999,
      "step": 813
    },
    {
      "epoch": 3.2560000000000002,
      "grad_norm": 0.15806514024734497,
      "learning_rate": 5.73470334061505e-05,
      "loss": 2.0421,
      "step": 814
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.13515135645866394,
      "learning_rate": 5.7112730791896207e-05,
      "loss": 1.7839,
      "step": 815
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 0.13681413233280182,
      "learning_rate": 5.687871633031754e-05,
      "loss": 1.6047,
      "step": 816
    },
    {
      "epoch": 3.268,
      "grad_norm": 0.1400761902332306,
      "learning_rate": 5.664499159372017e-05,
      "loss": 1.7335,
      "step": 817
    },
    {
      "epoch": 3.2720000000000002,
      "grad_norm": 0.14728492498397827,
      "learning_rate": 5.6411558152462894e-05,
      "loss": 1.7161,
      "step": 818
    },
    {
      "epoch": 3.276,
      "grad_norm": 0.17005541920661926,
      "learning_rate": 5.617841757494762e-05,
      "loss": 1.7215,
      "step": 819
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.15447860956192017,
      "learning_rate": 5.5945571427608526e-05,
      "loss": 1.711,
      "step": 820
    },
    {
      "epoch": 3.284,
      "grad_norm": 0.1346764713525772,
      "learning_rate": 5.5713021274901335e-05,
      "loss": 1.7967,
      "step": 821
    },
    {
      "epoch": 3.288,
      "grad_norm": 0.13371747732162476,
      "learning_rate": 5.54807686792933e-05,
      "loss": 2.0399,
      "step": 822
    },
    {
      "epoch": 3.292,
      "grad_norm": 0.1545538455247879,
      "learning_rate": 5.524881520125229e-05,
      "loss": 1.8478,
      "step": 823
    },
    {
      "epoch": 3.296,
      "grad_norm": 0.13462470471858978,
      "learning_rate": 5.501716239923642e-05,
      "loss": 1.9198,
      "step": 824
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.22406914830207825,
      "learning_rate": 5.4785811829683764e-05,
      "loss": 1.7366,
      "step": 825
    },
    {
      "epoch": 3.304,
      "grad_norm": 0.16156619787216187,
      "learning_rate": 5.4554765047001613e-05,
      "loss": 2.1378,
      "step": 826
    },
    {
      "epoch": 3.308,
      "grad_norm": 0.15384535491466522,
      "learning_rate": 5.432402360355615e-05,
      "loss": 1.6321,
      "step": 827
    },
    {
      "epoch": 3.312,
      "grad_norm": 0.22376853227615356,
      "learning_rate": 5.4093589049662175e-05,
      "loss": 1.6782,
      "step": 828
    },
    {
      "epoch": 3.316,
      "grad_norm": 0.13586805760860443,
      "learning_rate": 5.386346293357242e-05,
      "loss": 1.736,
      "step": 829
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.17356790602207184,
      "learning_rate": 5.363364680146725e-05,
      "loss": 1.8884,
      "step": 830
    },
    {
      "epoch": 3.324,
      "grad_norm": 0.1289406269788742,
      "learning_rate": 5.3404142197444506e-05,
      "loss": 1.9409,
      "step": 831
    },
    {
      "epoch": 3.328,
      "grad_norm": 0.14675115048885345,
      "learning_rate": 5.31749506635086e-05,
      "loss": 1.9161,
      "step": 832
    },
    {
      "epoch": 3.332,
      "grad_norm": 0.13437442481517792,
      "learning_rate": 5.2946073739560706e-05,
      "loss": 1.7526,
      "step": 833
    },
    {
      "epoch": 3.336,
      "grad_norm": 0.24118800461292267,
      "learning_rate": 5.271751296338823e-05,
      "loss": 2.2439,
      "step": 834
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.14214321970939636,
      "learning_rate": 5.248926987065417e-05,
      "loss": 1.8205,
      "step": 835
    },
    {
      "epoch": 3.344,
      "grad_norm": 0.14601503312587738,
      "learning_rate": 5.226134599488728e-05,
      "loss": 1.7105,
      "step": 836
    },
    {
      "epoch": 3.348,
      "grad_norm": 0.17752023041248322,
      "learning_rate": 5.203374286747158e-05,
      "loss": 1.9057,
      "step": 837
    },
    {
      "epoch": 3.352,
      "grad_norm": 0.1597767472267151,
      "learning_rate": 5.180646201763577e-05,
      "loss": 1.5346,
      "step": 838
    },
    {
      "epoch": 3.356,
      "grad_norm": 0.1357821822166443,
      "learning_rate": 5.15795049724435e-05,
      "loss": 1.498,
      "step": 839
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.1407257616519928,
      "learning_rate": 5.135287325678271e-05,
      "loss": 1.8962,
      "step": 840
    },
    {
      "epoch": 3.364,
      "grad_norm": 0.17050552368164062,
      "learning_rate": 5.112656839335543e-05,
      "loss": 1.922,
      "step": 841
    },
    {
      "epoch": 3.368,
      "grad_norm": 0.13714009523391724,
      "learning_rate": 5.090059190266779e-05,
      "loss": 1.7985,
      "step": 842
    },
    {
      "epoch": 3.372,
      "grad_norm": 0.13142937421798706,
      "learning_rate": 5.0674945303019526e-05,
      "loss": 1.7013,
      "step": 843
    },
    {
      "epoch": 3.376,
      "grad_norm": 0.30359378457069397,
      "learning_rate": 5.0449630110493836e-05,
      "loss": 2.3618,
      "step": 844
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.18151116371154785,
      "learning_rate": 5.022464783894744e-05,
      "loss": 1.9442,
      "step": 845
    },
    {
      "epoch": 3.384,
      "grad_norm": 0.15435537695884705,
      "learning_rate": 5.000000000000002e-05,
      "loss": 1.9775,
      "step": 846
    },
    {
      "epoch": 3.388,
      "grad_norm": 0.16649220883846283,
      "learning_rate": 4.977568810302432e-05,
      "loss": 1.8079,
      "step": 847
    },
    {
      "epoch": 3.392,
      "grad_norm": 0.22618311643600464,
      "learning_rate": 4.955171365513603e-05,
      "loss": 1.8836,
      "step": 848
    },
    {
      "epoch": 3.396,
      "grad_norm": 0.1690877228975296,
      "learning_rate": 4.9328078161183464e-05,
      "loss": 1.8109,
      "step": 849
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.36585745215415955,
      "learning_rate": 4.9104783123737566e-05,
      "loss": 1.9982,
      "step": 850
    },
    {
      "epoch": 3.404,
      "grad_norm": 0.1459648460149765,
      "learning_rate": 4.88818300430819e-05,
      "loss": 1.973,
      "step": 851
    },
    {
      "epoch": 3.408,
      "grad_norm": 0.13495580852031708,
      "learning_rate": 4.865922041720239e-05,
      "loss": 1.7005,
      "step": 852
    },
    {
      "epoch": 3.412,
      "grad_norm": 0.15739330649375916,
      "learning_rate": 4.843695574177737e-05,
      "loss": 1.9673,
      "step": 853
    },
    {
      "epoch": 3.416,
      "grad_norm": 0.22858813405036926,
      "learning_rate": 4.821503751016746e-05,
      "loss": 1.9882,
      "step": 854
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.14553114771842957,
      "learning_rate": 4.7993467213405706e-05,
      "loss": 2.0013,
      "step": 855
    },
    {
      "epoch": 3.424,
      "grad_norm": 0.11840030550956726,
      "learning_rate": 4.777224634018732e-05,
      "loss": 1.7671,
      "step": 856
    },
    {
      "epoch": 3.428,
      "grad_norm": 0.15690553188323975,
      "learning_rate": 4.755137637685979e-05,
      "loss": 1.6312,
      "step": 857
    },
    {
      "epoch": 3.432,
      "grad_norm": 0.14160272479057312,
      "learning_rate": 4.733085880741301e-05,
      "loss": 1.7412,
      "step": 858
    },
    {
      "epoch": 3.436,
      "grad_norm": 0.132757306098938,
      "learning_rate": 4.7110695113469085e-05,
      "loss": 1.8228,
      "step": 859
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.2600253224372864,
      "learning_rate": 4.689088677427249e-05,
      "loss": 1.5891,
      "step": 860
    },
    {
      "epoch": 3.444,
      "grad_norm": 0.14016735553741455,
      "learning_rate": 4.6671435266680216e-05,
      "loss": 1.6411,
      "step": 861
    },
    {
      "epoch": 3.448,
      "grad_norm": 0.32522040605545044,
      "learning_rate": 4.645234206515171e-05,
      "loss": 2.1176,
      "step": 862
    },
    {
      "epoch": 3.452,
      "grad_norm": 0.13668116927146912,
      "learning_rate": 4.623360864173893e-05,
      "loss": 1.8831,
      "step": 863
    },
    {
      "epoch": 3.456,
      "grad_norm": 0.16860292851924896,
      "learning_rate": 4.6015236466076747e-05,
      "loss": 1.7761,
      "step": 864
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.1436384618282318,
      "learning_rate": 4.579722700537268e-05,
      "loss": 1.7912,
      "step": 865
    },
    {
      "epoch": 3.464,
      "grad_norm": 0.15985871851444244,
      "learning_rate": 4.5579581724397255e-05,
      "loss": 1.7208,
      "step": 866
    },
    {
      "epoch": 3.468,
      "grad_norm": 0.16263322532176971,
      "learning_rate": 4.5362302085474254e-05,
      "loss": 1.9447,
      "step": 867
    },
    {
      "epoch": 3.472,
      "grad_norm": 0.2847965657711029,
      "learning_rate": 4.514538954847064e-05,
      "loss": 1.8248,
      "step": 868
    },
    {
      "epoch": 3.476,
      "grad_norm": 0.13279087841510773,
      "learning_rate": 4.492884557078688e-05,
      "loss": 1.4671,
      "step": 869
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.16990479826927185,
      "learning_rate": 4.471267160734731e-05,
      "loss": 1.935,
      "step": 870
    },
    {
      "epoch": 3.484,
      "grad_norm": 0.1566818356513977,
      "learning_rate": 4.449686911058992e-05,
      "loss": 1.7079,
      "step": 871
    },
    {
      "epoch": 3.488,
      "grad_norm": 0.16157273948192596,
      "learning_rate": 4.428143953045717e-05,
      "loss": 1.8235,
      "step": 872
    },
    {
      "epoch": 3.492,
      "grad_norm": 0.1910916119813919,
      "learning_rate": 4.406638431438576e-05,
      "loss": 1.9791,
      "step": 873
    },
    {
      "epoch": 3.496,
      "grad_norm": 0.22573094069957733,
      "learning_rate": 4.385170490729712e-05,
      "loss": 2.192,
      "step": 874
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.31405094265937805,
      "learning_rate": 4.36374027515878e-05,
      "loss": 2.0699,
      "step": 875
    },
    {
      "epoch": 3.504,
      "grad_norm": 0.15656381845474243,
      "learning_rate": 4.342347928711953e-05,
      "loss": 1.7404,
      "step": 876
    },
    {
      "epoch": 3.508,
      "grad_norm": 0.16184821724891663,
      "learning_rate": 4.320993595120969e-05,
      "loss": 2.0678,
      "step": 877
    },
    {
      "epoch": 3.512,
      "grad_norm": 0.14835785329341888,
      "learning_rate": 4.2996774178621736e-05,
      "loss": 1.6205,
      "step": 878
    },
    {
      "epoch": 3.516,
      "grad_norm": 0.13206909596920013,
      "learning_rate": 4.278399540155536e-05,
      "loss": 1.8014,
      "step": 879
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.16724851727485657,
      "learning_rate": 4.257160104963696e-05,
      "loss": 1.7197,
      "step": 880
    },
    {
      "epoch": 3.524,
      "grad_norm": 0.1532011181116104,
      "learning_rate": 4.2359592549910145e-05,
      "loss": 2.0102,
      "step": 881
    },
    {
      "epoch": 3.528,
      "grad_norm": 0.13110291957855225,
      "learning_rate": 4.2147971326825966e-05,
      "loss": 1.4997,
      "step": 882
    },
    {
      "epoch": 3.532,
      "grad_norm": 0.19153030216693878,
      "learning_rate": 4.193673880223339e-05,
      "loss": 1.8521,
      "step": 883
    },
    {
      "epoch": 3.536,
      "grad_norm": 0.1169450581073761,
      "learning_rate": 4.172589639536991e-05,
      "loss": 1.9887,
      "step": 884
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.15624582767486572,
      "learning_rate": 4.1515445522851784e-05,
      "loss": 1.8051,
      "step": 885
    },
    {
      "epoch": 3.544,
      "grad_norm": 0.22538842260837555,
      "learning_rate": 4.130538759866457e-05,
      "loss": 1.6552,
      "step": 886
    },
    {
      "epoch": 3.548,
      "grad_norm": 0.14353492856025696,
      "learning_rate": 4.109572403415386e-05,
      "loss": 1.6715,
      "step": 887
    },
    {
      "epoch": 3.552,
      "grad_norm": 0.16810473799705505,
      "learning_rate": 4.088645623801534e-05,
      "loss": 1.7595,
      "step": 888
    },
    {
      "epoch": 3.556,
      "grad_norm": 0.1511276513338089,
      "learning_rate": 4.0677585616285774e-05,
      "loss": 1.834,
      "step": 889
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.16189661622047424,
      "learning_rate": 4.046911357233343e-05,
      "loss": 1.7313,
      "step": 890
    },
    {
      "epoch": 3.564,
      "grad_norm": 0.1376354694366455,
      "learning_rate": 4.026104150684835e-05,
      "loss": 1.694,
      "step": 891
    },
    {
      "epoch": 3.568,
      "grad_norm": 0.11959335207939148,
      "learning_rate": 4.00533708178334e-05,
      "loss": 1.9907,
      "step": 892
    },
    {
      "epoch": 3.572,
      "grad_norm": 0.16976073384284973,
      "learning_rate": 3.984610290059467e-05,
      "loss": 1.7406,
      "step": 893
    },
    {
      "epoch": 3.576,
      "grad_norm": 0.13911281526088715,
      "learning_rate": 3.963923914773187e-05,
      "loss": 1.6704,
      "step": 894
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.23619332909584045,
      "learning_rate": 3.943278094912946e-05,
      "loss": 1.7611,
      "step": 895
    },
    {
      "epoch": 3.584,
      "grad_norm": 0.15896736085414886,
      "learning_rate": 3.922672969194686e-05,
      "loss": 1.6388,
      "step": 896
    },
    {
      "epoch": 3.588,
      "grad_norm": 0.14207327365875244,
      "learning_rate": 3.902108676060937e-05,
      "loss": 1.8985,
      "step": 897
    },
    {
      "epoch": 3.592,
      "grad_norm": 0.12225341796875,
      "learning_rate": 3.8815853536798904e-05,
      "loss": 2.0362,
      "step": 898
    },
    {
      "epoch": 3.596,
      "grad_norm": 0.15455278754234314,
      "learning_rate": 3.861103139944449e-05,
      "loss": 1.7776,
      "step": 899
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.1292208433151245,
      "learning_rate": 3.840662172471315e-05,
      "loss": 1.766,
      "step": 900
    },
    {
      "epoch": 3.604,
      "grad_norm": 0.12905584275722504,
      "learning_rate": 3.820262588600074e-05,
      "loss": 1.8638,
      "step": 901
    },
    {
      "epoch": 3.608,
      "grad_norm": 0.16067597270011902,
      "learning_rate": 3.79990452539225e-05,
      "loss": 1.7805,
      "step": 902
    },
    {
      "epoch": 3.612,
      "grad_norm": 0.14290887117385864,
      "learning_rate": 3.7795881196303995e-05,
      "loss": 1.7363,
      "step": 903
    },
    {
      "epoch": 3.616,
      "grad_norm": 0.16197633743286133,
      "learning_rate": 3.759313507817196e-05,
      "loss": 1.8754,
      "step": 904
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.12659421563148499,
      "learning_rate": 3.739080826174498e-05,
      "loss": 1.6755,
      "step": 905
    },
    {
      "epoch": 3.624,
      "grad_norm": 0.1850692480802536,
      "learning_rate": 3.7188902106424416e-05,
      "loss": 2.0765,
      "step": 906
    },
    {
      "epoch": 3.628,
      "grad_norm": 0.1987793743610382,
      "learning_rate": 3.6987417968785366e-05,
      "loss": 1.9415,
      "step": 907
    },
    {
      "epoch": 3.632,
      "grad_norm": 0.32363709807395935,
      "learning_rate": 3.678635720256737e-05,
      "loss": 1.9976,
      "step": 908
    },
    {
      "epoch": 3.636,
      "grad_norm": 0.297448992729187,
      "learning_rate": 3.658572115866541e-05,
      "loss": 2.0429,
      "step": 909
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.15959253907203674,
      "learning_rate": 3.638551118512089e-05,
      "loss": 1.6568,
      "step": 910
    },
    {
      "epoch": 3.644,
      "grad_norm": 0.15176483988761902,
      "learning_rate": 3.618572862711247e-05,
      "loss": 1.8101,
      "step": 911
    },
    {
      "epoch": 3.648,
      "grad_norm": 0.1622679978609085,
      "learning_rate": 3.5986374826947066e-05,
      "loss": 1.5449,
      "step": 912
    },
    {
      "epoch": 3.652,
      "grad_norm": 0.1446385383605957,
      "learning_rate": 3.578745112405083e-05,
      "loss": 1.9184,
      "step": 913
    },
    {
      "epoch": 3.656,
      "grad_norm": 0.14695441722869873,
      "learning_rate": 3.558895885496023e-05,
      "loss": 1.7546,
      "step": 914
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.1664234846830368,
      "learning_rate": 3.539089935331294e-05,
      "loss": 1.7271,
      "step": 915
    },
    {
      "epoch": 3.664,
      "grad_norm": 0.1286022961139679,
      "learning_rate": 3.519327394983888e-05,
      "loss": 1.7485,
      "step": 916
    },
    {
      "epoch": 3.668,
      "grad_norm": 0.15824460983276367,
      "learning_rate": 3.4996083972351515e-05,
      "loss": 1.7328,
      "step": 917
    },
    {
      "epoch": 3.672,
      "grad_norm": 0.1276688426733017,
      "learning_rate": 3.479933074573858e-05,
      "loss": 1.7378,
      "step": 918
    },
    {
      "epoch": 3.676,
      "grad_norm": 0.13634303212165833,
      "learning_rate": 3.4603015591953395e-05,
      "loss": 1.4538,
      "step": 919
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.13914011418819427,
      "learning_rate": 3.440713983000601e-05,
      "loss": 1.5064,
      "step": 920
    },
    {
      "epoch": 3.684,
      "grad_norm": 0.13353845477104187,
      "learning_rate": 3.421170477595419e-05,
      "loss": 1.7007,
      "step": 921
    },
    {
      "epoch": 3.6879999999999997,
      "grad_norm": 0.14930902421474457,
      "learning_rate": 3.401671174289469e-05,
      "loss": 1.9914,
      "step": 922
    },
    {
      "epoch": 3.692,
      "grad_norm": 0.13436488807201385,
      "learning_rate": 3.3822162040954354e-05,
      "loss": 1.6952,
      "step": 923
    },
    {
      "epoch": 3.6959999999999997,
      "grad_norm": 0.13487590849399567,
      "learning_rate": 3.362805697728145e-05,
      "loss": 1.6814,
      "step": 924
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.19812427461147308,
      "learning_rate": 3.34343978560367e-05,
      "loss": 1.8064,
      "step": 925
    },
    {
      "epoch": 3.7039999999999997,
      "grad_norm": 0.2511241137981415,
      "learning_rate": 3.324118597838464e-05,
      "loss": 2.1869,
      "step": 926
    },
    {
      "epoch": 3.708,
      "grad_norm": 0.15182413160800934,
      "learning_rate": 3.3048422642484886e-05,
      "loss": 1.5721,
      "step": 927
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 0.13985469937324524,
      "learning_rate": 3.285610914348332e-05,
      "loss": 1.7644,
      "step": 928
    },
    {
      "epoch": 3.716,
      "grad_norm": 0.1694997102022171,
      "learning_rate": 3.266424677350346e-05,
      "loss": 1.7337,
      "step": 929
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.193812757730484,
      "learning_rate": 3.2472836821637744e-05,
      "loss": 1.9275,
      "step": 930
    },
    {
      "epoch": 3.724,
      "grad_norm": 0.14206792414188385,
      "learning_rate": 3.228188057393895e-05,
      "loss": 1.7879,
      "step": 931
    },
    {
      "epoch": 3.7279999999999998,
      "grad_norm": 0.33644384145736694,
      "learning_rate": 3.209137931341143e-05,
      "loss": 2.101,
      "step": 932
    },
    {
      "epoch": 3.732,
      "grad_norm": 0.16268962621688843,
      "learning_rate": 3.190133432000252e-05,
      "loss": 1.6858,
      "step": 933
    },
    {
      "epoch": 3.7359999999999998,
      "grad_norm": 0.1421443521976471,
      "learning_rate": 3.1711746870594086e-05,
      "loss": 1.7464,
      "step": 934
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.1561165452003479,
      "learning_rate": 3.1522618238993725e-05,
      "loss": 1.787,
      "step": 935
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 0.16699685156345367,
      "learning_rate": 3.1333949695926324e-05,
      "loss": 1.9983,
      "step": 936
    },
    {
      "epoch": 3.748,
      "grad_norm": 0.13124489784240723,
      "learning_rate": 3.114574250902558e-05,
      "loss": 1.7515,
      "step": 937
    },
    {
      "epoch": 3.752,
      "grad_norm": 0.14669294655323029,
      "learning_rate": 3.0957997942825336e-05,
      "loss": 1.7731,
      "step": 938
    },
    {
      "epoch": 3.7560000000000002,
      "grad_norm": 0.12718607485294342,
      "learning_rate": 3.077071725875116e-05,
      "loss": 1.8849,
      "step": 939
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.13767005503177643,
      "learning_rate": 3.058390171511196e-05,
      "loss": 1.6872,
      "step": 940
    },
    {
      "epoch": 3.7640000000000002,
      "grad_norm": 0.19195427000522614,
      "learning_rate": 3.0397552567091337e-05,
      "loss": 1.937,
      "step": 941
    },
    {
      "epoch": 3.768,
      "grad_norm": 0.20458711683750153,
      "learning_rate": 3.021167106673928e-05,
      "loss": 1.6794,
      "step": 942
    },
    {
      "epoch": 3.7720000000000002,
      "grad_norm": 0.1235603615641594,
      "learning_rate": 3.0026258462963787e-05,
      "loss": 1.7486,
      "step": 943
    },
    {
      "epoch": 3.776,
      "grad_norm": 0.13024641573429108,
      "learning_rate": 2.9841316001522347e-05,
      "loss": 1.819,
      "step": 944
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 0.12738947570323944,
      "learning_rate": 2.9656844925013637e-05,
      "loss": 1.7989,
      "step": 945
    },
    {
      "epoch": 3.784,
      "grad_norm": 0.14615382254123688,
      "learning_rate": 2.9472846472869298e-05,
      "loss": 1.7796,
      "step": 946
    },
    {
      "epoch": 3.7880000000000003,
      "grad_norm": 0.2073487490415573,
      "learning_rate": 2.9289321881345254e-05,
      "loss": 1.8757,
      "step": 947
    },
    {
      "epoch": 3.792,
      "grad_norm": 0.16685791313648224,
      "learning_rate": 2.9106272383513835e-05,
      "loss": 1.8067,
      "step": 948
    },
    {
      "epoch": 3.7960000000000003,
      "grad_norm": 0.1858421415090561,
      "learning_rate": 2.8923699209255284e-05,
      "loss": 1.5559,
      "step": 949
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.2232963591814041,
      "learning_rate": 2.874160358524931e-05,
      "loss": 1.8529,
      "step": 950
    },
    {
      "epoch": 3.8040000000000003,
      "grad_norm": 0.14920435845851898,
      "learning_rate": 2.8559986734967282e-05,
      "loss": 1.7782,
      "step": 951
    },
    {
      "epoch": 3.808,
      "grad_norm": 0.14300169050693512,
      "learning_rate": 2.8378849878663628e-05,
      "loss": 1.6382,
      "step": 952
    },
    {
      "epoch": 3.8120000000000003,
      "grad_norm": 0.15156874060630798,
      "learning_rate": 2.819819423336775e-05,
      "loss": 1.8427,
      "step": 953
    },
    {
      "epoch": 3.816,
      "grad_norm": 0.14714127779006958,
      "learning_rate": 2.8018021012875994e-05,
      "loss": 2.0772,
      "step": 954
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.1278696060180664,
      "learning_rate": 2.7838331427743282e-05,
      "loss": 1.7001,
      "step": 955
    },
    {
      "epoch": 3.824,
      "grad_norm": 0.12148181349039078,
      "learning_rate": 2.7659126685275027e-05,
      "loss": 1.6889,
      "step": 956
    },
    {
      "epoch": 3.828,
      "grad_norm": 0.16706550121307373,
      "learning_rate": 2.7480407989519198e-05,
      "loss": 1.6859,
      "step": 957
    },
    {
      "epoch": 3.832,
      "grad_norm": 0.1894461065530777,
      "learning_rate": 2.7302176541257986e-05,
      "loss": 1.9692,
      "step": 958
    },
    {
      "epoch": 3.836,
      "grad_norm": 0.16092456877231598,
      "learning_rate": 2.712443353799984e-05,
      "loss": 1.6152,
      "step": 959
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.16209468245506287,
      "learning_rate": 2.6947180173971508e-05,
      "loss": 1.6677,
      "step": 960
    },
    {
      "epoch": 3.844,
      "grad_norm": 0.17368730902671814,
      "learning_rate": 2.677041764010988e-05,
      "loss": 1.7567,
      "step": 961
    },
    {
      "epoch": 3.848,
      "grad_norm": 0.1357336938381195,
      "learning_rate": 2.659414712405398e-05,
      "loss": 1.5933,
      "step": 962
    },
    {
      "epoch": 3.852,
      "grad_norm": 0.14102672040462494,
      "learning_rate": 2.6418369810137188e-05,
      "loss": 1.7471,
      "step": 963
    },
    {
      "epoch": 3.856,
      "grad_norm": 0.15084823966026306,
      "learning_rate": 2.6243086879379e-05,
      "loss": 1.7841,
      "step": 964
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.12286999076604843,
      "learning_rate": 2.6068299509477266e-05,
      "loss": 1.7315,
      "step": 965
    },
    {
      "epoch": 3.864,
      "grad_norm": 0.15039043128490448,
      "learning_rate": 2.5894008874800325e-05,
      "loss": 1.9645,
      "step": 966
    },
    {
      "epoch": 3.868,
      "grad_norm": 0.14316707849502563,
      "learning_rate": 2.5720216146378917e-05,
      "loss": 1.8316,
      "step": 967
    },
    {
      "epoch": 3.872,
      "grad_norm": 0.16512712836265564,
      "learning_rate": 2.5546922491898495e-05,
      "loss": 1.7126,
      "step": 968
    },
    {
      "epoch": 3.876,
      "grad_norm": 0.2533513605594635,
      "learning_rate": 2.5374129075691265e-05,
      "loss": 2.0555,
      "step": 969
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.19131293892860413,
      "learning_rate": 2.5201837058728505e-05,
      "loss": 1.9731,
      "step": 970
    },
    {
      "epoch": 3.884,
      "grad_norm": 0.12377158552408218,
      "learning_rate": 2.503004759861258e-05,
      "loss": 1.953,
      "step": 971
    },
    {
      "epoch": 3.888,
      "grad_norm": 0.15424002707004547,
      "learning_rate": 2.485876184956928e-05,
      "loss": 1.7492,
      "step": 972
    },
    {
      "epoch": 3.892,
      "grad_norm": 0.1462014764547348,
      "learning_rate": 2.4687980962440072e-05,
      "loss": 1.6561,
      "step": 973
    },
    {
      "epoch": 3.896,
      "grad_norm": 0.1688312441110611,
      "learning_rate": 2.451770608467432e-05,
      "loss": 1.7908,
      "step": 974
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.19845207035541534,
      "learning_rate": 2.4347938360321566e-05,
      "loss": 1.8555,
      "step": 975
    },
    {
      "epoch": 3.904,
      "grad_norm": 0.2253541797399521,
      "learning_rate": 2.417867893002387e-05,
      "loss": 2.0876,
      "step": 976
    },
    {
      "epoch": 3.908,
      "grad_norm": 0.22255121171474457,
      "learning_rate": 2.400992893100822e-05,
      "loss": 2.3916,
      "step": 977
    },
    {
      "epoch": 3.912,
      "grad_norm": 0.11830325424671173,
      "learning_rate": 2.3841689497078746e-05,
      "loss": 1.7559,
      "step": 978
    },
    {
      "epoch": 3.916,
      "grad_norm": 0.15174025297164917,
      "learning_rate": 2.3673961758609152e-05,
      "loss": 1.9225,
      "step": 979
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.146844282746315,
      "learning_rate": 2.3506746842535242e-05,
      "loss": 1.9241,
      "step": 980
    },
    {
      "epoch": 3.924,
      "grad_norm": 0.23159679770469666,
      "learning_rate": 2.334004587234717e-05,
      "loss": 1.8458,
      "step": 981
    },
    {
      "epoch": 3.928,
      "grad_norm": 0.17714627087116241,
      "learning_rate": 2.3173859968081944e-05,
      "loss": 1.7672,
      "step": 982
    },
    {
      "epoch": 3.932,
      "grad_norm": 0.41704103350639343,
      "learning_rate": 2.300819024631603e-05,
      "loss": 2.2556,
      "step": 983
    },
    {
      "epoch": 3.936,
      "grad_norm": 0.20525696873664856,
      "learning_rate": 2.2843037820157675e-05,
      "loss": 1.8747,
      "step": 984
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.1513192355632782,
      "learning_rate": 2.26784037992395e-05,
      "loss": 1.7267,
      "step": 985
    },
    {
      "epoch": 3.944,
      "grad_norm": 0.18455109000205994,
      "learning_rate": 2.251428928971102e-05,
      "loss": 1.9316,
      "step": 986
    },
    {
      "epoch": 3.948,
      "grad_norm": 0.15815649926662445,
      "learning_rate": 2.2350695394231345e-05,
      "loss": 1.6679,
      "step": 987
    },
    {
      "epoch": 3.952,
      "grad_norm": 0.1836026906967163,
      "learning_rate": 2.2187623211961562e-05,
      "loss": 1.7889,
      "step": 988
    },
    {
      "epoch": 3.956,
      "grad_norm": 0.18900907039642334,
      "learning_rate": 2.2025073838557454e-05,
      "loss": 1.6458,
      "step": 989
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.16276156902313232,
      "learning_rate": 2.1863048366162208e-05,
      "loss": 1.8338,
      "step": 990
    },
    {
      "epoch": 3.964,
      "grad_norm": 0.12265828251838684,
      "learning_rate": 2.1701547883398922e-05,
      "loss": 1.547,
      "step": 991
    },
    {
      "epoch": 3.968,
      "grad_norm": 0.1618589162826538,
      "learning_rate": 2.1540573475363402e-05,
      "loss": 1.6814,
      "step": 992
    },
    {
      "epoch": 3.972,
      "grad_norm": 0.18278934061527252,
      "learning_rate": 2.138012622361689e-05,
      "loss": 1.647,
      "step": 993
    },
    {
      "epoch": 3.976,
      "grad_norm": 0.12013430148363113,
      "learning_rate": 2.1220207206178688e-05,
      "loss": 1.4805,
      "step": 994
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.1987268179655075,
      "learning_rate": 2.106081749751897e-05,
      "loss": 1.6758,
      "step": 995
    },
    {
      "epoch": 3.984,
      "grad_norm": 0.12149348109960556,
      "learning_rate": 2.0901958168551638e-05,
      "loss": 1.8721,
      "step": 996
    },
    {
      "epoch": 3.988,
      "grad_norm": 0.17352192103862762,
      "learning_rate": 2.0743630286627002e-05,
      "loss": 1.6037,
      "step": 997
    },
    {
      "epoch": 3.992,
      "grad_norm": 0.20458659529685974,
      "learning_rate": 2.058583491552465e-05,
      "loss": 1.6921,
      "step": 998
    },
    {
      "epoch": 3.996,
      "grad_norm": 0.12363901734352112,
      "learning_rate": 2.0428573115446392e-05,
      "loss": 1.7911,
      "step": 999
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.22831536829471588,
      "learning_rate": 2.027184594300898e-05,
      "loss": 1.8984,
      "step": 1000
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.824775218963623,
      "eval_runtime": 23.7677,
      "eval_samples_per_second": 21.037,
      "eval_steps_per_second": 2.651,
      "step": 1000
    },
    {
      "epoch": 4.004,
      "grad_norm": 0.23808938264846802,
      "learning_rate": 2.011565445123711e-05,
      "loss": 1.8888,
      "step": 1001
    },
    {
      "epoch": 4.008,
      "grad_norm": 0.18479931354522705,
      "learning_rate": 1.995999968955641e-05,
      "loss": 1.8913,
      "step": 1002
    },
    {
      "epoch": 4.012,
      "grad_norm": 0.17329709231853485,
      "learning_rate": 1.980488270378612e-05,
      "loss": 1.7518,
      "step": 1003
    },
    {
      "epoch": 4.016,
      "grad_norm": 0.14933672547340393,
      "learning_rate": 1.9650304536132426e-05,
      "loss": 1.6708,
      "step": 1004
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.21011051535606384,
      "learning_rate": 1.9496266225181248e-05,
      "loss": 2.246,
      "step": 1005
    },
    {
      "epoch": 4.024,
      "grad_norm": 0.19673742353916168,
      "learning_rate": 1.9342768805891178e-05,
      "loss": 1.9908,
      "step": 1006
    },
    {
      "epoch": 4.028,
      "grad_norm": 0.1454567313194275,
      "learning_rate": 1.918981330958678e-05,
      "loss": 1.6105,
      "step": 1007
    },
    {
      "epoch": 4.032,
      "grad_norm": 0.2079913169145584,
      "learning_rate": 1.903740076395151e-05,
      "loss": 1.9072,
      "step": 1008
    },
    {
      "epoch": 4.036,
      "grad_norm": 0.13264235854148865,
      "learning_rate": 1.8885532193020704e-05,
      "loss": 1.6261,
      "step": 1009
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.19396278262138367,
      "learning_rate": 1.8734208617174988e-05,
      "loss": 2.0035,
      "step": 1010
    },
    {
      "epoch": 4.044,
      "grad_norm": 0.158916175365448,
      "learning_rate": 1.8583431053133127e-05,
      "loss": 1.4617,
      "step": 1011
    },
    {
      "epoch": 4.048,
      "grad_norm": 0.1497250348329544,
      "learning_rate": 1.8433200513945337e-05,
      "loss": 1.6956,
      "step": 1012
    },
    {
      "epoch": 4.052,
      "grad_norm": 0.16714049875736237,
      "learning_rate": 1.8283518008986567e-05,
      "loss": 1.704,
      "step": 1013
    },
    {
      "epoch": 4.056,
      "grad_norm": 0.15812087059020996,
      "learning_rate": 1.8134384543949478e-05,
      "loss": 1.4721,
      "step": 1014
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.19945967197418213,
      "learning_rate": 1.7985801120837865e-05,
      "loss": 1.7342,
      "step": 1015
    },
    {
      "epoch": 4.064,
      "grad_norm": 0.2052343785762787,
      "learning_rate": 1.783776873795994e-05,
      "loss": 2.2056,
      "step": 1016
    },
    {
      "epoch": 4.068,
      "grad_norm": 0.1845226287841797,
      "learning_rate": 1.7690288389921493e-05,
      "loss": 1.5216,
      "step": 1017
    },
    {
      "epoch": 4.072,
      "grad_norm": 0.21388433873653412,
      "learning_rate": 1.754336106761927e-05,
      "loss": 2.071,
      "step": 1018
    },
    {
      "epoch": 4.076,
      "grad_norm": 0.1309802085161209,
      "learning_rate": 1.739698775823442e-05,
      "loss": 1.9435,
      "step": 1019
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.13341234624385834,
      "learning_rate": 1.7251169445225657e-05,
      "loss": 1.663,
      "step": 1020
    },
    {
      "epoch": 4.084,
      "grad_norm": 0.14217042922973633,
      "learning_rate": 1.7105907108322816e-05,
      "loss": 1.5897,
      "step": 1021
    },
    {
      "epoch": 4.088,
      "grad_norm": 0.19493228197097778,
      "learning_rate": 1.696120172352025e-05,
      "loss": 2.0972,
      "step": 1022
    },
    {
      "epoch": 4.092,
      "grad_norm": 0.15778040885925293,
      "learning_rate": 1.6817054263070174e-05,
      "loss": 1.866,
      "step": 1023
    },
    {
      "epoch": 4.096,
      "grad_norm": 0.2696833312511444,
      "learning_rate": 1.6673465695476232e-05,
      "loss": 2.1348,
      "step": 1024
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.1493230164051056,
      "learning_rate": 1.6530436985486996e-05,
      "loss": 1.6066,
      "step": 1025
    },
    {
      "epoch": 4.104,
      "grad_norm": 0.11502595990896225,
      "learning_rate": 1.6387969094089316e-05,
      "loss": 1.6372,
      "step": 1026
    },
    {
      "epoch": 4.108,
      "grad_norm": 0.15548323094844818,
      "learning_rate": 1.6246062978502164e-05,
      "loss": 1.685,
      "step": 1027
    },
    {
      "epoch": 4.112,
      "grad_norm": 0.12781952321529388,
      "learning_rate": 1.6104719592169902e-05,
      "loss": 1.6255,
      "step": 1028
    },
    {
      "epoch": 4.116,
      "grad_norm": 0.11122239381074905,
      "learning_rate": 1.5963939884756042e-05,
      "loss": 1.9789,
      "step": 1029
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.11722549051046371,
      "learning_rate": 1.5823724802136865e-05,
      "loss": 1.8861,
      "step": 1030
    },
    {
      "epoch": 4.124,
      "grad_norm": 0.14034642279148102,
      "learning_rate": 1.5684075286394985e-05,
      "loss": 1.56,
      "step": 1031
    },
    {
      "epoch": 4.128,
      "grad_norm": 0.1647201031446457,
      "learning_rate": 1.5544992275813053e-05,
      "loss": 1.8993,
      "step": 1032
    },
    {
      "epoch": 4.132,
      "grad_norm": 0.16992512345314026,
      "learning_rate": 1.5406476704867524e-05,
      "loss": 1.878,
      "step": 1033
    },
    {
      "epoch": 4.136,
      "grad_norm": 0.1328699141740799,
      "learning_rate": 1.526852950422226e-05,
      "loss": 1.6745,
      "step": 1034
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.13620224595069885,
      "learning_rate": 1.5131151600722337e-05,
      "loss": 1.8157,
      "step": 1035
    },
    {
      "epoch": 4.144,
      "grad_norm": 0.13966518640518188,
      "learning_rate": 1.4994343917387854e-05,
      "loss": 1.6786,
      "step": 1036
    },
    {
      "epoch": 4.148,
      "grad_norm": 0.1598132997751236,
      "learning_rate": 1.485810737340767e-05,
      "loss": 1.7999,
      "step": 1037
    },
    {
      "epoch": 4.152,
      "grad_norm": 0.13652397692203522,
      "learning_rate": 1.4722442884133214e-05,
      "loss": 1.8532,
      "step": 1038
    },
    {
      "epoch": 4.156,
      "grad_norm": 0.17736223340034485,
      "learning_rate": 1.4587351361072454e-05,
      "loss": 1.5451,
      "step": 1039
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.14304953813552856,
      "learning_rate": 1.4452833711883628e-05,
      "loss": 1.8277,
      "step": 1040
    },
    {
      "epoch": 4.164,
      "grad_norm": 0.13398900628089905,
      "learning_rate": 1.4318890840369182e-05,
      "loss": 1.8527,
      "step": 1041
    },
    {
      "epoch": 4.168,
      "grad_norm": 0.15134942531585693,
      "learning_rate": 1.4185523646469822e-05,
      "loss": 1.5103,
      "step": 1042
    },
    {
      "epoch": 4.172,
      "grad_norm": 0.15203598141670227,
      "learning_rate": 1.4052733026258281e-05,
      "loss": 1.6127,
      "step": 1043
    },
    {
      "epoch": 4.176,
      "grad_norm": 0.12512914836406708,
      "learning_rate": 1.3920519871933424e-05,
      "loss": 1.5869,
      "step": 1044
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.1255650520324707,
      "learning_rate": 1.3788885071814172e-05,
      "loss": 1.8612,
      "step": 1045
    },
    {
      "epoch": 4.184,
      "grad_norm": 0.12080039829015732,
      "learning_rate": 1.3657829510333654e-05,
      "loss": 1.9981,
      "step": 1046
    },
    {
      "epoch": 4.188,
      "grad_norm": 0.2141483575105667,
      "learning_rate": 1.3527354068033139e-05,
      "loss": 1.7822,
      "step": 1047
    },
    {
      "epoch": 4.192,
      "grad_norm": 0.1311495304107666,
      "learning_rate": 1.339745962155613e-05,
      "loss": 1.931,
      "step": 1048
    },
    {
      "epoch": 4.196,
      "grad_norm": 0.14503873884677887,
      "learning_rate": 1.326814704364262e-05,
      "loss": 1.5807,
      "step": 1049
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.20720374584197998,
      "learning_rate": 1.3139417203123027e-05,
      "loss": 1.8006,
      "step": 1050
    },
    {
      "epoch": 4.204,
      "grad_norm": 0.15319478511810303,
      "learning_rate": 1.3011270964912459e-05,
      "loss": 1.7148,
      "step": 1051
    },
    {
      "epoch": 4.208,
      "grad_norm": 0.15263156592845917,
      "learning_rate": 1.2883709190004955e-05,
      "loss": 1.5575,
      "step": 1052
    },
    {
      "epoch": 4.212,
      "grad_norm": 0.17390523850917816,
      "learning_rate": 1.275673273546758e-05,
      "loss": 1.7752,
      "step": 1053
    },
    {
      "epoch": 4.216,
      "grad_norm": 0.1131986677646637,
      "learning_rate": 1.263034245443473e-05,
      "loss": 1.6976,
      "step": 1054
    },
    {
      "epoch": 4.22,
      "grad_norm": 0.11178093403577805,
      "learning_rate": 1.2504539196102439e-05,
      "loss": 1.8897,
      "step": 1055
    },
    {
      "epoch": 4.224,
      "grad_norm": 0.1470910906791687,
      "learning_rate": 1.2379323805722576e-05,
      "loss": 1.608,
      "step": 1056
    },
    {
      "epoch": 4.228,
      "grad_norm": 0.14391861855983734,
      "learning_rate": 1.2254697124597237e-05,
      "loss": 1.768,
      "step": 1057
    },
    {
      "epoch": 4.232,
      "grad_norm": 0.2928100526332855,
      "learning_rate": 1.2130659990073146e-05,
      "loss": 2.1136,
      "step": 1058
    },
    {
      "epoch": 4.236,
      "grad_norm": 0.16674114763736725,
      "learning_rate": 1.2007213235535786e-05,
      "loss": 1.6547,
      "step": 1059
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.1926121562719345,
      "learning_rate": 1.1884357690404158e-05,
      "loss": 1.9352,
      "step": 1060
    },
    {
      "epoch": 4.244,
      "grad_norm": 0.1430789977312088,
      "learning_rate": 1.176209418012495e-05,
      "loss": 1.8371,
      "step": 1061
    },
    {
      "epoch": 4.248,
      "grad_norm": 0.11976566165685654,
      "learning_rate": 1.1640423526166988e-05,
      "loss": 1.7623,
      "step": 1062
    },
    {
      "epoch": 4.252,
      "grad_norm": 0.1608245074748993,
      "learning_rate": 1.1519346546015907e-05,
      "loss": 1.6165,
      "step": 1063
    },
    {
      "epoch": 4.256,
      "grad_norm": 0.14619006216526031,
      "learning_rate": 1.1398864053168534e-05,
      "loss": 1.6036,
      "step": 1064
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.162991002202034,
      "learning_rate": 1.1278976857127311e-05,
      "loss": 1.7861,
      "step": 1065
    },
    {
      "epoch": 4.264,
      "grad_norm": 0.15879274904727936,
      "learning_rate": 1.1159685763395111e-05,
      "loss": 1.8519,
      "step": 1066
    },
    {
      "epoch": 4.268,
      "grad_norm": 0.14490625262260437,
      "learning_rate": 1.1040991573469629e-05,
      "loss": 1.6378,
      "step": 1067
    },
    {
      "epoch": 4.272,
      "grad_norm": 0.31925201416015625,
      "learning_rate": 1.0922895084838037e-05,
      "loss": 1.7394,
      "step": 1068
    },
    {
      "epoch": 4.276,
      "grad_norm": 0.28598886728286743,
      "learning_rate": 1.0805397090971737e-05,
      "loss": 2.272,
      "step": 1069
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.17359377443790436,
      "learning_rate": 1.0688498381320855e-05,
      "loss": 2.1411,
      "step": 1070
    },
    {
      "epoch": 4.284,
      "grad_norm": 0.13892929255962372,
      "learning_rate": 1.057219974130903e-05,
      "loss": 1.9836,
      "step": 1071
    },
    {
      "epoch": 4.288,
      "grad_norm": 0.1232033371925354,
      "learning_rate": 1.045650195232819e-05,
      "loss": 1.7547,
      "step": 1072
    },
    {
      "epoch": 4.292,
      "grad_norm": 0.11908156424760818,
      "learning_rate": 1.0341405791733183e-05,
      "loss": 1.5837,
      "step": 1073
    },
    {
      "epoch": 4.296,
      "grad_norm": 0.14134426414966583,
      "learning_rate": 1.0226912032836611e-05,
      "loss": 1.8376,
      "step": 1074
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.11128665506839752,
      "learning_rate": 1.0113021444903726e-05,
      "loss": 2.1109,
      "step": 1075
    },
    {
      "epoch": 4.304,
      "grad_norm": 0.13195087015628815,
      "learning_rate": 9.999734793146998e-06,
      "loss": 1.8608,
      "step": 1076
    },
    {
      "epoch": 4.308,
      "grad_norm": 0.12181559950113297,
      "learning_rate": 9.887052838721322e-06,
      "loss": 1.9367,
      "step": 1077
    },
    {
      "epoch": 4.312,
      "grad_norm": 0.1492137908935547,
      "learning_rate": 9.774976338718677e-06,
      "loss": 1.775,
      "step": 1078
    },
    {
      "epoch": 4.316,
      "grad_norm": 0.14898277819156647,
      "learning_rate": 9.663506046162985e-06,
      "loss": 1.6229,
      "step": 1079
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.15620821714401245,
      "learning_rate": 9.552642710005299e-06,
      "loss": 2.2586,
      "step": 1080
    },
    {
      "epoch": 4.324,
      "grad_norm": 0.13027425110340118,
      "learning_rate": 9.44238707511862e-06,
      "loss": 1.4998,
      "step": 1081
    },
    {
      "epoch": 4.328,
      "grad_norm": 0.14910948276519775,
      "learning_rate": 9.332739882292752e-06,
      "loss": 1.6978,
      "step": 1082
    },
    {
      "epoch": 4.332,
      "grad_norm": 0.21388715505599976,
      "learning_rate": 9.22370186822965e-06,
      "loss": 1.5297,
      "step": 1083
    },
    {
      "epoch": 4.336,
      "grad_norm": 0.17385390400886536,
      "learning_rate": 9.115273765538202e-06,
      "loss": 1.6833,
      "step": 1084
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.13991506397724152,
      "learning_rate": 9.0074563027294e-06,
      "loss": 1.7041,
      "step": 1085
    },
    {
      "epoch": 4.344,
      "grad_norm": 0.18503133952617645,
      "learning_rate": 8.900250204211514e-06,
      "loss": 1.5903,
      "step": 1086
    },
    {
      "epoch": 4.348,
      "grad_norm": 0.1522044539451599,
      "learning_rate": 8.79365619028507e-06,
      "loss": 1.8189,
      "step": 1087
    },
    {
      "epoch": 4.352,
      "grad_norm": 0.1539858877658844,
      "learning_rate": 8.687674977138116e-06,
      "loss": 1.6619,
      "step": 1088
    },
    {
      "epoch": 4.356,
      "grad_norm": 0.1455029994249344,
      "learning_rate": 8.582307276841462e-06,
      "loss": 1.6772,
      "step": 1089
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.1433190256357193,
      "learning_rate": 8.47755379734373e-06,
      "loss": 1.8237,
      "step": 1090
    },
    {
      "epoch": 4.364,
      "grad_norm": 0.11192115396261215,
      "learning_rate": 8.37341524246672e-06,
      "loss": 1.8013,
      "step": 1091
    },
    {
      "epoch": 4.368,
      "grad_norm": 0.13803225755691528,
      "learning_rate": 8.269892311900696e-06,
      "loss": 1.6554,
      "step": 1092
    },
    {
      "epoch": 4.372,
      "grad_norm": 0.20280106365680695,
      "learning_rate": 8.166985701199582e-06,
      "loss": 1.9797,
      "step": 1093
    },
    {
      "epoch": 4.376,
      "grad_norm": 0.15741965174674988,
      "learning_rate": 8.064696101776358e-06,
      "loss": 1.9567,
      "step": 1094
    },
    {
      "epoch": 4.38,
      "grad_norm": 0.15355932712554932,
      "learning_rate": 7.963024200898462e-06,
      "loss": 1.5097,
      "step": 1095
    },
    {
      "epoch": 4.384,
      "grad_norm": 0.19170024991035461,
      "learning_rate": 7.861970681683051e-06,
      "loss": 1.7157,
      "step": 1096
    },
    {
      "epoch": 4.388,
      "grad_norm": 0.10957661271095276,
      "learning_rate": 7.761536223092458e-06,
      "loss": 1.7687,
      "step": 1097
    },
    {
      "epoch": 4.392,
      "grad_norm": 0.1299196034669876,
      "learning_rate": 7.661721499929753e-06,
      "loss": 1.6718,
      "step": 1098
    },
    {
      "epoch": 4.396,
      "grad_norm": 0.15984152257442474,
      "learning_rate": 7.562527182833978e-06,
      "loss": 1.8159,
      "step": 1099
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.287174254655838,
      "learning_rate": 7.463953938275858e-06,
      "loss": 1.9048,
      "step": 1100
    },
    {
      "epoch": 4.404,
      "grad_norm": 0.17800219357013702,
      "learning_rate": 7.366002428553153e-06,
      "loss": 1.7105,
      "step": 1101
    },
    {
      "epoch": 4.408,
      "grad_norm": 0.1289452463388443,
      "learning_rate": 7.2686733117863784e-06,
      "loss": 1.9905,
      "step": 1102
    },
    {
      "epoch": 4.412,
      "grad_norm": 0.17204168438911438,
      "learning_rate": 7.171967241914224e-06,
      "loss": 1.5259,
      "step": 1103
    },
    {
      "epoch": 4.416,
      "grad_norm": 0.13202516734600067,
      "learning_rate": 7.07588486868922e-06,
      "loss": 1.5442,
      "step": 1104
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.1699722856283188,
      "learning_rate": 6.980426837673437e-06,
      "loss": 1.9466,
      "step": 1105
    },
    {
      "epoch": 4.424,
      "grad_norm": 0.14999698102474213,
      "learning_rate": 6.8855937902340576e-06,
      "loss": 1.6085,
      "step": 1106
    },
    {
      "epoch": 4.428,
      "grad_norm": 0.18005894124507904,
      "learning_rate": 6.791386363539065e-06,
      "loss": 1.7602,
      "step": 1107
    },
    {
      "epoch": 4.432,
      "grad_norm": 0.1429319530725479,
      "learning_rate": 6.6978051905530855e-06,
      "loss": 1.7075,
      "step": 1108
    },
    {
      "epoch": 4.436,
      "grad_norm": 0.1407107710838318,
      "learning_rate": 6.604850900032955e-06,
      "loss": 1.7199,
      "step": 1109
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.14282314479351044,
      "learning_rate": 6.512524116523633e-06,
      "loss": 1.7835,
      "step": 1110
    },
    {
      "epoch": 4.444,
      "grad_norm": 0.1356341391801834,
      "learning_rate": 6.420825460353974e-06,
      "loss": 1.9047,
      "step": 1111
    },
    {
      "epoch": 4.448,
      "grad_norm": 0.13658884167671204,
      "learning_rate": 6.329755547632499e-06,
      "loss": 1.6549,
      "step": 1112
    },
    {
      "epoch": 4.452,
      "grad_norm": 0.1335659772157669,
      "learning_rate": 6.239314990243339e-06,
      "loss": 1.686,
      "step": 1113
    },
    {
      "epoch": 4.456,
      "grad_norm": 0.29253482818603516,
      "learning_rate": 6.149504395842087e-06,
      "loss": 2.0275,
      "step": 1114
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.17454533278942108,
      "learning_rate": 6.0603243678516995e-06,
      "loss": 1.5854,
      "step": 1115
    },
    {
      "epoch": 4.464,
      "grad_norm": 0.2804255485534668,
      "learning_rate": 5.971775505458444e-06,
      "loss": 1.9726,
      "step": 1116
    },
    {
      "epoch": 4.468,
      "grad_norm": 0.1540774703025818,
      "learning_rate": 5.883858403607967e-06,
      "loss": 1.724,
      "step": 1117
    },
    {
      "epoch": 4.4719999999999995,
      "grad_norm": 0.13094155490398407,
      "learning_rate": 5.7965736530010916e-06,
      "loss": 1.7807,
      "step": 1118
    },
    {
      "epoch": 4.476,
      "grad_norm": 0.21620780229568481,
      "learning_rate": 5.7099218400900716e-06,
      "loss": 1.994,
      "step": 1119
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.11517107486724854,
      "learning_rate": 5.623903547074549e-06,
      "loss": 1.5976,
      "step": 1120
    },
    {
      "epoch": 4.484,
      "grad_norm": 0.13457967340946198,
      "learning_rate": 5.538519351897575e-06,
      "loss": 1.7967,
      "step": 1121
    },
    {
      "epoch": 4.4879999999999995,
      "grad_norm": 0.14231686294078827,
      "learning_rate": 5.453769828241872e-06,
      "loss": 1.7487,
      "step": 1122
    },
    {
      "epoch": 4.492,
      "grad_norm": 0.15069830417633057,
      "learning_rate": 5.369655545525909e-06,
      "loss": 1.5688,
      "step": 1123
    },
    {
      "epoch": 4.496,
      "grad_norm": 0.1750989556312561,
      "learning_rate": 5.286177068899989e-06,
      "loss": 1.7343,
      "step": 1124
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.1652929186820984,
      "learning_rate": 5.2033349592426335e-06,
      "loss": 1.9745,
      "step": 1125
    },
    {
      "epoch": 4.504,
      "grad_norm": 0.13020184636116028,
      "learning_rate": 5.121129773156663e-06,
      "loss": 1.7057,
      "step": 1126
    },
    {
      "epoch": 4.508,
      "grad_norm": 0.1966426968574524,
      "learning_rate": 5.039562062965508e-06,
      "loss": 1.6723,
      "step": 1127
    },
    {
      "epoch": 4.5120000000000005,
      "grad_norm": 0.3035782277584076,
      "learning_rate": 4.95863237670956e-06,
      "loss": 1.9836,
      "step": 1128
    },
    {
      "epoch": 4.516,
      "grad_norm": 0.17457516491413116,
      "learning_rate": 4.87834125814235e-06,
      "loss": 2.1533,
      "step": 1129
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.15242676436901093,
      "learning_rate": 4.798689246727006e-06,
      "loss": 1.6889,
      "step": 1130
    },
    {
      "epoch": 4.524,
      "grad_norm": 0.18708965182304382,
      "learning_rate": 4.719676877632639e-06,
      "loss": 1.7689,
      "step": 1131
    },
    {
      "epoch": 4.5280000000000005,
      "grad_norm": 0.1366831660270691,
      "learning_rate": 4.641304681730641e-06,
      "loss": 1.5121,
      "step": 1132
    },
    {
      "epoch": 4.532,
      "grad_norm": 0.1300063580274582,
      "learning_rate": 4.563573185591219e-06,
      "loss": 1.9156,
      "step": 1133
    },
    {
      "epoch": 4.536,
      "grad_norm": 0.12983980774879456,
      "learning_rate": 4.486482911479839e-06,
      "loss": 1.4986,
      "step": 1134
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.17479215562343597,
      "learning_rate": 4.4100343773536225e-06,
      "loss": 1.5157,
      "step": 1135
    },
    {
      "epoch": 4.5440000000000005,
      "grad_norm": 0.25191259384155273,
      "learning_rate": 4.3342280968580285e-06,
      "loss": 2.1015,
      "step": 1136
    },
    {
      "epoch": 4.548,
      "grad_norm": 0.16789278388023376,
      "learning_rate": 4.259064579323302e-06,
      "loss": 1.7657,
      "step": 1137
    },
    {
      "epoch": 4.552,
      "grad_norm": 0.19888466596603394,
      "learning_rate": 4.184544329761009e-06,
      "loss": 2.1059,
      "step": 1138
    },
    {
      "epoch": 4.556,
      "grad_norm": 0.17788982391357422,
      "learning_rate": 4.1106678488607495e-06,
      "loss": 1.5904,
      "step": 1139
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 0.11457834392786026,
      "learning_rate": 4.037435632986786e-06,
      "loss": 1.9242,
      "step": 1140
    },
    {
      "epoch": 4.564,
      "grad_norm": 0.12395275384187698,
      "learning_rate": 3.964848174174541e-06,
      "loss": 1.5649,
      "step": 1141
    },
    {
      "epoch": 4.568,
      "grad_norm": 0.18884466588497162,
      "learning_rate": 3.892905960127546e-06,
      "loss": 1.4943,
      "step": 1142
    },
    {
      "epoch": 4.572,
      "grad_norm": 0.12497373670339584,
      "learning_rate": 3.821609474213983e-06,
      "loss": 1.8375,
      "step": 1143
    },
    {
      "epoch": 4.576,
      "grad_norm": 0.12651681900024414,
      "learning_rate": 3.750959195463466e-06,
      "loss": 1.8073,
      "step": 1144
    },
    {
      "epoch": 4.58,
      "grad_norm": 0.1303396224975586,
      "learning_rate": 3.6809555985639068e-06,
      "loss": 1.6965,
      "step": 1145
    },
    {
      "epoch": 4.584,
      "grad_norm": 0.1598234325647354,
      "learning_rate": 3.611599153858214e-06,
      "loss": 1.7485,
      "step": 1146
    },
    {
      "epoch": 4.588,
      "grad_norm": 0.28895682096481323,
      "learning_rate": 3.5428903273411863e-06,
      "loss": 2.1791,
      "step": 1147
    },
    {
      "epoch": 4.592,
      "grad_norm": 0.16057033836841583,
      "learning_rate": 3.4748295806564356e-06,
      "loss": 2.2448,
      "step": 1148
    },
    {
      "epoch": 4.596,
      "grad_norm": 0.1439620554447174,
      "learning_rate": 3.40741737109318e-06,
      "loss": 1.8838,
      "step": 1149
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.15349629521369934,
      "learning_rate": 3.3406541515832003e-06,
      "loss": 1.5768,
      "step": 1150
    },
    {
      "epoch": 4.604,
      "grad_norm": 0.16149739921092987,
      "learning_rate": 3.2745403706978872e-06,
      "loss": 1.5889,
      "step": 1151
    },
    {
      "epoch": 4.608,
      "grad_norm": 0.16047951579093933,
      "learning_rate": 3.209076472645112e-06,
      "loss": 1.6149,
      "step": 1152
    },
    {
      "epoch": 4.612,
      "grad_norm": 0.15433432161808014,
      "learning_rate": 3.1442628972662704e-06,
      "loss": 1.8102,
      "step": 1153
    },
    {
      "epoch": 4.616,
      "grad_norm": 0.13145993649959564,
      "learning_rate": 3.0801000800333877e-06,
      "loss": 1.6246,
      "step": 1154
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.13042955100536346,
      "learning_rate": 3.0165884520461316e-06,
      "loss": 1.7933,
      "step": 1155
    },
    {
      "epoch": 4.624,
      "grad_norm": 0.17599888145923615,
      "learning_rate": 2.9537284400289355e-06,
      "loss": 1.6639,
      "step": 1156
    },
    {
      "epoch": 4.628,
      "grad_norm": 0.13135582208633423,
      "learning_rate": 2.8915204663281013e-06,
      "loss": 1.7831,
      "step": 1157
    },
    {
      "epoch": 4.632,
      "grad_norm": 0.1564401239156723,
      "learning_rate": 2.8299649489090475e-06,
      "loss": 1.7522,
      "step": 1158
    },
    {
      "epoch": 4.636,
      "grad_norm": 0.20832459628582,
      "learning_rate": 2.7690623013533976e-06,
      "loss": 1.8868,
      "step": 1159
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.13758771121501923,
      "learning_rate": 2.708812932856253e-06,
      "loss": 1.7916,
      "step": 1160
    },
    {
      "epoch": 4.644,
      "grad_norm": 0.14862686395645142,
      "learning_rate": 2.649217248223468e-06,
      "loss": 1.7501,
      "step": 1161
    },
    {
      "epoch": 4.648,
      "grad_norm": 0.1663656234741211,
      "learning_rate": 2.590275647868867e-06,
      "loss": 1.6753,
      "step": 1162
    },
    {
      "epoch": 4.652,
      "grad_norm": 0.13159507513046265,
      "learning_rate": 2.5319885278115906e-06,
      "loss": 1.7853,
      "step": 1163
    },
    {
      "epoch": 4.656,
      "grad_norm": 0.14392049610614777,
      "learning_rate": 2.4743562796734622e-06,
      "loss": 1.5864,
      "step": 1164
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.1837579607963562,
      "learning_rate": 2.4173792906762804e-06,
      "loss": 1.6393,
      "step": 1165
    },
    {
      "epoch": 4.664,
      "grad_norm": 0.1330523043870926,
      "learning_rate": 2.3610579436393e-06,
      "loss": 1.6275,
      "step": 1166
    },
    {
      "epoch": 4.668,
      "grad_norm": 0.18904438614845276,
      "learning_rate": 2.3053926169765984e-06,
      "loss": 1.6993,
      "step": 1167
    },
    {
      "epoch": 4.672,
      "grad_norm": 0.2269921451807022,
      "learning_rate": 2.250383684694579e-06,
      "loss": 1.6807,
      "step": 1168
    },
    {
      "epoch": 4.676,
      "grad_norm": 0.12511563301086426,
      "learning_rate": 2.1960315163894075e-06,
      "loss": 1.6606,
      "step": 1169
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.14448386430740356,
      "learning_rate": 2.1423364772445887e-06,
      "loss": 1.9334,
      "step": 1170
    },
    {
      "epoch": 4.684,
      "grad_norm": 0.12829935550689697,
      "learning_rate": 2.0892989280284823e-06,
      "loss": 1.892,
      "step": 1171
    },
    {
      "epoch": 4.688,
      "grad_norm": 0.13381318747997284,
      "learning_rate": 2.036919225091827e-06,
      "loss": 1.7434,
      "step": 1172
    },
    {
      "epoch": 4.692,
      "grad_norm": 0.136515811085701,
      "learning_rate": 1.9851977203654835e-06,
      "loss": 1.7423,
      "step": 1173
    },
    {
      "epoch": 4.696,
      "grad_norm": 0.12540490925312042,
      "learning_rate": 1.9341347613579087e-06,
      "loss": 1.5503,
      "step": 1174
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.10799551010131836,
      "learning_rate": 1.8837306911529184e-06,
      "loss": 1.9287,
      "step": 1175
    },
    {
      "epoch": 4.704,
      "grad_norm": 0.16565652191638947,
      "learning_rate": 1.8339858484073935e-06,
      "loss": 1.6249,
      "step": 1176
    },
    {
      "epoch": 4.708,
      "grad_norm": 0.14278404414653778,
      "learning_rate": 1.7849005673489127e-06,
      "loss": 2.1559,
      "step": 1177
    },
    {
      "epoch": 4.712,
      "grad_norm": 0.1326790302991867,
      "learning_rate": 1.7364751777736332e-06,
      "loss": 1.699,
      "step": 1178
    },
    {
      "epoch": 4.716,
      "grad_norm": 0.13588517904281616,
      "learning_rate": 1.6887100050439587e-06,
      "loss": 1.7208,
      "step": 1179
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.14988045394420624,
      "learning_rate": 1.6416053700863964e-06,
      "loss": 1.9246,
      "step": 1180
    },
    {
      "epoch": 4.724,
      "grad_norm": 0.12888695299625397,
      "learning_rate": 1.595161589389449e-06,
      "loss": 1.6981,
      "step": 1181
    },
    {
      "epoch": 4.728,
      "grad_norm": 0.12409037351608276,
      "learning_rate": 1.5493789750014031e-06,
      "loss": 1.5214,
      "step": 1182
    },
    {
      "epoch": 4.732,
      "grad_norm": 0.18988093733787537,
      "learning_rate": 1.5042578345283108e-06,
      "loss": 1.8513,
      "step": 1183
    },
    {
      "epoch": 4.736,
      "grad_norm": 0.20572853088378906,
      "learning_rate": 1.459798471131868e-06,
      "loss": 1.9202,
      "step": 1184
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.14276732504367828,
      "learning_rate": 1.4160011835273934e-06,
      "loss": 2.0443,
      "step": 1185
    },
    {
      "epoch": 4.744,
      "grad_norm": 0.14897659420967102,
      "learning_rate": 1.3728662659818204e-06,
      "loss": 1.3077,
      "step": 1186
    },
    {
      "epoch": 4.748,
      "grad_norm": 0.19155430793762207,
      "learning_rate": 1.3303940083117527e-06,
      "loss": 1.639,
      "step": 1187
    },
    {
      "epoch": 4.752,
      "grad_norm": 0.2459820657968521,
      "learning_rate": 1.2885846958814673e-06,
      "loss": 1.7118,
      "step": 1188
    },
    {
      "epoch": 4.756,
      "grad_norm": 0.12827768921852112,
      "learning_rate": 1.2474386096010039e-06,
      "loss": 1.5196,
      "step": 1189
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.18579216301441193,
      "learning_rate": 1.2069560259243328e-06,
      "loss": 1.8164,
      "step": 1190
    },
    {
      "epoch": 4.764,
      "grad_norm": 0.11911207437515259,
      "learning_rate": 1.1671372168474138e-06,
      "loss": 1.4966,
      "step": 1191
    },
    {
      "epoch": 4.768,
      "grad_norm": 0.16928520798683167,
      "learning_rate": 1.1279824499064396e-06,
      "loss": 1.8304,
      "step": 1192
    },
    {
      "epoch": 4.772,
      "grad_norm": 0.11362146586179733,
      "learning_rate": 1.089491988176017e-06,
      "loss": 1.8743,
      "step": 1193
    },
    {
      "epoch": 4.776,
      "grad_norm": 0.1600981205701828,
      "learning_rate": 1.0516660902673448e-06,
      "loss": 1.7468,
      "step": 1194
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.14930763840675354,
      "learning_rate": 1.014505010326583e-06,
      "loss": 1.7079,
      "step": 1195
    },
    {
      "epoch": 4.784,
      "grad_norm": 0.1278790980577469,
      "learning_rate": 9.780089980330642e-07,
      "loss": 1.7383,
      "step": 1196
    },
    {
      "epoch": 4.788,
      "grad_norm": 0.14077618718147278,
      "learning_rate": 9.421782985976068e-07,
      "loss": 2.1028,
      "step": 1197
    },
    {
      "epoch": 4.792,
      "grad_norm": 0.14857670664787292,
      "learning_rate": 9.070131527609604e-07,
      "loss": 1.6186,
      "step": 1198
    },
    {
      "epoch": 4.796,
      "grad_norm": 0.368460088968277,
      "learning_rate": 8.725137967920738e-07,
      "loss": 2.1025,
      "step": 1199
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.12952463328838348,
      "learning_rate": 8.386804624865851e-07,
      "loss": 1.7275,
      "step": 1200
    },
    {
      "epoch": 4.804,
      "grad_norm": 0.14700107276439667,
      "learning_rate": 8.055133771652345e-07,
      "loss": 1.9564,
      "step": 1201
    },
    {
      "epoch": 4.808,
      "grad_norm": 0.14160555601119995,
      "learning_rate": 7.730127636723539e-07,
      "loss": 1.5092,
      "step": 1202
    },
    {
      "epoch": 4.812,
      "grad_norm": 0.13559886813163757,
      "learning_rate": 7.411788403743237e-07,
      "loss": 1.9578,
      "step": 1203
    },
    {
      "epoch": 4.816,
      "grad_norm": 0.14456912875175476,
      "learning_rate": 7.100118211581852e-07,
      "loss": 1.5119,
      "step": 1204
    },
    {
      "epoch": 4.82,
      "grad_norm": 0.13705547153949738,
      "learning_rate": 6.7951191543012e-07,
      "loss": 1.6808,
      "step": 1205
    },
    {
      "epoch": 4.824,
      "grad_norm": 0.16058595478534698,
      "learning_rate": 6.496793281141056e-07,
      "loss": 1.9024,
      "step": 1206
    },
    {
      "epoch": 4.828,
      "grad_norm": 0.12898442149162292,
      "learning_rate": 6.205142596505176e-07,
      "loss": 1.6942,
      "step": 1207
    },
    {
      "epoch": 4.832,
      "grad_norm": 0.1221541091799736,
      "learning_rate": 5.920169059947411e-07,
      "loss": 1.8842,
      "step": 1208
    },
    {
      "epoch": 4.836,
      "grad_norm": 0.39227160811424255,
      "learning_rate": 5.64187458615939e-07,
      "loss": 1.8103,
      "step": 1209
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.1579180210828781,
      "learning_rate": 5.370261044956971e-07,
      "loss": 1.7338,
      "step": 1210
    },
    {
      "epoch": 4.844,
      "grad_norm": 0.16653914749622345,
      "learning_rate": 5.105330261267916e-07,
      "loss": 1.7341,
      "step": 1211
    },
    {
      "epoch": 4.848,
      "grad_norm": 0.13848458230495453,
      "learning_rate": 4.847084015119574e-07,
      "loss": 1.6821,
      "step": 1212
    },
    {
      "epoch": 4.852,
      "grad_norm": 0.13323792815208435,
      "learning_rate": 4.5955240416271084e-07,
      "loss": 1.7004,
      "step": 1213
    },
    {
      "epoch": 4.856,
      "grad_norm": 0.20529811084270477,
      "learning_rate": 4.3506520309813947e-07,
      "loss": 1.8016,
      "step": 1214
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.17215237021446228,
      "learning_rate": 4.112469628438365e-07,
      "loss": 1.9427,
      "step": 1215
    },
    {
      "epoch": 4.864,
      "grad_norm": 0.21023520827293396,
      "learning_rate": 3.8809784343072366e-07,
      "loss": 2.0626,
      "step": 1216
    },
    {
      "epoch": 4.868,
      "grad_norm": 0.14112381637096405,
      "learning_rate": 3.6561800039403016e-07,
      "loss": 1.6304,
      "step": 1217
    },
    {
      "epoch": 4.872,
      "grad_norm": 0.16995862126350403,
      "learning_rate": 3.4380758477219333e-07,
      "loss": 1.718,
      "step": 1218
    },
    {
      "epoch": 4.876,
      "grad_norm": 0.16298572719097137,
      "learning_rate": 3.2266674310589273e-07,
      "loss": 1.6037,
      "step": 1219
    },
    {
      "epoch": 4.88,
      "grad_norm": 0.13825032114982605,
      "learning_rate": 3.0219561743707326e-07,
      "loss": 1.9258,
      "step": 1220
    },
    {
      "epoch": 4.884,
      "grad_norm": 0.1567644476890564,
      "learning_rate": 2.8239434530792365e-07,
      "loss": 1.6793,
      "step": 1221
    },
    {
      "epoch": 4.888,
      "grad_norm": 0.1791917085647583,
      "learning_rate": 2.6326305976001055e-07,
      "loss": 1.726,
      "step": 1222
    },
    {
      "epoch": 4.892,
      "grad_norm": 0.13811874389648438,
      "learning_rate": 2.448018893333681e-07,
      "loss": 1.7662,
      "step": 1223
    },
    {
      "epoch": 4.896,
      "grad_norm": 0.16864252090454102,
      "learning_rate": 2.2701095806565432e-07,
      "loss": 1.6056,
      "step": 1224
    },
    {
      "epoch": 4.9,
      "grad_norm": 0.11749818921089172,
      "learning_rate": 2.098903854912515e-07,
      "loss": 1.6608,
      "step": 1225
    },
    {
      "epoch": 4.904,
      "grad_norm": 0.1611095368862152,
      "learning_rate": 1.9344028664056713e-07,
      "loss": 1.9917,
      "step": 1226
    },
    {
      "epoch": 4.908,
      "grad_norm": 0.1539628803730011,
      "learning_rate": 1.7766077203915655e-07,
      "loss": 1.7966,
      "step": 1227
    },
    {
      "epoch": 4.912,
      "grad_norm": 0.13494712114334106,
      "learning_rate": 1.6255194770704586e-07,
      "loss": 1.6277,
      "step": 1228
    },
    {
      "epoch": 4.916,
      "grad_norm": 0.1389809101819992,
      "learning_rate": 1.481139151579991e-07,
      "loss": 1.9931,
      "step": 1229
    },
    {
      "epoch": 4.92,
      "grad_norm": 0.16226287186145782,
      "learning_rate": 1.3434677139885222e-07,
      "loss": 1.7976,
      "step": 1230
    },
    {
      "epoch": 4.924,
      "grad_norm": 0.1586417257785797,
      "learning_rate": 1.2125060892881346e-07,
      "loss": 1.7906,
      "step": 1231
    },
    {
      "epoch": 4.928,
      "grad_norm": 0.15262752771377563,
      "learning_rate": 1.0882551573891953e-07,
      "loss": 1.4412,
      "step": 1232
    },
    {
      "epoch": 4.932,
      "grad_norm": 0.16330096125602722,
      "learning_rate": 9.707157531134713e-08,
      "loss": 1.5819,
      "step": 1233
    },
    {
      "epoch": 4.936,
      "grad_norm": 0.14067837595939636,
      "learning_rate": 8.598886661895788e-08,
      "loss": 1.6302,
      "step": 1234
    },
    {
      "epoch": 4.9399999999999995,
      "grad_norm": 0.18124538660049438,
      "learning_rate": 7.557746412468758e-08,
      "loss": 1.7177,
      "step": 1235
    },
    {
      "epoch": 4.944,
      "grad_norm": 0.1256203055381775,
      "learning_rate": 6.583743778106887e-08,
      "loss": 1.8642,
      "step": 1236
    },
    {
      "epoch": 4.948,
      "grad_norm": 0.1754581481218338,
      "learning_rate": 5.6768853029787184e-08,
      "loss": 1.8182,
      "step": 1237
    },
    {
      "epoch": 4.952,
      "grad_norm": 0.12627682089805603,
      "learning_rate": 4.837177080119215e-08,
      "loss": 1.5912,
      "step": 1238
    },
    {
      "epoch": 4.9559999999999995,
      "grad_norm": 0.11430428922176361,
      "learning_rate": 4.064624751394242e-08,
      "loss": 1.8501,
      "step": 1239
    },
    {
      "epoch": 4.96,
      "grad_norm": 0.1414104551076889,
      "learning_rate": 3.359233507459481e-08,
      "loss": 1.8551,
      "step": 1240
    },
    {
      "epoch": 4.964,
      "grad_norm": 0.141187384724617,
      "learning_rate": 2.7210080877237976e-08,
      "loss": 1.7721,
      "step": 1241
    },
    {
      "epoch": 4.968,
      "grad_norm": 0.2245040386915207,
      "learning_rate": 2.1499527803214846e-08,
      "loss": 1.8562,
      "step": 1242
    },
    {
      "epoch": 4.9719999999999995,
      "grad_norm": 0.14019371569156647,
      "learning_rate": 1.646071422083395e-08,
      "loss": 1.8547,
      "step": 1243
    },
    {
      "epoch": 4.976,
      "grad_norm": 0.14556215703487396,
      "learning_rate": 1.209367398504746e-08,
      "loss": 1.7208,
      "step": 1244
    },
    {
      "epoch": 4.98,
      "grad_norm": 0.13618595898151398,
      "learning_rate": 8.398436437317969e-09,
      "loss": 1.497,
      "step": 1245
    },
    {
      "epoch": 4.984,
      "grad_norm": 0.14591217041015625,
      "learning_rate": 5.375026405352035e-09,
      "loss": 2.0931,
      "step": 1246
    },
    {
      "epoch": 4.9879999999999995,
      "grad_norm": 0.11961893737316132,
      "learning_rate": 3.023464202944748e-09,
      "loss": 1.96,
      "step": 1247
    },
    {
      "epoch": 4.992,
      "grad_norm": 0.357136607170105,
      "learning_rate": 1.3437656298687097e-09,
      "loss": 2.232,
      "step": 1248
    },
    {
      "epoch": 4.996,
      "grad_norm": 0.2860781252384186,
      "learning_rate": 3.3594197175190745e-10,
      "loss": 2.0735,
      "step": 1249
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.15874364972114563,
      "learning_rate": 0.0,
      "loss": 1.4251,
      "step": 1250
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.8082343339920044,
      "eval_runtime": 23.7879,
      "eval_samples_per_second": 21.019,
      "eval_steps_per_second": 2.648,
      "step": 1250
    }
  ],
  "logging_steps": 1,
  "max_steps": 1250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 250,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.0882524867690496e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
