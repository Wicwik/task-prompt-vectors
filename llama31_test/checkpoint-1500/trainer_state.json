{
  "best_metric": 1.1484078168869019,
  "best_model_checkpoint": "llama31_test/checkpoint-1500",
  "epoch": 2.0,
  "eval_steps": 1500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0013333333333333333,
      "grad_norm": 0.3555504083633423,
      "learning_rate": 8.888888888888888e-06,
      "loss": 6.939,
      "step": 1
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 0.2820369005203247,
      "learning_rate": 1.7777777777777777e-05,
      "loss": 7.1212,
      "step": 2
    },
    {
      "epoch": 0.004,
      "grad_norm": 0.3040475845336914,
      "learning_rate": 2.666666666666667e-05,
      "loss": 7.2469,
      "step": 3
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 0.3130744695663452,
      "learning_rate": 3.555555555555555e-05,
      "loss": 7.1404,
      "step": 4
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 0.3134247660636902,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 7.2064,
      "step": 5
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.33323732018470764,
      "learning_rate": 5.333333333333334e-05,
      "loss": 6.659,
      "step": 6
    },
    {
      "epoch": 0.009333333333333334,
      "grad_norm": 0.3007800579071045,
      "learning_rate": 6.222222222222222e-05,
      "loss": 6.9849,
      "step": 7
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.2976188063621521,
      "learning_rate": 7.11111111111111e-05,
      "loss": 7.0471,
      "step": 8
    },
    {
      "epoch": 0.012,
      "grad_norm": 0.32053157687187195,
      "learning_rate": 8e-05,
      "loss": 6.9614,
      "step": 9
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.328104704618454,
      "learning_rate": 8.888888888888889e-05,
      "loss": 6.9047,
      "step": 10
    },
    {
      "epoch": 0.014666666666666666,
      "grad_norm": 0.3572377562522888,
      "learning_rate": 9.777777777777778e-05,
      "loss": 7.0393,
      "step": 11
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.3350681662559509,
      "learning_rate": 0.00010666666666666668,
      "loss": 6.9802,
      "step": 12
    },
    {
      "epoch": 0.017333333333333333,
      "grad_norm": 0.3303508162498474,
      "learning_rate": 0.00011555555555555555,
      "loss": 6.9706,
      "step": 13
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.32809674739837646,
      "learning_rate": 0.00012444444444444444,
      "loss": 6.9325,
      "step": 14
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.3702910840511322,
      "learning_rate": 0.00013333333333333334,
      "loss": 6.7135,
      "step": 15
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.32899269461631775,
      "learning_rate": 0.0001422222222222222,
      "loss": 6.8394,
      "step": 16
    },
    {
      "epoch": 0.02266666666666667,
      "grad_norm": 0.30374035239219666,
      "learning_rate": 0.0001511111111111111,
      "loss": 6.8322,
      "step": 17
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.366574764251709,
      "learning_rate": 0.00016,
      "loss": 6.7342,
      "step": 18
    },
    {
      "epoch": 0.025333333333333333,
      "grad_norm": 0.34597158432006836,
      "learning_rate": 0.00016888888888888889,
      "loss": 7.1006,
      "step": 19
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.3574230968952179,
      "learning_rate": 0.00017777777777777779,
      "loss": 7.0246,
      "step": 20
    },
    {
      "epoch": 0.028,
      "grad_norm": 0.32099682092666626,
      "learning_rate": 0.0001866666666666667,
      "loss": 6.9404,
      "step": 21
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 0.3596108555793762,
      "learning_rate": 0.00019555555555555556,
      "loss": 6.9141,
      "step": 22
    },
    {
      "epoch": 0.030666666666666665,
      "grad_norm": 0.323851078748703,
      "learning_rate": 0.00020444444444444446,
      "loss": 6.7382,
      "step": 23
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.290274977684021,
      "learning_rate": 0.00021333333333333336,
      "loss": 6.8497,
      "step": 24
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.3392907679080963,
      "learning_rate": 0.0002222222222222222,
      "loss": 6.7182,
      "step": 25
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 0.3558824360370636,
      "learning_rate": 0.0002311111111111111,
      "loss": 6.5068,
      "step": 26
    },
    {
      "epoch": 0.036,
      "grad_norm": 0.3710341453552246,
      "learning_rate": 0.00024,
      "loss": 6.4784,
      "step": 27
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 0.3727741241455078,
      "learning_rate": 0.0002488888888888889,
      "loss": 6.5286,
      "step": 28
    },
    {
      "epoch": 0.03866666666666667,
      "grad_norm": 0.3603370189666748,
      "learning_rate": 0.0002577777777777778,
      "loss": 6.6161,
      "step": 29
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.38295966386795044,
      "learning_rate": 0.0002666666666666667,
      "loss": 6.5721,
      "step": 30
    },
    {
      "epoch": 0.04133333333333333,
      "grad_norm": 0.41009706258773804,
      "learning_rate": 0.0002755555555555556,
      "loss": 6.6032,
      "step": 31
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.30817705392837524,
      "learning_rate": 0.0002844444444444444,
      "loss": 6.6088,
      "step": 32
    },
    {
      "epoch": 0.044,
      "grad_norm": 0.4053432047367096,
      "learning_rate": 0.0002933333333333333,
      "loss": 6.3758,
      "step": 33
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 0.3585493266582489,
      "learning_rate": 0.0003022222222222222,
      "loss": 6.5408,
      "step": 34
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.3294336497783661,
      "learning_rate": 0.0003111111111111111,
      "loss": 6.4788,
      "step": 35
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.38775748014450073,
      "learning_rate": 0.00032,
      "loss": 5.8985,
      "step": 36
    },
    {
      "epoch": 0.04933333333333333,
      "grad_norm": 0.5096060037612915,
      "learning_rate": 0.0003288888888888889,
      "loss": 5.8477,
      "step": 37
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.30484312772750854,
      "learning_rate": 0.00033777777777777777,
      "loss": 6.2385,
      "step": 38
    },
    {
      "epoch": 0.052,
      "grad_norm": 0.36814066767692566,
      "learning_rate": 0.00034666666666666667,
      "loss": 6.1029,
      "step": 39
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.35196802020072937,
      "learning_rate": 0.00035555555555555557,
      "loss": 6.3254,
      "step": 40
    },
    {
      "epoch": 0.05466666666666667,
      "grad_norm": 0.36300069093704224,
      "learning_rate": 0.00036444444444444447,
      "loss": 5.6745,
      "step": 41
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.3681320250034332,
      "learning_rate": 0.0003733333333333334,
      "loss": 5.7915,
      "step": 42
    },
    {
      "epoch": 0.05733333333333333,
      "grad_norm": 0.40314555168151855,
      "learning_rate": 0.0003822222222222223,
      "loss": 5.9162,
      "step": 43
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.39633047580718994,
      "learning_rate": 0.0003911111111111111,
      "loss": 5.7487,
      "step": 44
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.39122119545936584,
      "learning_rate": 0.0004,
      "loss": 6.0053,
      "step": 45
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.3738775849342346,
      "learning_rate": 0.0004088888888888889,
      "loss": 5.8295,
      "step": 46
    },
    {
      "epoch": 0.06266666666666666,
      "grad_norm": 0.4502626359462738,
      "learning_rate": 0.0004177777777777778,
      "loss": 5.602,
      "step": 47
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.41406819224357605,
      "learning_rate": 0.0004266666666666667,
      "loss": 5.5885,
      "step": 48
    },
    {
      "epoch": 0.06533333333333333,
      "grad_norm": 0.3728400766849518,
      "learning_rate": 0.0004355555555555555,
      "loss": 5.3816,
      "step": 49
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.43621519207954407,
      "learning_rate": 0.0004444444444444444,
      "loss": 5.5194,
      "step": 50
    },
    {
      "epoch": 0.068,
      "grad_norm": 0.40215903520584106,
      "learning_rate": 0.0004533333333333333,
      "loss": 5.3399,
      "step": 51
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.35502320528030396,
      "learning_rate": 0.0004622222222222222,
      "loss": 5.5855,
      "step": 52
    },
    {
      "epoch": 0.07066666666666667,
      "grad_norm": 0.3777894079685211,
      "learning_rate": 0.0004711111111111111,
      "loss": 5.02,
      "step": 53
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.40208253264427185,
      "learning_rate": 0.00048,
      "loss": 5.239,
      "step": 54
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.4758330285549164,
      "learning_rate": 0.0004888888888888889,
      "loss": 4.995,
      "step": 55
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.4006824195384979,
      "learning_rate": 0.0004977777777777778,
      "loss": 4.9496,
      "step": 56
    },
    {
      "epoch": 0.076,
      "grad_norm": 0.34312161803245544,
      "learning_rate": 0.0005066666666666668,
      "loss": 4.6876,
      "step": 57
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.390020489692688,
      "learning_rate": 0.0005155555555555556,
      "loss": 4.8512,
      "step": 58
    },
    {
      "epoch": 0.07866666666666666,
      "grad_norm": 0.3633885085582733,
      "learning_rate": 0.0005244444444444445,
      "loss": 5.0354,
      "step": 59
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.33825743198394775,
      "learning_rate": 0.0005333333333333334,
      "loss": 5.0089,
      "step": 60
    },
    {
      "epoch": 0.08133333333333333,
      "grad_norm": 0.27596643567085266,
      "learning_rate": 0.0005422222222222223,
      "loss": 4.5417,
      "step": 61
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.2744942903518677,
      "learning_rate": 0.0005511111111111112,
      "loss": 4.5155,
      "step": 62
    },
    {
      "epoch": 0.084,
      "grad_norm": 0.2609971761703491,
      "learning_rate": 0.0005600000000000001,
      "loss": 4.3623,
      "step": 63
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.25846701860427856,
      "learning_rate": 0.0005688888888888889,
      "loss": 4.539,
      "step": 64
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.24574969708919525,
      "learning_rate": 0.0005777777777777778,
      "loss": 4.2871,
      "step": 65
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.2544957995414734,
      "learning_rate": 0.0005866666666666667,
      "loss": 4.4581,
      "step": 66
    },
    {
      "epoch": 0.08933333333333333,
      "grad_norm": 0.2602943480014801,
      "learning_rate": 0.0005955555555555556,
      "loss": 4.2923,
      "step": 67
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.21910735964775085,
      "learning_rate": 0.0006044444444444445,
      "loss": 4.001,
      "step": 68
    },
    {
      "epoch": 0.092,
      "grad_norm": 0.2860652506351471,
      "learning_rate": 0.0006133333333333334,
      "loss": 4.2471,
      "step": 69
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.23105306923389435,
      "learning_rate": 0.0006222222222222223,
      "loss": 4.1282,
      "step": 70
    },
    {
      "epoch": 0.09466666666666666,
      "grad_norm": 0.2389366626739502,
      "learning_rate": 0.000631111111111111,
      "loss": 3.8047,
      "step": 71
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.21928375959396362,
      "learning_rate": 0.00064,
      "loss": 4.0391,
      "step": 72
    },
    {
      "epoch": 0.09733333333333333,
      "grad_norm": 0.24404151737689972,
      "learning_rate": 0.0006488888888888888,
      "loss": 3.9932,
      "step": 73
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.2796590328216553,
      "learning_rate": 0.0006577777777777779,
      "loss": 4.1413,
      "step": 74
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.29691430926322937,
      "learning_rate": 0.0006666666666666666,
      "loss": 4.0533,
      "step": 75
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.2289622575044632,
      "learning_rate": 0.0006755555555555555,
      "loss": 3.9604,
      "step": 76
    },
    {
      "epoch": 0.10266666666666667,
      "grad_norm": 0.25216829776763916,
      "learning_rate": 0.0006844444444444444,
      "loss": 4.0024,
      "step": 77
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.18387772142887115,
      "learning_rate": 0.0006933333333333333,
      "loss": 3.7311,
      "step": 78
    },
    {
      "epoch": 0.10533333333333333,
      "grad_norm": 0.2159779965877533,
      "learning_rate": 0.0007022222222222222,
      "loss": 4.0338,
      "step": 79
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.23165352642536163,
      "learning_rate": 0.0007111111111111111,
      "loss": 3.6967,
      "step": 80
    },
    {
      "epoch": 0.108,
      "grad_norm": 0.25378698110580444,
      "learning_rate": 0.0007199999999999999,
      "loss": 3.5939,
      "step": 81
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.20034092664718628,
      "learning_rate": 0.0007288888888888889,
      "loss": 3.8341,
      "step": 82
    },
    {
      "epoch": 0.11066666666666666,
      "grad_norm": 0.17457611858844757,
      "learning_rate": 0.0007377777777777777,
      "loss": 3.5018,
      "step": 83
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.2083585560321808,
      "learning_rate": 0.0007466666666666667,
      "loss": 3.81,
      "step": 84
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.2119496464729309,
      "learning_rate": 0.0007555555555555555,
      "loss": 3.8609,
      "step": 85
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.22008116543293,
      "learning_rate": 0.0007644444444444445,
      "loss": 3.6746,
      "step": 86
    },
    {
      "epoch": 0.116,
      "grad_norm": 0.21630693972110748,
      "learning_rate": 0.0007733333333333333,
      "loss": 3.5259,
      "step": 87
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.18918000161647797,
      "learning_rate": 0.0007822222222222222,
      "loss": 3.417,
      "step": 88
    },
    {
      "epoch": 0.11866666666666667,
      "grad_norm": 0.24622909724712372,
      "learning_rate": 0.0007911111111111111,
      "loss": 3.504,
      "step": 89
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.2659607231616974,
      "learning_rate": 0.0008,
      "loss": 3.6211,
      "step": 90
    },
    {
      "epoch": 0.12133333333333333,
      "grad_norm": 0.29008498787879944,
      "learning_rate": 0.0008088888888888889,
      "loss": 3.4069,
      "step": 91
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.2224079668521881,
      "learning_rate": 0.0008177777777777778,
      "loss": 3.2265,
      "step": 92
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.19913670420646667,
      "learning_rate": 0.0008266666666666666,
      "loss": 3.1952,
      "step": 93
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.2076563686132431,
      "learning_rate": 0.0008355555555555556,
      "loss": 3.2571,
      "step": 94
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.1831824779510498,
      "learning_rate": 0.0008444444444444444,
      "loss": 3.2619,
      "step": 95
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.23425628244876862,
      "learning_rate": 0.0008533333333333334,
      "loss": 3.0934,
      "step": 96
    },
    {
      "epoch": 0.12933333333333333,
      "grad_norm": 0.25713077187538147,
      "learning_rate": 0.0008622222222222222,
      "loss": 3.4453,
      "step": 97
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.19652040302753448,
      "learning_rate": 0.000871111111111111,
      "loss": 3.0896,
      "step": 98
    },
    {
      "epoch": 0.132,
      "grad_norm": 0.1827600747346878,
      "learning_rate": 0.00088,
      "loss": 2.8709,
      "step": 99
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.20682170987129211,
      "learning_rate": 0.0008888888888888888,
      "loss": 3.0675,
      "step": 100
    },
    {
      "epoch": 0.13466666666666666,
      "grad_norm": 0.1917726695537567,
      "learning_rate": 0.0008977777777777778,
      "loss": 3.0282,
      "step": 101
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.18286015093326569,
      "learning_rate": 0.0009066666666666666,
      "loss": 3.3707,
      "step": 102
    },
    {
      "epoch": 0.13733333333333334,
      "grad_norm": 0.18966540694236755,
      "learning_rate": 0.0009155555555555556,
      "loss": 3.1299,
      "step": 103
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.18103714287281036,
      "learning_rate": 0.0009244444444444444,
      "loss": 2.9937,
      "step": 104
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.17885956168174744,
      "learning_rate": 0.0009333333333333333,
      "loss": 3.2555,
      "step": 105
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.35943517088890076,
      "learning_rate": 0.0009422222222222222,
      "loss": 3.0403,
      "step": 106
    },
    {
      "epoch": 0.14266666666666666,
      "grad_norm": 0.20058420300483704,
      "learning_rate": 0.0009511111111111111,
      "loss": 2.9642,
      "step": 107
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.2611333727836609,
      "learning_rate": 0.00096,
      "loss": 3.096,
      "step": 108
    },
    {
      "epoch": 0.14533333333333334,
      "grad_norm": 0.3253711760044098,
      "learning_rate": 0.0009688888888888889,
      "loss": 3.0214,
      "step": 109
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.2833825945854187,
      "learning_rate": 0.0009777777777777777,
      "loss": 3.0148,
      "step": 110
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.15768861770629883,
      "learning_rate": 0.0009866666666666667,
      "loss": 3.2658,
      "step": 111
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.19665218889713287,
      "learning_rate": 0.0009955555555555555,
      "loss": 2.9948,
      "step": 112
    },
    {
      "epoch": 0.15066666666666667,
      "grad_norm": 0.22823019325733185,
      "learning_rate": 0.0010044444444444445,
      "loss": 2.7265,
      "step": 113
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.15567077696323395,
      "learning_rate": 0.0010133333333333335,
      "loss": 2.8005,
      "step": 114
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.2153978943824768,
      "learning_rate": 0.0010222222222222221,
      "loss": 2.7751,
      "step": 115
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.43959227204322815,
      "learning_rate": 0.0010311111111111111,
      "loss": 2.7895,
      "step": 116
    },
    {
      "epoch": 0.156,
      "grad_norm": 0.19013641774654388,
      "learning_rate": 0.0010400000000000001,
      "loss": 2.774,
      "step": 117
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.21319551765918732,
      "learning_rate": 0.001048888888888889,
      "loss": 2.9346,
      "step": 118
    },
    {
      "epoch": 0.15866666666666668,
      "grad_norm": 0.3194159269332886,
      "learning_rate": 0.0010577777777777777,
      "loss": 2.753,
      "step": 119
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.24583186209201813,
      "learning_rate": 0.0010666666666666667,
      "loss": 3.0352,
      "step": 120
    },
    {
      "epoch": 0.16133333333333333,
      "grad_norm": 0.2755632996559143,
      "learning_rate": 0.0010755555555555557,
      "loss": 2.6773,
      "step": 121
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.1570361852645874,
      "learning_rate": 0.0010844444444444445,
      "loss": 2.895,
      "step": 122
    },
    {
      "epoch": 0.164,
      "grad_norm": 0.31439799070358276,
      "learning_rate": 0.0010933333333333333,
      "loss": 2.8218,
      "step": 123
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.1796349436044693,
      "learning_rate": 0.0011022222222222223,
      "loss": 2.6005,
      "step": 124
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.1848248541355133,
      "learning_rate": 0.0011111111111111111,
      "loss": 2.715,
      "step": 125
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.19730344414710999,
      "learning_rate": 0.0011200000000000001,
      "loss": 2.7502,
      "step": 126
    },
    {
      "epoch": 0.16933333333333334,
      "grad_norm": 0.23708689212799072,
      "learning_rate": 0.001128888888888889,
      "loss": 2.4887,
      "step": 127
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.1518843024969101,
      "learning_rate": 0.0011377777777777777,
      "loss": 2.9067,
      "step": 128
    },
    {
      "epoch": 0.172,
      "grad_norm": 0.23372532427310944,
      "learning_rate": 0.0011466666666666667,
      "loss": 2.5531,
      "step": 129
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.19489368796348572,
      "learning_rate": 0.0011555555555555555,
      "loss": 2.5173,
      "step": 130
    },
    {
      "epoch": 0.17466666666666666,
      "grad_norm": 0.17375832796096802,
      "learning_rate": 0.0011644444444444445,
      "loss": 2.7289,
      "step": 131
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.17223307490348816,
      "learning_rate": 0.0011733333333333333,
      "loss": 2.3688,
      "step": 132
    },
    {
      "epoch": 0.17733333333333334,
      "grad_norm": 0.1609000563621521,
      "learning_rate": 0.0011822222222222223,
      "loss": 2.6588,
      "step": 133
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.2193676382303238,
      "learning_rate": 0.001191111111111111,
      "loss": 2.407,
      "step": 134
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.2354118674993515,
      "learning_rate": 0.0012,
      "loss": 2.5228,
      "step": 135
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.2277151644229889,
      "learning_rate": 0.001208888888888889,
      "loss": 2.5359,
      "step": 136
    },
    {
      "epoch": 0.18266666666666667,
      "grad_norm": 0.2764875888824463,
      "learning_rate": 0.001217777777777778,
      "loss": 2.8204,
      "step": 137
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.18017089366912842,
      "learning_rate": 0.0012266666666666667,
      "loss": 2.3479,
      "step": 138
    },
    {
      "epoch": 0.18533333333333332,
      "grad_norm": 0.2732390761375427,
      "learning_rate": 0.0012355555555555555,
      "loss": 2.6582,
      "step": 139
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.24082329869270325,
      "learning_rate": 0.0012444444444444445,
      "loss": 2.1702,
      "step": 140
    },
    {
      "epoch": 0.188,
      "grad_norm": 0.23642857372760773,
      "learning_rate": 0.0012533333333333335,
      "loss": 2.6012,
      "step": 141
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.22302553057670593,
      "learning_rate": 0.001262222222222222,
      "loss": 2.5584,
      "step": 142
    },
    {
      "epoch": 0.19066666666666668,
      "grad_norm": 0.21631298959255219,
      "learning_rate": 0.001271111111111111,
      "loss": 2.4059,
      "step": 143
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.23782074451446533,
      "learning_rate": 0.00128,
      "loss": 2.4692,
      "step": 144
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.1809307038784027,
      "learning_rate": 0.001288888888888889,
      "loss": 2.442,
      "step": 145
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.22575511038303375,
      "learning_rate": 0.0012977777777777777,
      "loss": 2.3062,
      "step": 146
    },
    {
      "epoch": 0.196,
      "grad_norm": 0.22840678691864014,
      "learning_rate": 0.0013066666666666667,
      "loss": 2.4491,
      "step": 147
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.2556595504283905,
      "learning_rate": 0.0013155555555555557,
      "loss": 2.309,
      "step": 148
    },
    {
      "epoch": 0.19866666666666666,
      "grad_norm": 0.25059255957603455,
      "learning_rate": 0.0013244444444444445,
      "loss": 2.2907,
      "step": 149
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.2540203332901001,
      "learning_rate": 0.0013333333333333333,
      "loss": 2.4229,
      "step": 150
    },
    {
      "epoch": 0.20133333333333334,
      "grad_norm": 0.3189737796783447,
      "learning_rate": 0.0013422222222222223,
      "loss": 2.3595,
      "step": 151
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.2814345061779022,
      "learning_rate": 0.001351111111111111,
      "loss": 2.3265,
      "step": 152
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.23742927610874176,
      "learning_rate": 0.00136,
      "loss": 2.2254,
      "step": 153
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.4587046205997467,
      "learning_rate": 0.0013688888888888889,
      "loss": 2.1579,
      "step": 154
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.30864301323890686,
      "learning_rate": 0.001377777777777778,
      "loss": 2.0703,
      "step": 155
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.22583816945552826,
      "learning_rate": 0.0013866666666666667,
      "loss": 2.3132,
      "step": 156
    },
    {
      "epoch": 0.20933333333333334,
      "grad_norm": 0.22097212076187134,
      "learning_rate": 0.0013955555555555557,
      "loss": 2.3189,
      "step": 157
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.5596842765808105,
      "learning_rate": 0.0014044444444444445,
      "loss": 2.3481,
      "step": 158
    },
    {
      "epoch": 0.212,
      "grad_norm": 0.24860230088233948,
      "learning_rate": 0.0014133333333333333,
      "loss": 2.2695,
      "step": 159
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.5702952742576599,
      "learning_rate": 0.0014222222222222223,
      "loss": 2.2506,
      "step": 160
    },
    {
      "epoch": 0.21466666666666667,
      "grad_norm": 0.2053312063217163,
      "learning_rate": 0.001431111111111111,
      "loss": 2.2972,
      "step": 161
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.26408371329307556,
      "learning_rate": 0.0014399999999999999,
      "loss": 2.1839,
      "step": 162
    },
    {
      "epoch": 0.21733333333333332,
      "grad_norm": 0.21351370215415955,
      "learning_rate": 0.0014488888888888889,
      "loss": 2.1841,
      "step": 163
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.25986039638519287,
      "learning_rate": 0.0014577777777777779,
      "loss": 1.887,
      "step": 164
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.2630249559879303,
      "learning_rate": 0.0014666666666666667,
      "loss": 2.1449,
      "step": 165
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.2603936493396759,
      "learning_rate": 0.0014755555555555555,
      "loss": 1.8729,
      "step": 166
    },
    {
      "epoch": 0.22266666666666668,
      "grad_norm": 0.22913362085819244,
      "learning_rate": 0.0014844444444444445,
      "loss": 2.0419,
      "step": 167
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.2890835106372833,
      "learning_rate": 0.0014933333333333335,
      "loss": 2.0237,
      "step": 168
    },
    {
      "epoch": 0.22533333333333333,
      "grad_norm": 0.24949640035629272,
      "learning_rate": 0.001502222222222222,
      "loss": 2.1428,
      "step": 169
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.2959970235824585,
      "learning_rate": 0.001511111111111111,
      "loss": 2.3783,
      "step": 170
    },
    {
      "epoch": 0.228,
      "grad_norm": 0.3019074499607086,
      "learning_rate": 0.00152,
      "loss": 2.5129,
      "step": 171
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.37460583448410034,
      "learning_rate": 0.001528888888888889,
      "loss": 1.7151,
      "step": 172
    },
    {
      "epoch": 0.23066666666666666,
      "grad_norm": 0.3367548882961273,
      "learning_rate": 0.0015377777777777777,
      "loss": 1.77,
      "step": 173
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.27657562494277954,
      "learning_rate": 0.0015466666666666667,
      "loss": 2.1734,
      "step": 174
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.25605666637420654,
      "learning_rate": 0.0015555555555555557,
      "loss": 1.8297,
      "step": 175
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.32427042722702026,
      "learning_rate": 0.0015644444444444445,
      "loss": 1.9176,
      "step": 176
    },
    {
      "epoch": 0.236,
      "grad_norm": 0.30497390031814575,
      "learning_rate": 0.0015733333333333333,
      "loss": 1.9386,
      "step": 177
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.24644477665424347,
      "learning_rate": 0.0015822222222222223,
      "loss": 1.7461,
      "step": 178
    },
    {
      "epoch": 0.23866666666666667,
      "grad_norm": 0.26502078771591187,
      "learning_rate": 0.001591111111111111,
      "loss": 1.8744,
      "step": 179
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.4013954699039459,
      "learning_rate": 0.0016,
      "loss": 1.7072,
      "step": 180
    },
    {
      "epoch": 0.24133333333333334,
      "grad_norm": 0.26181158423423767,
      "learning_rate": 0.0016088888888888889,
      "loss": 1.9893,
      "step": 181
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.38677167892456055,
      "learning_rate": 0.0016177777777777779,
      "loss": 1.7762,
      "step": 182
    },
    {
      "epoch": 0.244,
      "grad_norm": 0.2490021288394928,
      "learning_rate": 0.0016266666666666667,
      "loss": 2.2498,
      "step": 183
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.3483845293521881,
      "learning_rate": 0.0016355555555555557,
      "loss": 1.7619,
      "step": 184
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.21299581229686737,
      "learning_rate": 0.0016444444444444445,
      "loss": 1.8468,
      "step": 185
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.2674828767776489,
      "learning_rate": 0.0016533333333333333,
      "loss": 2.0769,
      "step": 186
    },
    {
      "epoch": 0.24933333333333332,
      "grad_norm": 0.18275614082813263,
      "learning_rate": 0.0016622222222222223,
      "loss": 1.816,
      "step": 187
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.357870489358902,
      "learning_rate": 0.0016711111111111113,
      "loss": 1.9828,
      "step": 188
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.22154709696769714,
      "learning_rate": 0.00168,
      "loss": 1.327,
      "step": 189
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.1553424596786499,
      "learning_rate": 0.0016888888888888889,
      "loss": 1.7697,
      "step": 190
    },
    {
      "epoch": 0.25466666666666665,
      "grad_norm": 0.35465845465660095,
      "learning_rate": 0.0016977777777777779,
      "loss": 2.1192,
      "step": 191
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.16351741552352905,
      "learning_rate": 0.0017066666666666669,
      "loss": 1.5972,
      "step": 192
    },
    {
      "epoch": 0.25733333333333336,
      "grad_norm": 0.16911204159259796,
      "learning_rate": 0.0017155555555555555,
      "loss": 1.4392,
      "step": 193
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 0.3353917896747589,
      "learning_rate": 0.0017244444444444445,
      "loss": 1.453,
      "step": 194
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.18812192976474762,
      "learning_rate": 0.0017333333333333335,
      "loss": 1.6527,
      "step": 195
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.13612323999404907,
      "learning_rate": 0.001742222222222222,
      "loss": 1.7609,
      "step": 196
    },
    {
      "epoch": 0.26266666666666666,
      "grad_norm": 0.18409249186515808,
      "learning_rate": 0.001751111111111111,
      "loss": 1.731,
      "step": 197
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.20915021002292633,
      "learning_rate": 0.00176,
      "loss": 1.3184,
      "step": 198
    },
    {
      "epoch": 0.2653333333333333,
      "grad_norm": 0.14540532231330872,
      "learning_rate": 0.001768888888888889,
      "loss": 1.7068,
      "step": 199
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.1447267383337021,
      "learning_rate": 0.0017777777777777776,
      "loss": 1.4373,
      "step": 200
    },
    {
      "epoch": 0.268,
      "grad_norm": 0.15977557003498077,
      "learning_rate": 0.0017866666666666667,
      "loss": 1.5344,
      "step": 201
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.23101578652858734,
      "learning_rate": 0.0017955555555555557,
      "loss": 1.8091,
      "step": 202
    },
    {
      "epoch": 0.27066666666666667,
      "grad_norm": 0.11719998717308044,
      "learning_rate": 0.0018044444444444445,
      "loss": 1.7493,
      "step": 203
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.14386355876922607,
      "learning_rate": 0.0018133333333333332,
      "loss": 1.6227,
      "step": 204
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.29026785492897034,
      "learning_rate": 0.0018222222222222223,
      "loss": 1.7728,
      "step": 205
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.12349116802215576,
      "learning_rate": 0.0018311111111111113,
      "loss": 1.5605,
      "step": 206
    },
    {
      "epoch": 0.276,
      "grad_norm": 0.2713688611984253,
      "learning_rate": 0.00184,
      "loss": 1.6615,
      "step": 207
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.14579862356185913,
      "learning_rate": 0.0018488888888888888,
      "loss": 1.4276,
      "step": 208
    },
    {
      "epoch": 0.2786666666666667,
      "grad_norm": 0.15123353898525238,
      "learning_rate": 0.0018577777777777779,
      "loss": 1.2556,
      "step": 209
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.2123190015554428,
      "learning_rate": 0.0018666666666666666,
      "loss": 1.4817,
      "step": 210
    },
    {
      "epoch": 0.2813333333333333,
      "grad_norm": 0.09837790578603745,
      "learning_rate": 0.0018755555555555557,
      "loss": 1.509,
      "step": 211
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.13006064295768738,
      "learning_rate": 0.0018844444444444444,
      "loss": 1.4932,
      "step": 212
    },
    {
      "epoch": 0.284,
      "grad_norm": 0.1544894278049469,
      "learning_rate": 0.0018933333333333335,
      "loss": 1.8282,
      "step": 213
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.1085115373134613,
      "learning_rate": 0.0019022222222222222,
      "loss": 1.364,
      "step": 214
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.2004687786102295,
      "learning_rate": 0.0019111111111111113,
      "loss": 1.8523,
      "step": 215
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.1133243516087532,
      "learning_rate": 0.00192,
      "loss": 1.1968,
      "step": 216
    },
    {
      "epoch": 0.28933333333333333,
      "grad_norm": 0.14336636662483215,
      "learning_rate": 0.0019288888888888888,
      "loss": 1.2636,
      "step": 217
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.10528384894132614,
      "learning_rate": 0.0019377777777777778,
      "loss": 1.2506,
      "step": 218
    },
    {
      "epoch": 0.292,
      "grad_norm": 0.11885785311460495,
      "learning_rate": 0.0019466666666666669,
      "loss": 1.528,
      "step": 219
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.10311006009578705,
      "learning_rate": 0.0019555555555555554,
      "loss": 1.4176,
      "step": 220
    },
    {
      "epoch": 0.2946666666666667,
      "grad_norm": 0.08208838850259781,
      "learning_rate": 0.0019644444444444444,
      "loss": 1.3646,
      "step": 221
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.0704248771071434,
      "learning_rate": 0.0019733333333333334,
      "loss": 1.3744,
      "step": 222
    },
    {
      "epoch": 0.29733333333333334,
      "grad_norm": 0.17308077216148376,
      "learning_rate": 0.0019822222222222225,
      "loss": 1.6596,
      "step": 223
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.09888805449008942,
      "learning_rate": 0.001991111111111111,
      "loss": 1.2529,
      "step": 224
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.11442390829324722,
      "learning_rate": 0.002,
      "loss": 1.59,
      "step": 225
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.08426883816719055,
      "learning_rate": 0.001999999906759682,
      "loss": 1.3271,
      "step": 226
    },
    {
      "epoch": 0.30266666666666664,
      "grad_norm": 0.4455508589744568,
      "learning_rate": 0.001999999627038744,
      "loss": 1.9235,
      "step": 227
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.23622238636016846,
      "learning_rate": 0.001999999160837239,
      "loss": 1.5459,
      "step": 228
    },
    {
      "epoch": 0.30533333333333335,
      "grad_norm": 0.15035676956176758,
      "learning_rate": 0.001999998508155254,
      "loss": 1.5617,
      "step": 229
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.1432712823152542,
      "learning_rate": 0.001999997668992911,
      "loss": 1.3752,
      "step": 230
    },
    {
      "epoch": 0.308,
      "grad_norm": 0.19954468309879303,
      "learning_rate": 0.001999996643350365,
      "loss": 1.2875,
      "step": 231
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.2520114779472351,
      "learning_rate": 0.0019999954312278087,
      "loss": 1.7282,
      "step": 232
    },
    {
      "epoch": 0.31066666666666665,
      "grad_norm": 0.15023937821388245,
      "learning_rate": 0.0019999940326254676,
      "loss": 1.3115,
      "step": 233
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.1070207878947258,
      "learning_rate": 0.001999992447543603,
      "loss": 1.1875,
      "step": 234
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.17872443795204163,
      "learning_rate": 0.0019999906759825097,
      "loss": 1.3943,
      "step": 235
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.11692877113819122,
      "learning_rate": 0.0019999887179425187,
      "loss": 1.5984,
      "step": 236
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.10689838975667953,
      "learning_rate": 0.0019999865734239945,
      "loss": 1.4206,
      "step": 237
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.09283257275819778,
      "learning_rate": 0.001999984242427338,
      "loss": 1.3056,
      "step": 238
    },
    {
      "epoch": 0.31866666666666665,
      "grad_norm": 0.20318013429641724,
      "learning_rate": 0.0019999817249529827,
      "loss": 1.4931,
      "step": 239
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.09454657882452011,
      "learning_rate": 0.001999979021001399,
      "loss": 1.3915,
      "step": 240
    },
    {
      "epoch": 0.32133333333333336,
      "grad_norm": 0.0849783793091774,
      "learning_rate": 0.00199997613057309,
      "loss": 1.5463,
      "step": 241
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.19206766784191132,
      "learning_rate": 0.0019999730536685964,
      "loss": 1.7484,
      "step": 242
    },
    {
      "epoch": 0.324,
      "grad_norm": 0.10975363105535507,
      "learning_rate": 0.0019999697902884908,
      "loss": 1.5411,
      "step": 243
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.1025867611169815,
      "learning_rate": 0.001999966340433382,
      "loss": 1.5015,
      "step": 244
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.07621404528617859,
      "learning_rate": 0.0019999627041039133,
      "loss": 1.4361,
      "step": 245
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.11332632601261139,
      "learning_rate": 0.0019999588813007633,
      "loss": 1.5896,
      "step": 246
    },
    {
      "epoch": 0.3293333333333333,
      "grad_norm": 0.08630890399217606,
      "learning_rate": 0.001999954872024644,
      "loss": 1.1898,
      "step": 247
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.10983987897634506,
      "learning_rate": 0.0019999506762763035,
      "loss": 1.4508,
      "step": 248
    },
    {
      "epoch": 0.332,
      "grad_norm": 0.06788527965545654,
      "learning_rate": 0.001999946294056524,
      "loss": 1.1999,
      "step": 249
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.5273227691650391,
      "learning_rate": 0.0019999417253661234,
      "loss": 1.8123,
      "step": 250
    },
    {
      "epoch": 0.33466666666666667,
      "grad_norm": 0.08027791976928711,
      "learning_rate": 0.0019999369702059532,
      "loss": 1.443,
      "step": 251
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.1597403585910797,
      "learning_rate": 0.0019999320285769,
      "loss": 1.5052,
      "step": 252
    },
    {
      "epoch": 0.3373333333333333,
      "grad_norm": 0.11737503856420517,
      "learning_rate": 0.001999926900479885,
      "loss": 1.5635,
      "step": 253
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.14986412227153778,
      "learning_rate": 0.0019999215859158657,
      "loss": 1.4726,
      "step": 254
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.17985229194164276,
      "learning_rate": 0.001999916084885832,
      "loss": 1.5511,
      "step": 255
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.13349486887454987,
      "learning_rate": 0.00199991039739081,
      "loss": 1.1446,
      "step": 256
    },
    {
      "epoch": 0.3426666666666667,
      "grad_norm": 0.26438799500465393,
      "learning_rate": 0.0019999045234318606,
      "loss": 1.4797,
      "step": 257
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.09764478355646133,
      "learning_rate": 0.001999898463010079,
      "loss": 1.1688,
      "step": 258
    },
    {
      "epoch": 0.3453333333333333,
      "grad_norm": 0.07612636685371399,
      "learning_rate": 0.001999892216126596,
      "loss": 1.4874,
      "step": 259
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.14608581364154816,
      "learning_rate": 0.001999885782782575,
      "loss": 1.393,
      "step": 260
    },
    {
      "epoch": 0.348,
      "grad_norm": 0.21489006280899048,
      "learning_rate": 0.001999879162979217,
      "loss": 1.3894,
      "step": 261
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 0.06065966188907623,
      "learning_rate": 0.0019998723567177562,
      "loss": 1.2482,
      "step": 262
    },
    {
      "epoch": 0.3506666666666667,
      "grad_norm": 0.0650487095117569,
      "learning_rate": 0.001999865363999461,
      "loss": 1.432,
      "step": 263
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.05423147231340408,
      "learning_rate": 0.0019998581848256368,
      "loss": 1.3507,
      "step": 264
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.07804792374372482,
      "learning_rate": 0.0019998508191976217,
      "loss": 1.3087,
      "step": 265
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.09763214737176895,
      "learning_rate": 0.001999843267116789,
      "loss": 1.2827,
      "step": 266
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.0960591584444046,
      "learning_rate": 0.0019998355285845474,
      "loss": 1.531,
      "step": 267
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.13922002911567688,
      "learning_rate": 0.0019998276036023396,
      "loss": 1.5412,
      "step": 268
    },
    {
      "epoch": 0.3586666666666667,
      "grad_norm": 0.05023603141307831,
      "learning_rate": 0.001999819492171644,
      "loss": 1.4064,
      "step": 269
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.10569024085998535,
      "learning_rate": 0.0019998111942939726,
      "loss": 1.3646,
      "step": 270
    },
    {
      "epoch": 0.36133333333333334,
      "grad_norm": 0.11148208379745483,
      "learning_rate": 0.0019998027099708735,
      "loss": 1.293,
      "step": 271
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.10627737641334534,
      "learning_rate": 0.001999794039203928,
      "loss": 1.4043,
      "step": 272
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.08687829971313477,
      "learning_rate": 0.0019997851819947535,
      "loss": 1.4107,
      "step": 273
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.07988125085830688,
      "learning_rate": 0.001999776138345002,
      "loss": 1.259,
      "step": 274
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.1479395031929016,
      "learning_rate": 0.0019997669082563595,
      "loss": 1.595,
      "step": 275
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.05802853778004646,
      "learning_rate": 0.001999757491730548,
      "loss": 1.4499,
      "step": 276
    },
    {
      "epoch": 0.36933333333333335,
      "grad_norm": 0.06045597046613693,
      "learning_rate": 0.001999747888769322,
      "loss": 1.5376,
      "step": 277
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.07695658504962921,
      "learning_rate": 0.001999738099374474,
      "loss": 1.3521,
      "step": 278
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.08334402740001678,
      "learning_rate": 0.001999728123547828,
      "loss": 1.3981,
      "step": 279
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.06757953017950058,
      "learning_rate": 0.001999717961291245,
      "loss": 1.3287,
      "step": 280
    },
    {
      "epoch": 0.37466666666666665,
      "grad_norm": 0.04772596433758736,
      "learning_rate": 0.0019997076126066207,
      "loss": 1.3087,
      "step": 281
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.05686959996819496,
      "learning_rate": 0.0019996970774958838,
      "loss": 1.2932,
      "step": 282
    },
    {
      "epoch": 0.37733333333333335,
      "grad_norm": 0.05310272052884102,
      "learning_rate": 0.001999686355960999,
      "loss": 1.5018,
      "step": 283
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.05681496486067772,
      "learning_rate": 0.0019996754480039666,
      "loss": 1.234,
      "step": 284
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.05258401110768318,
      "learning_rate": 0.00199966435362682,
      "loss": 1.3291,
      "step": 285
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.07269256561994553,
      "learning_rate": 0.001999653072831629,
      "loss": 1.2992,
      "step": 286
    },
    {
      "epoch": 0.38266666666666665,
      "grad_norm": 0.04860762506723404,
      "learning_rate": 0.0019996416056204955,
      "loss": 1.4232,
      "step": 287
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.044220492243766785,
      "learning_rate": 0.001999629951995559,
      "loss": 1.4881,
      "step": 288
    },
    {
      "epoch": 0.38533333333333336,
      "grad_norm": 0.14887695014476776,
      "learning_rate": 0.0019996181119589927,
      "loss": 1.0483,
      "step": 289
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.09845087677240372,
      "learning_rate": 0.001999606085513005,
      "loss": 1.713,
      "step": 290
    },
    {
      "epoch": 0.388,
      "grad_norm": 0.07102132588624954,
      "learning_rate": 0.0019995938726598372,
      "loss": 1.4657,
      "step": 291
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 0.06169552728533745,
      "learning_rate": 0.001999581473401768,
      "loss": 1.3369,
      "step": 292
    },
    {
      "epoch": 0.39066666666666666,
      "grad_norm": 0.052081745117902756,
      "learning_rate": 0.001999568887741109,
      "loss": 1.3564,
      "step": 293
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.05409561097621918,
      "learning_rate": 0.0019995561156802076,
      "loss": 1.3551,
      "step": 294
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.05737118050456047,
      "learning_rate": 0.0019995431572214454,
      "loss": 1.4825,
      "step": 295
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.12405510246753693,
      "learning_rate": 0.001999530012367239,
      "loss": 1.2869,
      "step": 296
    },
    {
      "epoch": 0.396,
      "grad_norm": 0.04846273735165596,
      "learning_rate": 0.001999516681120039,
      "loss": 1.5018,
      "step": 297
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.07069501280784607,
      "learning_rate": 0.001999503163482332,
      "loss": 1.5463,
      "step": 298
    },
    {
      "epoch": 0.39866666666666667,
      "grad_norm": 0.07002240419387817,
      "learning_rate": 0.001999489459456639,
      "loss": 1.1883,
      "step": 299
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.036256637424230576,
      "learning_rate": 0.0019994755690455153,
      "loss": 1.2057,
      "step": 300
    },
    {
      "epoch": 0.4013333333333333,
      "grad_norm": 0.07686804234981537,
      "learning_rate": 0.0019994614922515504,
      "loss": 1.2535,
      "step": 301
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.06085726246237755,
      "learning_rate": 0.0019994472290773705,
      "loss": 1.3783,
      "step": 302
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.04615631699562073,
      "learning_rate": 0.0019994327795256352,
      "loss": 1.2315,
      "step": 303
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.07021823525428772,
      "learning_rate": 0.0019994181435990382,
      "loss": 1.5601,
      "step": 304
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.06546615064144135,
      "learning_rate": 0.0019994033213003104,
      "loss": 1.2778,
      "step": 305
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.053704507648944855,
      "learning_rate": 0.001999388312632214,
      "loss": 1.3663,
      "step": 306
    },
    {
      "epoch": 0.4093333333333333,
      "grad_norm": 0.045590389519929886,
      "learning_rate": 0.0019993731175975494,
      "loss": 1.1747,
      "step": 307
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.08280221372842789,
      "learning_rate": 0.001999357736199149,
      "loss": 1.2012,
      "step": 308
    },
    {
      "epoch": 0.412,
      "grad_norm": 0.03712066262960434,
      "learning_rate": 0.001999342168439882,
      "loss": 1.4598,
      "step": 309
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.09471578896045685,
      "learning_rate": 0.0019993264143226513,
      "loss": 1.3591,
      "step": 310
    },
    {
      "epoch": 0.4146666666666667,
      "grad_norm": 0.2632363736629486,
      "learning_rate": 0.0019993104738503945,
      "loss": 1.2627,
      "step": 311
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.06657509505748749,
      "learning_rate": 0.001999294347026084,
      "loss": 1.3266,
      "step": 312
    },
    {
      "epoch": 0.41733333333333333,
      "grad_norm": 0.06597900390625,
      "learning_rate": 0.0019992780338527276,
      "loss": 1.4617,
      "step": 313
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.0662798061966896,
      "learning_rate": 0.0019992615343333675,
      "loss": 1.1541,
      "step": 314
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.09077456593513489,
      "learning_rate": 0.0019992448484710797,
      "loss": 1.1156,
      "step": 315
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.049744147807359695,
      "learning_rate": 0.001999227976268977,
      "loss": 1.2986,
      "step": 316
    },
    {
      "epoch": 0.4226666666666667,
      "grad_norm": 0.07648691534996033,
      "learning_rate": 0.0019992109177302043,
      "loss": 1.3802,
      "step": 317
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.30873656272888184,
      "learning_rate": 0.0019991936728579436,
      "loss": 1.7303,
      "step": 318
    },
    {
      "epoch": 0.42533333333333334,
      "grad_norm": 0.10793206095695496,
      "learning_rate": 0.001999176241655411,
      "loss": 1.2319,
      "step": 319
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.04792880639433861,
      "learning_rate": 0.0019991586241258565,
      "loss": 1.2385,
      "step": 320
    },
    {
      "epoch": 0.428,
      "grad_norm": 0.10951211303472519,
      "learning_rate": 0.0019991408202725655,
      "loss": 1.4573,
      "step": 321
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.0815192237496376,
      "learning_rate": 0.0019991228300988585,
      "loss": 1.1636,
      "step": 322
    },
    {
      "epoch": 0.43066666666666664,
      "grad_norm": 0.09811433404684067,
      "learning_rate": 0.0019991046536080898,
      "loss": 1.4158,
      "step": 323
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.07855795323848724,
      "learning_rate": 0.0019990862908036487,
      "loss": 1.4849,
      "step": 324
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.06553905457258224,
      "learning_rate": 0.0019990677416889605,
      "loss": 1.2736,
      "step": 325
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.05087639391422272,
      "learning_rate": 0.001999049006267484,
      "loss": 1.406,
      "step": 326
    },
    {
      "epoch": 0.436,
      "grad_norm": 0.06218847632408142,
      "learning_rate": 0.0019990300845427124,
      "loss": 1.5146,
      "step": 327
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.06691920012235641,
      "learning_rate": 0.0019990109765181743,
      "loss": 1.4756,
      "step": 328
    },
    {
      "epoch": 0.43866666666666665,
      "grad_norm": 0.05301818996667862,
      "learning_rate": 0.0019989916821974334,
      "loss": 1.5027,
      "step": 329
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.05837976187467575,
      "learning_rate": 0.001998972201584088,
      "loss": 1.2203,
      "step": 330
    },
    {
      "epoch": 0.44133333333333336,
      "grad_norm": 0.05512513220310211,
      "learning_rate": 0.00199895253468177,
      "loss": 1.294,
      "step": 331
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.07397747039794922,
      "learning_rate": 0.0019989326814941473,
      "loss": 1.3471,
      "step": 332
    },
    {
      "epoch": 0.444,
      "grad_norm": 0.06325335800647736,
      "learning_rate": 0.0019989126420249218,
      "loss": 1.3156,
      "step": 333
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.05998796969652176,
      "learning_rate": 0.001998892416277831,
      "loss": 1.1595,
      "step": 334
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.10149934887886047,
      "learning_rate": 0.0019988720042566467,
      "loss": 1.3689,
      "step": 335
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.05460279434919357,
      "learning_rate": 0.001998851405965175,
      "loss": 1.6546,
      "step": 336
    },
    {
      "epoch": 0.4493333333333333,
      "grad_norm": 0.054483331739902496,
      "learning_rate": 0.0019988306214072573,
      "loss": 1.2249,
      "step": 337
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.08378994464874268,
      "learning_rate": 0.001998809650586769,
      "loss": 1.8498,
      "step": 338
    },
    {
      "epoch": 0.452,
      "grad_norm": 0.09979964047670364,
      "learning_rate": 0.001998788493507621,
      "loss": 1.2252,
      "step": 339
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.04588304087519646,
      "learning_rate": 0.001998767150173759,
      "loss": 1.5296,
      "step": 340
    },
    {
      "epoch": 0.45466666666666666,
      "grad_norm": 0.0771678239107132,
      "learning_rate": 0.001998745620589163,
      "loss": 1.3201,
      "step": 341
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.057599280029535294,
      "learning_rate": 0.001998723904757848,
      "loss": 1.2344,
      "step": 342
    },
    {
      "epoch": 0.4573333333333333,
      "grad_norm": 0.04583011195063591,
      "learning_rate": 0.0019987020026838633,
      "loss": 1.3051,
      "step": 343
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.03659690544009209,
      "learning_rate": 0.001998679914371293,
      "loss": 1.1183,
      "step": 344
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.0475444570183754,
      "learning_rate": 0.0019986576398242565,
      "loss": 1.1643,
      "step": 345
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.09210855513811111,
      "learning_rate": 0.0019986351790469074,
      "loss": 1.1875,
      "step": 346
    },
    {
      "epoch": 0.46266666666666667,
      "grad_norm": 0.050942420959472656,
      "learning_rate": 0.0019986125320434344,
      "loss": 1.2574,
      "step": 347
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.08606415241956711,
      "learning_rate": 0.0019985896988180605,
      "loss": 1.652,
      "step": 348
    },
    {
      "epoch": 0.4653333333333333,
      "grad_norm": 0.08358950912952423,
      "learning_rate": 0.001998566679375044,
      "loss": 1.562,
      "step": 349
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.05088535696268082,
      "learning_rate": 0.001998543473718677,
      "loss": 1.456,
      "step": 350
    },
    {
      "epoch": 0.468,
      "grad_norm": 0.0559992715716362,
      "learning_rate": 0.0019985200818532873,
      "loss": 1.5776,
      "step": 351
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.05187581852078438,
      "learning_rate": 0.001998496503783237,
      "loss": 1.3837,
      "step": 352
    },
    {
      "epoch": 0.4706666666666667,
      "grad_norm": 0.03263884037733078,
      "learning_rate": 0.0019984727395129234,
      "loss": 1.2556,
      "step": 353
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.04913047328591347,
      "learning_rate": 0.001998448789046777,
      "loss": 1.3417,
      "step": 354
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.05219447240233421,
      "learning_rate": 0.001998424652389265,
      "loss": 1.4745,
      "step": 355
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.057174138724803925,
      "learning_rate": 0.001998400329544888,
      "loss": 1.2246,
      "step": 356
    },
    {
      "epoch": 0.476,
      "grad_norm": 0.041574083268642426,
      "learning_rate": 0.001998375820518182,
      "loss": 1.3928,
      "step": 357
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.047312237322330475,
      "learning_rate": 0.001998351125313717,
      "loss": 1.065,
      "step": 358
    },
    {
      "epoch": 0.4786666666666667,
      "grad_norm": 0.03565726429224014,
      "learning_rate": 0.001998326243936099,
      "loss": 1.1635,
      "step": 359
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.09561602771282196,
      "learning_rate": 0.0019983011763899674,
      "loss": 1.3387,
      "step": 360
    },
    {
      "epoch": 0.48133333333333334,
      "grad_norm": 0.04501534253358841,
      "learning_rate": 0.0019982759226799965,
      "loss": 1.2516,
      "step": 361
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.05453461408615112,
      "learning_rate": 0.001998250482810896,
      "loss": 1.5275,
      "step": 362
    },
    {
      "epoch": 0.484,
      "grad_norm": 0.038464803248643875,
      "learning_rate": 0.0019982248567874095,
      "loss": 1.2146,
      "step": 363
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.04511315003037453,
      "learning_rate": 0.001998199044614317,
      "loss": 1.4316,
      "step": 364
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.0429406613111496,
      "learning_rate": 0.0019981730462964305,
      "loss": 1.4252,
      "step": 365
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.03547189384698868,
      "learning_rate": 0.001998146861838599,
      "loss": 1.4203,
      "step": 366
    },
    {
      "epoch": 0.48933333333333334,
      "grad_norm": 0.07850974053144455,
      "learning_rate": 0.0019981204912457046,
      "loss": 1.2955,
      "step": 367
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.04389988258481026,
      "learning_rate": 0.001998093934522666,
      "loss": 1.2423,
      "step": 368
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.046507686376571655,
      "learning_rate": 0.001998067191674435,
      "loss": 1.0064,
      "step": 369
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.051280561834573746,
      "learning_rate": 0.001998040262705999,
      "loss": 1.4747,
      "step": 370
    },
    {
      "epoch": 0.49466666666666664,
      "grad_norm": 0.04559962823987007,
      "learning_rate": 0.001998013147622379,
      "loss": 1.4199,
      "step": 371
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.04297352582216263,
      "learning_rate": 0.0019979858464286315,
      "loss": 1.1705,
      "step": 372
    },
    {
      "epoch": 0.49733333333333335,
      "grad_norm": 0.04963921383023262,
      "learning_rate": 0.0019979583591298485,
      "loss": 1.1705,
      "step": 373
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.18203385174274445,
      "learning_rate": 0.0019979306857311548,
      "loss": 1.5593,
      "step": 374
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.05946153402328491,
      "learning_rate": 0.0019979028262377117,
      "loss": 1.4933,
      "step": 375
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.04380752146244049,
      "learning_rate": 0.0019978747806547142,
      "loss": 1.206,
      "step": 376
    },
    {
      "epoch": 0.5026666666666667,
      "grad_norm": 0.04074501991271973,
      "learning_rate": 0.001997846548987392,
      "loss": 1.2303,
      "step": 377
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.04231489077210426,
      "learning_rate": 0.0019978181312410104,
      "loss": 1.2513,
      "step": 378
    },
    {
      "epoch": 0.5053333333333333,
      "grad_norm": 0.05422556772828102,
      "learning_rate": 0.001997789527420868,
      "loss": 1.3631,
      "step": 379
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.0528741180896759,
      "learning_rate": 0.0019977607375323,
      "loss": 1.2854,
      "step": 380
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.044870343059301376,
      "learning_rate": 0.0019977317615806735,
      "loss": 1.3378,
      "step": 381
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.044257208704948425,
      "learning_rate": 0.001997702599571393,
      "loss": 1.1967,
      "step": 382
    },
    {
      "epoch": 0.5106666666666667,
      "grad_norm": 0.04860832914710045,
      "learning_rate": 0.001997673251509897,
      "loss": 1.3478,
      "step": 383
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.045245107263326645,
      "learning_rate": 0.001997643717401657,
      "loss": 1.285,
      "step": 384
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.04363589733839035,
      "learning_rate": 0.001997613997252182,
      "loss": 1.0023,
      "step": 385
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.05787007510662079,
      "learning_rate": 0.001997584091067014,
      "loss": 1.4132,
      "step": 386
    },
    {
      "epoch": 0.516,
      "grad_norm": 0.03477425128221512,
      "learning_rate": 0.0019975539988517288,
      "loss": 1.1194,
      "step": 387
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.05705839768052101,
      "learning_rate": 0.001997523720611939,
      "loss": 1.2487,
      "step": 388
    },
    {
      "epoch": 0.5186666666666667,
      "grad_norm": 0.05525148659944534,
      "learning_rate": 0.0019974932563532905,
      "loss": 0.8382,
      "step": 389
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.05530788376927376,
      "learning_rate": 0.0019974626060814647,
      "loss": 1.3131,
      "step": 390
    },
    {
      "epoch": 0.5213333333333333,
      "grad_norm": 0.05661793425679207,
      "learning_rate": 0.001997431769802177,
      "loss": 1.0936,
      "step": 391
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.05634351447224617,
      "learning_rate": 0.0019974007475211776,
      "loss": 1.3485,
      "step": 392
    },
    {
      "epoch": 0.524,
      "grad_norm": 0.04908985644578934,
      "learning_rate": 0.001997369539244252,
      "loss": 1.1945,
      "step": 393
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.14249204099178314,
      "learning_rate": 0.0019973381449772194,
      "loss": 1.6922,
      "step": 394
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.04738152399659157,
      "learning_rate": 0.0019973065647259348,
      "loss": 0.9895,
      "step": 395
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.0476241409778595,
      "learning_rate": 0.0019972747984962867,
      "loss": 1.2069,
      "step": 396
    },
    {
      "epoch": 0.5293333333333333,
      "grad_norm": 0.15888147056102753,
      "learning_rate": 0.0019972428462941994,
      "loss": 1.3894,
      "step": 397
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.046857211738824844,
      "learning_rate": 0.0019972107081256316,
      "loss": 1.4062,
      "step": 398
    },
    {
      "epoch": 0.532,
      "grad_norm": 0.034172605723142624,
      "learning_rate": 0.0019971783839965755,
      "loss": 1.3365,
      "step": 399
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.041829273104667664,
      "learning_rate": 0.0019971458739130596,
      "loss": 0.9306,
      "step": 400
    },
    {
      "epoch": 0.5346666666666666,
      "grad_norm": 0.14157475531101227,
      "learning_rate": 0.0019971131778811465,
      "loss": 1.2573,
      "step": 401
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.05568951368331909,
      "learning_rate": 0.0019970802959069327,
      "loss": 1.3749,
      "step": 402
    },
    {
      "epoch": 0.5373333333333333,
      "grad_norm": 0.05192892253398895,
      "learning_rate": 0.0019970472279965505,
      "loss": 1.3734,
      "step": 403
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.05479362979531288,
      "learning_rate": 0.001997013974156167,
      "loss": 1.3693,
      "step": 404
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.04695793241262436,
      "learning_rate": 0.001996980534391982,
      "loss": 1.4303,
      "step": 405
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 0.049125052988529205,
      "learning_rate": 0.0019969469087102324,
      "loss": 1.3034,
      "step": 406
    },
    {
      "epoch": 0.5426666666666666,
      "grad_norm": 0.04361307993531227,
      "learning_rate": 0.0019969130971171888,
      "loss": 1.1984,
      "step": 407
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.050563666969537735,
      "learning_rate": 0.001996879099619156,
      "loss": 1.2095,
      "step": 408
    },
    {
      "epoch": 0.5453333333333333,
      "grad_norm": 0.05336029827594757,
      "learning_rate": 0.001996844916222474,
      "loss": 1.4589,
      "step": 409
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.04923446476459503,
      "learning_rate": 0.001996810546933517,
      "loss": 1.0218,
      "step": 410
    },
    {
      "epoch": 0.548,
      "grad_norm": 0.4374910891056061,
      "learning_rate": 0.0019967759917586952,
      "loss": 1.3898,
      "step": 411
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.0457625612616539,
      "learning_rate": 0.001996741250704451,
      "loss": 1.2638,
      "step": 412
    },
    {
      "epoch": 0.5506666666666666,
      "grad_norm": 0.07485470920801163,
      "learning_rate": 0.001996706323777264,
      "loss": 1.1842,
      "step": 413
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.16584523022174835,
      "learning_rate": 0.0019966712109836474,
      "loss": 1.2966,
      "step": 414
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.24083121120929718,
      "learning_rate": 0.001996635912330149,
      "loss": 1.1396,
      "step": 415
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.04897521436214447,
      "learning_rate": 0.001996600427823351,
      "loss": 1.1033,
      "step": 416
    },
    {
      "epoch": 0.556,
      "grad_norm": 0.04386454448103905,
      "learning_rate": 0.0019965647574698704,
      "loss": 1.3412,
      "step": 417
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.17673587799072266,
      "learning_rate": 0.0019965289012763596,
      "loss": 1.2012,
      "step": 418
    },
    {
      "epoch": 0.5586666666666666,
      "grad_norm": 0.09123574942350388,
      "learning_rate": 0.0019964928592495043,
      "loss": 1.3724,
      "step": 419
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.30419105291366577,
      "learning_rate": 0.0019964566313960264,
      "loss": 1.2557,
      "step": 420
    },
    {
      "epoch": 0.5613333333333334,
      "grad_norm": 0.06432117521762848,
      "learning_rate": 0.001996420217722682,
      "loss": 1.3414,
      "step": 421
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 0.11227492988109589,
      "learning_rate": 0.0019963836182362604,
      "loss": 1.2183,
      "step": 422
    },
    {
      "epoch": 0.564,
      "grad_norm": 0.2097117155790329,
      "learning_rate": 0.001996346832943587,
      "loss": 1.5035,
      "step": 423
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.06525377184152603,
      "learning_rate": 0.0019963098618515224,
      "loss": 1.2455,
      "step": 424
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.11905170977115631,
      "learning_rate": 0.00199627270496696,
      "loss": 1.4152,
      "step": 425
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.08599259704351425,
      "learning_rate": 0.0019962353622968295,
      "loss": 1.4766,
      "step": 426
    },
    {
      "epoch": 0.5693333333333334,
      "grad_norm": 0.05159797891974449,
      "learning_rate": 0.001996197833848094,
      "loss": 1.4331,
      "step": 427
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.05167139321565628,
      "learning_rate": 0.0019961601196277524,
      "loss": 1.0232,
      "step": 428
    },
    {
      "epoch": 0.572,
      "grad_norm": 0.06547737866640091,
      "learning_rate": 0.0019961222196428377,
      "loss": 1.3607,
      "step": 429
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.04885708540678024,
      "learning_rate": 0.001996084133900417,
      "loss": 1.3514,
      "step": 430
    },
    {
      "epoch": 0.5746666666666667,
      "grad_norm": 0.06885898113250732,
      "learning_rate": 0.001996045862407593,
      "loss": 1.2983,
      "step": 431
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.11762596666812897,
      "learning_rate": 0.001996007405171502,
      "loss": 1.4115,
      "step": 432
    },
    {
      "epoch": 0.5773333333333334,
      "grad_norm": 0.045557085424661636,
      "learning_rate": 0.001995968762199316,
      "loss": 1.2843,
      "step": 433
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.0447065569460392,
      "learning_rate": 0.0019959299334982414,
      "loss": 1.126,
      "step": 434
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.06316620856523514,
      "learning_rate": 0.0019958909190755185,
      "loss": 1.2184,
      "step": 435
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.17869415879249573,
      "learning_rate": 0.001995851718938423,
      "loss": 1.335,
      "step": 436
    },
    {
      "epoch": 0.5826666666666667,
      "grad_norm": 0.041679441928863525,
      "learning_rate": 0.001995812333094265,
      "loss": 1.2453,
      "step": 437
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.06389506906270981,
      "learning_rate": 0.0019957727615503885,
      "loss": 1.4884,
      "step": 438
    },
    {
      "epoch": 0.5853333333333334,
      "grad_norm": 0.05041440948843956,
      "learning_rate": 0.001995733004314174,
      "loss": 1.2995,
      "step": 439
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.039798665791749954,
      "learning_rate": 0.001995693061393035,
      "loss": 1.2662,
      "step": 440
    },
    {
      "epoch": 0.588,
      "grad_norm": 0.05408851057291031,
      "learning_rate": 0.00199565293279442,
      "loss": 1.3753,
      "step": 441
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.039038997143507004,
      "learning_rate": 0.0019956126185258116,
      "loss": 1.2177,
      "step": 442
    },
    {
      "epoch": 0.5906666666666667,
      "grad_norm": 0.04923113062977791,
      "learning_rate": 0.0019955721185947284,
      "loss": 1.2576,
      "step": 443
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.05296563357114792,
      "learning_rate": 0.0019955314330087222,
      "loss": 1.2748,
      "step": 444
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.04187644645571709,
      "learning_rate": 0.0019954905617753814,
      "loss": 1.2104,
      "step": 445
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.12507914006710052,
      "learning_rate": 0.0019954495049023264,
      "loss": 1.2816,
      "step": 446
    },
    {
      "epoch": 0.596,
      "grad_norm": 0.03879881650209427,
      "learning_rate": 0.001995408262397214,
      "loss": 1.3522,
      "step": 447
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.03624623641371727,
      "learning_rate": 0.001995366834267735,
      "loss": 1.1835,
      "step": 448
    },
    {
      "epoch": 0.5986666666666667,
      "grad_norm": 0.08685246109962463,
      "learning_rate": 0.001995325220521615,
      "loss": 1.1703,
      "step": 449
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.05578622221946716,
      "learning_rate": 0.0019952834211666138,
      "loss": 1.3275,
      "step": 450
    },
    {
      "epoch": 0.6013333333333334,
      "grad_norm": 0.042471833527088165,
      "learning_rate": 0.001995241436210527,
      "loss": 1.2373,
      "step": 451
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 0.05162429064512253,
      "learning_rate": 0.0019951992656611836,
      "loss": 1.178,
      "step": 452
    },
    {
      "epoch": 0.604,
      "grad_norm": 0.05223008617758751,
      "learning_rate": 0.0019951569095264473,
      "loss": 1.15,
      "step": 453
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 0.03955993056297302,
      "learning_rate": 0.0019951143678142167,
      "loss": 1.3227,
      "step": 454
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.05032728984951973,
      "learning_rate": 0.0019950716405324254,
      "loss": 1.3173,
      "step": 455
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.0402766689658165,
      "learning_rate": 0.0019950287276890412,
      "loss": 1.067,
      "step": 456
    },
    {
      "epoch": 0.6093333333333333,
      "grad_norm": 0.053399451076984406,
      "learning_rate": 0.001994985629292066,
      "loss": 1.3012,
      "step": 457
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.06857909262180328,
      "learning_rate": 0.001994942345349537,
      "loss": 1.129,
      "step": 458
    },
    {
      "epoch": 0.612,
      "grad_norm": 0.03352987393736839,
      "learning_rate": 0.0019948988758695264,
      "loss": 1.2819,
      "step": 459
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.07027451694011688,
      "learning_rate": 0.00199485522086014,
      "loss": 1.6333,
      "step": 460
    },
    {
      "epoch": 0.6146666666666667,
      "grad_norm": 0.05211083963513374,
      "learning_rate": 0.001994811380329518,
      "loss": 1.5878,
      "step": 461
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.040701575577259064,
      "learning_rate": 0.0019947673542858365,
      "loss": 1.1966,
      "step": 462
    },
    {
      "epoch": 0.6173333333333333,
      "grad_norm": 0.039525751024484634,
      "learning_rate": 0.0019947231427373062,
      "loss": 1.5503,
      "step": 463
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.04832125082612038,
      "learning_rate": 0.00199467874569217,
      "loss": 1.2978,
      "step": 464
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.04553849995136261,
      "learning_rate": 0.0019946341631587087,
      "loss": 1.2427,
      "step": 465
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.32125380635261536,
      "learning_rate": 0.001994589395145235,
      "loss": 1.4024,
      "step": 466
    },
    {
      "epoch": 0.6226666666666667,
      "grad_norm": 0.040016330778598785,
      "learning_rate": 0.001994544441660098,
      "loss": 1.2246,
      "step": 467
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.10011456906795502,
      "learning_rate": 0.0019944993027116798,
      "loss": 1.4486,
      "step": 468
    },
    {
      "epoch": 0.6253333333333333,
      "grad_norm": 0.03368661180138588,
      "learning_rate": 0.0019944539783083985,
      "loss": 1.279,
      "step": 469
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.04721704125404358,
      "learning_rate": 0.001994408468458706,
      "loss": 1.0057,
      "step": 470
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.14364473521709442,
      "learning_rate": 0.0019943627731710896,
      "loss": 1.7496,
      "step": 471
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.03712965548038483,
      "learning_rate": 0.00199431689245407,
      "loss": 0.95,
      "step": 472
    },
    {
      "epoch": 0.6306666666666667,
      "grad_norm": 0.06311935931444168,
      "learning_rate": 0.001994270826316203,
      "loss": 1.3062,
      "step": 473
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.15321768820285797,
      "learning_rate": 0.0019942245747660795,
      "loss": 1.422,
      "step": 474
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.0938134640455246,
      "learning_rate": 0.001994178137812324,
      "loss": 1.3303,
      "step": 475
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.04027548059821129,
      "learning_rate": 0.0019941315154635973,
      "loss": 1.4462,
      "step": 476
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.043631475418806076,
      "learning_rate": 0.0019940847077285916,
      "loss": 1.2526,
      "step": 477
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.07198604196310043,
      "learning_rate": 0.001994037714616037,
      "loss": 1.0239,
      "step": 478
    },
    {
      "epoch": 0.6386666666666667,
      "grad_norm": 0.07211945205926895,
      "learning_rate": 0.0019939905361346963,
      "loss": 1.4472,
      "step": 479
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.04374998062849045,
      "learning_rate": 0.0019939431722933677,
      "loss": 1.3215,
      "step": 480
    },
    {
      "epoch": 0.6413333333333333,
      "grad_norm": 0.04757560417056084,
      "learning_rate": 0.0019938956231008837,
      "loss": 1.3205,
      "step": 481
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.037524498999118805,
      "learning_rate": 0.001993847888566111,
      "loss": 1.3772,
      "step": 482
    },
    {
      "epoch": 0.644,
      "grad_norm": 0.03711952269077301,
      "learning_rate": 0.001993799968697951,
      "loss": 1.3041,
      "step": 483
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 0.05570259690284729,
      "learning_rate": 0.0019937518635053404,
      "loss": 1.5562,
      "step": 484
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.04483720660209656,
      "learning_rate": 0.0019937035729972494,
      "loss": 1.0846,
      "step": 485
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.05575212463736534,
      "learning_rate": 0.0019936550971826833,
      "loss": 1.6606,
      "step": 486
    },
    {
      "epoch": 0.6493333333333333,
      "grad_norm": 0.07995740324258804,
      "learning_rate": 0.001993606436070682,
      "loss": 1.205,
      "step": 487
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.05766502395272255,
      "learning_rate": 0.0019935575896703204,
      "loss": 1.5778,
      "step": 488
    },
    {
      "epoch": 0.652,
      "grad_norm": 0.16146396100521088,
      "learning_rate": 0.0019935085579907063,
      "loss": 1.3767,
      "step": 489
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.05064406991004944,
      "learning_rate": 0.001993459341040984,
      "loss": 1.3276,
      "step": 490
    },
    {
      "epoch": 0.6546666666666666,
      "grad_norm": 0.05369731783866882,
      "learning_rate": 0.001993409938830331,
      "loss": 1.4342,
      "step": 491
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.03992399945855141,
      "learning_rate": 0.0019933603513679603,
      "loss": 1.2062,
      "step": 492
    },
    {
      "epoch": 0.6573333333333333,
      "grad_norm": 0.032426804304122925,
      "learning_rate": 0.001993310578663119,
      "loss": 1.4297,
      "step": 493
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.04238004609942436,
      "learning_rate": 0.0019932606207250883,
      "loss": 1.0547,
      "step": 494
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.04586634784936905,
      "learning_rate": 0.0019932104775631843,
      "loss": 1.0463,
      "step": 495
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.047575145959854126,
      "learning_rate": 0.0019931601491867588,
      "loss": 1.4064,
      "step": 496
    },
    {
      "epoch": 0.6626666666666666,
      "grad_norm": 0.05401758849620819,
      "learning_rate": 0.001993109635605196,
      "loss": 1.2379,
      "step": 497
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.07208560407161713,
      "learning_rate": 0.001993058936827916,
      "loss": 1.2841,
      "step": 498
    },
    {
      "epoch": 0.6653333333333333,
      "grad_norm": 0.03680965304374695,
      "learning_rate": 0.001993008052864373,
      "loss": 1.195,
      "step": 499
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.03922390192747116,
      "learning_rate": 0.0019929569837240564,
      "loss": 1.4405,
      "step": 500
    },
    {
      "epoch": 0.668,
      "grad_norm": 0.11053036898374557,
      "learning_rate": 0.001992905729416489,
      "loss": 1.2263,
      "step": 501
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.32040610909461975,
      "learning_rate": 0.0019928542899512293,
      "loss": 1.4134,
      "step": 502
    },
    {
      "epoch": 0.6706666666666666,
      "grad_norm": 0.04465348646044731,
      "learning_rate": 0.001992802665337869,
      "loss": 1.2449,
      "step": 503
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.05368734896183014,
      "learning_rate": 0.001992750855586036,
      "loss": 1.3804,
      "step": 504
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.13195988535881042,
      "learning_rate": 0.0019926988607053913,
      "loss": 1.3817,
      "step": 505
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.10302291810512543,
      "learning_rate": 0.0019926466807056306,
      "loss": 1.221,
      "step": 506
    },
    {
      "epoch": 0.676,
      "grad_norm": 0.05810856074094772,
      "learning_rate": 0.0019925943155964855,
      "loss": 1.1166,
      "step": 507
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 0.035961881279945374,
      "learning_rate": 0.0019925417653877202,
      "loss": 1.2398,
      "step": 508
    },
    {
      "epoch": 0.6786666666666666,
      "grad_norm": 0.15068960189819336,
      "learning_rate": 0.0019924890300891344,
      "loss": 1.2033,
      "step": 509
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.15369467437267303,
      "learning_rate": 0.0019924361097105625,
      "loss": 1.5533,
      "step": 510
    },
    {
      "epoch": 0.6813333333333333,
      "grad_norm": 0.03356549143791199,
      "learning_rate": 0.001992383004261873,
      "loss": 1.3505,
      "step": 511
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.060285672545433044,
      "learning_rate": 0.0019923297137529688,
      "loss": 1.2064,
      "step": 512
    },
    {
      "epoch": 0.684,
      "grad_norm": 0.03869315981864929,
      "learning_rate": 0.001992276238193788,
      "loss": 1.1933,
      "step": 513
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 0.0386049710214138,
      "learning_rate": 0.0019922225775943023,
      "loss": 1.0469,
      "step": 514
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.10605252534151077,
      "learning_rate": 0.0019921687319645184,
      "loss": 1.1665,
      "step": 515
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.05392318591475487,
      "learning_rate": 0.001992114701314478,
      "loss": 0.9972,
      "step": 516
    },
    {
      "epoch": 0.6893333333333334,
      "grad_norm": 0.049436092376708984,
      "learning_rate": 0.0019920604856542563,
      "loss": 1.3673,
      "step": 517
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 0.10797630995512009,
      "learning_rate": 0.0019920060849939634,
      "loss": 1.3112,
      "step": 518
    },
    {
      "epoch": 0.692,
      "grad_norm": 0.2239043116569519,
      "learning_rate": 0.0019919514993437444,
      "loss": 1.6899,
      "step": 519
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.09764652699232101,
      "learning_rate": 0.0019918967287137785,
      "loss": 1.4775,
      "step": 520
    },
    {
      "epoch": 0.6946666666666667,
      "grad_norm": 0.07788337022066116,
      "learning_rate": 0.0019918417731142786,
      "loss": 1.3581,
      "step": 521
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.05877821892499924,
      "learning_rate": 0.001991786632555494,
      "loss": 1.2085,
      "step": 522
    },
    {
      "epoch": 0.6973333333333334,
      "grad_norm": 0.03842935338616371,
      "learning_rate": 0.0019917313070477055,
      "loss": 1.1956,
      "step": 523
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.08199071139097214,
      "learning_rate": 0.0019916757966012325,
      "loss": 1.178,
      "step": 524
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.04760304465889931,
      "learning_rate": 0.001991620101226425,
      "loss": 1.2226,
      "step": 525
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.039904844015836716,
      "learning_rate": 0.00199156422093367,
      "loss": 1.1394,
      "step": 526
    },
    {
      "epoch": 0.7026666666666667,
      "grad_norm": 0.0473652258515358,
      "learning_rate": 0.0019915081557333875,
      "loss": 1.2372,
      "step": 527
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.04281729459762573,
      "learning_rate": 0.001991451905636033,
      "loss": 1.191,
      "step": 528
    },
    {
      "epoch": 0.7053333333333334,
      "grad_norm": 0.05050241947174072,
      "learning_rate": 0.001991395470652096,
      "loss": 1.1128,
      "step": 529
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.2780245244503021,
      "learning_rate": 0.0019913388507921,
      "loss": 1.4138,
      "step": 530
    },
    {
      "epoch": 0.708,
      "grad_norm": 0.06240695342421532,
      "learning_rate": 0.0019912820460666046,
      "loss": 1.379,
      "step": 531
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 0.07397565245628357,
      "learning_rate": 0.0019912250564862017,
      "loss": 1.3243,
      "step": 532
    },
    {
      "epoch": 0.7106666666666667,
      "grad_norm": 0.06275905668735504,
      "learning_rate": 0.0019911678820615187,
      "loss": 1.2253,
      "step": 533
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.07825533300638199,
      "learning_rate": 0.0019911105228032186,
      "loss": 1.266,
      "step": 534
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.0673811212182045,
      "learning_rate": 0.0019910529787219968,
      "loss": 1.2804,
      "step": 535
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.04868144541978836,
      "learning_rate": 0.0019909952498285846,
      "loss": 1.3076,
      "step": 536
    },
    {
      "epoch": 0.716,
      "grad_norm": 0.06706177443265915,
      "learning_rate": 0.0019909373361337475,
      "loss": 1.399,
      "step": 537
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 0.055421821773052216,
      "learning_rate": 0.001990879237648285,
      "loss": 1.1078,
      "step": 538
    },
    {
      "epoch": 0.7186666666666667,
      "grad_norm": 0.05965227261185646,
      "learning_rate": 0.0019908209543830313,
      "loss": 1.263,
      "step": 539
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.035955995321273804,
      "learning_rate": 0.001990762486348855,
      "loss": 0.9878,
      "step": 540
    },
    {
      "epoch": 0.7213333333333334,
      "grad_norm": 0.03754599392414093,
      "learning_rate": 0.0019907038335566594,
      "loss": 1.2353,
      "step": 541
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.05093695968389511,
      "learning_rate": 0.001990644996017382,
      "loss": 1.2934,
      "step": 542
    },
    {
      "epoch": 0.724,
      "grad_norm": 0.04218876734375954,
      "learning_rate": 0.0019905859737419955,
      "loss": 1.2224,
      "step": 543
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.04036647453904152,
      "learning_rate": 0.0019905267667415056,
      "loss": 1.1762,
      "step": 544
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.07038184255361557,
      "learning_rate": 0.0019904673750269536,
      "loss": 1.1477,
      "step": 545
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.03646077215671539,
      "learning_rate": 0.001990407798609415,
      "loss": 1.0942,
      "step": 546
    },
    {
      "epoch": 0.7293333333333333,
      "grad_norm": 0.05545599013566971,
      "learning_rate": 0.0019903480374999995,
      "loss": 1.147,
      "step": 547
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.04763709753751755,
      "learning_rate": 0.0019902880917098513,
      "loss": 1.1576,
      "step": 548
    },
    {
      "epoch": 0.732,
      "grad_norm": 0.04583637788891792,
      "learning_rate": 0.001990227961250149,
      "loss": 1.0715,
      "step": 549
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.04283996298909187,
      "learning_rate": 0.0019901676461321067,
      "loss": 1.195,
      "step": 550
    },
    {
      "epoch": 0.7346666666666667,
      "grad_norm": 0.030183978378772736,
      "learning_rate": 0.001990107146366971,
      "loss": 1.2644,
      "step": 551
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.03771253675222397,
      "learning_rate": 0.001990046461966024,
      "loss": 1.105,
      "step": 552
    },
    {
      "epoch": 0.7373333333333333,
      "grad_norm": 0.03785443305969238,
      "learning_rate": 0.0019899855929405826,
      "loss": 1.2575,
      "step": 553
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 0.17935176193714142,
      "learning_rate": 0.0019899245393019977,
      "loss": 1.5944,
      "step": 554
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.05186612159013748,
      "learning_rate": 0.001989863301061654,
      "loss": 1.3438,
      "step": 555
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.04307100921869278,
      "learning_rate": 0.001989801878230972,
      "loss": 1.3061,
      "step": 556
    },
    {
      "epoch": 0.7426666666666667,
      "grad_norm": 0.03372262418270111,
      "learning_rate": 0.0019897402708214055,
      "loss": 1.0965,
      "step": 557
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.04772927612066269,
      "learning_rate": 0.0019896784788444428,
      "loss": 1.1738,
      "step": 558
    },
    {
      "epoch": 0.7453333333333333,
      "grad_norm": 0.044674813747406006,
      "learning_rate": 0.0019896165023116077,
      "loss": 1.1048,
      "step": 559
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.05813051760196686,
      "learning_rate": 0.0019895543412344566,
      "loss": 1.2387,
      "step": 560
    },
    {
      "epoch": 0.748,
      "grad_norm": 0.10255257040262222,
      "learning_rate": 0.0019894919956245827,
      "loss": 1.369,
      "step": 561
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.038473162800073624,
      "learning_rate": 0.0019894294654936107,
      "loss": 1.279,
      "step": 562
    },
    {
      "epoch": 0.7506666666666667,
      "grad_norm": 0.0671471357345581,
      "learning_rate": 0.0019893667508532023,
      "loss": 1.4185,
      "step": 563
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.04327027499675751,
      "learning_rate": 0.001989303851715052,
      "loss": 0.8439,
      "step": 564
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.04634663090109825,
      "learning_rate": 0.00198924076809089,
      "loss": 1.1487,
      "step": 565
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 0.03611162677407265,
      "learning_rate": 0.0019891774999924797,
      "loss": 1.0885,
      "step": 566
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.04367527738213539,
      "learning_rate": 0.0019891140474316196,
      "loss": 1.5282,
      "step": 567
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.04957663640379906,
      "learning_rate": 0.0019890504104201415,
      "loss": 1.2081,
      "step": 568
    },
    {
      "epoch": 0.7586666666666667,
      "grad_norm": 0.04294877499341965,
      "learning_rate": 0.0019889865889699135,
      "loss": 1.066,
      "step": 569
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.04970566928386688,
      "learning_rate": 0.0019889225830928363,
      "loss": 1.3773,
      "step": 570
    },
    {
      "epoch": 0.7613333333333333,
      "grad_norm": 0.04009956121444702,
      "learning_rate": 0.0019888583928008466,
      "loss": 1.4071,
      "step": 571
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.05686815828084946,
      "learning_rate": 0.0019887940181059142,
      "loss": 1.2829,
      "step": 572
    },
    {
      "epoch": 0.764,
      "grad_norm": 0.042250555008649826,
      "learning_rate": 0.0019887294590200436,
      "loss": 1.3649,
      "step": 573
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.04666374251246452,
      "learning_rate": 0.001988664715555274,
      "loss": 1.3635,
      "step": 574
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.03796734660863876,
      "learning_rate": 0.0019885997877236786,
      "loss": 1.2567,
      "step": 575
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.042796675115823746,
      "learning_rate": 0.0019885346755373658,
      "loss": 1.202,
      "step": 576
    },
    {
      "epoch": 0.7693333333333333,
      "grad_norm": 0.038811858743429184,
      "learning_rate": 0.001988469379008477,
      "loss": 1.1965,
      "step": 577
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.04945554956793785,
      "learning_rate": 0.001988403898149189,
      "loss": 1.2736,
      "step": 578
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.04024607688188553,
      "learning_rate": 0.0019883382329717128,
      "loss": 1.0842,
      "step": 579
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.09316089749336243,
      "learning_rate": 0.0019882723834882933,
      "loss": 1.3453,
      "step": 580
    },
    {
      "epoch": 0.7746666666666666,
      "grad_norm": 0.0477818064391613,
      "learning_rate": 0.001988206349711211,
      "loss": 1.2358,
      "step": 581
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.0430501364171505,
      "learning_rate": 0.0019881401316527796,
      "loss": 1.2793,
      "step": 582
    },
    {
      "epoch": 0.7773333333333333,
      "grad_norm": 0.04995362088084221,
      "learning_rate": 0.001988073729325347,
      "loss": 1.1499,
      "step": 583
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.0571594201028347,
      "learning_rate": 0.0019880071427412957,
      "loss": 1.3315,
      "step": 584
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.04272359237074852,
      "learning_rate": 0.001987940371913044,
      "loss": 1.1035,
      "step": 585
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 0.09237006306648254,
      "learning_rate": 0.0019878734168530428,
      "loss": 1.171,
      "step": 586
    },
    {
      "epoch": 0.7826666666666666,
      "grad_norm": 0.06027572974562645,
      "learning_rate": 0.0019878062775737777,
      "loss": 1.2347,
      "step": 587
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.07006745785474777,
      "learning_rate": 0.0019877389540877686,
      "loss": 1.2659,
      "step": 588
    },
    {
      "epoch": 0.7853333333333333,
      "grad_norm": 0.034680936485528946,
      "learning_rate": 0.001987671446407571,
      "loss": 1.31,
      "step": 589
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.03421400114893913,
      "learning_rate": 0.001987603754545773,
      "loss": 1.0262,
      "step": 590
    },
    {
      "epoch": 0.788,
      "grad_norm": 0.035117678344249725,
      "learning_rate": 0.001987535878514998,
      "loss": 1.1246,
      "step": 591
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.0343889556825161,
      "learning_rate": 0.001987467818327904,
      "loss": 1.1339,
      "step": 592
    },
    {
      "epoch": 0.7906666666666666,
      "grad_norm": 0.0352587066590786,
      "learning_rate": 0.0019873995739971818,
      "loss": 1.199,
      "step": 593
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.048659373074769974,
      "learning_rate": 0.001987331145535559,
      "loss": 1.277,
      "step": 594
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.05149490758776665,
      "learning_rate": 0.0019872625329557954,
      "loss": 1.4934,
      "step": 595
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.21961577236652374,
      "learning_rate": 0.001987193736270686,
      "loss": 1.3801,
      "step": 596
    },
    {
      "epoch": 0.796,
      "grad_norm": 0.061457935720682144,
      "learning_rate": 0.00198712475549306,
      "loss": 1.2059,
      "step": 597
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 0.07134959101676941,
      "learning_rate": 0.001987055590635781,
      "loss": 1.1302,
      "step": 598
    },
    {
      "epoch": 0.7986666666666666,
      "grad_norm": 0.04010030999779701,
      "learning_rate": 0.0019869862417117475,
      "loss": 1.0476,
      "step": 599
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.057877711951732635,
      "learning_rate": 0.0019869167087338906,
      "loss": 1.2614,
      "step": 600
    },
    {
      "epoch": 0.8013333333333333,
      "grad_norm": 0.0511474646627903,
      "learning_rate": 0.001986846991715178,
      "loss": 1.0175,
      "step": 601
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 0.07443512976169586,
      "learning_rate": 0.0019867770906686097,
      "loss": 0.989,
      "step": 602
    },
    {
      "epoch": 0.804,
      "grad_norm": 0.050948869436979294,
      "learning_rate": 0.0019867070056072216,
      "loss": 1.1243,
      "step": 603
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.08675958216190338,
      "learning_rate": 0.0019866367365440826,
      "loss": 1.3732,
      "step": 604
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.05105762556195259,
      "learning_rate": 0.001986566283492297,
      "loss": 1.456,
      "step": 605
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.03602202981710434,
      "learning_rate": 0.0019864956464650026,
      "loss": 1.0652,
      "step": 606
    },
    {
      "epoch": 0.8093333333333333,
      "grad_norm": 0.042252231389284134,
      "learning_rate": 0.0019864248254753717,
      "loss": 1.549,
      "step": 607
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.038269586861133575,
      "learning_rate": 0.0019863538205366115,
      "loss": 1.2007,
      "step": 608
    },
    {
      "epoch": 0.812,
      "grad_norm": 0.041908953338861465,
      "learning_rate": 0.001986282631661963,
      "loss": 1.5537,
      "step": 609
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.04403570666909218,
      "learning_rate": 0.0019862112588647013,
      "loss": 1.1792,
      "step": 610
    },
    {
      "epoch": 0.8146666666666667,
      "grad_norm": 0.048691533505916595,
      "learning_rate": 0.001986139702158136,
      "loss": 1.3752,
      "step": 611
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.03814046084880829,
      "learning_rate": 0.001986067961555611,
      "loss": 1.337,
      "step": 612
    },
    {
      "epoch": 0.8173333333333334,
      "grad_norm": 0.04022346809506416,
      "learning_rate": 0.001985996037070505,
      "loss": 1.1274,
      "step": 613
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.047556325793266296,
      "learning_rate": 0.00198592392871623,
      "loss": 1.3994,
      "step": 614
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.03753335401415825,
      "learning_rate": 0.0019858516365062334,
      "loss": 1.0339,
      "step": 615
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.04924526438117027,
      "learning_rate": 0.0019857791604539956,
      "loss": 1.238,
      "step": 616
    },
    {
      "epoch": 0.8226666666666667,
      "grad_norm": 0.0611661821603775,
      "learning_rate": 0.0019857065005730325,
      "loss": 1.3735,
      "step": 617
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.06356005370616913,
      "learning_rate": 0.0019856336568768933,
      "loss": 1.2529,
      "step": 618
    },
    {
      "epoch": 0.8253333333333334,
      "grad_norm": 0.040762778371572495,
      "learning_rate": 0.0019855606293791624,
      "loss": 1.0775,
      "step": 619
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.04204663261771202,
      "learning_rate": 0.001985487418093458,
      "loss": 1.2484,
      "step": 620
    },
    {
      "epoch": 0.828,
      "grad_norm": 0.043720610439777374,
      "learning_rate": 0.001985414023033432,
      "loss": 1.351,
      "step": 621
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 0.04349161311984062,
      "learning_rate": 0.001985340444212772,
      "loss": 1.2079,
      "step": 622
    },
    {
      "epoch": 0.8306666666666667,
      "grad_norm": 0.04948198050260544,
      "learning_rate": 0.0019852666816451985,
      "loss": 1.4174,
      "step": 623
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.03907691314816475,
      "learning_rate": 0.001985192735344467,
      "loss": 1.2594,
      "step": 624
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.036606643348932266,
      "learning_rate": 0.0019851186053243667,
      "loss": 1.3409,
      "step": 625
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.03999806568026543,
      "learning_rate": 0.0019850442915987213,
      "loss": 1.3813,
      "step": 626
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.05849708616733551,
      "learning_rate": 0.00198496979418139,
      "loss": 1.3855,
      "step": 627
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.030042242258787155,
      "learning_rate": 0.001984895113086264,
      "loss": 1.2365,
      "step": 628
    },
    {
      "epoch": 0.8386666666666667,
      "grad_norm": 0.0543988011777401,
      "learning_rate": 0.0019848202483272698,
      "loss": 1.3074,
      "step": 629
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.03862643614411354,
      "learning_rate": 0.001984745199918369,
      "loss": 1.3165,
      "step": 630
    },
    {
      "epoch": 0.8413333333333334,
      "grad_norm": 0.033345337957143784,
      "learning_rate": 0.0019846699678735566,
      "loss": 1.0515,
      "step": 631
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.04215315356850624,
      "learning_rate": 0.0019845945522068615,
      "loss": 1.1388,
      "step": 632
    },
    {
      "epoch": 0.844,
      "grad_norm": 0.04496956616640091,
      "learning_rate": 0.0019845189529323474,
      "loss": 1.2424,
      "step": 633
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 0.05219142884016037,
      "learning_rate": 0.001984443170064112,
      "loss": 1.2956,
      "step": 634
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 0.035296279937028885,
      "learning_rate": 0.001984367203616288,
      "loss": 1.159,
      "step": 635
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.039884909987449646,
      "learning_rate": 0.00198429105360304,
      "loss": 1.286,
      "step": 636
    },
    {
      "epoch": 0.8493333333333334,
      "grad_norm": 0.034350570291280746,
      "learning_rate": 0.0019842147200385706,
      "loss": 1.1479,
      "step": 637
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 0.03566160053014755,
      "learning_rate": 0.001984138202937113,
      "loss": 1.4377,
      "step": 638
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.04362429305911064,
      "learning_rate": 0.001984061502312937,
      "loss": 1.2657,
      "step": 639
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.04142385348677635,
      "learning_rate": 0.0019839846181803457,
      "loss": 1.1858,
      "step": 640
    },
    {
      "epoch": 0.8546666666666667,
      "grad_norm": 0.03652351722121239,
      "learning_rate": 0.001983907550553676,
      "loss": 0.834,
      "step": 641
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.040226809680461884,
      "learning_rate": 0.0019838302994472997,
      "loss": 0.955,
      "step": 642
    },
    {
      "epoch": 0.8573333333333333,
      "grad_norm": 0.030205493792891502,
      "learning_rate": 0.001983752864875623,
      "loss": 1.1313,
      "step": 643
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.042249441146850586,
      "learning_rate": 0.0019836752468530856,
      "loss": 1.1725,
      "step": 644
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.07298334687948227,
      "learning_rate": 0.0019835974453941622,
      "loss": 1.2121,
      "step": 645
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.046084094792604446,
      "learning_rate": 0.00198351946051336,
      "loss": 1.3234,
      "step": 646
    },
    {
      "epoch": 0.8626666666666667,
      "grad_norm": 0.0505850613117218,
      "learning_rate": 0.0019834412922252235,
      "loss": 1.4608,
      "step": 647
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.03963499888777733,
      "learning_rate": 0.0019833629405443284,
      "loss": 1.2604,
      "step": 648
    },
    {
      "epoch": 0.8653333333333333,
      "grad_norm": 0.0329366996884346,
      "learning_rate": 0.001983284405485286,
      "loss": 0.9693,
      "step": 649
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.04918372631072998,
      "learning_rate": 0.001983205687062742,
      "loss": 1.3107,
      "step": 650
    },
    {
      "epoch": 0.868,
      "grad_norm": 0.1054145097732544,
      "learning_rate": 0.001983126785291375,
      "loss": 1.3209,
      "step": 651
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.04114145413041115,
      "learning_rate": 0.001983047700185899,
      "loss": 0.9805,
      "step": 652
    },
    {
      "epoch": 0.8706666666666667,
      "grad_norm": 0.06032363697886467,
      "learning_rate": 0.001982968431761062,
      "loss": 1.2847,
      "step": 653
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.030844679102301598,
      "learning_rate": 0.0019828889800316465,
      "loss": 1.055,
      "step": 654
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 0.04572083055973053,
      "learning_rate": 0.0019828093450124677,
      "loss": 1.459,
      "step": 655
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.05414131283760071,
      "learning_rate": 0.0019827295267183767,
      "loss": 1.3006,
      "step": 656
    },
    {
      "epoch": 0.876,
      "grad_norm": 0.03188403695821762,
      "learning_rate": 0.001982649525164258,
      "loss": 1.1935,
      "step": 657
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.047899000346660614,
      "learning_rate": 0.00198256934036503,
      "loss": 1.357,
      "step": 658
    },
    {
      "epoch": 0.8786666666666667,
      "grad_norm": 0.03193974867463112,
      "learning_rate": 0.0019824889723356457,
      "loss": 1.2908,
      "step": 659
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.05039690062403679,
      "learning_rate": 0.0019824084210910923,
      "loss": 1.2561,
      "step": 660
    },
    {
      "epoch": 0.8813333333333333,
      "grad_norm": 0.15111958980560303,
      "learning_rate": 0.001982327686646391,
      "loss": 1.4664,
      "step": 661
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.039017800241708755,
      "learning_rate": 0.0019822467690165974,
      "loss": 0.9607,
      "step": 662
    },
    {
      "epoch": 0.884,
      "grad_norm": 0.04465925693511963,
      "learning_rate": 0.001982165668216801,
      "loss": 1.3703,
      "step": 663
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.09993806481361389,
      "learning_rate": 0.001982084384262126,
      "loss": 1.3284,
      "step": 664
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 0.04884537309408188,
      "learning_rate": 0.0019820029171677286,
      "loss": 1.3242,
      "step": 665
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.034920789301395416,
      "learning_rate": 0.0019819212669488027,
      "loss": 1.2328,
      "step": 666
    },
    {
      "epoch": 0.8893333333333333,
      "grad_norm": 0.04159178584814072,
      "learning_rate": 0.001981839433620573,
      "loss": 1.3912,
      "step": 667
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.054168060421943665,
      "learning_rate": 0.0019817574171983013,
      "loss": 1.2546,
      "step": 668
    },
    {
      "epoch": 0.892,
      "grad_norm": 0.035854071378707886,
      "learning_rate": 0.0019816752176972812,
      "loss": 1.0169,
      "step": 669
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.05354318767786026,
      "learning_rate": 0.0019815928351328413,
      "loss": 1.3071,
      "step": 670
    },
    {
      "epoch": 0.8946666666666667,
      "grad_norm": 0.03891652822494507,
      "learning_rate": 0.001981510269520345,
      "loss": 1.423,
      "step": 671
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.058840181678533554,
      "learning_rate": 0.001981427520875188,
      "loss": 1.2855,
      "step": 672
    },
    {
      "epoch": 0.8973333333333333,
      "grad_norm": 0.043568678200244904,
      "learning_rate": 0.0019813445892128026,
      "loss": 1.058,
      "step": 673
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.03873732313513756,
      "learning_rate": 0.001981261474548653,
      "loss": 1.0552,
      "step": 674
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.043549519032239914,
      "learning_rate": 0.0019811781768982392,
      "loss": 1.5924,
      "step": 675
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.04407823830842972,
      "learning_rate": 0.001981094696277094,
      "loss": 1.1208,
      "step": 676
    },
    {
      "epoch": 0.9026666666666666,
      "grad_norm": 0.040066592395305634,
      "learning_rate": 0.001981011032700785,
      "loss": 1.1359,
      "step": 677
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.042482078075408936,
      "learning_rate": 0.0019809271861849147,
      "loss": 1.2618,
      "step": 678
    },
    {
      "epoch": 0.9053333333333333,
      "grad_norm": 0.04104184731841087,
      "learning_rate": 0.0019808431567451177,
      "loss": 0.9655,
      "step": 679
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.043194301426410675,
      "learning_rate": 0.001980758944397064,
      "loss": 1.0895,
      "step": 680
    },
    {
      "epoch": 0.908,
      "grad_norm": 0.04229630529880524,
      "learning_rate": 0.0019806745491564586,
      "loss": 0.9725,
      "step": 681
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.05327345058321953,
      "learning_rate": 0.0019805899710390383,
      "loss": 1.3442,
      "step": 682
    },
    {
      "epoch": 0.9106666666666666,
      "grad_norm": 0.03620363771915436,
      "learning_rate": 0.001980505210060576,
      "loss": 1.3453,
      "step": 683
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.04740983992815018,
      "learning_rate": 0.001980420266236878,
      "loss": 1.1521,
      "step": 684
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 0.03394995257258415,
      "learning_rate": 0.001980335139583785,
      "loss": 1.2801,
      "step": 685
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.03685692697763443,
      "learning_rate": 0.00198024983011717,
      "loss": 1.1639,
      "step": 686
    },
    {
      "epoch": 0.916,
      "grad_norm": 0.038006071001291275,
      "learning_rate": 0.001980164337852943,
      "loss": 1.3461,
      "step": 687
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.03821340203285217,
      "learning_rate": 0.0019800786628070464,
      "loss": 1.2299,
      "step": 688
    },
    {
      "epoch": 0.9186666666666666,
      "grad_norm": 0.028552325442433357,
      "learning_rate": 0.0019799928049954564,
      "loss": 1.1825,
      "step": 689
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.03532811999320984,
      "learning_rate": 0.0019799067644341844,
      "loss": 1.5049,
      "step": 690
    },
    {
      "epoch": 0.9213333333333333,
      "grad_norm": 0.1984838843345642,
      "learning_rate": 0.001979820541139275,
      "loss": 1.0872,
      "step": 691
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.05993962287902832,
      "learning_rate": 0.0019797341351268072,
      "loss": 1.2646,
      "step": 692
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.0827043280005455,
      "learning_rate": 0.001979647546412894,
      "loss": 1.415,
      "step": 693
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.03462410345673561,
      "learning_rate": 0.001979560775013683,
      "loss": 1.1991,
      "step": 694
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 0.04694166034460068,
      "learning_rate": 0.0019794738209453545,
      "loss": 1.0894,
      "step": 695
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.10221926867961884,
      "learning_rate": 0.0019793866842241245,
      "loss": 1.3328,
      "step": 696
    },
    {
      "epoch": 0.9293333333333333,
      "grad_norm": 0.040791817009449005,
      "learning_rate": 0.0019792993648662417,
      "loss": 1.0221,
      "step": 697
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 0.04611221328377724,
      "learning_rate": 0.0019792118628879905,
      "loss": 1.0564,
      "step": 698
    },
    {
      "epoch": 0.932,
      "grad_norm": 0.0477466955780983,
      "learning_rate": 0.001979124178305687,
      "loss": 1.1124,
      "step": 699
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.04596417024731636,
      "learning_rate": 0.0019790363111356836,
      "loss": 1.2323,
      "step": 700
    },
    {
      "epoch": 0.9346666666666666,
      "grad_norm": 0.029860856011509895,
      "learning_rate": 0.0019789482613943657,
      "loss": 0.8308,
      "step": 701
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.039923761039972305,
      "learning_rate": 0.0019788600290981525,
      "loss": 1.1521,
      "step": 702
    },
    {
      "epoch": 0.9373333333333334,
      "grad_norm": 0.04907752200961113,
      "learning_rate": 0.001978771614263498,
      "loss": 0.9647,
      "step": 703
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.036198344081640244,
      "learning_rate": 0.0019786830169068893,
      "loss": 1.5854,
      "step": 704
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.051110878586769104,
      "learning_rate": 0.0019785942370448488,
      "loss": 1.2361,
      "step": 705
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.050387296825647354,
      "learning_rate": 0.001978505274693932,
      "loss": 1.2785,
      "step": 706
    },
    {
      "epoch": 0.9426666666666667,
      "grad_norm": 0.03628532588481903,
      "learning_rate": 0.001978416129870728,
      "loss": 1.0033,
      "step": 707
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.04510756582021713,
      "learning_rate": 0.0019783268025918618,
      "loss": 1.1307,
      "step": 708
    },
    {
      "epoch": 0.9453333333333334,
      "grad_norm": 0.05902193859219551,
      "learning_rate": 0.0019782372928739906,
      "loss": 1.4857,
      "step": 709
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.02775685489177704,
      "learning_rate": 0.0019781476007338056,
      "loss": 1.1896,
      "step": 710
    },
    {
      "epoch": 0.948,
      "grad_norm": 0.049294859170913696,
      "learning_rate": 0.0019780577261880334,
      "loss": 1.1861,
      "step": 711
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.04747086763381958,
      "learning_rate": 0.001977967669253434,
      "loss": 1.446,
      "step": 712
    },
    {
      "epoch": 0.9506666666666667,
      "grad_norm": 0.040599673986434937,
      "learning_rate": 0.0019778774299468007,
      "loss": 1.3227,
      "step": 713
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.061495207250118256,
      "learning_rate": 0.0019777870082849616,
      "loss": 1.276,
      "step": 714
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 0.04132350534200668,
      "learning_rate": 0.001977696404284779,
      "loss": 1.0075,
      "step": 715
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.0538661926984787,
      "learning_rate": 0.0019776056179631484,
      "loss": 1.2371,
      "step": 716
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.04239802062511444,
      "learning_rate": 0.001977514649336999,
      "loss": 1.3505,
      "step": 717
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.03769887238740921,
      "learning_rate": 0.0019774234984232964,
      "loss": 1.1395,
      "step": 718
    },
    {
      "epoch": 0.9586666666666667,
      "grad_norm": 0.05681364983320236,
      "learning_rate": 0.001977332165239037,
      "loss": 1.5656,
      "step": 719
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.05129548907279968,
      "learning_rate": 0.001977240649801253,
      "loss": 1.6277,
      "step": 720
    },
    {
      "epoch": 0.9613333333333334,
      "grad_norm": 0.2924656569957733,
      "learning_rate": 0.0019771489521270107,
      "loss": 1.407,
      "step": 721
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 0.0376729890704155,
      "learning_rate": 0.0019770570722334093,
      "loss": 1.4534,
      "step": 722
    },
    {
      "epoch": 0.964,
      "grad_norm": 0.05404624715447426,
      "learning_rate": 0.0019769650101375837,
      "loss": 0.9892,
      "step": 723
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 0.16851240396499634,
      "learning_rate": 0.0019768727658567003,
      "loss": 1.3257,
      "step": 724
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.06689499318599701,
      "learning_rate": 0.0019767803394079614,
      "loss": 1.1741,
      "step": 725
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.060719843953847885,
      "learning_rate": 0.0019766877308086037,
      "loss": 1.0683,
      "step": 726
    },
    {
      "epoch": 0.9693333333333334,
      "grad_norm": 0.048348911106586456,
      "learning_rate": 0.0019765949400758955,
      "loss": 1.225,
      "step": 727
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.17444469034671783,
      "learning_rate": 0.0019765019672271416,
      "loss": 1.3784,
      "step": 728
    },
    {
      "epoch": 0.972,
      "grad_norm": 0.04657571762800217,
      "learning_rate": 0.0019764088122796782,
      "loss": 1.2849,
      "step": 729
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.05217059329152107,
      "learning_rate": 0.0019763154752508783,
      "loss": 1.2152,
      "step": 730
    },
    {
      "epoch": 0.9746666666666667,
      "grad_norm": 0.09551148116588593,
      "learning_rate": 0.0019762219561581472,
      "loss": 0.9818,
      "step": 731
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.042320579290390015,
      "learning_rate": 0.001976128255018924,
      "loss": 1.5143,
      "step": 732
    },
    {
      "epoch": 0.9773333333333334,
      "grad_norm": 0.043850623071193695,
      "learning_rate": 0.001976034371850682,
      "loss": 1.0459,
      "step": 733
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.06457269191741943,
      "learning_rate": 0.0019759403066709294,
      "loss": 1.1412,
      "step": 734
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.05029945820569992,
      "learning_rate": 0.001975846059497207,
      "loss": 1.2979,
      "step": 735
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.036278240382671356,
      "learning_rate": 0.0019757516303470896,
      "loss": 1.3369,
      "step": 736
    },
    {
      "epoch": 0.9826666666666667,
      "grad_norm": 0.050299856811761856,
      "learning_rate": 0.0019756570192381875,
      "loss": 1.1386,
      "step": 737
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.057707980275154114,
      "learning_rate": 0.0019755622261881426,
      "loss": 1.0933,
      "step": 738
    },
    {
      "epoch": 0.9853333333333333,
      "grad_norm": 0.05284956470131874,
      "learning_rate": 0.001975467251214633,
      "loss": 1.2255,
      "step": 739
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.030747197568416595,
      "learning_rate": 0.0019753720943353694,
      "loss": 1.1293,
      "step": 740
    },
    {
      "epoch": 0.988,
      "grad_norm": 0.038681916892528534,
      "learning_rate": 0.0019752767555680966,
      "loss": 1.2917,
      "step": 741
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 0.03715824335813522,
      "learning_rate": 0.001975181234930593,
      "loss": 1.1178,
      "step": 742
    },
    {
      "epoch": 0.9906666666666667,
      "grad_norm": 0.04513728618621826,
      "learning_rate": 0.0019750855324406724,
      "loss": 1.1234,
      "step": 743
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.05069280043244362,
      "learning_rate": 0.001974989648116181,
      "loss": 1.4125,
      "step": 744
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 0.03868097439408302,
      "learning_rate": 0.0019748935819749987,
      "loss": 1.2644,
      "step": 745
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 0.05064034461975098,
      "learning_rate": 0.001974797334035041,
      "loss": 1.2755,
      "step": 746
    },
    {
      "epoch": 0.996,
      "grad_norm": 0.052486125379800797,
      "learning_rate": 0.0019747009043142552,
      "loss": 1.2067,
      "step": 747
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.0350789949297905,
      "learning_rate": 0.0019746042928306244,
      "loss": 1.1133,
      "step": 748
    },
    {
      "epoch": 0.9986666666666667,
      "grad_norm": 0.061461661010980606,
      "learning_rate": 0.0019745074996021647,
      "loss": 1.1749,
      "step": 749
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.04764922335743904,
      "learning_rate": 0.001974410524646926,
      "loss": 1.0886,
      "step": 750
    },
    {
      "epoch": 1.0013333333333334,
      "grad_norm": 0.05492081493139267,
      "learning_rate": 0.0019743133679829923,
      "loss": 1.1155,
      "step": 751
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 0.028692753985524178,
      "learning_rate": 0.001974216029628481,
      "loss": 1.0918,
      "step": 752
    },
    {
      "epoch": 1.004,
      "grad_norm": 0.045952919870615005,
      "learning_rate": 0.001974118509601545,
      "loss": 1.3218,
      "step": 753
    },
    {
      "epoch": 1.0053333333333334,
      "grad_norm": 0.03732681646943092,
      "learning_rate": 0.001974020807920368,
      "loss": 1.2281,
      "step": 754
    },
    {
      "epoch": 1.0066666666666666,
      "grad_norm": 0.039693862199783325,
      "learning_rate": 0.0019739229246031717,
      "loss": 0.9007,
      "step": 755
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.03235549479722977,
      "learning_rate": 0.0019738248596682076,
      "loss": 0.9669,
      "step": 756
    },
    {
      "epoch": 1.0093333333333334,
      "grad_norm": 0.028382059186697006,
      "learning_rate": 0.0019737266131337637,
      "loss": 1.3491,
      "step": 757
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.04543567821383476,
      "learning_rate": 0.0019736281850181612,
      "loss": 1.3708,
      "step": 758
    },
    {
      "epoch": 1.012,
      "grad_norm": 0.035272035747766495,
      "learning_rate": 0.001973529575339755,
      "loss": 1.0629,
      "step": 759
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.032112959772348404,
      "learning_rate": 0.0019734307841169337,
      "loss": 1.1489,
      "step": 760
    },
    {
      "epoch": 1.0146666666666666,
      "grad_norm": 0.04895009100437164,
      "learning_rate": 0.00197333181136812,
      "loss": 1.1928,
      "step": 761
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.033490847796201706,
      "learning_rate": 0.00197323265711177,
      "loss": 1.2498,
      "step": 762
    },
    {
      "epoch": 1.0173333333333334,
      "grad_norm": 0.03367787227034569,
      "learning_rate": 0.0019731333213663747,
      "loss": 1.2191,
      "step": 763
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 0.030910920351743698,
      "learning_rate": 0.0019730338041504584,
      "loss": 1.2605,
      "step": 764
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.055230170488357544,
      "learning_rate": 0.0019729341054825784,
      "loss": 1.3101,
      "step": 765
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.04118988662958145,
      "learning_rate": 0.0019728342253813266,
      "loss": 1.0732,
      "step": 766
    },
    {
      "epoch": 1.0226666666666666,
      "grad_norm": 0.0421115942299366,
      "learning_rate": 0.0019727341638653296,
      "loss": 1.4456,
      "step": 767
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.044679395854473114,
      "learning_rate": 0.001972633920953246,
      "loss": 1.351,
      "step": 768
    },
    {
      "epoch": 1.0253333333333334,
      "grad_norm": 0.04334598407149315,
      "learning_rate": 0.00197253349666377,
      "loss": 1.3485,
      "step": 769
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.0557689443230629,
      "learning_rate": 0.001972432891015628,
      "loss": 1.1754,
      "step": 770
    },
    {
      "epoch": 1.028,
      "grad_norm": 0.054303061217069626,
      "learning_rate": 0.0019723321040275815,
      "loss": 1.1276,
      "step": 771
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 0.04731139540672302,
      "learning_rate": 0.001972231135718425,
      "loss": 1.0513,
      "step": 772
    },
    {
      "epoch": 1.0306666666666666,
      "grad_norm": 0.03875308111310005,
      "learning_rate": 0.0019721299861069873,
      "loss": 1.301,
      "step": 773
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.03325248509645462,
      "learning_rate": 0.001972028655212131,
      "loss": 1.0212,
      "step": 774
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.19474375247955322,
      "learning_rate": 0.001971927143052752,
      "loss": 1.3438,
      "step": 775
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 0.04760857671499252,
      "learning_rate": 0.0019718254496477803,
      "loss": 1.2521,
      "step": 776
    },
    {
      "epoch": 1.036,
      "grad_norm": 0.04879898205399513,
      "learning_rate": 0.001971723575016181,
      "loss": 1.0142,
      "step": 777
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 0.04724428802728653,
      "learning_rate": 0.00197162151917695,
      "loss": 1.3684,
      "step": 778
    },
    {
      "epoch": 1.0386666666666666,
      "grad_norm": 0.03401247039437294,
      "learning_rate": 0.001971519282149119,
      "loss": 1.1758,
      "step": 779
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.08395465463399887,
      "learning_rate": 0.001971416863951754,
      "loss": 1.4199,
      "step": 780
    },
    {
      "epoch": 1.0413333333333332,
      "grad_norm": 0.06653392314910889,
      "learning_rate": 0.0019713142646039543,
      "loss": 1.147,
      "step": 781
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.043024055659770966,
      "learning_rate": 0.001971211484124852,
      "loss": 1.143,
      "step": 782
    },
    {
      "epoch": 1.044,
      "grad_norm": 0.12867620587348938,
      "learning_rate": 0.0019711085225336132,
      "loss": 1.3113,
      "step": 783
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 0.05742860957980156,
      "learning_rate": 0.001971005379849439,
      "loss": 1.1678,
      "step": 784
    },
    {
      "epoch": 1.0466666666666666,
      "grad_norm": 0.043350595980882645,
      "learning_rate": 0.0019709020560915638,
      "loss": 1.4979,
      "step": 785
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.06436274945735931,
      "learning_rate": 0.001970798551279254,
      "loss": 1.2417,
      "step": 786
    },
    {
      "epoch": 1.0493333333333332,
      "grad_norm": 0.0517854206264019,
      "learning_rate": 0.0019706948654318133,
      "loss": 1.0394,
      "step": 787
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 0.0455281138420105,
      "learning_rate": 0.0019705909985685754,
      "loss": 1.17,
      "step": 788
    },
    {
      "epoch": 1.052,
      "grad_norm": 0.03233114257454872,
      "learning_rate": 0.0019704869507089105,
      "loss": 1.2047,
      "step": 789
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.11199016124010086,
      "learning_rate": 0.001970382721872221,
      "loss": 1.0844,
      "step": 790
    },
    {
      "epoch": 1.0546666666666666,
      "grad_norm": 0.046638764441013336,
      "learning_rate": 0.0019702783120779436,
      "loss": 1.3331,
      "step": 791
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.06628764420747757,
      "learning_rate": 0.0019701737213455492,
      "loss": 1.1167,
      "step": 792
    },
    {
      "epoch": 1.0573333333333332,
      "grad_norm": 0.04928062856197357,
      "learning_rate": 0.0019700689496945406,
      "loss": 1.2975,
      "step": 793
    },
    {
      "epoch": 1.0586666666666666,
      "grad_norm": 0.03600689396262169,
      "learning_rate": 0.0019699639971444576,
      "loss": 1.2342,
      "step": 794
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.044804465025663376,
      "learning_rate": 0.00196985886371487,
      "loss": 1.2783,
      "step": 795
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 0.046508245170116425,
      "learning_rate": 0.001969753549425385,
      "loss": 1.1335,
      "step": 796
    },
    {
      "epoch": 1.0626666666666666,
      "grad_norm": 0.04667240008711815,
      "learning_rate": 0.0019696480542956397,
      "loss": 1.0865,
      "step": 797
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.06626936048269272,
      "learning_rate": 0.0019695423783453085,
      "loss": 1.221,
      "step": 798
    },
    {
      "epoch": 1.0653333333333332,
      "grad_norm": 0.037824466824531555,
      "learning_rate": 0.0019694365215940967,
      "loss": 1.3444,
      "step": 799
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.08075495809316635,
      "learning_rate": 0.0019693304840617456,
      "loss": 1.5071,
      "step": 800
    },
    {
      "epoch": 1.068,
      "grad_norm": 0.041821498423814774,
      "learning_rate": 0.0019692242657680286,
      "loss": 1.2076,
      "step": 801
    },
    {
      "epoch": 1.0693333333333332,
      "grad_norm": 0.042703695595264435,
      "learning_rate": 0.0019691178667327535,
      "loss": 1.2563,
      "step": 802
    },
    {
      "epoch": 1.0706666666666667,
      "grad_norm": 0.04531577229499817,
      "learning_rate": 0.001969011286975761,
      "loss": 1.3219,
      "step": 803
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.05538894236087799,
      "learning_rate": 0.0019689045265169273,
      "loss": 1.1357,
      "step": 804
    },
    {
      "epoch": 1.0733333333333333,
      "grad_norm": 0.033032774925231934,
      "learning_rate": 0.0019687975853761603,
      "loss": 1.2057,
      "step": 805
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.03953256085515022,
      "learning_rate": 0.0019686904635734027,
      "loss": 1.1293,
      "step": 806
    },
    {
      "epoch": 1.076,
      "grad_norm": 0.03795613721013069,
      "learning_rate": 0.001968583161128631,
      "loss": 1.1765,
      "step": 807
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 0.04781962186098099,
      "learning_rate": 0.001968475678061855,
      "loss": 1.1039,
      "step": 808
    },
    {
      "epoch": 1.0786666666666667,
      "grad_norm": 0.04316357895731926,
      "learning_rate": 0.001968368014393117,
      "loss": 1.2861,
      "step": 809
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.029855255037546158,
      "learning_rate": 0.001968260170142496,
      "loss": 1.2248,
      "step": 810
    },
    {
      "epoch": 1.0813333333333333,
      "grad_norm": 0.033655691891908646,
      "learning_rate": 0.0019681521453301016,
      "loss": 1.2381,
      "step": 811
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 0.035517774522304535,
      "learning_rate": 0.0019680439399760784,
      "loss": 1.0373,
      "step": 812
    },
    {
      "epoch": 1.084,
      "grad_norm": 0.04572390392422676,
      "learning_rate": 0.0019679355541006053,
      "loss": 0.9502,
      "step": 813
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.04740644246339798,
      "learning_rate": 0.0019678269877238938,
      "loss": 0.9651,
      "step": 814
    },
    {
      "epoch": 1.0866666666666667,
      "grad_norm": 0.040754277259111404,
      "learning_rate": 0.0019677182408661892,
      "loss": 1.3105,
      "step": 815
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.034917715936899185,
      "learning_rate": 0.0019676093135477715,
      "loss": 1.3288,
      "step": 816
    },
    {
      "epoch": 1.0893333333333333,
      "grad_norm": 0.03089931793510914,
      "learning_rate": 0.0019675002057889523,
      "loss": 1.3395,
      "step": 817
    },
    {
      "epoch": 1.0906666666666667,
      "grad_norm": 0.031500186771154404,
      "learning_rate": 0.0019673909176100785,
      "loss": 1.0815,
      "step": 818
    },
    {
      "epoch": 1.092,
      "grad_norm": 0.041324492543935776,
      "learning_rate": 0.001967281449031531,
      "loss": 1.3037,
      "step": 819
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.03816927224397659,
      "learning_rate": 0.0019671718000737227,
      "loss": 1.4001,
      "step": 820
    },
    {
      "epoch": 1.0946666666666667,
      "grad_norm": 0.0363711453974247,
      "learning_rate": 0.0019670619707571014,
      "loss": 1.304,
      "step": 821
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.03685825318098068,
      "learning_rate": 0.0019669519611021486,
      "loss": 1.2014,
      "step": 822
    },
    {
      "epoch": 1.0973333333333333,
      "grad_norm": 0.05055256560444832,
      "learning_rate": 0.001966841771129378,
      "loss": 1.1765,
      "step": 823
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 0.062012407928705215,
      "learning_rate": 0.001966731400859338,
      "loss": 1.0428,
      "step": 824
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.04109656810760498,
      "learning_rate": 0.001966620850312611,
      "loss": 1.3648,
      "step": 825
    },
    {
      "epoch": 1.1013333333333333,
      "grad_norm": 0.08261837810277939,
      "learning_rate": 0.001966510119509813,
      "loss": 1.3917,
      "step": 826
    },
    {
      "epoch": 1.1026666666666667,
      "grad_norm": 0.04505375772714615,
      "learning_rate": 0.0019663992084715917,
      "loss": 1.5472,
      "step": 827
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.03917964547872543,
      "learning_rate": 0.001966288117218631,
      "loss": 1.1403,
      "step": 828
    },
    {
      "epoch": 1.1053333333333333,
      "grad_norm": 0.054943814873695374,
      "learning_rate": 0.001966176845771647,
      "loss": 1.3214,
      "step": 829
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.03743620216846466,
      "learning_rate": 0.00196606539415139,
      "loss": 1.124,
      "step": 830
    },
    {
      "epoch": 1.108,
      "grad_norm": 0.03882941976189613,
      "learning_rate": 0.0019659537623786427,
      "loss": 1.0724,
      "step": 831
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 0.0449087917804718,
      "learning_rate": 0.001965841950474223,
      "loss": 1.1441,
      "step": 832
    },
    {
      "epoch": 1.1106666666666667,
      "grad_norm": 0.03611493483185768,
      "learning_rate": 0.0019657299584589817,
      "loss": 1.2189,
      "step": 833
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.21207331120967865,
      "learning_rate": 0.0019656177863538026,
      "loss": 1.076,
      "step": 834
    },
    {
      "epoch": 1.1133333333333333,
      "grad_norm": 0.0416511632502079,
      "learning_rate": 0.0019655054341796035,
      "loss": 1.3412,
      "step": 835
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 0.046583060175180435,
      "learning_rate": 0.0019653929019573368,
      "loss": 1.3777,
      "step": 836
    },
    {
      "epoch": 1.116,
      "grad_norm": 0.03714112937450409,
      "learning_rate": 0.001965280189707987,
      "loss": 1.0972,
      "step": 837
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 0.04214714094996452,
      "learning_rate": 0.0019651672974525724,
      "loss": 1.0115,
      "step": 838
    },
    {
      "epoch": 1.1186666666666667,
      "grad_norm": 0.04557332769036293,
      "learning_rate": 0.001965054225212146,
      "loss": 1.1312,
      "step": 839
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.05759608373045921,
      "learning_rate": 0.0019649409730077933,
      "loss": 1.1488,
      "step": 840
    },
    {
      "epoch": 1.1213333333333333,
      "grad_norm": 0.040716059505939484,
      "learning_rate": 0.0019648275408606338,
      "loss": 0.9685,
      "step": 841
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 0.15011951327323914,
      "learning_rate": 0.00196471392879182,
      "loss": 1.3555,
      "step": 842
    },
    {
      "epoch": 1.124,
      "grad_norm": 0.029683878645300865,
      "learning_rate": 0.001964600136822538,
      "loss": 1.1814,
      "step": 843
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 0.033196765929460526,
      "learning_rate": 0.0019644861649740085,
      "loss": 1.3367,
      "step": 844
    },
    {
      "epoch": 1.1266666666666667,
      "grad_norm": 0.04839568957686424,
      "learning_rate": 0.0019643720132674855,
      "loss": 1.2071,
      "step": 845
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.04349754750728607,
      "learning_rate": 0.001964257681724255,
      "loss": 1.0548,
      "step": 846
    },
    {
      "epoch": 1.1293333333333333,
      "grad_norm": 0.038985058665275574,
      "learning_rate": 0.001964143170365638,
      "loss": 1.1327,
      "step": 847
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 0.04016222059726715,
      "learning_rate": 0.0019640284792129888,
      "loss": 1.2203,
      "step": 848
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 0.0388663075864315,
      "learning_rate": 0.0019639136082876952,
      "loss": 1.0207,
      "step": 849
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.05538376420736313,
      "learning_rate": 0.001963798557611178,
      "loss": 1.2075,
      "step": 850
    },
    {
      "epoch": 1.1346666666666667,
      "grad_norm": 0.05111375451087952,
      "learning_rate": 0.001963683327204892,
      "loss": 1.2315,
      "step": 851
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.07650972157716751,
      "learning_rate": 0.001963567917090326,
      "loss": 1.5605,
      "step": 852
    },
    {
      "epoch": 1.1373333333333333,
      "grad_norm": 0.034140679985284805,
      "learning_rate": 0.001963452327289001,
      "loss": 0.9406,
      "step": 853
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.06674176454544067,
      "learning_rate": 0.0019633365578224727,
      "loss": 1.097,
      "step": 854
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.043116141110658646,
      "learning_rate": 0.0019632206087123296,
      "loss": 1.2446,
      "step": 855
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 0.042417071759700775,
      "learning_rate": 0.0019631044799801943,
      "loss": 1.3021,
      "step": 856
    },
    {
      "epoch": 1.1426666666666667,
      "grad_norm": 0.29406219720840454,
      "learning_rate": 0.0019629881716477222,
      "loss": 1.6659,
      "step": 857
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.040381766855716705,
      "learning_rate": 0.0019628716837366026,
      "loss": 1.0983,
      "step": 858
    },
    {
      "epoch": 1.1453333333333333,
      "grad_norm": 0.0932982787489891,
      "learning_rate": 0.0019627550162685586,
      "loss": 1.1384,
      "step": 859
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.1289646178483963,
      "learning_rate": 0.001962638169265346,
      "loss": 1.2801,
      "step": 860
    },
    {
      "epoch": 1.148,
      "grad_norm": 0.06896897405385971,
      "learning_rate": 0.0019625211427487547,
      "loss": 1.2081,
      "step": 861
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 0.04520745202898979,
      "learning_rate": 0.001962403936740608,
      "loss": 1.3,
      "step": 862
    },
    {
      "epoch": 1.1506666666666667,
      "grad_norm": 0.057265505194664,
      "learning_rate": 0.001962286551262762,
      "loss": 1.1457,
      "step": 863
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.06256968528032303,
      "learning_rate": 0.001962168986337108,
      "loss": 1.2759,
      "step": 864
    },
    {
      "epoch": 1.1533333333333333,
      "grad_norm": 0.23656372725963593,
      "learning_rate": 0.001962051241985568,
      "loss": 1.2796,
      "step": 865
    },
    {
      "epoch": 1.1546666666666667,
      "grad_norm": 0.03691737353801727,
      "learning_rate": 0.0019619333182301006,
      "loss": 1.1457,
      "step": 866
    },
    {
      "epoch": 1.156,
      "grad_norm": 0.04464427009224892,
      "learning_rate": 0.0019618152150926954,
      "loss": 1.2173,
      "step": 867
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 0.2768774926662445,
      "learning_rate": 0.0019616969325953765,
      "loss": 1.1512,
      "step": 868
    },
    {
      "epoch": 1.1586666666666667,
      "grad_norm": 0.12546859681606293,
      "learning_rate": 0.001961578470760201,
      "loss": 1.3206,
      "step": 869
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.04840635880827904,
      "learning_rate": 0.0019614598296092602,
      "loss": 1.0681,
      "step": 870
    },
    {
      "epoch": 1.1613333333333333,
      "grad_norm": 0.03620665520429611,
      "learning_rate": 0.001961341009164678,
      "loss": 0.9532,
      "step": 871
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 0.050271548330783844,
      "learning_rate": 0.0019612220094486123,
      "loss": 1.1742,
      "step": 872
    },
    {
      "epoch": 1.164,
      "grad_norm": 0.05934023857116699,
      "learning_rate": 0.0019611028304832544,
      "loss": 1.145,
      "step": 873
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 0.11515268683433533,
      "learning_rate": 0.001960983472290829,
      "loss": 1.4425,
      "step": 874
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.07152783125638962,
      "learning_rate": 0.0019608639348935937,
      "loss": 1.2687,
      "step": 875
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.041697029024362564,
      "learning_rate": 0.00196074421831384,
      "loss": 1.3197,
      "step": 876
    },
    {
      "epoch": 1.1693333333333333,
      "grad_norm": 0.03429299220442772,
      "learning_rate": 0.0019606243225738927,
      "loss": 1.1547,
      "step": 877
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.044424984604120255,
      "learning_rate": 0.00196050424769611,
      "loss": 1.1934,
      "step": 878
    },
    {
      "epoch": 1.172,
      "grad_norm": 0.37057796120643616,
      "learning_rate": 0.001960383993702884,
      "loss": 1.076,
      "step": 879
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.037166111171245575,
      "learning_rate": 0.001960263560616639,
      "loss": 1.2215,
      "step": 880
    },
    {
      "epoch": 1.1746666666666667,
      "grad_norm": 0.11643952131271362,
      "learning_rate": 0.001960142948459834,
      "loss": 1.2489,
      "step": 881
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.338459849357605,
      "learning_rate": 0.0019600221572549604,
      "loss": 1.1654,
      "step": 882
    },
    {
      "epoch": 1.1773333333333333,
      "grad_norm": 0.04853470250964165,
      "learning_rate": 0.0019599011870245443,
      "loss": 1.1954,
      "step": 883
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 0.04907139390707016,
      "learning_rate": 0.001959780037791143,
      "loss": 1.3119,
      "step": 884
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.041177451610565186,
      "learning_rate": 0.0019596587095773495,
      "loss": 1.1804,
      "step": 885
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 0.07074406743049622,
      "learning_rate": 0.0019595372024057887,
      "loss": 1.1682,
      "step": 886
    },
    {
      "epoch": 1.1826666666666668,
      "grad_norm": 0.21728043258190155,
      "learning_rate": 0.0019594155162991193,
      "loss": 1.0897,
      "step": 887
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.20473513007164001,
      "learning_rate": 0.001959293651280034,
      "loss": 1.1523,
      "step": 888
    },
    {
      "epoch": 1.1853333333333333,
      "grad_norm": 0.07304471731185913,
      "learning_rate": 0.0019591716073712575,
      "loss": 1.0748,
      "step": 889
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.22039680182933807,
      "learning_rate": 0.001959049384595549,
      "loss": 1.0558,
      "step": 890
    },
    {
      "epoch": 1.188,
      "grad_norm": 0.040471937507390976,
      "learning_rate": 0.001958926982975701,
      "loss": 1.1499,
      "step": 891
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 0.0521916039288044,
      "learning_rate": 0.001958804402534538,
      "loss": 1.1655,
      "step": 892
    },
    {
      "epoch": 1.1906666666666668,
      "grad_norm": 0.07169001549482346,
      "learning_rate": 0.00195868164329492,
      "loss": 1.2261,
      "step": 893
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.04304784536361694,
      "learning_rate": 0.0019585587052797387,
      "loss": 1.174,
      "step": 894
    },
    {
      "epoch": 1.1933333333333334,
      "grad_norm": 0.034983307123184204,
      "learning_rate": 0.0019584355885119194,
      "loss": 1.2627,
      "step": 895
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 0.04538155719637871,
      "learning_rate": 0.001958312293014422,
      "loss": 0.977,
      "step": 896
    },
    {
      "epoch": 1.196,
      "grad_norm": 0.038281772285699844,
      "learning_rate": 0.0019581888188102375,
      "loss": 1.1638,
      "step": 897
    },
    {
      "epoch": 1.1973333333333334,
      "grad_norm": 0.04049857705831528,
      "learning_rate": 0.0019580651659223923,
      "loss": 1.1048,
      "step": 898
    },
    {
      "epoch": 1.1986666666666665,
      "grad_norm": 0.05089913681149483,
      "learning_rate": 0.0019579413343739447,
      "loss": 1.1091,
      "step": 899
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.032131582498550415,
      "learning_rate": 0.001957817324187987,
      "loss": 1.1479,
      "step": 900
    },
    {
      "epoch": 1.2013333333333334,
      "grad_norm": 0.04219766706228256,
      "learning_rate": 0.0019576931353876455,
      "loss": 1.0884,
      "step": 901
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.034969180822372437,
      "learning_rate": 0.0019575687679960776,
      "loss": 1.3627,
      "step": 902
    },
    {
      "epoch": 1.204,
      "grad_norm": 0.032337967306375504,
      "learning_rate": 0.0019574442220364765,
      "loss": 0.9053,
      "step": 903
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 0.03637808933854103,
      "learning_rate": 0.001957319497532067,
      "loss": 1.3538,
      "step": 904
    },
    {
      "epoch": 1.2066666666666666,
      "grad_norm": 0.039203744381666183,
      "learning_rate": 0.0019571945945061086,
      "loss": 1.2707,
      "step": 905
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.036725353449583054,
      "learning_rate": 0.0019570695129818927,
      "loss": 1.07,
      "step": 906
    },
    {
      "epoch": 1.2093333333333334,
      "grad_norm": 0.03589681163430214,
      "learning_rate": 0.0019569442529827445,
      "loss": 1.0078,
      "step": 907
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 0.052406907081604004,
      "learning_rate": 0.0019568188145320225,
      "loss": 1.4642,
      "step": 908
    },
    {
      "epoch": 1.212,
      "grad_norm": 0.030685564503073692,
      "learning_rate": 0.001956693197653119,
      "loss": 1.1603,
      "step": 909
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.08401060104370117,
      "learning_rate": 0.0019565674023694587,
      "loss": 1.2702,
      "step": 910
    },
    {
      "epoch": 1.2146666666666666,
      "grad_norm": 0.0396094024181366,
      "learning_rate": 0.0019564414287045005,
      "loss": 1.0741,
      "step": 911
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.053863316774368286,
      "learning_rate": 0.0019563152766817354,
      "loss": 0.7975,
      "step": 912
    },
    {
      "epoch": 1.2173333333333334,
      "grad_norm": 0.043405681848526,
      "learning_rate": 0.0019561889463246885,
      "loss": 1.1493,
      "step": 913
    },
    {
      "epoch": 1.2186666666666666,
      "grad_norm": 0.02939494140446186,
      "learning_rate": 0.0019560624376569187,
      "loss": 1.1243,
      "step": 914
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.043171703815460205,
      "learning_rate": 0.001955935750702016,
      "loss": 1.1387,
      "step": 915
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 0.03859351575374603,
      "learning_rate": 0.0019558088854836064,
      "loss": 0.8999,
      "step": 916
    },
    {
      "epoch": 1.2226666666666666,
      "grad_norm": 0.04007256403565407,
      "learning_rate": 0.001955681842025347,
      "loss": 1.2114,
      "step": 917
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.030193137004971504,
      "learning_rate": 0.0019555546203509295,
      "loss": 1.3066,
      "step": 918
    },
    {
      "epoch": 1.2253333333333334,
      "grad_norm": 0.043922893702983856,
      "learning_rate": 0.0019554272204840776,
      "loss": 0.9868,
      "step": 919
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.028725994750857353,
      "learning_rate": 0.00195529964244855,
      "loss": 1.2071,
      "step": 920
    },
    {
      "epoch": 1.228,
      "grad_norm": 0.037407130002975464,
      "learning_rate": 0.001955171886268136,
      "loss": 1.2809,
      "step": 921
    },
    {
      "epoch": 1.2293333333333334,
      "grad_norm": 0.03820015862584114,
      "learning_rate": 0.001955043951966661,
      "loss": 1.3424,
      "step": 922
    },
    {
      "epoch": 1.2306666666666666,
      "grad_norm": 0.026921585202217102,
      "learning_rate": 0.0019549158395679818,
      "loss": 0.9847,
      "step": 923
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.06583460420370102,
      "learning_rate": 0.0019547875490959882,
      "loss": 1.3747,
      "step": 924
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 0.032380588352680206,
      "learning_rate": 0.0019546590805746052,
      "loss": 0.9501,
      "step": 925
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.058085694909095764,
      "learning_rate": 0.001954530434027789,
      "loss": 1.2671,
      "step": 926
    },
    {
      "epoch": 1.236,
      "grad_norm": 0.025331571698188782,
      "learning_rate": 0.0019544016094795295,
      "loss": 0.9797,
      "step": 927
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 0.034199487417936325,
      "learning_rate": 0.0019542726069538504,
      "loss": 1.1158,
      "step": 928
    },
    {
      "epoch": 1.2386666666666666,
      "grad_norm": 0.03000166453421116,
      "learning_rate": 0.0019541434264748075,
      "loss": 1.1205,
      "step": 929
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.09325883537530899,
      "learning_rate": 0.0019540140680664913,
      "loss": 1.4931,
      "step": 930
    },
    {
      "epoch": 1.2413333333333334,
      "grad_norm": 0.035580094903707504,
      "learning_rate": 0.0019538845317530243,
      "loss": 0.9834,
      "step": 931
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 0.032512106001377106,
      "learning_rate": 0.0019537548175585623,
      "loss": 1.1614,
      "step": 932
    },
    {
      "epoch": 1.244,
      "grad_norm": 0.030432922765612602,
      "learning_rate": 0.0019536249255072947,
      "loss": 1.3405,
      "step": 933
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 0.05341111123561859,
      "learning_rate": 0.001953494855623444,
      "loss": 1.1252,
      "step": 934
    },
    {
      "epoch": 1.2466666666666666,
      "grad_norm": 0.058438364416360855,
      "learning_rate": 0.0019533646079312656,
      "loss": 1.1084,
      "step": 935
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.08664575219154358,
      "learning_rate": 0.001953234182455048,
      "loss": 1.4712,
      "step": 936
    },
    {
      "epoch": 1.2493333333333334,
      "grad_norm": 0.03525920212268829,
      "learning_rate": 0.0019531035792191126,
      "loss": 1.2232,
      "step": 937
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 0.059603240340948105,
      "learning_rate": 0.0019529727982478154,
      "loss": 1.0196,
      "step": 938
    },
    {
      "epoch": 1.252,
      "grad_norm": 0.043205082416534424,
      "learning_rate": 0.0019528418395655441,
      "loss": 1.1563,
      "step": 939
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.040585629642009735,
      "learning_rate": 0.0019527107031967197,
      "loss": 1.179,
      "step": 940
    },
    {
      "epoch": 1.2546666666666666,
      "grad_norm": 0.07096292078495026,
      "learning_rate": 0.001952579389165797,
      "loss": 1.2439,
      "step": 941
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.29361552000045776,
      "learning_rate": 0.001952447897497263,
      "loss": 1.3428,
      "step": 942
    },
    {
      "epoch": 1.2573333333333334,
      "grad_norm": 0.04388132318854332,
      "learning_rate": 0.0019523162282156388,
      "loss": 1.4457,
      "step": 943
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 0.06553230434656143,
      "learning_rate": 0.001952184381345478,
      "loss": 1.2623,
      "step": 944
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.0411115437746048,
      "learning_rate": 0.0019520523569113678,
      "loss": 1.3557,
      "step": 945
    },
    {
      "epoch": 1.2613333333333334,
      "grad_norm": 0.07150384783744812,
      "learning_rate": 0.0019519201549379273,
      "loss": 1.2459,
      "step": 946
    },
    {
      "epoch": 1.2626666666666666,
      "grad_norm": 0.10820503532886505,
      "learning_rate": 0.0019517877754498107,
      "loss": 1.0689,
      "step": 947
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.18272580206394196,
      "learning_rate": 0.0019516552184717037,
      "loss": 1.227,
      "step": 948
    },
    {
      "epoch": 1.2653333333333334,
      "grad_norm": 0.04019171744585037,
      "learning_rate": 0.0019515224840283255,
      "loss": 1.486,
      "step": 949
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.035130444914102554,
      "learning_rate": 0.0019513895721444286,
      "loss": 1.308,
      "step": 950
    },
    {
      "epoch": 1.268,
      "grad_norm": 0.06054501608014107,
      "learning_rate": 0.0019512564828447988,
      "loss": 1.2732,
      "step": 951
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 0.05069601163268089,
      "learning_rate": 0.0019511232161542541,
      "loss": 1.0948,
      "step": 952
    },
    {
      "epoch": 1.2706666666666666,
      "grad_norm": 0.12024369090795517,
      "learning_rate": 0.0019509897720976466,
      "loss": 1.2654,
      "step": 953
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.03756503760814667,
      "learning_rate": 0.001950856150699861,
      "loss": 1.1607,
      "step": 954
    },
    {
      "epoch": 1.2733333333333334,
      "grad_norm": 0.06278540939092636,
      "learning_rate": 0.0019507223519858146,
      "loss": 1.4081,
      "step": 955
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 0.051109179854393005,
      "learning_rate": 0.001950588375980459,
      "loss": 1.1908,
      "step": 956
    },
    {
      "epoch": 1.276,
      "grad_norm": 0.02911866270005703,
      "learning_rate": 0.0019504542227087778,
      "loss": 1.2778,
      "step": 957
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.041811563074588776,
      "learning_rate": 0.001950319892195788,
      "loss": 1.187,
      "step": 958
    },
    {
      "epoch": 1.2786666666666666,
      "grad_norm": 0.05014164373278618,
      "learning_rate": 0.0019501853844665399,
      "loss": 1.1465,
      "step": 959
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.036165930330753326,
      "learning_rate": 0.0019500506995461157,
      "loss": 1.094,
      "step": 960
    },
    {
      "epoch": 1.2813333333333334,
      "grad_norm": 0.033391740173101425,
      "learning_rate": 0.0019499158374596327,
      "loss": 0.868,
      "step": 961
    },
    {
      "epoch": 1.2826666666666666,
      "grad_norm": 0.030474362894892693,
      "learning_rate": 0.0019497807982322393,
      "loss": 1.04,
      "step": 962
    },
    {
      "epoch": 1.284,
      "grad_norm": 0.03946472704410553,
      "learning_rate": 0.0019496455818891181,
      "loss": 1.0843,
      "step": 963
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 0.038722872734069824,
      "learning_rate": 0.0019495101884554839,
      "loss": 1.244,
      "step": 964
    },
    {
      "epoch": 1.2866666666666666,
      "grad_norm": 0.03766161948442459,
      "learning_rate": 0.0019493746179565852,
      "loss": 1.3545,
      "step": 965
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.034053198993206024,
      "learning_rate": 0.0019492388704177035,
      "loss": 0.9666,
      "step": 966
    },
    {
      "epoch": 1.2893333333333334,
      "grad_norm": 0.04746085777878761,
      "learning_rate": 0.0019491029458641527,
      "loss": 1.3431,
      "step": 967
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 0.03471178933978081,
      "learning_rate": 0.0019489668443212805,
      "loss": 1.1371,
      "step": 968
    },
    {
      "epoch": 1.292,
      "grad_norm": 0.03200376406311989,
      "learning_rate": 0.0019488305658144666,
      "loss": 1.2418,
      "step": 969
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.03146272525191307,
      "learning_rate": 0.0019486941103691246,
      "loss": 1.3092,
      "step": 970
    },
    {
      "epoch": 1.2946666666666666,
      "grad_norm": 0.033365488052368164,
      "learning_rate": 0.0019485574780107014,
      "loss": 1.2712,
      "step": 971
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.03075232543051243,
      "learning_rate": 0.0019484206687646753,
      "loss": 1.2443,
      "step": 972
    },
    {
      "epoch": 1.2973333333333334,
      "grad_norm": 0.051488567143678665,
      "learning_rate": 0.0019482836826565594,
      "loss": 1.1158,
      "step": 973
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.030538994818925858,
      "learning_rate": 0.0019481465197118983,
      "loss": 1.1642,
      "step": 974
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.038301169872283936,
      "learning_rate": 0.0019480091799562705,
      "loss": 1.359,
      "step": 975
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 0.06107578054070473,
      "learning_rate": 0.0019478716634152872,
      "loss": 0.9968,
      "step": 976
    },
    {
      "epoch": 1.3026666666666666,
      "grad_norm": 0.0440947525203228,
      "learning_rate": 0.001947733970114593,
      "loss": 1.2118,
      "step": 977
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.02929622121155262,
      "learning_rate": 0.0019475961000798643,
      "loss": 0.9592,
      "step": 978
    },
    {
      "epoch": 1.3053333333333335,
      "grad_norm": 0.049520161002874374,
      "learning_rate": 0.0019474580533368115,
      "loss": 1.2133,
      "step": 979
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.026915408670902252,
      "learning_rate": 0.0019473198299111778,
      "loss": 1.25,
      "step": 980
    },
    {
      "epoch": 1.308,
      "grad_norm": 0.03466947004199028,
      "learning_rate": 0.0019471814298287389,
      "loss": 1.3719,
      "step": 981
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.037364665418863297,
      "learning_rate": 0.0019470428531153038,
      "loss": 1.0986,
      "step": 982
    },
    {
      "epoch": 1.3106666666666666,
      "grad_norm": 0.04226904362440109,
      "learning_rate": 0.001946904099796715,
      "loss": 1.3798,
      "step": 983
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.03864710405468941,
      "learning_rate": 0.0019467651698988461,
      "loss": 1.3063,
      "step": 984
    },
    {
      "epoch": 1.3133333333333335,
      "grad_norm": 0.048180874437093735,
      "learning_rate": 0.001946626063447606,
      "loss": 0.9279,
      "step": 985
    },
    {
      "epoch": 1.3146666666666667,
      "grad_norm": 0.04228246212005615,
      "learning_rate": 0.0019464867804689348,
      "loss": 1.1569,
      "step": 986
    },
    {
      "epoch": 1.316,
      "grad_norm": 0.02623417228460312,
      "learning_rate": 0.001946347320988806,
      "loss": 1.225,
      "step": 987
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 0.047404587268829346,
      "learning_rate": 0.0019462076850332265,
      "loss": 1.1843,
      "step": 988
    },
    {
      "epoch": 1.3186666666666667,
      "grad_norm": 0.03544633090496063,
      "learning_rate": 0.001946067872628235,
      "loss": 1.2119,
      "step": 989
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.05128188803792,
      "learning_rate": 0.0019459278837999046,
      "loss": 1.449,
      "step": 990
    },
    {
      "epoch": 1.3213333333333335,
      "grad_norm": 0.030097145587205887,
      "learning_rate": 0.0019457877185743403,
      "loss": 1.1071,
      "step": 991
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 0.031008301302790642,
      "learning_rate": 0.0019456473769776795,
      "loss": 1.2859,
      "step": 992
    },
    {
      "epoch": 1.324,
      "grad_norm": 0.034020811319351196,
      "learning_rate": 0.0019455068590360943,
      "loss": 1.0749,
      "step": 993
    },
    {
      "epoch": 1.3253333333333333,
      "grad_norm": 0.03134119510650635,
      "learning_rate": 0.0019453661647757877,
      "loss": 1.0457,
      "step": 994
    },
    {
      "epoch": 1.3266666666666667,
      "grad_norm": 0.04797649011015892,
      "learning_rate": 0.001945225294222997,
      "loss": 1.3599,
      "step": 995
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.0311111006885767,
      "learning_rate": 0.0019450842474039913,
      "loss": 1.29,
      "step": 996
    },
    {
      "epoch": 1.3293333333333333,
      "grad_norm": 0.03706010803580284,
      "learning_rate": 0.0019449430243450737,
      "loss": 1.1249,
      "step": 997
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.038652338087558746,
      "learning_rate": 0.0019448016250725791,
      "loss": 0.921,
      "step": 998
    },
    {
      "epoch": 1.332,
      "grad_norm": 0.034926656633615494,
      "learning_rate": 0.0019446600496128758,
      "loss": 1.1532,
      "step": 999
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.027791153639554977,
      "learning_rate": 0.0019445182979923655,
      "loss": 1.1127,
      "step": 1000
    },
    {
      "epoch": 1.3346666666666667,
      "grad_norm": 0.0369306243956089,
      "learning_rate": 0.0019443763702374811,
      "loss": 1.2877,
      "step": 1001
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.03968014568090439,
      "learning_rate": 0.00194423426637469,
      "loss": 1.1902,
      "step": 1002
    },
    {
      "epoch": 1.3373333333333333,
      "grad_norm": 0.041777413338422775,
      "learning_rate": 0.0019440919864304918,
      "loss": 1.1582,
      "step": 1003
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 0.03023817017674446,
      "learning_rate": 0.0019439495304314188,
      "loss": 1.1592,
      "step": 1004
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.03844117000699043,
      "learning_rate": 0.0019438068984040365,
      "loss": 1.0502,
      "step": 1005
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.033100344240665436,
      "learning_rate": 0.0019436640903749427,
      "loss": 1.2267,
      "step": 1006
    },
    {
      "epoch": 1.3426666666666667,
      "grad_norm": 0.03463374078273773,
      "learning_rate": 0.0019435211063707687,
      "loss": 1.0481,
      "step": 1007
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.053173381835222244,
      "learning_rate": 0.0019433779464181778,
      "loss": 1.2908,
      "step": 1008
    },
    {
      "epoch": 1.3453333333333333,
      "grad_norm": 0.03280322253704071,
      "learning_rate": 0.001943234610543867,
      "loss": 0.8925,
      "step": 1009
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.025117285549640656,
      "learning_rate": 0.0019430910987745654,
      "loss": 1.1763,
      "step": 1010
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 0.0350940078496933,
      "learning_rate": 0.001942947411137035,
      "loss": 1.1532,
      "step": 1011
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 0.03816450014710426,
      "learning_rate": 0.0019428035476580713,
      "loss": 1.0659,
      "step": 1012
    },
    {
      "epoch": 1.3506666666666667,
      "grad_norm": 0.031671345233917236,
      "learning_rate": 0.0019426595083645015,
      "loss": 1.4603,
      "step": 1013
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.08946898579597473,
      "learning_rate": 0.001942515293283187,
      "loss": 1.324,
      "step": 1014
    },
    {
      "epoch": 1.3533333333333333,
      "grad_norm": 0.03596218675374985,
      "learning_rate": 0.0019423709024410196,
      "loss": 1.2267,
      "step": 1015
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 0.04250172898173332,
      "learning_rate": 0.0019422263358649268,
      "loss": 1.0925,
      "step": 1016
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 0.06566435098648071,
      "learning_rate": 0.0019420815935818673,
      "loss": 1.4852,
      "step": 1017
    },
    {
      "epoch": 1.3573333333333333,
      "grad_norm": 0.03196272999048233,
      "learning_rate": 0.0019419366756188317,
      "loss": 1.0055,
      "step": 1018
    },
    {
      "epoch": 1.3586666666666667,
      "grad_norm": 0.036296598613262177,
      "learning_rate": 0.0019417915820028457,
      "loss": 0.8954,
      "step": 1019
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.03516979143023491,
      "learning_rate": 0.0019416463127609656,
      "loss": 1.1029,
      "step": 1020
    },
    {
      "epoch": 1.3613333333333333,
      "grad_norm": 0.03265445679426193,
      "learning_rate": 0.0019415008679202815,
      "loss": 1.2619,
      "step": 1021
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.04200386628508568,
      "learning_rate": 0.0019413552475079161,
      "loss": 1.0716,
      "step": 1022
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 0.03689655289053917,
      "learning_rate": 0.0019412094515510248,
      "loss": 1.3979,
      "step": 1023
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 0.03170574828982353,
      "learning_rate": 0.0019410634800767958,
      "loss": 1.1837,
      "step": 1024
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.04890434443950653,
      "learning_rate": 0.0019409173331124499,
      "loss": 0.9876,
      "step": 1025
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.033238139003515244,
      "learning_rate": 0.0019407710106852402,
      "loss": 1.2522,
      "step": 1026
    },
    {
      "epoch": 1.3693333333333333,
      "grad_norm": 0.03388666361570358,
      "learning_rate": 0.001940624512822454,
      "loss": 0.9276,
      "step": 1027
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 0.03537699580192566,
      "learning_rate": 0.0019404778395514094,
      "loss": 1.0985,
      "step": 1028
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 0.04850810021162033,
      "learning_rate": 0.0019403309908994587,
      "loss": 1.1092,
      "step": 1029
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.24810157716274261,
      "learning_rate": 0.001940183966893986,
      "loss": 1.0112,
      "step": 1030
    },
    {
      "epoch": 1.3746666666666667,
      "grad_norm": 0.03380877152085304,
      "learning_rate": 0.0019400367675624084,
      "loss": 0.9848,
      "step": 1031
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.036490194499492645,
      "learning_rate": 0.0019398893929321761,
      "loss": 1.3524,
      "step": 1032
    },
    {
      "epoch": 1.3773333333333333,
      "grad_norm": 0.044986024498939514,
      "learning_rate": 0.0019397418430307713,
      "loss": 1.2282,
      "step": 1033
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 0.10466448217630386,
      "learning_rate": 0.0019395941178857092,
      "loss": 1.2598,
      "step": 1034
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.04024912044405937,
      "learning_rate": 0.001939446217524538,
      "loss": 1.1575,
      "step": 1035
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 0.2530352473258972,
      "learning_rate": 0.0019392981419748376,
      "loss": 1.143,
      "step": 1036
    },
    {
      "epoch": 1.3826666666666667,
      "grad_norm": 0.03773501142859459,
      "learning_rate": 0.001939149891264222,
      "loss": 1.1443,
      "step": 1037
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.0643220916390419,
      "learning_rate": 0.0019390014654203367,
      "loss": 1.1808,
      "step": 1038
    },
    {
      "epoch": 1.3853333333333333,
      "grad_norm": 0.03536034747958183,
      "learning_rate": 0.0019388528644708602,
      "loss": 1.1469,
      "step": 1039
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.06130973622202873,
      "learning_rate": 0.0019387040884435037,
      "loss": 1.109,
      "step": 1040
    },
    {
      "epoch": 1.388,
      "grad_norm": 0.1922924816608429,
      "learning_rate": 0.0019385551373660112,
      "loss": 1.5175,
      "step": 1041
    },
    {
      "epoch": 1.3893333333333333,
      "grad_norm": 0.048888418823480606,
      "learning_rate": 0.001938406011266159,
      "loss": 1.3275,
      "step": 1042
    },
    {
      "epoch": 1.3906666666666667,
      "grad_norm": 0.07075276225805283,
      "learning_rate": 0.0019382567101717564,
      "loss": 1.1408,
      "step": 1043
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.06521464884281158,
      "learning_rate": 0.0019381072341106453,
      "loss": 1.248,
      "step": 1044
    },
    {
      "epoch": 1.3933333333333333,
      "grad_norm": 0.051839422434568405,
      "learning_rate": 0.0019379575831106994,
      "loss": 1.2363,
      "step": 1045
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.042511679232120514,
      "learning_rate": 0.0019378077571998264,
      "loss": 1.2944,
      "step": 1046
    },
    {
      "epoch": 1.396,
      "grad_norm": 0.07609397917985916,
      "learning_rate": 0.001937657756405966,
      "loss": 1.1261,
      "step": 1047
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 0.06251794099807739,
      "learning_rate": 0.00193750758075709,
      "loss": 1.5067,
      "step": 1048
    },
    {
      "epoch": 1.3986666666666667,
      "grad_norm": 0.04129412770271301,
      "learning_rate": 0.0019373572302812034,
      "loss": 1.3242,
      "step": 1049
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.029453624039888382,
      "learning_rate": 0.0019372067050063438,
      "loss": 1.1113,
      "step": 1050
    },
    {
      "epoch": 1.4013333333333333,
      "grad_norm": 0.06899867951869965,
      "learning_rate": 0.001937056004960581,
      "loss": 1.1747,
      "step": 1051
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 0.047641996294260025,
      "learning_rate": 0.0019369051301720177,
      "loss": 1.2007,
      "step": 1052
    },
    {
      "epoch": 1.404,
      "grad_norm": 0.05185546725988388,
      "learning_rate": 0.0019367540806687893,
      "loss": 1.1742,
      "step": 1053
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.031130235642194748,
      "learning_rate": 0.0019366028564790634,
      "loss": 1.207,
      "step": 1054
    },
    {
      "epoch": 1.4066666666666667,
      "grad_norm": 0.03203819692134857,
      "learning_rate": 0.0019364514576310408,
      "loss": 1.2248,
      "step": 1055
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.033899154514074326,
      "learning_rate": 0.001936299884152954,
      "loss": 1.324,
      "step": 1056
    },
    {
      "epoch": 1.4093333333333333,
      "grad_norm": 0.03956972062587738,
      "learning_rate": 0.0019361481360730686,
      "loss": 1.2148,
      "step": 1057
    },
    {
      "epoch": 1.4106666666666667,
      "grad_norm": 0.04931002855300903,
      "learning_rate": 0.0019359962134196827,
      "loss": 1.1771,
      "step": 1058
    },
    {
      "epoch": 1.412,
      "grad_norm": 0.03282837197184563,
      "learning_rate": 0.0019358441162211268,
      "loss": 1.4793,
      "step": 1059
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.02552546001970768,
      "learning_rate": 0.0019356918445057648,
      "loss": 1.3109,
      "step": 1060
    },
    {
      "epoch": 1.4146666666666667,
      "grad_norm": 0.02982131391763687,
      "learning_rate": 0.0019355393983019912,
      "loss": 1.1649,
      "step": 1061
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.034890398383140564,
      "learning_rate": 0.0019353867776382354,
      "loss": 1.1318,
      "step": 1062
    },
    {
      "epoch": 1.4173333333333333,
      "grad_norm": 0.036309607326984406,
      "learning_rate": 0.001935233982542958,
      "loss": 1.254,
      "step": 1063
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 0.027689358219504356,
      "learning_rate": 0.0019350810130446515,
      "loss": 1.1954,
      "step": 1064
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.0335623137652874,
      "learning_rate": 0.0019349278691718427,
      "loss": 1.315,
      "step": 1065
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 0.03918091207742691,
      "learning_rate": 0.0019347745509530894,
      "loss": 1.3466,
      "step": 1066
    },
    {
      "epoch": 1.4226666666666667,
      "grad_norm": 0.04572087153792381,
      "learning_rate": 0.0019346210584169827,
      "loss": 0.8661,
      "step": 1067
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.03770998865365982,
      "learning_rate": 0.0019344673915921461,
      "loss": 1.2371,
      "step": 1068
    },
    {
      "epoch": 1.4253333333333333,
      "grad_norm": 0.03576504811644554,
      "learning_rate": 0.0019343135505072349,
      "loss": 1.3754,
      "step": 1069
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.03012077324092388,
      "learning_rate": 0.0019341595351909384,
      "loss": 1.3323,
      "step": 1070
    },
    {
      "epoch": 1.428,
      "grad_norm": 0.0364651121199131,
      "learning_rate": 0.0019340053456719769,
      "loss": 0.9957,
      "step": 1071
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 0.033979207277297974,
      "learning_rate": 0.0019338509819791037,
      "loss": 0.9219,
      "step": 1072
    },
    {
      "epoch": 1.4306666666666668,
      "grad_norm": 0.05296367406845093,
      "learning_rate": 0.001933696444141105,
      "loss": 1.309,
      "step": 1073
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.029917173087596893,
      "learning_rate": 0.0019335417321867988,
      "loss": 1.3027,
      "step": 1074
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.15252470970153809,
      "learning_rate": 0.0019333868461450358,
      "loss": 1.1495,
      "step": 1075
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 0.04634843021631241,
      "learning_rate": 0.0019332317860446997,
      "loss": 1.3094,
      "step": 1076
    },
    {
      "epoch": 1.436,
      "grad_norm": 0.029510192573070526,
      "learning_rate": 0.0019330765519147057,
      "loss": 0.8929,
      "step": 1077
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 0.03733333945274353,
      "learning_rate": 0.0019329211437840025,
      "loss": 0.9797,
      "step": 1078
    },
    {
      "epoch": 1.4386666666666668,
      "grad_norm": 0.04122123494744301,
      "learning_rate": 0.0019327655616815705,
      "loss": 1.34,
      "step": 1079
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.0380447693169117,
      "learning_rate": 0.0019326098056364222,
      "loss": 1.3913,
      "step": 1080
    },
    {
      "epoch": 1.4413333333333334,
      "grad_norm": 0.03763454034924507,
      "learning_rate": 0.001932453875677604,
      "loss": 1.037,
      "step": 1081
    },
    {
      "epoch": 1.4426666666666668,
      "grad_norm": 0.09115712344646454,
      "learning_rate": 0.0019322977718341933,
      "loss": 1.1692,
      "step": 1082
    },
    {
      "epoch": 1.444,
      "grad_norm": 0.03941146284341812,
      "learning_rate": 0.0019321414941353005,
      "loss": 1.1363,
      "step": 1083
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 0.09977391362190247,
      "learning_rate": 0.001931985042610068,
      "loss": 1.1467,
      "step": 1084
    },
    {
      "epoch": 1.4466666666666668,
      "grad_norm": 0.025791233405470848,
      "learning_rate": 0.0019318284172876719,
      "loss": 1.3123,
      "step": 1085
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.03638454154133797,
      "learning_rate": 0.001931671618197319,
      "loss": 0.8348,
      "step": 1086
    },
    {
      "epoch": 1.4493333333333334,
      "grad_norm": 0.03779686242341995,
      "learning_rate": 0.0019315146453682496,
      "loss": 1.2207,
      "step": 1087
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 0.036933474242687225,
      "learning_rate": 0.0019313574988297358,
      "loss": 1.2993,
      "step": 1088
    },
    {
      "epoch": 1.452,
      "grad_norm": 0.06168394163250923,
      "learning_rate": 0.0019312001786110828,
      "loss": 1.434,
      "step": 1089
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.05353248119354248,
      "learning_rate": 0.0019310426847416275,
      "loss": 1.2748,
      "step": 1090
    },
    {
      "epoch": 1.4546666666666668,
      "grad_norm": 0.06427458673715591,
      "learning_rate": 0.0019308850172507397,
      "loss": 1.3133,
      "step": 1091
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.03762117400765419,
      "learning_rate": 0.0019307271761678214,
      "loss": 1.1852,
      "step": 1092
    },
    {
      "epoch": 1.4573333333333334,
      "grad_norm": 0.039141394197940826,
      "learning_rate": 0.0019305691615223064,
      "loss": 1.3403,
      "step": 1093
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.02479461021721363,
      "learning_rate": 0.0019304109733436616,
      "loss": 1.0433,
      "step": 1094
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.038292624056339264,
      "learning_rate": 0.0019302526116613864,
      "loss": 1.37,
      "step": 1095
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 0.03812011331319809,
      "learning_rate": 0.0019300940765050113,
      "loss": 1.5786,
      "step": 1096
    },
    {
      "epoch": 1.4626666666666668,
      "grad_norm": 0.047531358897686005,
      "learning_rate": 0.001929935367904101,
      "loss": 0.9266,
      "step": 1097
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.06007589399814606,
      "learning_rate": 0.0019297764858882513,
      "loss": 1.0865,
      "step": 1098
    },
    {
      "epoch": 1.4653333333333334,
      "grad_norm": 0.03907125070691109,
      "learning_rate": 0.0019296174304870906,
      "loss": 1.1008,
      "step": 1099
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.03853308781981468,
      "learning_rate": 0.0019294582017302796,
      "loss": 1.3794,
      "step": 1100
    },
    {
      "epoch": 1.468,
      "grad_norm": 0.0294423159211874,
      "learning_rate": 0.001929298799647511,
      "loss": 1.2066,
      "step": 1101
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 0.042536910623311996,
      "learning_rate": 0.0019291392242685107,
      "loss": 0.9692,
      "step": 1102
    },
    {
      "epoch": 1.4706666666666668,
      "grad_norm": 0.06810896843671799,
      "learning_rate": 0.0019289794756230363,
      "loss": 1.6728,
      "step": 1103
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.03829686716198921,
      "learning_rate": 0.0019288195537408778,
      "loss": 1.3638,
      "step": 1104
    },
    {
      "epoch": 1.4733333333333334,
      "grad_norm": 0.03567301854491234,
      "learning_rate": 0.0019286594586518575,
      "loss": 1.3868,
      "step": 1105
    },
    {
      "epoch": 1.4746666666666668,
      "grad_norm": 0.032131630927324295,
      "learning_rate": 0.0019284991903858301,
      "loss": 1.2609,
      "step": 1106
    },
    {
      "epoch": 1.476,
      "grad_norm": 0.03357576206326485,
      "learning_rate": 0.0019283387489726826,
      "loss": 0.8685,
      "step": 1107
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 0.026589877903461456,
      "learning_rate": 0.0019281781344423342,
      "loss": 0.905,
      "step": 1108
    },
    {
      "epoch": 1.4786666666666668,
      "grad_norm": 0.027862684801220894,
      "learning_rate": 0.0019280173468247362,
      "loss": 1.062,
      "step": 1109
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.032053142786026,
      "learning_rate": 0.0019278563861498724,
      "loss": 1.2809,
      "step": 1110
    },
    {
      "epoch": 1.4813333333333334,
      "grad_norm": 0.06968472898006439,
      "learning_rate": 0.0019276952524477592,
      "loss": 1.0491,
      "step": 1111
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 0.030142201110720634,
      "learning_rate": 0.0019275339457484443,
      "loss": 0.964,
      "step": 1112
    },
    {
      "epoch": 1.484,
      "grad_norm": 0.05732380971312523,
      "learning_rate": 0.0019273724660820086,
      "loss": 1.129,
      "step": 1113
    },
    {
      "epoch": 1.4853333333333334,
      "grad_norm": 0.025522222742438316,
      "learning_rate": 0.0019272108134785655,
      "loss": 1.0828,
      "step": 1114
    },
    {
      "epoch": 1.4866666666666668,
      "grad_norm": 0.03420479968190193,
      "learning_rate": 0.001927048987968259,
      "loss": 1.2274,
      "step": 1115
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.03673067316412926,
      "learning_rate": 0.0019268869895812672,
      "loss": 1.3353,
      "step": 1116
    },
    {
      "epoch": 1.4893333333333334,
      "grad_norm": 0.03361811861395836,
      "learning_rate": 0.0019267248183477993,
      "loss": 1.0382,
      "step": 1117
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.036503035575151443,
      "learning_rate": 0.0019265624742980977,
      "loss": 1.2459,
      "step": 1118
    },
    {
      "epoch": 1.492,
      "grad_norm": 0.04614037647843361,
      "learning_rate": 0.0019263999574624356,
      "loss": 1.2337,
      "step": 1119
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.032770320773124695,
      "learning_rate": 0.0019262372678711195,
      "loss": 1.2042,
      "step": 1120
    },
    {
      "epoch": 1.4946666666666666,
      "grad_norm": 0.03707144409418106,
      "learning_rate": 0.001926074405554488,
      "loss": 1.2283,
      "step": 1121
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.03890113905072212,
      "learning_rate": 0.0019259113705429119,
      "loss": 1.1419,
      "step": 1122
    },
    {
      "epoch": 1.4973333333333334,
      "grad_norm": 0.04665500298142433,
      "learning_rate": 0.001925748162866794,
      "loss": 1.3614,
      "step": 1123
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 0.029024560004472733,
      "learning_rate": 0.0019255847825565687,
      "loss": 1.0714,
      "step": 1124
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.027847202494740486,
      "learning_rate": 0.0019254212296427042,
      "loss": 1.066,
      "step": 1125
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.036507509648799896,
      "learning_rate": 0.0019252575041556997,
      "loss": 1.2496,
      "step": 1126
    },
    {
      "epoch": 1.5026666666666668,
      "grad_norm": 0.037839457392692566,
      "learning_rate": 0.0019250936061260865,
      "loss": 1.2916,
      "step": 1127
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.031323038041591644,
      "learning_rate": 0.0019249295355844285,
      "loss": 1.1231,
      "step": 1128
    },
    {
      "epoch": 1.5053333333333332,
      "grad_norm": 0.0476144477725029,
      "learning_rate": 0.0019247652925613221,
      "loss": 1.2372,
      "step": 1129
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.03580286726355553,
      "learning_rate": 0.0019246008770873952,
      "loss": 1.2995,
      "step": 1130
    },
    {
      "epoch": 1.508,
      "grad_norm": 0.053861796855926514,
      "learning_rate": 0.0019244362891933076,
      "loss": 0.9273,
      "step": 1131
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 0.07648592442274094,
      "learning_rate": 0.0019242715289097526,
      "loss": 1.2355,
      "step": 1132
    },
    {
      "epoch": 1.5106666666666668,
      "grad_norm": 0.0389610230922699,
      "learning_rate": 0.0019241065962674541,
      "loss": 1.4008,
      "step": 1133
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.029886845499277115,
      "learning_rate": 0.0019239414912971696,
      "loss": 1.2381,
      "step": 1134
    },
    {
      "epoch": 1.5133333333333332,
      "grad_norm": 0.03296320512890816,
      "learning_rate": 0.0019237762140296873,
      "loss": 1.3392,
      "step": 1135
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 0.043230537325143814,
      "learning_rate": 0.0019236107644958283,
      "loss": 1.5222,
      "step": 1136
    },
    {
      "epoch": 1.516,
      "grad_norm": 0.036820344626903534,
      "learning_rate": 0.0019234451427264459,
      "loss": 1.2157,
      "step": 1137
    },
    {
      "epoch": 1.5173333333333332,
      "grad_norm": 0.038154181092977524,
      "learning_rate": 0.0019232793487524256,
      "loss": 1.2014,
      "step": 1138
    },
    {
      "epoch": 1.5186666666666668,
      "grad_norm": 0.038935672491788864,
      "learning_rate": 0.0019231133826046842,
      "loss": 1.3046,
      "step": 1139
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.08492407947778702,
      "learning_rate": 0.0019229472443141717,
      "loss": 0.9978,
      "step": 1140
    },
    {
      "epoch": 1.5213333333333332,
      "grad_norm": 0.03445436432957649,
      "learning_rate": 0.001922780933911869,
      "loss": 1.2715,
      "step": 1141
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.03141622617840767,
      "learning_rate": 0.0019226144514287906,
      "loss": 1.4054,
      "step": 1142
    },
    {
      "epoch": 1.524,
      "grad_norm": 0.03992561623454094,
      "learning_rate": 0.001922447796895982,
      "loss": 1.1322,
      "step": 1143
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 0.0334891602396965,
      "learning_rate": 0.0019222809703445206,
      "loss": 1.0604,
      "step": 1144
    },
    {
      "epoch": 1.5266666666666666,
      "grad_norm": 0.06263735145330429,
      "learning_rate": 0.001922113971805517,
      "loss": 1.1775,
      "step": 1145
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.08937094360589981,
      "learning_rate": 0.0019219468013101121,
      "loss": 1.0979,
      "step": 1146
    },
    {
      "epoch": 1.5293333333333332,
      "grad_norm": 0.0393209345638752,
      "learning_rate": 0.0019217794588894815,
      "loss": 1.2765,
      "step": 1147
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 0.06908867508172989,
      "learning_rate": 0.0019216119445748302,
      "loss": 1.2652,
      "step": 1148
    },
    {
      "epoch": 1.532,
      "grad_norm": 0.04567311704158783,
      "learning_rate": 0.0019214442583973966,
      "loss": 1.2852,
      "step": 1149
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.03834959492087364,
      "learning_rate": 0.001921276400388451,
      "loss": 1.2297,
      "step": 1150
    },
    {
      "epoch": 1.5346666666666666,
      "grad_norm": 0.06769412755966187,
      "learning_rate": 0.0019211083705792957,
      "loss": 1.2351,
      "step": 1151
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.0361933633685112,
      "learning_rate": 0.0019209401690012651,
      "loss": 1.1698,
      "step": 1152
    },
    {
      "epoch": 1.5373333333333332,
      "grad_norm": 0.039950497448444366,
      "learning_rate": 0.0019207717956857254,
      "loss": 1.3037,
      "step": 1153
    },
    {
      "epoch": 1.5386666666666666,
      "grad_norm": 0.0356692336499691,
      "learning_rate": 0.0019206032506640747,
      "loss": 1.4069,
      "step": 1154
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.030030788853764534,
      "learning_rate": 0.001920434533967744,
      "loss": 1.1806,
      "step": 1155
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 0.04032178595662117,
      "learning_rate": 0.0019202656456281952,
      "loss": 1.2436,
      "step": 1156
    },
    {
      "epoch": 1.5426666666666666,
      "grad_norm": 0.0379839763045311,
      "learning_rate": 0.001920096585676923,
      "loss": 1.1481,
      "step": 1157
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.05142031982541084,
      "learning_rate": 0.0019199273541454537,
      "loss": 1.4854,
      "step": 1158
    },
    {
      "epoch": 1.5453333333333332,
      "grad_norm": 0.04140099883079529,
      "learning_rate": 0.0019197579510653456,
      "loss": 1.2322,
      "step": 1159
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.03596137836575508,
      "learning_rate": 0.0019195883764681892,
      "loss": 1.121,
      "step": 1160
    },
    {
      "epoch": 1.548,
      "grad_norm": 0.029852155596017838,
      "learning_rate": 0.0019194186303856068,
      "loss": 0.9737,
      "step": 1161
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 0.04490175470709801,
      "learning_rate": 0.0019192487128492529,
      "loss": 1.0951,
      "step": 1162
    },
    {
      "epoch": 1.5506666666666666,
      "grad_norm": 0.03802970051765442,
      "learning_rate": 0.0019190786238908136,
      "loss": 1.1863,
      "step": 1163
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.033025212585926056,
      "learning_rate": 0.0019189083635420075,
      "loss": 1.5652,
      "step": 1164
    },
    {
      "epoch": 1.5533333333333332,
      "grad_norm": 0.03308124467730522,
      "learning_rate": 0.0019187379318345845,
      "loss": 1.0581,
      "step": 1165
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.04073697328567505,
      "learning_rate": 0.0019185673288003274,
      "loss": 1.2326,
      "step": 1166
    },
    {
      "epoch": 1.556,
      "grad_norm": 0.028796609491109848,
      "learning_rate": 0.0019183965544710495,
      "loss": 1.1706,
      "step": 1167
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 0.032067958265542984,
      "learning_rate": 0.0019182256088785977,
      "loss": 0.9782,
      "step": 1168
    },
    {
      "epoch": 1.5586666666666666,
      "grad_norm": 0.04135400429368019,
      "learning_rate": 0.0019180544920548495,
      "loss": 1.0871,
      "step": 1169
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.05263848230242729,
      "learning_rate": 0.0019178832040317154,
      "loss": 1.3031,
      "step": 1170
    },
    {
      "epoch": 1.5613333333333332,
      "grad_norm": 0.03904447704553604,
      "learning_rate": 0.0019177117448411366,
      "loss": 1.232,
      "step": 1171
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 0.03162356838583946,
      "learning_rate": 0.0019175401145150874,
      "loss": 1.2634,
      "step": 1172
    },
    {
      "epoch": 1.564,
      "grad_norm": 0.03250851482152939,
      "learning_rate": 0.0019173683130855738,
      "loss": 1.1357,
      "step": 1173
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.02929721400141716,
      "learning_rate": 0.0019171963405846327,
      "loss": 1.1657,
      "step": 1174
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.03873145207762718,
      "learning_rate": 0.0019170241970443342,
      "loss": 1.3201,
      "step": 1175
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.03734424710273743,
      "learning_rate": 0.0019168518824967797,
      "loss": 1.3253,
      "step": 1176
    },
    {
      "epoch": 1.5693333333333332,
      "grad_norm": 0.050296295434236526,
      "learning_rate": 0.0019166793969741024,
      "loss": 1.3498,
      "step": 1177
    },
    {
      "epoch": 1.5706666666666667,
      "grad_norm": 0.05395993962883949,
      "learning_rate": 0.0019165067405084672,
      "loss": 1.3635,
      "step": 1178
    },
    {
      "epoch": 1.572,
      "grad_norm": 0.08466081321239471,
      "learning_rate": 0.0019163339131320716,
      "loss": 1.1913,
      "step": 1179
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.040522653609514236,
      "learning_rate": 0.0019161609148771444,
      "loss": 1.4815,
      "step": 1180
    },
    {
      "epoch": 1.5746666666666667,
      "grad_norm": 0.031109554693102837,
      "learning_rate": 0.0019159877457759467,
      "loss": 0.9617,
      "step": 1181
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.05868156999349594,
      "learning_rate": 0.0019158144058607706,
      "loss": 1.4265,
      "step": 1182
    },
    {
      "epoch": 1.5773333333333333,
      "grad_norm": 0.039943210780620575,
      "learning_rate": 0.0019156408951639414,
      "loss": 1.078,
      "step": 1183
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 0.04538954421877861,
      "learning_rate": 0.0019154672137178148,
      "loss": 1.1227,
      "step": 1184
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.032892413437366486,
      "learning_rate": 0.0019152933615547796,
      "loss": 1.0505,
      "step": 1185
    },
    {
      "epoch": 1.5813333333333333,
      "grad_norm": 0.039090242236852646,
      "learning_rate": 0.0019151193387072555,
      "loss": 0.9823,
      "step": 1186
    },
    {
      "epoch": 1.5826666666666667,
      "grad_norm": 0.03311231732368469,
      "learning_rate": 0.0019149451452076943,
      "loss": 1.0706,
      "step": 1187
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.03480307012796402,
      "learning_rate": 0.0019147707810885798,
      "loss": 1.1343,
      "step": 1188
    },
    {
      "epoch": 1.5853333333333333,
      "grad_norm": 0.03385277837514877,
      "learning_rate": 0.0019145962463824279,
      "loss": 1.1136,
      "step": 1189
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.041126929223537445,
      "learning_rate": 0.001914421541121785,
      "loss": 1.1069,
      "step": 1190
    },
    {
      "epoch": 1.588,
      "grad_norm": 0.040245819836854935,
      "learning_rate": 0.0019142466653392317,
      "loss": 1.1831,
      "step": 1191
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 0.03378815948963165,
      "learning_rate": 0.0019140716190673777,
      "loss": 1.4464,
      "step": 1192
    },
    {
      "epoch": 1.5906666666666667,
      "grad_norm": 0.07238516956567764,
      "learning_rate": 0.0019138964023388662,
      "loss": 1.118,
      "step": 1193
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.03839018568396568,
      "learning_rate": 0.001913721015186372,
      "loss": 1.1486,
      "step": 1194
    },
    {
      "epoch": 1.5933333333333333,
      "grad_norm": 0.02827485091984272,
      "learning_rate": 0.001913545457642601,
      "loss": 0.9859,
      "step": 1195
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 0.03224482014775276,
      "learning_rate": 0.0019133697297402912,
      "loss": 1.1681,
      "step": 1196
    },
    {
      "epoch": 1.596,
      "grad_norm": 0.035218946635723114,
      "learning_rate": 0.0019131938315122131,
      "loss": 1.3026,
      "step": 1197
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.04090793803334236,
      "learning_rate": 0.0019130177629911674,
      "loss": 1.1723,
      "step": 1198
    },
    {
      "epoch": 1.5986666666666667,
      "grad_norm": 0.06373558193445206,
      "learning_rate": 0.0019128415242099881,
      "loss": 1.2005,
      "step": 1199
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.027570605278015137,
      "learning_rate": 0.0019126651152015402,
      "loss": 1.0651,
      "step": 1200
    },
    {
      "epoch": 1.6013333333333333,
      "grad_norm": 0.03094179928302765,
      "learning_rate": 0.0019124885359987206,
      "loss": 1.3301,
      "step": 1201
    },
    {
      "epoch": 1.6026666666666667,
      "grad_norm": 0.039830271154642105,
      "learning_rate": 0.0019123117866344575,
      "loss": 1.3648,
      "step": 1202
    },
    {
      "epoch": 1.604,
      "grad_norm": 0.033579643815755844,
      "learning_rate": 0.0019121348671417117,
      "loss": 1.2406,
      "step": 1203
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 0.036577798426151276,
      "learning_rate": 0.0019119577775534755,
      "loss": 1.1239,
      "step": 1204
    },
    {
      "epoch": 1.6066666666666667,
      "grad_norm": 0.028768688440322876,
      "learning_rate": 0.0019117805179027722,
      "loss": 1.1041,
      "step": 1205
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.03136226907372475,
      "learning_rate": 0.001911603088222657,
      "loss": 1.398,
      "step": 1206
    },
    {
      "epoch": 1.6093333333333333,
      "grad_norm": 0.029745595529675484,
      "learning_rate": 0.0019114254885462176,
      "loss": 1.0693,
      "step": 1207
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 0.0319194495677948,
      "learning_rate": 0.001911247718906573,
      "loss": 1.2434,
      "step": 1208
    },
    {
      "epoch": 1.612,
      "grad_norm": 0.04638802260160446,
      "learning_rate": 0.0019110697793368733,
      "loss": 1.2395,
      "step": 1209
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.029794272035360336,
      "learning_rate": 0.0019108916698703014,
      "loss": 1.1865,
      "step": 1210
    },
    {
      "epoch": 1.6146666666666667,
      "grad_norm": 0.03279958665370941,
      "learning_rate": 0.0019107133905400709,
      "loss": 1.4016,
      "step": 1211
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.031919293105602264,
      "learning_rate": 0.001910534941379427,
      "loss": 1.1496,
      "step": 1212
    },
    {
      "epoch": 1.6173333333333333,
      "grad_norm": 0.25600144267082214,
      "learning_rate": 0.001910356322421648,
      "loss": 1.222,
      "step": 1213
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.05811659246683121,
      "learning_rate": 0.0019101775337000422,
      "loss": 1.2157,
      "step": 1214
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.3031785488128662,
      "learning_rate": 0.0019099985752479506,
      "loss": 1.0924,
      "step": 1215
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 0.028796110302209854,
      "learning_rate": 0.0019098194470987448,
      "loss": 1.1603,
      "step": 1216
    },
    {
      "epoch": 1.6226666666666667,
      "grad_norm": 0.038465362042188644,
      "learning_rate": 0.0019096401492858298,
      "loss": 1.0193,
      "step": 1217
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.07102926820516586,
      "learning_rate": 0.0019094606818426403,
      "loss": 1.1355,
      "step": 1218
    },
    {
      "epoch": 1.6253333333333333,
      "grad_norm": 0.13549208641052246,
      "learning_rate": 0.0019092810448026439,
      "loss": 1.3408,
      "step": 1219
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.03507966175675392,
      "learning_rate": 0.001909101238199339,
      "loss": 1.0847,
      "step": 1220
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 0.041864726692438126,
      "learning_rate": 0.0019089212620662568,
      "loss": 1.2147,
      "step": 1221
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 0.2200983762741089,
      "learning_rate": 0.0019087411164369589,
      "loss": 1.1567,
      "step": 1222
    },
    {
      "epoch": 1.6306666666666667,
      "grad_norm": 0.05468539148569107,
      "learning_rate": 0.0019085608013450388,
      "loss": 1.1195,
      "step": 1223
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.07808142155408859,
      "learning_rate": 0.0019083803168241224,
      "loss": 1.1464,
      "step": 1224
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.058709025382995605,
      "learning_rate": 0.0019081996629078655,
      "loss": 1.2303,
      "step": 1225
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 0.3061031997203827,
      "learning_rate": 0.0019080188396299576,
      "loss": 1.1295,
      "step": 1226
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 0.25929364562034607,
      "learning_rate": 0.0019078378470241183,
      "loss": 1.2193,
      "step": 1227
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 0.03649763762950897,
      "learning_rate": 0.0019076566851240994,
      "loss": 0.8796,
      "step": 1228
    },
    {
      "epoch": 1.6386666666666667,
      "grad_norm": 0.050126027315855026,
      "learning_rate": 0.0019074753539636835,
      "loss": 1.2626,
      "step": 1229
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.09182040393352509,
      "learning_rate": 0.0019072938535766863,
      "loss": 1.034,
      "step": 1230
    },
    {
      "epoch": 1.6413333333333333,
      "grad_norm": 0.16734810173511505,
      "learning_rate": 0.0019071121839969534,
      "loss": 1.0777,
      "step": 1231
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 0.046076979488134384,
      "learning_rate": 0.0019069303452583627,
      "loss": 1.3502,
      "step": 1232
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 0.04674212634563446,
      "learning_rate": 0.0019067483373948243,
      "loss": 1.0992,
      "step": 1233
    },
    {
      "epoch": 1.6453333333333333,
      "grad_norm": 0.04751649498939514,
      "learning_rate": 0.0019065661604402782,
      "loss": 1.0023,
      "step": 1234
    },
    {
      "epoch": 1.6466666666666665,
      "grad_norm": 0.2590172290802002,
      "learning_rate": 0.0019063838144286974,
      "loss": 1.3586,
      "step": 1235
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.03021070919930935,
      "learning_rate": 0.0019062012993940859,
      "loss": 1.0303,
      "step": 1236
    },
    {
      "epoch": 1.6493333333333333,
      "grad_norm": 0.04040015861392021,
      "learning_rate": 0.0019060186153704787,
      "loss": 1.2953,
      "step": 1237
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.032560091465711594,
      "learning_rate": 0.0019058357623919437,
      "loss": 1.1655,
      "step": 1238
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 0.03935252130031586,
      "learning_rate": 0.0019056527404925788,
      "loss": 1.1003,
      "step": 1239
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.0936133936047554,
      "learning_rate": 0.0019054695497065142,
      "loss": 0.8688,
      "step": 1240
    },
    {
      "epoch": 1.6546666666666665,
      "grad_norm": 0.06002434343099594,
      "learning_rate": 0.0019052861900679115,
      "loss": 1.3319,
      "step": 1241
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.03721441701054573,
      "learning_rate": 0.0019051026616109636,
      "loss": 1.2336,
      "step": 1242
    },
    {
      "epoch": 1.6573333333333333,
      "grad_norm": 0.03649011254310608,
      "learning_rate": 0.0019049189643698952,
      "loss": 1.1829,
      "step": 1243
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 0.029769908636808395,
      "learning_rate": 0.0019047350983789622,
      "loss": 1.0471,
      "step": 1244
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.09674405306577682,
      "learning_rate": 0.0019045510636724518,
      "loss": 1.3474,
      "step": 1245
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.03294941037893295,
      "learning_rate": 0.0019043668602846833,
      "loss": 1.1993,
      "step": 1246
    },
    {
      "epoch": 1.6626666666666665,
      "grad_norm": 0.06474320590496063,
      "learning_rate": 0.001904182488250007,
      "loss": 1.0684,
      "step": 1247
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.06734839081764221,
      "learning_rate": 0.001903997947602804,
      "loss": 1.1451,
      "step": 1248
    },
    {
      "epoch": 1.6653333333333333,
      "grad_norm": 0.03365247696638107,
      "learning_rate": 0.0019038132383774888,
      "loss": 1.1108,
      "step": 1249
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.04782658442854881,
      "learning_rate": 0.0019036283606085054,
      "loss": 1.4607,
      "step": 1250
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 0.025025110691785812,
      "learning_rate": 0.0019034433143303299,
      "loss": 0.9268,
      "step": 1251
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 0.04126270115375519,
      "learning_rate": 0.0019032580995774697,
      "loss": 1.172,
      "step": 1252
    },
    {
      "epoch": 1.6706666666666665,
      "grad_norm": 0.05352315679192543,
      "learning_rate": 0.0019030727163844641,
      "loss": 1.2918,
      "step": 1253
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.03063308261334896,
      "learning_rate": 0.0019028871647858833,
      "loss": 1.1123,
      "step": 1254
    },
    {
      "epoch": 1.6733333333333333,
      "grad_norm": 0.078941710293293,
      "learning_rate": 0.0019027014448163295,
      "loss": 1.1963,
      "step": 1255
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 0.028724735602736473,
      "learning_rate": 0.0019025155565104352,
      "loss": 1.3837,
      "step": 1256
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 0.03329942002892494,
      "learning_rate": 0.0019023294999028653,
      "loss": 1.2551,
      "step": 1257
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 0.025968533009290695,
      "learning_rate": 0.001902143275028316,
      "loss": 1.3551,
      "step": 1258
    },
    {
      "epoch": 1.6786666666666665,
      "grad_norm": 0.02595193311572075,
      "learning_rate": 0.001901956881921514,
      "loss": 1.0747,
      "step": 1259
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.03346455469727516,
      "learning_rate": 0.0019017703206172186,
      "loss": 1.0206,
      "step": 1260
    },
    {
      "epoch": 1.6813333333333333,
      "grad_norm": 0.0357394702732563,
      "learning_rate": 0.0019015835911502196,
      "loss": 1.3811,
      "step": 1261
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.03815038874745369,
      "learning_rate": 0.0019013966935553385,
      "loss": 1.0889,
      "step": 1262
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 0.03146841749548912,
      "learning_rate": 0.001901209627867428,
      "loss": 1.0898,
      "step": 1263
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 0.028193963691592216,
      "learning_rate": 0.0019010223941213723,
      "loss": 1.1958,
      "step": 1264
    },
    {
      "epoch": 1.6866666666666665,
      "grad_norm": 0.033551715314388275,
      "learning_rate": 0.001900834992352087,
      "loss": 0.9596,
      "step": 1265
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.04016895219683647,
      "learning_rate": 0.0019006474225945187,
      "loss": 1.1662,
      "step": 1266
    },
    {
      "epoch": 1.6893333333333334,
      "grad_norm": 0.031781382858753204,
      "learning_rate": 0.0019004596848836454,
      "loss": 1.3564,
      "step": 1267
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 0.023324353620409966,
      "learning_rate": 0.001900271779254477,
      "loss": 1.1682,
      "step": 1268
    },
    {
      "epoch": 1.692,
      "grad_norm": 0.03504998981952667,
      "learning_rate": 0.0019000837057420539,
      "loss": 1.3668,
      "step": 1269
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.029398400336503983,
      "learning_rate": 0.0018998954643814484,
      "loss": 1.2265,
      "step": 1270
    },
    {
      "epoch": 1.6946666666666665,
      "grad_norm": 0.04528603330254555,
      "learning_rate": 0.0018997070552077635,
      "loss": 1.2289,
      "step": 1271
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.03444022312760353,
      "learning_rate": 0.0018995184782561343,
      "loss": 0.7848,
      "step": 1272
    },
    {
      "epoch": 1.6973333333333334,
      "grad_norm": 0.026356151327490807,
      "learning_rate": 0.0018993297335617263,
      "loss": 1.129,
      "step": 1273
    },
    {
      "epoch": 1.6986666666666665,
      "grad_norm": 0.039837796241045,
      "learning_rate": 0.0018991408211597371,
      "loss": 1.2663,
      "step": 1274
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.029256027191877365,
      "learning_rate": 0.0018989517410853954,
      "loss": 1.2285,
      "step": 1275
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 0.030302047729492188,
      "learning_rate": 0.0018987624933739604,
      "loss": 0.9946,
      "step": 1276
    },
    {
      "epoch": 1.7026666666666666,
      "grad_norm": 0.03322473168373108,
      "learning_rate": 0.0018985730780607237,
      "loss": 1.2188,
      "step": 1277
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.029829341918230057,
      "learning_rate": 0.0018983834951810069,
      "loss": 1.0724,
      "step": 1278
    },
    {
      "epoch": 1.7053333333333334,
      "grad_norm": 0.030723629519343376,
      "learning_rate": 0.0018981937447701638,
      "loss": 1.0318,
      "step": 1279
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.028177203610539436,
      "learning_rate": 0.0018980038268635794,
      "loss": 1.104,
      "step": 1280
    },
    {
      "epoch": 1.708,
      "grad_norm": 0.027784399688243866,
      "learning_rate": 0.0018978137414966698,
      "loss": 1.2739,
      "step": 1281
    },
    {
      "epoch": 1.7093333333333334,
      "grad_norm": 0.023871077224612236,
      "learning_rate": 0.001897623488704882,
      "loss": 1.2207,
      "step": 1282
    },
    {
      "epoch": 1.7106666666666666,
      "grad_norm": 0.033844079822301865,
      "learning_rate": 0.0018974330685236945,
      "loss": 1.2293,
      "step": 1283
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.052604105323553085,
      "learning_rate": 0.001897242480988617,
      "loss": 1.1445,
      "step": 1284
    },
    {
      "epoch": 1.7133333333333334,
      "grad_norm": 0.03092321753501892,
      "learning_rate": 0.00189705172613519,
      "loss": 1.1699,
      "step": 1285
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.03373512625694275,
      "learning_rate": 0.0018968608039989865,
      "loss": 1.2784,
      "step": 1286
    },
    {
      "epoch": 1.716,
      "grad_norm": 0.03810304030776024,
      "learning_rate": 0.001896669714615609,
      "loss": 1.2912,
      "step": 1287
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 0.035578105598688126,
      "learning_rate": 0.0018964784580206922,
      "loss": 1.0598,
      "step": 1288
    },
    {
      "epoch": 1.7186666666666666,
      "grad_norm": 0.03400229662656784,
      "learning_rate": 0.001896287034249902,
      "loss": 1.2402,
      "step": 1289
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.02596878819167614,
      "learning_rate": 0.0018960954433389346,
      "loss": 0.9581,
      "step": 1290
    },
    {
      "epoch": 1.7213333333333334,
      "grad_norm": 0.03508739173412323,
      "learning_rate": 0.0018959036853235186,
      "loss": 1.2146,
      "step": 1291
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 0.03211157023906708,
      "learning_rate": 0.0018957117602394129,
      "loss": 1.1856,
      "step": 1292
    },
    {
      "epoch": 1.724,
      "grad_norm": 0.031712885946035385,
      "learning_rate": 0.001895519668122408,
      "loss": 1.1832,
      "step": 1293
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.03060387633740902,
      "learning_rate": 0.001895327409008325,
      "loss": 1.1561,
      "step": 1294
    },
    {
      "epoch": 1.7266666666666666,
      "grad_norm": 0.026773180812597275,
      "learning_rate": 0.0018951349829330166,
      "loss": 1.0478,
      "step": 1295
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.03134344145655632,
      "learning_rate": 0.001894942389932367,
      "loss": 1.2289,
      "step": 1296
    },
    {
      "epoch": 1.7293333333333334,
      "grad_norm": 0.03330351412296295,
      "learning_rate": 0.0018947496300422903,
      "loss": 1.1096,
      "step": 1297
    },
    {
      "epoch": 1.7306666666666666,
      "grad_norm": 0.02788815274834633,
      "learning_rate": 0.0018945567032987332,
      "loss": 1.1562,
      "step": 1298
    },
    {
      "epoch": 1.732,
      "grad_norm": 0.03094436414539814,
      "learning_rate": 0.0018943636097376727,
      "loss": 1.1661,
      "step": 1299
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.04924730584025383,
      "learning_rate": 0.0018941703493951163,
      "loss": 1.2369,
      "step": 1300
    },
    {
      "epoch": 1.7346666666666666,
      "grad_norm": 0.034816328436136246,
      "learning_rate": 0.0018939769223071043,
      "loss": 1.0322,
      "step": 1301
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.0391564704477787,
      "learning_rate": 0.0018937833285097066,
      "loss": 1.3479,
      "step": 1302
    },
    {
      "epoch": 1.7373333333333334,
      "grad_norm": 0.032706983387470245,
      "learning_rate": 0.0018935895680390242,
      "loss": 1.0646,
      "step": 1303
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 0.03629806637763977,
      "learning_rate": 0.0018933956409311907,
      "loss": 0.7184,
      "step": 1304
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.036405935883522034,
      "learning_rate": 0.0018932015472223692,
      "loss": 1.0584,
      "step": 1305
    },
    {
      "epoch": 1.7413333333333334,
      "grad_norm": 0.041054125875234604,
      "learning_rate": 0.0018930072869487544,
      "loss": 1.2518,
      "step": 1306
    },
    {
      "epoch": 1.7426666666666666,
      "grad_norm": 0.03304671123623848,
      "learning_rate": 0.0018928128601465723,
      "loss": 1.254,
      "step": 1307
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.03576839715242386,
      "learning_rate": 0.0018926182668520793,
      "loss": 0.8299,
      "step": 1308
    },
    {
      "epoch": 1.7453333333333334,
      "grad_norm": 0.04643280431628227,
      "learning_rate": 0.0018924235071015637,
      "loss": 1.0034,
      "step": 1309
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.03677359223365784,
      "learning_rate": 0.0018922285809313442,
      "loss": 1.409,
      "step": 1310
    },
    {
      "epoch": 1.748,
      "grad_norm": 0.10000596940517426,
      "learning_rate": 0.0018920334883777707,
      "loss": 1.4515,
      "step": 1311
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 0.033641062676906586,
      "learning_rate": 0.0018918382294772246,
      "loss": 0.9019,
      "step": 1312
    },
    {
      "epoch": 1.7506666666666666,
      "grad_norm": 0.02857152372598648,
      "learning_rate": 0.0018916428042661177,
      "loss": 0.9846,
      "step": 1313
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.044123005121946335,
      "learning_rate": 0.0018914472127808928,
      "loss": 1.2375,
      "step": 1314
    },
    {
      "epoch": 1.7533333333333334,
      "grad_norm": 0.03313343971967697,
      "learning_rate": 0.0018912514550580243,
      "loss": 1.3124,
      "step": 1315
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 0.02761625498533249,
      "learning_rate": 0.001891055531134017,
      "loss": 1.2075,
      "step": 1316
    },
    {
      "epoch": 1.756,
      "grad_norm": 0.04096516966819763,
      "learning_rate": 0.0018908594410454067,
      "loss": 1.1835,
      "step": 1317
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.033655934035778046,
      "learning_rate": 0.0018906631848287605,
      "loss": 1.2573,
      "step": 1318
    },
    {
      "epoch": 1.7586666666666666,
      "grad_norm": 0.03689945116639137,
      "learning_rate": 0.0018904667625206767,
      "loss": 1.2539,
      "step": 1319
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.034489117562770844,
      "learning_rate": 0.0018902701741577842,
      "loss": 1.0822,
      "step": 1320
    },
    {
      "epoch": 1.7613333333333334,
      "grad_norm": 0.028308356180787086,
      "learning_rate": 0.0018900734197767424,
      "loss": 1.1201,
      "step": 1321
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 0.031972646713256836,
      "learning_rate": 0.001889876499414243,
      "loss": 1.3334,
      "step": 1322
    },
    {
      "epoch": 1.764,
      "grad_norm": 0.03217318281531334,
      "learning_rate": 0.0018896794131070072,
      "loss": 1.0036,
      "step": 1323
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 0.07792829722166061,
      "learning_rate": 0.001889482160891788,
      "loss": 0.9451,
      "step": 1324
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.04310810565948486,
      "learning_rate": 0.0018892847428053693,
      "loss": 1.1754,
      "step": 1325
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.03274562209844589,
      "learning_rate": 0.001889087158884565,
      "loss": 1.2035,
      "step": 1326
    },
    {
      "epoch": 1.7693333333333334,
      "grad_norm": 0.028197236359119415,
      "learning_rate": 0.0018888894091662217,
      "loss": 0.9115,
      "step": 1327
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 0.03760989382863045,
      "learning_rate": 0.0018886914936872153,
      "loss": 1.227,
      "step": 1328
    },
    {
      "epoch": 1.772,
      "grad_norm": 0.03211091458797455,
      "learning_rate": 0.0018884934124844533,
      "loss": 1.04,
      "step": 1329
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.04761089012026787,
      "learning_rate": 0.001888295165594874,
      "loss": 1.2624,
      "step": 1330
    },
    {
      "epoch": 1.7746666666666666,
      "grad_norm": 0.035953789949417114,
      "learning_rate": 0.001888096753055447,
      "loss": 1.2851,
      "step": 1331
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.12217722833156586,
      "learning_rate": 0.0018878981749031716,
      "loss": 1.2275,
      "step": 1332
    },
    {
      "epoch": 1.7773333333333334,
      "grad_norm": 0.027957692742347717,
      "learning_rate": 0.0018876994311750792,
      "loss": 1.1839,
      "step": 1333
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 0.030590467154979706,
      "learning_rate": 0.001887500521908232,
      "loss": 0.9701,
      "step": 1334
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.04305307939648628,
      "learning_rate": 0.0018873014471397222,
      "loss": 1.0544,
      "step": 1335
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 0.027654387056827545,
      "learning_rate": 0.0018871022069066737,
      "loss": 1.015,
      "step": 1336
    },
    {
      "epoch": 1.7826666666666666,
      "grad_norm": 0.031566549092531204,
      "learning_rate": 0.0018869028012462408,
      "loss": 1.1429,
      "step": 1337
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.03159976750612259,
      "learning_rate": 0.0018867032301956089,
      "loss": 0.9927,
      "step": 1338
    },
    {
      "epoch": 1.7853333333333334,
      "grad_norm": 0.04417363926768303,
      "learning_rate": 0.0018865034937919937,
      "loss": 1.2067,
      "step": 1339
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.027856070548295975,
      "learning_rate": 0.001886303592072643,
      "loss": 1.089,
      "step": 1340
    },
    {
      "epoch": 1.788,
      "grad_norm": 0.040572457015514374,
      "learning_rate": 0.0018861035250748342,
      "loss": 1.3645,
      "step": 1341
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.0314355194568634,
      "learning_rate": 0.0018859032928358755,
      "loss": 1.2355,
      "step": 1342
    },
    {
      "epoch": 1.7906666666666666,
      "grad_norm": 0.039292823523283005,
      "learning_rate": 0.0018857028953931068,
      "loss": 1.2262,
      "step": 1343
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.05921813100576401,
      "learning_rate": 0.0018855023327838984,
      "loss": 1.2787,
      "step": 1344
    },
    {
      "epoch": 1.7933333333333334,
      "grad_norm": 0.10428109019994736,
      "learning_rate": 0.001885301605045651,
      "loss": 0.9102,
      "step": 1345
    },
    {
      "epoch": 1.7946666666666666,
      "grad_norm": 0.029718823730945587,
      "learning_rate": 0.0018851007122157966,
      "loss": 1.0667,
      "step": 1346
    },
    {
      "epoch": 1.796,
      "grad_norm": 0.025827350094914436,
      "learning_rate": 0.001884899654331798,
      "loss": 1.2169,
      "step": 1347
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 0.02552994154393673,
      "learning_rate": 0.0018846984314311484,
      "loss": 0.9912,
      "step": 1348
    },
    {
      "epoch": 1.7986666666666666,
      "grad_norm": 0.040677592158317566,
      "learning_rate": 0.0018844970435513719,
      "loss": 1.076,
      "step": 1349
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.03254977613687515,
      "learning_rate": 0.0018842954907300237,
      "loss": 0.9481,
      "step": 1350
    },
    {
      "epoch": 1.8013333333333335,
      "grad_norm": 0.03555314615368843,
      "learning_rate": 0.0018840937730046892,
      "loss": 1.0393,
      "step": 1351
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 0.03102840669453144,
      "learning_rate": 0.001883891890412985,
      "loss": 1.0495,
      "step": 1352
    },
    {
      "epoch": 1.804,
      "grad_norm": 0.041418544948101044,
      "learning_rate": 0.0018836898429925584,
      "loss": 1.1912,
      "step": 1353
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 0.04032829776406288,
      "learning_rate": 0.0018834876307810876,
      "loss": 1.0502,
      "step": 1354
    },
    {
      "epoch": 1.8066666666666666,
      "grad_norm": 0.03677399829030037,
      "learning_rate": 0.0018832852538162804,
      "loss": 1.1741,
      "step": 1355
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.032436199486255646,
      "learning_rate": 0.0018830827121358769,
      "loss": 0.9785,
      "step": 1356
    },
    {
      "epoch": 1.8093333333333335,
      "grad_norm": 0.03219539299607277,
      "learning_rate": 0.001882880005777647,
      "loss": 1.0005,
      "step": 1357
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 0.05093623325228691,
      "learning_rate": 0.0018826771347793911,
      "loss": 0.9879,
      "step": 1358
    },
    {
      "epoch": 1.812,
      "grad_norm": 0.03988831490278244,
      "learning_rate": 0.0018824740991789416,
      "loss": 1.2959,
      "step": 1359
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.03173240274190903,
      "learning_rate": 0.00188227089901416,
      "loss": 1.1432,
      "step": 1360
    },
    {
      "epoch": 1.8146666666666667,
      "grad_norm": 0.053757261484861374,
      "learning_rate": 0.0018820675343229395,
      "loss": 1.0454,
      "step": 1361
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.031653717160224915,
      "learning_rate": 0.0018818640051432034,
      "loss": 1.076,
      "step": 1362
    },
    {
      "epoch": 1.8173333333333335,
      "grad_norm": 0.034455813467502594,
      "learning_rate": 0.0018816603115129062,
      "loss": 1.0076,
      "step": 1363
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 0.03756426274776459,
      "learning_rate": 0.001881456453470033,
      "loss": 1.3615,
      "step": 1364
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.03341095894575119,
      "learning_rate": 0.001881252431052599,
      "loss": 1.4133,
      "step": 1365
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.050927259027957916,
      "learning_rate": 0.0018810482442986503,
      "loss": 1.1797,
      "step": 1366
    },
    {
      "epoch": 1.8226666666666667,
      "grad_norm": 0.07957785576581955,
      "learning_rate": 0.0018808438932462642,
      "loss": 1.1293,
      "step": 1367
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.03390580043196678,
      "learning_rate": 0.0018806393779335483,
      "loss": 1.1763,
      "step": 1368
    },
    {
      "epoch": 1.8253333333333335,
      "grad_norm": 0.025584766641259193,
      "learning_rate": 0.0018804346983986402,
      "loss": 1.0189,
      "step": 1369
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.05006355792284012,
      "learning_rate": 0.0018802298546797091,
      "loss": 1.4497,
      "step": 1370
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 0.027011804282665253,
      "learning_rate": 0.0018800248468149544,
      "loss": 1.2305,
      "step": 1371
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 0.035984527319669724,
      "learning_rate": 0.0018798196748426056,
      "loss": 1.0747,
      "step": 1372
    },
    {
      "epoch": 1.8306666666666667,
      "grad_norm": 0.04567969590425491,
      "learning_rate": 0.0018796143388009239,
      "loss": 1.3167,
      "step": 1373
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.028441516682505608,
      "learning_rate": 0.0018794088387282,
      "loss": 1.1614,
      "step": 1374
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.029467375949025154,
      "learning_rate": 0.0018792031746627563,
      "loss": 0.9297,
      "step": 1375
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 0.03096092864871025,
      "learning_rate": 0.0018789973466429447,
      "loss": 1.3011,
      "step": 1376
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 0.04032488167285919,
      "learning_rate": 0.0018787913547071483,
      "loss": 1.405,
      "step": 1377
    },
    {
      "epoch": 1.8373333333333335,
      "grad_norm": 0.04002244397997856,
      "learning_rate": 0.0018785851988937802,
      "loss": 1.1646,
      "step": 1378
    },
    {
      "epoch": 1.8386666666666667,
      "grad_norm": 0.03360747918486595,
      "learning_rate": 0.001878378879241285,
      "loss": 1.1413,
      "step": 1379
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.03476371988654137,
      "learning_rate": 0.001878172395788137,
      "loss": 1.1269,
      "step": 1380
    },
    {
      "epoch": 1.8413333333333335,
      "grad_norm": 0.039157234132289886,
      "learning_rate": 0.001877965748572842,
      "loss": 1.2591,
      "step": 1381
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.04895796626806259,
      "learning_rate": 0.001877758937633935,
      "loss": 1.3431,
      "step": 1382
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 0.0792015865445137,
      "learning_rate": 0.001877551963009982,
      "loss": 1.1679,
      "step": 1383
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 0.03211560100317001,
      "learning_rate": 0.0018773448247395806,
      "loss": 1.3012,
      "step": 1384
    },
    {
      "epoch": 1.8466666666666667,
      "grad_norm": 0.034284643828868866,
      "learning_rate": 0.0018771375228613576,
      "loss": 1.2236,
      "step": 1385
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.027183273807168007,
      "learning_rate": 0.0018769300574139709,
      "loss": 0.9811,
      "step": 1386
    },
    {
      "epoch": 1.8493333333333335,
      "grad_norm": 0.030465709045529366,
      "learning_rate": 0.0018767224284361088,
      "loss": 1.3686,
      "step": 1387
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 0.04073521867394447,
      "learning_rate": 0.0018765146359664899,
      "loss": 1.0492,
      "step": 1388
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 0.030783599242568016,
      "learning_rate": 0.0018763066800438636,
      "loss": 1.1797,
      "step": 1389
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.030743855983018875,
      "learning_rate": 0.0018760985607070098,
      "loss": 1.0134,
      "step": 1390
    },
    {
      "epoch": 1.8546666666666667,
      "grad_norm": 0.031264662742614746,
      "learning_rate": 0.0018758902779947384,
      "loss": 0.9404,
      "step": 1391
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.03574087843298912,
      "learning_rate": 0.0018756818319458906,
      "loss": 1.315,
      "step": 1392
    },
    {
      "epoch": 1.8573333333333333,
      "grad_norm": 0.042973555624485016,
      "learning_rate": 0.0018754732225993369,
      "loss": 1.1153,
      "step": 1393
    },
    {
      "epoch": 1.8586666666666667,
      "grad_norm": 0.02912137471139431,
      "learning_rate": 0.0018752644499939795,
      "loss": 0.8428,
      "step": 1394
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.024553248658776283,
      "learning_rate": 0.00187505551416875,
      "loss": 1.2375,
      "step": 1395
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 0.031365782022476196,
      "learning_rate": 0.001874846415162611,
      "loss": 1.0577,
      "step": 1396
    },
    {
      "epoch": 1.8626666666666667,
      "grad_norm": 0.04311109706759453,
      "learning_rate": 0.0018746371530145556,
      "loss": 0.9856,
      "step": 1397
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.0397888720035553,
      "learning_rate": 0.001874427727763607,
      "loss": 1.2438,
      "step": 1398
    },
    {
      "epoch": 1.8653333333333333,
      "grad_norm": 0.03499394282698631,
      "learning_rate": 0.0018742181394488192,
      "loss": 1.1497,
      "step": 1399
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.030373457819223404,
      "learning_rate": 0.0018740083881092758,
      "loss": 1.4715,
      "step": 1400
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 0.030928129330277443,
      "learning_rate": 0.001873798473784092,
      "loss": 1.0604,
      "step": 1401
    },
    {
      "epoch": 1.8693333333333333,
      "grad_norm": 0.06468886137008667,
      "learning_rate": 0.0018735883965124121,
      "loss": 1.2356,
      "step": 1402
    },
    {
      "epoch": 1.8706666666666667,
      "grad_norm": 0.18336279690265656,
      "learning_rate": 0.0018733781563334121,
      "loss": 1.1076,
      "step": 1403
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.038702357560396194,
      "learning_rate": 0.0018731677532862975,
      "loss": 1.0531,
      "step": 1404
    },
    {
      "epoch": 1.8733333333333333,
      "grad_norm": 0.03170780465006828,
      "learning_rate": 0.001872957187410304,
      "loss": 1.1963,
      "step": 1405
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.03813354671001434,
      "learning_rate": 0.0018727464587446985,
      "loss": 1.079,
      "step": 1406
    },
    {
      "epoch": 1.876,
      "grad_norm": 0.03671356663107872,
      "learning_rate": 0.0018725355673287777,
      "loss": 0.8986,
      "step": 1407
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 0.03545207157731056,
      "learning_rate": 0.0018723245132018689,
      "loss": 0.9366,
      "step": 1408
    },
    {
      "epoch": 1.8786666666666667,
      "grad_norm": 0.15159215033054352,
      "learning_rate": 0.0018721132964033293,
      "loss": 1.4992,
      "step": 1409
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.033975109457969666,
      "learning_rate": 0.001871901916972547,
      "loss": 1.1877,
      "step": 1410
    },
    {
      "epoch": 1.8813333333333333,
      "grad_norm": 0.03172123432159424,
      "learning_rate": 0.00187169037494894,
      "loss": 0.9717,
      "step": 1411
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 0.030028928071260452,
      "learning_rate": 0.0018714786703719572,
      "loss": 1.1786,
      "step": 1412
    },
    {
      "epoch": 1.884,
      "grad_norm": 0.03153274953365326,
      "learning_rate": 0.0018712668032810768,
      "loss": 1.0732,
      "step": 1413
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.042484577745199203,
      "learning_rate": 0.001871054773715808,
      "loss": 1.1957,
      "step": 1414
    },
    {
      "epoch": 1.8866666666666667,
      "grad_norm": 0.040974248200654984,
      "learning_rate": 0.001870842581715691,
      "loss": 0.9419,
      "step": 1415
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.08846309781074524,
      "learning_rate": 0.0018706302273202942,
      "loss": 1.1518,
      "step": 1416
    },
    {
      "epoch": 1.8893333333333333,
      "grad_norm": 0.03399694710969925,
      "learning_rate": 0.0018704177105692186,
      "loss": 1.0013,
      "step": 1417
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 0.03126509487628937,
      "learning_rate": 0.0018702050315020941,
      "loss": 1.1241,
      "step": 1418
    },
    {
      "epoch": 1.892,
      "grad_norm": 0.028887374326586723,
      "learning_rate": 0.0018699921901585812,
      "loss": 1.4328,
      "step": 1419
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.038699761033058167,
      "learning_rate": 0.001869779186578371,
      "loss": 1.0091,
      "step": 1420
    },
    {
      "epoch": 1.8946666666666667,
      "grad_norm": 0.04095184803009033,
      "learning_rate": 0.0018695660208011841,
      "loss": 1.2887,
      "step": 1421
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.04157808795571327,
      "learning_rate": 0.0018693526928667721,
      "loss": 1.2746,
      "step": 1422
    },
    {
      "epoch": 1.8973333333333333,
      "grad_norm": 0.10388131439685822,
      "learning_rate": 0.0018691392028149163,
      "loss": 1.3753,
      "step": 1423
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 0.03579718992114067,
      "learning_rate": 0.0018689255506854287,
      "loss": 1.0806,
      "step": 1424
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.026865215972065926,
      "learning_rate": 0.0018687117365181513,
      "loss": 1.0447,
      "step": 1425
    },
    {
      "epoch": 1.9013333333333333,
      "grad_norm": 0.030520541593432426,
      "learning_rate": 0.0018684977603529557,
      "loss": 0.9882,
      "step": 1426
    },
    {
      "epoch": 1.9026666666666667,
      "grad_norm": 0.027295174077153206,
      "learning_rate": 0.001868283622229745,
      "loss": 1.2894,
      "step": 1427
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.029721692204475403,
      "learning_rate": 0.0018680693221884517,
      "loss": 1.016,
      "step": 1428
    },
    {
      "epoch": 1.9053333333333333,
      "grad_norm": 0.02969093807041645,
      "learning_rate": 0.0018678548602690387,
      "loss": 1.0913,
      "step": 1429
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.034553758800029755,
      "learning_rate": 0.0018676402365114982,
      "loss": 1.2696,
      "step": 1430
    },
    {
      "epoch": 1.908,
      "grad_norm": 0.03832172602415085,
      "learning_rate": 0.0018674254509558543,
      "loss": 1.0985,
      "step": 1431
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 0.03448032587766647,
      "learning_rate": 0.00186721050364216,
      "loss": 1.3501,
      "step": 1432
    },
    {
      "epoch": 1.9106666666666667,
      "grad_norm": 0.04191401228308678,
      "learning_rate": 0.001866995394610499,
      "loss": 1.2344,
      "step": 1433
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.03059355355799198,
      "learning_rate": 0.0018667801239009845,
      "loss": 1.274,
      "step": 1434
    },
    {
      "epoch": 1.9133333333333333,
      "grad_norm": 0.03467783331871033,
      "learning_rate": 0.001866564691553761,
      "loss": 0.9425,
      "step": 1435
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 0.027513697743415833,
      "learning_rate": 0.0018663490976090016,
      "loss": 1.0682,
      "step": 1436
    },
    {
      "epoch": 1.916,
      "grad_norm": 0.026467466726899147,
      "learning_rate": 0.001866133342106911,
      "loss": 1.0783,
      "step": 1437
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.047839466482400894,
      "learning_rate": 0.0018659174250877236,
      "loss": 1.4764,
      "step": 1438
    },
    {
      "epoch": 1.9186666666666667,
      "grad_norm": 0.025585174560546875,
      "learning_rate": 0.0018657013465917032,
      "loss": 1.1768,
      "step": 1439
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.04048372432589531,
      "learning_rate": 0.0018654851066591447,
      "loss": 1.221,
      "step": 1440
    },
    {
      "epoch": 1.9213333333333333,
      "grad_norm": 0.029877396300435066,
      "learning_rate": 0.0018652687053303722,
      "loss": 1.2758,
      "step": 1441
    },
    {
      "epoch": 1.9226666666666667,
      "grad_norm": 0.038539037108421326,
      "learning_rate": 0.0018650521426457406,
      "loss": 1.1292,
      "step": 1442
    },
    {
      "epoch": 1.924,
      "grad_norm": 0.03709743171930313,
      "learning_rate": 0.0018648354186456349,
      "loss": 0.914,
      "step": 1443
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 0.033556193113327026,
      "learning_rate": 0.0018646185333704695,
      "loss": 1.018,
      "step": 1444
    },
    {
      "epoch": 1.9266666666666667,
      "grad_norm": 0.03646198660135269,
      "learning_rate": 0.0018644014868606895,
      "loss": 1.1389,
      "step": 1445
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.029426690191030502,
      "learning_rate": 0.00186418427915677,
      "loss": 1.0026,
      "step": 1446
    },
    {
      "epoch": 1.9293333333333333,
      "grad_norm": 0.029403401538729668,
      "learning_rate": 0.0018639669102992159,
      "loss": 1.1398,
      "step": 1447
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 0.040749043226242065,
      "learning_rate": 0.001863749380328562,
      "loss": 1.2134,
      "step": 1448
    },
    {
      "epoch": 1.932,
      "grad_norm": 0.04176736995577812,
      "learning_rate": 0.0018635316892853739,
      "loss": 1.0495,
      "step": 1449
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.03915014490485191,
      "learning_rate": 0.0018633138372102468,
      "loss": 1.4383,
      "step": 1450
    },
    {
      "epoch": 1.9346666666666668,
      "grad_norm": 0.035090718418359756,
      "learning_rate": 0.0018630958241438052,
      "loss": 1.0151,
      "step": 1451
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.04583970457315445,
      "learning_rate": 0.001862877650126705,
      "loss": 1.4779,
      "step": 1452
    },
    {
      "epoch": 1.9373333333333334,
      "grad_norm": 0.03524994105100632,
      "learning_rate": 0.0018626593151996314,
      "loss": 0.9529,
      "step": 1453
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.02554062008857727,
      "learning_rate": 0.0018624408194032993,
      "loss": 1.1822,
      "step": 1454
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.037093088030815125,
      "learning_rate": 0.0018622221627784539,
      "loss": 0.9309,
      "step": 1455
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 0.034735362976789474,
      "learning_rate": 0.001862003345365871,
      "loss": 0.9868,
      "step": 1456
    },
    {
      "epoch": 1.9426666666666668,
      "grad_norm": 0.034348879009485245,
      "learning_rate": 0.0018617843672063552,
      "loss": 0.9333,
      "step": 1457
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.05417470633983612,
      "learning_rate": 0.0018615652283407419,
      "loss": 1.2609,
      "step": 1458
    },
    {
      "epoch": 1.9453333333333334,
      "grad_norm": 0.039587054401636124,
      "learning_rate": 0.0018613459288098961,
      "loss": 1.2004,
      "step": 1459
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.0480683334171772,
      "learning_rate": 0.0018611264686547134,
      "loss": 1.0641,
      "step": 1460
    },
    {
      "epoch": 1.948,
      "grad_norm": 0.07752077281475067,
      "learning_rate": 0.0018609068479161182,
      "loss": 1.2008,
      "step": 1461
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 0.028380312025547028,
      "learning_rate": 0.0018606870666350661,
      "loss": 0.9766,
      "step": 1462
    },
    {
      "epoch": 1.9506666666666668,
      "grad_norm": 0.029873916879296303,
      "learning_rate": 0.0018604671248525417,
      "loss": 0.8542,
      "step": 1463
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.03763354942202568,
      "learning_rate": 0.0018602470226095603,
      "loss": 1.0896,
      "step": 1464
    },
    {
      "epoch": 1.9533333333333334,
      "grad_norm": 0.04448999464511871,
      "learning_rate": 0.001860026759947166,
      "loss": 1.2345,
      "step": 1465
    },
    {
      "epoch": 1.9546666666666668,
      "grad_norm": 0.032382331788539886,
      "learning_rate": 0.0018598063369064343,
      "loss": 0.9043,
      "step": 1466
    },
    {
      "epoch": 1.956,
      "grad_norm": 0.037869833409786224,
      "learning_rate": 0.001859585753528469,
      "loss": 0.9883,
      "step": 1467
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 0.036326002329587936,
      "learning_rate": 0.0018593650098544052,
      "loss": 0.9629,
      "step": 1468
    },
    {
      "epoch": 1.9586666666666668,
      "grad_norm": 0.03744564950466156,
      "learning_rate": 0.0018591441059254074,
      "loss": 1.2657,
      "step": 1469
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.02835443802177906,
      "learning_rate": 0.0018589230417826697,
      "loss": 1.2481,
      "step": 1470
    },
    {
      "epoch": 1.9613333333333334,
      "grad_norm": 0.20039622485637665,
      "learning_rate": 0.0018587018174674164,
      "loss": 1.3225,
      "step": 1471
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 0.03728345036506653,
      "learning_rate": 0.001858480433020901,
      "loss": 1.1595,
      "step": 1472
    },
    {
      "epoch": 1.964,
      "grad_norm": 0.03157305717468262,
      "learning_rate": 0.0018582588884844084,
      "loss": 1.2043,
      "step": 1473
    },
    {
      "epoch": 1.9653333333333334,
      "grad_norm": 0.22094811499118805,
      "learning_rate": 0.0018580371838992514,
      "loss": 1.0541,
      "step": 1474
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.10128224641084671,
      "learning_rate": 0.0018578153193067745,
      "loss": 1.1698,
      "step": 1475
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.1280946284532547,
      "learning_rate": 0.0018575932947483503,
      "loss": 1.2659,
      "step": 1476
    },
    {
      "epoch": 1.9693333333333334,
      "grad_norm": 0.09836617857217789,
      "learning_rate": 0.0018573711102653824,
      "loss": 1.0374,
      "step": 1477
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.031240783631801605,
      "learning_rate": 0.0018571487658993041,
      "loss": 1.0866,
      "step": 1478
    },
    {
      "epoch": 1.972,
      "grad_norm": 0.12695123255252838,
      "learning_rate": 0.0018569262616915782,
      "loss": 1.5457,
      "step": 1479
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.04127512127161026,
      "learning_rate": 0.0018567035976836974,
      "loss": 0.9624,
      "step": 1480
    },
    {
      "epoch": 1.9746666666666668,
      "grad_norm": 0.039520107209682465,
      "learning_rate": 0.0018564807739171842,
      "loss": 1.0489,
      "step": 1481
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.04577441141009331,
      "learning_rate": 0.001856257790433591,
      "loss": 1.0165,
      "step": 1482
    },
    {
      "epoch": 1.9773333333333334,
      "grad_norm": 0.08957981318235397,
      "learning_rate": 0.0018560346472745,
      "loss": 1.4289,
      "step": 1483
    },
    {
      "epoch": 1.9786666666666668,
      "grad_norm": 0.043472450226545334,
      "learning_rate": 0.001855811344481523,
      "loss": 1.1109,
      "step": 1484
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.07138729095458984,
      "learning_rate": 0.0018555878820963013,
      "loss": 0.9984,
      "step": 1485
    },
    {
      "epoch": 1.9813333333333332,
      "grad_norm": 0.045047372579574585,
      "learning_rate": 0.0018553642601605068,
      "loss": 1.1115,
      "step": 1486
    },
    {
      "epoch": 1.9826666666666668,
      "grad_norm": 0.03237010911107063,
      "learning_rate": 0.0018551404787158403,
      "loss": 1.008,
      "step": 1487
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.055632084608078,
      "learning_rate": 0.0018549165378040327,
      "loss": 1.3125,
      "step": 1488
    },
    {
      "epoch": 1.9853333333333332,
      "grad_norm": 0.09588129818439484,
      "learning_rate": 0.001854692437466845,
      "loss": 1.2365,
      "step": 1489
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.03002503141760826,
      "learning_rate": 0.0018544681777460674,
      "loss": 1.1224,
      "step": 1490
    },
    {
      "epoch": 1.988,
      "grad_norm": 0.04386824369430542,
      "learning_rate": 0.00185424375868352,
      "loss": 1.2378,
      "step": 1491
    },
    {
      "epoch": 1.9893333333333332,
      "grad_norm": 0.02813909202814102,
      "learning_rate": 0.001854019180321053,
      "loss": 1.1875,
      "step": 1492
    },
    {
      "epoch": 1.9906666666666668,
      "grad_norm": 0.03873402252793312,
      "learning_rate": 0.0018537944427005448,
      "loss": 1.1552,
      "step": 1493
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.037198666483163834,
      "learning_rate": 0.0018535695458639055,
      "loss": 1.1678,
      "step": 1494
    },
    {
      "epoch": 1.9933333333333332,
      "grad_norm": 0.031215107068419456,
      "learning_rate": 0.001853344489853074,
      "loss": 1.2746,
      "step": 1495
    },
    {
      "epoch": 1.9946666666666668,
      "grad_norm": 0.03172533959150314,
      "learning_rate": 0.0018531192747100185,
      "loss": 1.0674,
      "step": 1496
    },
    {
      "epoch": 1.996,
      "grad_norm": 0.06409084051847458,
      "learning_rate": 0.0018528939004767377,
      "loss": 1.2441,
      "step": 1497
    },
    {
      "epoch": 1.9973333333333332,
      "grad_norm": 0.05110764876008034,
      "learning_rate": 0.001852668367195259,
      "loss": 1.3827,
      "step": 1498
    },
    {
      "epoch": 1.9986666666666668,
      "grad_norm": 0.0384422168135643,
      "learning_rate": 0.0018524426749076406,
      "loss": 1.0968,
      "step": 1499
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.03537039831280708,
      "learning_rate": 0.0018522168236559692,
      "loss": 1.4937,
      "step": 1500
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.1484078168869019,
      "eval_runtime": 24.0374,
      "eval_samples_per_second": 20.801,
      "eval_steps_per_second": 2.621,
      "step": 1500
    }
  ],
  "logging_steps": 1,
  "max_steps": 7500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 1500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.693431600499917e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
