{
  "best_metric": 1.9331860542297363,
  "best_model_checkpoint": "llama31_test/checkpoint-750",
  "epoch": 3.0,
  "eval_steps": 250,
  "global_step": 750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004,
      "grad_norm": 0.4784339964389801,
      "learning_rate": 5.263157894736842e-06,
      "loss": 6.214,
      "step": 1
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.4392923414707184,
      "learning_rate": 1.0526315789473684e-05,
      "loss": 5.934,
      "step": 2
    },
    {
      "epoch": 0.012,
      "grad_norm": 0.5393584370613098,
      "learning_rate": 1.5789473684210526e-05,
      "loss": 6.0088,
      "step": 3
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.4156465530395508,
      "learning_rate": 2.105263157894737e-05,
      "loss": 6.2645,
      "step": 4
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.4890686273574829,
      "learning_rate": 2.6315789473684212e-05,
      "loss": 6.0765,
      "step": 5
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.3873763382434845,
      "learning_rate": 3.157894736842105e-05,
      "loss": 6.366,
      "step": 6
    },
    {
      "epoch": 0.028,
      "grad_norm": 0.39986780285835266,
      "learning_rate": 3.6842105263157895e-05,
      "loss": 6.3909,
      "step": 7
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.4439467787742615,
      "learning_rate": 4.210526315789474e-05,
      "loss": 6.1008,
      "step": 8
    },
    {
      "epoch": 0.036,
      "grad_norm": 0.4216740131378174,
      "learning_rate": 4.736842105263158e-05,
      "loss": 6.1863,
      "step": 9
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.4025573432445526,
      "learning_rate": 5.2631578947368424e-05,
      "loss": 6.3604,
      "step": 10
    },
    {
      "epoch": 0.044,
      "grad_norm": 0.45535969734191895,
      "learning_rate": 5.789473684210527e-05,
      "loss": 5.7532,
      "step": 11
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.4248896837234497,
      "learning_rate": 6.31578947368421e-05,
      "loss": 5.9956,
      "step": 12
    },
    {
      "epoch": 0.052,
      "grad_norm": 0.4309670627117157,
      "learning_rate": 6.842105263157895e-05,
      "loss": 6.2867,
      "step": 13
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.4236027002334595,
      "learning_rate": 7.368421052631579e-05,
      "loss": 6.1302,
      "step": 14
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.4374712407588959,
      "learning_rate": 7.894736842105263e-05,
      "loss": 6.2312,
      "step": 15
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.4149826467037201,
      "learning_rate": 8.421052631578948e-05,
      "loss": 5.8842,
      "step": 16
    },
    {
      "epoch": 0.068,
      "grad_norm": 0.445383220911026,
      "learning_rate": 8.947368421052632e-05,
      "loss": 5.615,
      "step": 17
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.4713725447654724,
      "learning_rate": 9.473684210526316e-05,
      "loss": 5.802,
      "step": 18
    },
    {
      "epoch": 0.076,
      "grad_norm": 0.4527994394302368,
      "learning_rate": 0.0001,
      "loss": 6.1326,
      "step": 19
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.5209766030311584,
      "learning_rate": 0.00010526315789473685,
      "loss": 5.9146,
      "step": 20
    },
    {
      "epoch": 0.084,
      "grad_norm": 0.4161616265773773,
      "learning_rate": 0.0001105263157894737,
      "loss": 5.4767,
      "step": 21
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.4446420967578888,
      "learning_rate": 0.00011578947368421053,
      "loss": 5.9121,
      "step": 22
    },
    {
      "epoch": 0.092,
      "grad_norm": 0.4683161973953247,
      "learning_rate": 0.00012105263157894738,
      "loss": 5.9461,
      "step": 23
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.5022540092468262,
      "learning_rate": 0.0001263157894736842,
      "loss": 5.988,
      "step": 24
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.5101831555366516,
      "learning_rate": 0.00013157894736842108,
      "loss": 6.0137,
      "step": 25
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.46853986382484436,
      "learning_rate": 0.0001368421052631579,
      "loss": 6.1885,
      "step": 26
    },
    {
      "epoch": 0.108,
      "grad_norm": 0.513238787651062,
      "learning_rate": 0.00014210526315789474,
      "loss": 5.6406,
      "step": 27
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.5133530497550964,
      "learning_rate": 0.00014736842105263158,
      "loss": 5.5967,
      "step": 28
    },
    {
      "epoch": 0.116,
      "grad_norm": 0.5018254518508911,
      "learning_rate": 0.00015263157894736845,
      "loss": 5.8969,
      "step": 29
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.4603661298751831,
      "learning_rate": 0.00015789473684210527,
      "loss": 5.9683,
      "step": 30
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.5139949321746826,
      "learning_rate": 0.0001631578947368421,
      "loss": 5.7497,
      "step": 31
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.5443980693817139,
      "learning_rate": 0.00016842105263157895,
      "loss": 5.4531,
      "step": 32
    },
    {
      "epoch": 0.132,
      "grad_norm": 0.44543200731277466,
      "learning_rate": 0.0001736842105263158,
      "loss": 6.1245,
      "step": 33
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.5164801478385925,
      "learning_rate": 0.00017894736842105264,
      "loss": 5.5738,
      "step": 34
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.5367088913917542,
      "learning_rate": 0.00018421052631578948,
      "loss": 5.3527,
      "step": 35
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.5045109391212463,
      "learning_rate": 0.00018947368421052632,
      "loss": 5.4463,
      "step": 36
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.4642709791660309,
      "learning_rate": 0.00019473684210526317,
      "loss": 5.492,
      "step": 37
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.46966925263404846,
      "learning_rate": 0.0002,
      "loss": 5.7257,
      "step": 38
    },
    {
      "epoch": 0.156,
      "grad_norm": 0.4432699382305145,
      "learning_rate": 0.00019999966405802826,
      "loss": 5.3454,
      "step": 39
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.45373111963272095,
      "learning_rate": 0.00019999865623437013,
      "loss": 5.4597,
      "step": 40
    },
    {
      "epoch": 0.164,
      "grad_norm": 0.45346054434776306,
      "learning_rate": 0.00019999697653579705,
      "loss": 5.6333,
      "step": 41
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.4783134162425995,
      "learning_rate": 0.00019999462497359466,
      "loss": 5.0227,
      "step": 42
    },
    {
      "epoch": 0.172,
      "grad_norm": 0.4400278627872467,
      "learning_rate": 0.0001999916015635627,
      "loss": 5.4072,
      "step": 43
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.393835186958313,
      "learning_rate": 0.00019998790632601496,
      "loss": 4.9995,
      "step": 44
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.4605180621147156,
      "learning_rate": 0.00019998353928577919,
      "loss": 5.1187,
      "step": 45
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.38661497831344604,
      "learning_rate": 0.0001999785004721968,
      "loss": 5.214,
      "step": 46
    },
    {
      "epoch": 0.188,
      "grad_norm": 0.5314667224884033,
      "learning_rate": 0.0001999727899191228,
      "loss": 5.4626,
      "step": 47
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.3709624707698822,
      "learning_rate": 0.00019996640766492543,
      "loss": 5.0064,
      "step": 48
    },
    {
      "epoch": 0.196,
      "grad_norm": 0.448943555355072,
      "learning_rate": 0.00019995935375248606,
      "loss": 4.8794,
      "step": 49
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.386583149433136,
      "learning_rate": 0.00019995162822919883,
      "loss": 4.8934,
      "step": 50
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.41233915090560913,
      "learning_rate": 0.00019994323114697022,
      "loss": 4.9477,
      "step": 51
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.35428494215011597,
      "learning_rate": 0.00019993416256221895,
      "loss": 4.9791,
      "step": 52
    },
    {
      "epoch": 0.212,
      "grad_norm": 0.3947752118110657,
      "learning_rate": 0.0001999244225358753,
      "loss": 5.1636,
      "step": 53
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.3787893056869507,
      "learning_rate": 0.00019991401113338104,
      "loss": 5.2237,
      "step": 54
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.35882118344306946,
      "learning_rate": 0.00019990292842468868,
      "loss": 4.8061,
      "step": 55
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.4556887149810791,
      "learning_rate": 0.00019989117448426108,
      "loss": 4.8841,
      "step": 56
    },
    {
      "epoch": 0.228,
      "grad_norm": 0.3702075481414795,
      "learning_rate": 0.0001998787493910712,
      "loss": 4.9646,
      "step": 57
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.35284265875816345,
      "learning_rate": 0.00019986565322860115,
      "loss": 4.9417,
      "step": 58
    },
    {
      "epoch": 0.236,
      "grad_norm": 0.340786874294281,
      "learning_rate": 0.000199851886084842,
      "loss": 4.3131,
      "step": 59
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.473077654838562,
      "learning_rate": 0.00019983744805229296,
      "loss": 4.6141,
      "step": 60
    },
    {
      "epoch": 0.244,
      "grad_norm": 0.33983147144317627,
      "learning_rate": 0.00019982233922796085,
      "loss": 4.7423,
      "step": 61
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.37472233176231384,
      "learning_rate": 0.00019980655971335945,
      "loss": 4.9183,
      "step": 62
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.44570332765579224,
      "learning_rate": 0.00019979010961450878,
      "loss": 4.7789,
      "step": 63
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.356198787689209,
      "learning_rate": 0.00019977298904193437,
      "loss": 4.6349,
      "step": 64
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.35470113158226013,
      "learning_rate": 0.00019975519811066663,
      "loss": 4.5819,
      "step": 65
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.3320401906967163,
      "learning_rate": 0.00019973673694024,
      "loss": 4.606,
      "step": 66
    },
    {
      "epoch": 0.268,
      "grad_norm": 0.3716651499271393,
      "learning_rate": 0.0001997176056546921,
      "loss": 4.6923,
      "step": 67
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.3037758767604828,
      "learning_rate": 0.00019969780438256293,
      "loss": 4.4587,
      "step": 68
    },
    {
      "epoch": 0.276,
      "grad_norm": 0.2729921340942383,
      "learning_rate": 0.0001996773332568941,
      "loss": 4.695,
      "step": 69
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.3160737454891205,
      "learning_rate": 0.0001996561924152278,
      "loss": 4.8538,
      "step": 70
    },
    {
      "epoch": 0.284,
      "grad_norm": 0.3136674165725708,
      "learning_rate": 0.00019963438199960599,
      "loss": 4.3914,
      "step": 71
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.27826938033103943,
      "learning_rate": 0.0001996119021565693,
      "loss": 4.3679,
      "step": 72
    },
    {
      "epoch": 0.292,
      "grad_norm": 0.27338913083076477,
      "learning_rate": 0.00019958875303715615,
      "loss": 4.5426,
      "step": 73
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.2611103057861328,
      "learning_rate": 0.0001995649347969019,
      "loss": 4.1769,
      "step": 74
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.2666304111480713,
      "learning_rate": 0.0001995404475958373,
      "loss": 4.3617,
      "step": 75
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.2848054766654968,
      "learning_rate": 0.00019951529159848805,
      "loss": 4.3225,
      "step": 76
    },
    {
      "epoch": 0.308,
      "grad_norm": 0.25759223103523254,
      "learning_rate": 0.0001994894669738732,
      "loss": 4.0179,
      "step": 77
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.24610958993434906,
      "learning_rate": 0.00019946297389550433,
      "loss": 4.2192,
      "step": 78
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.27180609107017517,
      "learning_rate": 0.0001994358125413841,
      "loss": 4.3505,
      "step": 79
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.2435264140367508,
      "learning_rate": 0.00019940798309400526,
      "loss": 4.0923,
      "step": 80
    },
    {
      "epoch": 0.324,
      "grad_norm": 0.26358747482299805,
      "learning_rate": 0.0001993794857403495,
      "loss": 4.0708,
      "step": 81
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.22146102786064148,
      "learning_rate": 0.0001993503206718859,
      "loss": 3.9416,
      "step": 82
    },
    {
      "epoch": 0.332,
      "grad_norm": 0.2412416785955429,
      "learning_rate": 0.0001993204880845699,
      "loss": 4.3656,
      "step": 83
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.24067983031272888,
      "learning_rate": 0.00019928998817884182,
      "loss": 4.4357,
      "step": 84
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.20288822054862976,
      "learning_rate": 0.00019925882115962568,
      "loss": 4.2211,
      "step": 85
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.2635680139064789,
      "learning_rate": 0.00019922698723632767,
      "loss": 4.636,
      "step": 86
    },
    {
      "epoch": 0.348,
      "grad_norm": 0.2533942461013794,
      "learning_rate": 0.00019919448662283478,
      "loss": 4.218,
      "step": 87
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.2142430692911148,
      "learning_rate": 0.00019916131953751342,
      "loss": 4.2783,
      "step": 88
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.22049568593502045,
      "learning_rate": 0.00019912748620320794,
      "loss": 3.9361,
      "step": 89
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.228661447763443,
      "learning_rate": 0.00019909298684723904,
      "loss": 4.3481,
      "step": 90
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.23301751911640167,
      "learning_rate": 0.00019905782170140238,
      "loss": 4.1397,
      "step": 91
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.23621246218681335,
      "learning_rate": 0.00019902199100196697,
      "loss": 4.0657,
      "step": 92
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.23004989326000214,
      "learning_rate": 0.00019898549498967343,
      "loss": 4.3948,
      "step": 93
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.2750054597854614,
      "learning_rate": 0.00019894833390973266,
      "loss": 4.5767,
      "step": 94
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.23061150312423706,
      "learning_rate": 0.000198910508011824,
      "loss": 4.2543,
      "step": 95
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.21883933246135712,
      "learning_rate": 0.00019887201755009357,
      "loss": 4.0949,
      "step": 96
    },
    {
      "epoch": 0.388,
      "grad_norm": 0.22329269349575043,
      "learning_rate": 0.00019883286278315262,
      "loss": 4.0383,
      "step": 97
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.18738536536693573,
      "learning_rate": 0.0001987930439740757,
      "loss": 3.9865,
      "step": 98
    },
    {
      "epoch": 0.396,
      "grad_norm": 0.22126416862010956,
      "learning_rate": 0.00019875256139039902,
      "loss": 4.1082,
      "step": 99
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.18853801488876343,
      "learning_rate": 0.00019871141530411853,
      "loss": 3.9139,
      "step": 100
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.19810861349105835,
      "learning_rate": 0.00019866960599168826,
      "loss": 3.9028,
      "step": 101
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.21376661956310272,
      "learning_rate": 0.0001986271337340182,
      "loss": 3.996,
      "step": 102
    },
    {
      "epoch": 0.412,
      "grad_norm": 0.22600379586219788,
      "learning_rate": 0.0001985839988164726,
      "loss": 3.7888,
      "step": 103
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.22871556878089905,
      "learning_rate": 0.00019854020152886814,
      "loss": 3.6307,
      "step": 104
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.19921822845935822,
      "learning_rate": 0.00019849574216547171,
      "loss": 3.8658,
      "step": 105
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.21603383123874664,
      "learning_rate": 0.0001984506210249986,
      "loss": 4.023,
      "step": 106
    },
    {
      "epoch": 0.428,
      "grad_norm": 0.2065233290195465,
      "learning_rate": 0.00019840483841061058,
      "loss": 3.9083,
      "step": 107
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.204634889960289,
      "learning_rate": 0.00019835839462991361,
      "loss": 3.8998,
      "step": 108
    },
    {
      "epoch": 0.436,
      "grad_norm": 0.1894771307706833,
      "learning_rate": 0.00019831128999495606,
      "loss": 3.8346,
      "step": 109
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.21026335656642914,
      "learning_rate": 0.00019826352482222638,
      "loss": 3.9877,
      "step": 110
    },
    {
      "epoch": 0.444,
      "grad_norm": 0.2095920741558075,
      "learning_rate": 0.0001982150994326511,
      "loss": 3.8244,
      "step": 111
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.21727772057056427,
      "learning_rate": 0.00019816601415159263,
      "loss": 3.8133,
      "step": 112
    },
    {
      "epoch": 0.452,
      "grad_norm": 0.26117390394210815,
      "learning_rate": 0.0001981162693088471,
      "loss": 4.0344,
      "step": 113
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.20109862089157104,
      "learning_rate": 0.0001980658652386421,
      "loss": 3.8446,
      "step": 114
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.20960460603237152,
      "learning_rate": 0.0001980148022796345,
      "loss": 4.0537,
      "step": 115
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.19522885978221893,
      "learning_rate": 0.00019796308077490817,
      "loss": 3.6823,
      "step": 116
    },
    {
      "epoch": 0.468,
      "grad_norm": 0.24077893793582916,
      "learning_rate": 0.00019791070107197153,
      "loss": 3.9274,
      "step": 117
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.25236910581588745,
      "learning_rate": 0.00019785766352275542,
      "loss": 4.0123,
      "step": 118
    },
    {
      "epoch": 0.476,
      "grad_norm": 0.22876451909542084,
      "learning_rate": 0.0001978039684836106,
      "loss": 3.9554,
      "step": 119
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.1948583424091339,
      "learning_rate": 0.00019774961631530545,
      "loss": 3.6754,
      "step": 120
    },
    {
      "epoch": 0.484,
      "grad_norm": 0.1958794891834259,
      "learning_rate": 0.0001976946073830234,
      "loss": 3.9746,
      "step": 121
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.19073235988616943,
      "learning_rate": 0.00019763894205636072,
      "loss": 3.6453,
      "step": 122
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.18706361949443817,
      "learning_rate": 0.00019758262070932375,
      "loss": 3.9003,
      "step": 123
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.28321877121925354,
      "learning_rate": 0.00019752564372032657,
      "loss": 3.9541,
      "step": 124
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.19286976754665375,
      "learning_rate": 0.00019746801147218842,
      "loss": 3.6473,
      "step": 125
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.20077985525131226,
      "learning_rate": 0.00019740972435213115,
      "loss": 3.6844,
      "step": 126
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.18777069449424744,
      "learning_rate": 0.00019735078275177654,
      "loss": 3.5302,
      "step": 127
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.21749374270439148,
      "learning_rate": 0.00019729118706714375,
      "loss": 3.8206,
      "step": 128
    },
    {
      "epoch": 0.516,
      "grad_norm": 0.22461704909801483,
      "learning_rate": 0.00019723093769864663,
      "loss": 3.6288,
      "step": 129
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.207794189453125,
      "learning_rate": 0.00019717003505109095,
      "loss": 3.8557,
      "step": 130
    },
    {
      "epoch": 0.524,
      "grad_norm": 0.1765696108341217,
      "learning_rate": 0.0001971084795336719,
      "loss": 3.8331,
      "step": 131
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.18416836857795715,
      "learning_rate": 0.00019704627155997108,
      "loss": 3.5759,
      "step": 132
    },
    {
      "epoch": 0.532,
      "grad_norm": 0.17642898857593536,
      "learning_rate": 0.00019698341154795389,
      "loss": 3.8103,
      "step": 133
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.1897030770778656,
      "learning_rate": 0.00019691989991996663,
      "loss": 3.4696,
      "step": 134
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.17490051686763763,
      "learning_rate": 0.00019685573710273376,
      "loss": 3.7594,
      "step": 135
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.19629855453968048,
      "learning_rate": 0.0001967909235273549,
      "loss": 3.6005,
      "step": 136
    },
    {
      "epoch": 0.548,
      "grad_norm": 0.1772780567407608,
      "learning_rate": 0.00019672545962930215,
      "loss": 3.3996,
      "step": 137
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.21250469982624054,
      "learning_rate": 0.00019665934584841682,
      "loss": 3.533,
      "step": 138
    },
    {
      "epoch": 0.556,
      "grad_norm": 0.1839977204799652,
      "learning_rate": 0.00019659258262890683,
      "loss": 3.7061,
      "step": 139
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.20183798670768738,
      "learning_rate": 0.00019652517041934356,
      "loss": 3.7575,
      "step": 140
    },
    {
      "epoch": 0.564,
      "grad_norm": 0.20151212811470032,
      "learning_rate": 0.00019645710967265882,
      "loss": 3.6179,
      "step": 141
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.17049460113048553,
      "learning_rate": 0.00019638840084614182,
      "loss": 3.798,
      "step": 142
    },
    {
      "epoch": 0.572,
      "grad_norm": 0.20975027978420258,
      "learning_rate": 0.00019631904440143612,
      "loss": 3.7226,
      "step": 143
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.18792618811130524,
      "learning_rate": 0.00019624904080453655,
      "loss": 3.641,
      "step": 144
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.19618527591228485,
      "learning_rate": 0.00019617839052578603,
      "loss": 3.6207,
      "step": 145
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.19512119889259338,
      "learning_rate": 0.00019610709403987246,
      "loss": 3.5474,
      "step": 146
    },
    {
      "epoch": 0.588,
      "grad_norm": 0.16474847495555878,
      "learning_rate": 0.0001960351518258255,
      "loss": 3.5127,
      "step": 147
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.16645503044128418,
      "learning_rate": 0.00019596256436701324,
      "loss": 3.4305,
      "step": 148
    },
    {
      "epoch": 0.596,
      "grad_norm": 0.1710980087518692,
      "learning_rate": 0.00019588933215113926,
      "loss": 3.503,
      "step": 149
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.18621136248111725,
      "learning_rate": 0.000195815455670239,
      "loss": 3.9021,
      "step": 150
    },
    {
      "epoch": 0.604,
      "grad_norm": 0.19290271401405334,
      "learning_rate": 0.00019574093542067673,
      "loss": 3.781,
      "step": 151
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.17418935894966125,
      "learning_rate": 0.00019566577190314197,
      "loss": 3.4184,
      "step": 152
    },
    {
      "epoch": 0.612,
      "grad_norm": 0.1979985237121582,
      "learning_rate": 0.0001955899656226464,
      "loss": 3.8018,
      "step": 153
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.19483564794063568,
      "learning_rate": 0.0001955135170885202,
      "loss": 3.3272,
      "step": 154
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.17463375627994537,
      "learning_rate": 0.0001954364268144088,
      "loss": 3.4532,
      "step": 155
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.17619574069976807,
      "learning_rate": 0.00019535869531826937,
      "loss": 3.5688,
      "step": 156
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.1940525621175766,
      "learning_rate": 0.00019528032312236736,
      "loss": 3.5482,
      "step": 157
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.16512367129325867,
      "learning_rate": 0.00019520131075327298,
      "loss": 3.475,
      "step": 158
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.18556439876556396,
      "learning_rate": 0.00019512165874185767,
      "loss": 3.4638,
      "step": 159
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.1898597776889801,
      "learning_rate": 0.00019504136762329047,
      "loss": 3.6337,
      "step": 160
    },
    {
      "epoch": 0.644,
      "grad_norm": 0.21816740930080414,
      "learning_rate": 0.0001949604379370345,
      "loss": 3.628,
      "step": 161
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.2109871804714203,
      "learning_rate": 0.00019487887022684336,
      "loss": 3.8156,
      "step": 162
    },
    {
      "epoch": 0.652,
      "grad_norm": 0.2020304650068283,
      "learning_rate": 0.00019479666504075736,
      "loss": 3.5084,
      "step": 163
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.20162932574748993,
      "learning_rate": 0.00019471382293110003,
      "loss": 3.6731,
      "step": 164
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.19267459213733673,
      "learning_rate": 0.0001946303444544741,
      "loss": 3.4903,
      "step": 165
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.16757938265800476,
      "learning_rate": 0.00019454623017175812,
      "loss": 3.3496,
      "step": 166
    },
    {
      "epoch": 0.668,
      "grad_norm": 0.17372755706310272,
      "learning_rate": 0.00019446148064810242,
      "loss": 3.6256,
      "step": 167
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.18136070668697357,
      "learning_rate": 0.00019437609645292546,
      "loss": 3.4332,
      "step": 168
    },
    {
      "epoch": 0.676,
      "grad_norm": 0.23116272687911987,
      "learning_rate": 0.00019429007815990993,
      "loss": 3.396,
      "step": 169
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.2031964510679245,
      "learning_rate": 0.0001942034263469989,
      "loss": 3.3292,
      "step": 170
    },
    {
      "epoch": 0.684,
      "grad_norm": 0.1872299164533615,
      "learning_rate": 0.00019411614159639204,
      "loss": 3.4754,
      "step": 171
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.18025097250938416,
      "learning_rate": 0.00019402822449454153,
      "loss": 3.0741,
      "step": 172
    },
    {
      "epoch": 0.692,
      "grad_norm": 0.180389404296875,
      "learning_rate": 0.00019393967563214833,
      "loss": 3.612,
      "step": 173
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.17443440854549408,
      "learning_rate": 0.00019385049560415794,
      "loss": 3.5024,
      "step": 174
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.2160956710577011,
      "learning_rate": 0.00019376068500975667,
      "loss": 3.5082,
      "step": 175
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.2168867141008377,
      "learning_rate": 0.00019367024445236754,
      "loss": 3.3685,
      "step": 176
    },
    {
      "epoch": 0.708,
      "grad_norm": 0.20543432235717773,
      "learning_rate": 0.000193579174539646,
      "loss": 3.7719,
      "step": 177
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.19736941158771515,
      "learning_rate": 0.00019348747588347637,
      "loss": 3.5088,
      "step": 178
    },
    {
      "epoch": 0.716,
      "grad_norm": 0.17034073173999786,
      "learning_rate": 0.00019339514909996706,
      "loss": 3.3853,
      "step": 179
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.18376456201076508,
      "learning_rate": 0.00019330219480944694,
      "loss": 3.4838,
      "step": 180
    },
    {
      "epoch": 0.724,
      "grad_norm": 0.1802004873752594,
      "learning_rate": 0.00019320861363646095,
      "loss": 3.6067,
      "step": 181
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.26620882749557495,
      "learning_rate": 0.00019311440620976597,
      "loss": 3.4619,
      "step": 182
    },
    {
      "epoch": 0.732,
      "grad_norm": 0.173911914229393,
      "learning_rate": 0.00019301957316232658,
      "loss": 3.259,
      "step": 183
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.1645464152097702,
      "learning_rate": 0.0001929241151313108,
      "loss": 3.2834,
      "step": 184
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.15920527279376984,
      "learning_rate": 0.0001928280327580858,
      "loss": 3.2604,
      "step": 185
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.18966422975063324,
      "learning_rate": 0.00019273132668821364,
      "loss": 3.3534,
      "step": 186
    },
    {
      "epoch": 0.748,
      "grad_norm": 0.17744332551956177,
      "learning_rate": 0.00019263399757144683,
      "loss": 3.3653,
      "step": 187
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.21870556473731995,
      "learning_rate": 0.00019253604606172417,
      "loss": 3.3405,
      "step": 188
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.1836492419242859,
      "learning_rate": 0.000192437472817166,
      "loss": 3.222,
      "step": 189
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.15214946866035461,
      "learning_rate": 0.00019233827850007027,
      "loss": 3.3366,
      "step": 190
    },
    {
      "epoch": 0.764,
      "grad_norm": 0.16666916012763977,
      "learning_rate": 0.00019223846377690754,
      "loss": 3.1794,
      "step": 191
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.16685420274734497,
      "learning_rate": 0.00019213802931831696,
      "loss": 3.3915,
      "step": 192
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.19861777126789093,
      "learning_rate": 0.00019203697579910154,
      "loss": 3.5565,
      "step": 193
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.1829143464565277,
      "learning_rate": 0.00019193530389822363,
      "loss": 3.2966,
      "step": 194
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.160686194896698,
      "learning_rate": 0.00019183301429880043,
      "loss": 3.4499,
      "step": 195
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.1680517941713333,
      "learning_rate": 0.00019173010768809933,
      "loss": 3.197,
      "step": 196
    },
    {
      "epoch": 0.788,
      "grad_norm": 0.16934868693351746,
      "learning_rate": 0.00019162658475753327,
      "loss": 3.5043,
      "step": 197
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.18683680891990662,
      "learning_rate": 0.0001915224462026563,
      "loss": 3.2846,
      "step": 198
    },
    {
      "epoch": 0.796,
      "grad_norm": 0.20635023713111877,
      "learning_rate": 0.00019141769272315858,
      "loss": 3.3011,
      "step": 199
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.18586567044258118,
      "learning_rate": 0.00019131232502286188,
      "loss": 3.3198,
      "step": 200
    },
    {
      "epoch": 0.804,
      "grad_norm": 0.162764310836792,
      "learning_rate": 0.00019120634380971496,
      "loss": 2.9819,
      "step": 201
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.19849567115306854,
      "learning_rate": 0.0001910997497957885,
      "loss": 3.1487,
      "step": 202
    },
    {
      "epoch": 0.812,
      "grad_norm": 0.14675414562225342,
      "learning_rate": 0.0001909925436972706,
      "loss": 3.1662,
      "step": 203
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.17686086893081665,
      "learning_rate": 0.00019088472623446183,
      "loss": 3.2521,
      "step": 204
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.15433034300804138,
      "learning_rate": 0.00019077629813177036,
      "loss": 2.9268,
      "step": 205
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.1729031652212143,
      "learning_rate": 0.00019066726011770726,
      "loss": 3.0227,
      "step": 206
    },
    {
      "epoch": 0.828,
      "grad_norm": 0.1801833063364029,
      "learning_rate": 0.00019055761292488142,
      "loss": 3.0575,
      "step": 207
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.23211678862571716,
      "learning_rate": 0.0001904473572899947,
      "loss": 3.0818,
      "step": 208
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.1703633815050125,
      "learning_rate": 0.00019033649395383702,
      "loss": 3.0053,
      "step": 209
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.18966303765773773,
      "learning_rate": 0.00019022502366128135,
      "loss": 3.5013,
      "step": 210
    },
    {
      "epoch": 0.844,
      "grad_norm": 0.20013146102428436,
      "learning_rate": 0.00019011294716127867,
      "loss": 3.1926,
      "step": 211
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.21084697544574738,
      "learning_rate": 0.00019000026520685302,
      "loss": 3.3481,
      "step": 212
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.1806865632534027,
      "learning_rate": 0.0001898869785550963,
      "loss": 3.0052,
      "step": 213
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.18296949565410614,
      "learning_rate": 0.0001897730879671634,
      "loss": 3.2331,
      "step": 214
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.15883435308933258,
      "learning_rate": 0.00018965859420826684,
      "loss": 3.0514,
      "step": 215
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.25622278451919556,
      "learning_rate": 0.00018954349804767184,
      "loss": 3.4411,
      "step": 216
    },
    {
      "epoch": 0.868,
      "grad_norm": 0.15008386969566345,
      "learning_rate": 0.00018942780025869098,
      "loss": 3.09,
      "step": 217
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.2581194341182709,
      "learning_rate": 0.00018931150161867916,
      "loss": 3.5092,
      "step": 218
    },
    {
      "epoch": 0.876,
      "grad_norm": 0.19812160730361938,
      "learning_rate": 0.00018919460290902826,
      "loss": 3.2435,
      "step": 219
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.23643657565116882,
      "learning_rate": 0.00018907710491516199,
      "loss": 3.2776,
      "step": 220
    },
    {
      "epoch": 0.884,
      "grad_norm": 0.1677044928073883,
      "learning_rate": 0.0001889590084265304,
      "loss": 3.1014,
      "step": 221
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.16789324581623077,
      "learning_rate": 0.0001888403142366049,
      "loss": 3.1991,
      "step": 222
    },
    {
      "epoch": 0.892,
      "grad_norm": 0.17056231200695038,
      "learning_rate": 0.0001887210231428727,
      "loss": 2.9674,
      "step": 223
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.2036849409341812,
      "learning_rate": 0.00018860113594683148,
      "loss": 2.9393,
      "step": 224
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.14891906082630157,
      "learning_rate": 0.0001884806534539841,
      "loss": 3.2221,
      "step": 225
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.17370694875717163,
      "learning_rate": 0.00018835957647383303,
      "loss": 3.2765,
      "step": 226
    },
    {
      "epoch": 0.908,
      "grad_norm": 0.18648014962673187,
      "learning_rate": 0.0001882379058198751,
      "loss": 2.9754,
      "step": 227
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.17965064942836761,
      "learning_rate": 0.00018811564230959588,
      "loss": 3.1177,
      "step": 228
    },
    {
      "epoch": 0.916,
      "grad_norm": 0.19484280049800873,
      "learning_rate": 0.00018799278676446423,
      "loss": 3.2657,
      "step": 229
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.174091175198555,
      "learning_rate": 0.00018786934000992688,
      "loss": 3.0531,
      "step": 230
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.1678793728351593,
      "learning_rate": 0.00018774530287540278,
      "loss": 2.8636,
      "step": 231
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.1563955694437027,
      "learning_rate": 0.00018762067619427746,
      "loss": 3.03,
      "step": 232
    },
    {
      "epoch": 0.932,
      "grad_norm": 0.21710093319416046,
      "learning_rate": 0.00018749546080389757,
      "loss": 3.3249,
      "step": 233
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.173653706908226,
      "learning_rate": 0.00018736965754556528,
      "loss": 3.1334,
      "step": 234
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.1981709599494934,
      "learning_rate": 0.00018724326726453244,
      "loss": 2.7557,
      "step": 235
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.1846802681684494,
      "learning_rate": 0.00018711629080999504,
      "loss": 3.2447,
      "step": 236
    },
    {
      "epoch": 0.948,
      "grad_norm": 0.19806210696697235,
      "learning_rate": 0.00018698872903508755,
      "loss": 3.2318,
      "step": 237
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.19059094786643982,
      "learning_rate": 0.00018686058279687698,
      "loss": 3.1543,
      "step": 238
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.18573921918869019,
      "learning_rate": 0.0001867318529563574,
      "loss": 3.1641,
      "step": 239
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.14330364763736725,
      "learning_rate": 0.00018660254037844388,
      "loss": 2.8336,
      "step": 240
    },
    {
      "epoch": 0.964,
      "grad_norm": 0.21990638971328735,
      "learning_rate": 0.00018647264593196688,
      "loss": 3.1004,
      "step": 241
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.1869647055864334,
      "learning_rate": 0.00018634217048966637,
      "loss": 3.3031,
      "step": 242
    },
    {
      "epoch": 0.972,
      "grad_norm": 0.19950971007347107,
      "learning_rate": 0.00018621111492818585,
      "loss": 2.9891,
      "step": 243
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.1722448319196701,
      "learning_rate": 0.0001860794801280666,
      "loss": 3.001,
      "step": 244
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.15594443678855896,
      "learning_rate": 0.00018594726697374175,
      "loss": 2.8377,
      "step": 245
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.1793275624513626,
      "learning_rate": 0.0001858144763535302,
      "loss": 3.1687,
      "step": 246
    },
    {
      "epoch": 0.988,
      "grad_norm": 0.15000538527965546,
      "learning_rate": 0.0001856811091596308,
      "loss": 3.0487,
      "step": 247
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.1787269115447998,
      "learning_rate": 0.0001855471662881164,
      "loss": 2.7906,
      "step": 248
    },
    {
      "epoch": 0.996,
      "grad_norm": 0.17408689856529236,
      "learning_rate": 0.00018541264863892754,
      "loss": 3.2872,
      "step": 249
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.17045819759368896,
      "learning_rate": 0.00018527755711586678,
      "loss": 3.1007,
      "step": 250
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.990558624267578,
      "eval_runtime": 24.051,
      "eval_samples_per_second": 20.789,
      "eval_steps_per_second": 2.619,
      "step": 250
    },
    {
      "epoch": 1.004,
      "grad_norm": 0.2082027792930603,
      "learning_rate": 0.00018514189262659235,
      "loss": 2.8898,
      "step": 251
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.18725883960723877,
      "learning_rate": 0.00018500565608261214,
      "loss": 3.0481,
      "step": 252
    },
    {
      "epoch": 1.012,
      "grad_norm": 0.1778808832168579,
      "learning_rate": 0.00018486884839927768,
      "loss": 3.0432,
      "step": 253
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.15298837423324585,
      "learning_rate": 0.00018473147049577774,
      "loss": 3.0926,
      "step": 254
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.22449415922164917,
      "learning_rate": 0.0001845935232951325,
      "loss": 2.9081,
      "step": 255
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.15181326866149902,
      "learning_rate": 0.00018445500772418697,
      "loss": 2.882,
      "step": 256
    },
    {
      "epoch": 1.028,
      "grad_norm": 0.18487092852592468,
      "learning_rate": 0.00018431592471360503,
      "loss": 3.2031,
      "step": 257
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.21008017659187317,
      "learning_rate": 0.00018417627519786315,
      "loss": 2.908,
      "step": 258
    },
    {
      "epoch": 1.036,
      "grad_norm": 0.1959112584590912,
      "learning_rate": 0.000184036060115244,
      "loss": 2.8672,
      "step": 259
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.15446992218494415,
      "learning_rate": 0.00018389528040783012,
      "loss": 2.8602,
      "step": 260
    },
    {
      "epoch": 1.044,
      "grad_norm": 0.15976910293102264,
      "learning_rate": 0.00018375393702149787,
      "loss": 2.9407,
      "step": 261
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.1932137906551361,
      "learning_rate": 0.00018361203090591071,
      "loss": 3.0609,
      "step": 262
    },
    {
      "epoch": 1.052,
      "grad_norm": 0.19297915697097778,
      "learning_rate": 0.00018346956301451304,
      "loss": 3.1188,
      "step": 263
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.17886877059936523,
      "learning_rate": 0.00018332653430452376,
      "loss": 2.9767,
      "step": 264
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.22046789526939392,
      "learning_rate": 0.00018318294573692985,
      "loss": 3.0687,
      "step": 265
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.14293304085731506,
      "learning_rate": 0.00018303879827647975,
      "loss": 3.1285,
      "step": 266
    },
    {
      "epoch": 1.068,
      "grad_norm": 0.21329905092716217,
      "learning_rate": 0.0001828940928916772,
      "loss": 2.9014,
      "step": 267
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.18550504744052887,
      "learning_rate": 0.00018274883055477436,
      "loss": 2.6986,
      "step": 268
    },
    {
      "epoch": 1.076,
      "grad_norm": 0.1602412313222885,
      "learning_rate": 0.00018260301224176558,
      "loss": 3.1524,
      "step": 269
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.19258257746696472,
      "learning_rate": 0.00018245663893238075,
      "loss": 2.7578,
      "step": 270
    },
    {
      "epoch": 1.084,
      "grad_norm": 0.15281230211257935,
      "learning_rate": 0.00018230971161007853,
      "loss": 2.7409,
      "step": 271
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.15957097709178925,
      "learning_rate": 0.00018216223126204007,
      "loss": 2.8374,
      "step": 272
    },
    {
      "epoch": 1.092,
      "grad_norm": 0.1723804622888565,
      "learning_rate": 0.00018201419887916214,
      "loss": 2.7459,
      "step": 273
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.18181458115577698,
      "learning_rate": 0.00018186561545605054,
      "loss": 2.801,
      "step": 274
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.213564932346344,
      "learning_rate": 0.00018171648199101346,
      "loss": 2.779,
      "step": 275
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.1618279367685318,
      "learning_rate": 0.00018156679948605467,
      "loss": 2.7939,
      "step": 276
    },
    {
      "epoch": 1.108,
      "grad_norm": 0.17854879796504974,
      "learning_rate": 0.00018141656894686689,
      "loss": 2.9885,
      "step": 277
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.16151148080825806,
      "learning_rate": 0.00018126579138282503,
      "loss": 3.0043,
      "step": 278
    },
    {
      "epoch": 1.116,
      "grad_norm": 0.16941747069358826,
      "learning_rate": 0.00018111446780697929,
      "loss": 2.6011,
      "step": 279
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.14842823147773743,
      "learning_rate": 0.0001809625992360485,
      "loss": 2.7062,
      "step": 280
    },
    {
      "epoch": 1.124,
      "grad_norm": 0.1542978286743164,
      "learning_rate": 0.00018081018669041324,
      "loss": 2.6876,
      "step": 281
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.2459702044725418,
      "learning_rate": 0.00018065723119410884,
      "loss": 3.2285,
      "step": 282
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 0.16819116473197937,
      "learning_rate": 0.00018050373377481878,
      "loss": 2.9791,
      "step": 283
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.17059993743896484,
      "learning_rate": 0.00018034969546386757,
      "loss": 2.6799,
      "step": 284
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.18292608857154846,
      "learning_rate": 0.0001801951172962139,
      "loss": 2.7351,
      "step": 285
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.18434052169322968,
      "learning_rate": 0.0001800400003104436,
      "loss": 3.1496,
      "step": 286
    },
    {
      "epoch": 1.148,
      "grad_norm": 0.18487919867038727,
      "learning_rate": 0.0001798843455487629,
      "loss": 2.7495,
      "step": 287
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.15079112350940704,
      "learning_rate": 0.00017972815405699103,
      "loss": 2.8065,
      "step": 288
    },
    {
      "epoch": 1.156,
      "grad_norm": 0.15222522616386414,
      "learning_rate": 0.00017957142688455362,
      "loss": 2.6825,
      "step": 289
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.16851770877838135,
      "learning_rate": 0.00017941416508447536,
      "loss": 2.9736,
      "step": 290
    },
    {
      "epoch": 1.164,
      "grad_norm": 0.20419618487358093,
      "learning_rate": 0.00017925636971337304,
      "loss": 2.837,
      "step": 291
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.2203681468963623,
      "learning_rate": 0.0001790980418314484,
      "loss": 2.5853,
      "step": 292
    },
    {
      "epoch": 1.172,
      "grad_norm": 0.1571640968322754,
      "learning_rate": 0.00017893918250248104,
      "loss": 3.032,
      "step": 293
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.17670083045959473,
      "learning_rate": 0.00017877979279382135,
      "loss": 2.9062,
      "step": 294
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.1997249722480774,
      "learning_rate": 0.00017861987377638312,
      "loss": 2.8049,
      "step": 295
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.21550047397613525,
      "learning_rate": 0.0001784594265246366,
      "loss": 2.8209,
      "step": 296
    },
    {
      "epoch": 1.188,
      "grad_norm": 0.15742239356040955,
      "learning_rate": 0.0001782984521166011,
      "loss": 2.6473,
      "step": 297
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.16464199125766754,
      "learning_rate": 0.0001781369516338378,
      "loss": 2.9639,
      "step": 298
    },
    {
      "epoch": 1.196,
      "grad_norm": 0.16636008024215698,
      "learning_rate": 0.00017797492616144256,
      "loss": 2.8825,
      "step": 299
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.27598729729652405,
      "learning_rate": 0.00017781237678803847,
      "loss": 2.5044,
      "step": 300
    },
    {
      "epoch": 1.204,
      "grad_norm": 0.27128639817237854,
      "learning_rate": 0.00017764930460576866,
      "loss": 3.0665,
      "step": 301
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.18155336380004883,
      "learning_rate": 0.000177485710710289,
      "loss": 2.8386,
      "step": 302
    },
    {
      "epoch": 1.212,
      "grad_norm": 0.20413954555988312,
      "learning_rate": 0.00017732159620076053,
      "loss": 2.9846,
      "step": 303
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.1970483809709549,
      "learning_rate": 0.00017715696217984235,
      "loss": 2.7463,
      "step": 304
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.16914483904838562,
      "learning_rate": 0.00017699180975368396,
      "loss": 2.692,
      "step": 305
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.22873033583164215,
      "learning_rate": 0.00017682614003191807,
      "loss": 2.6617,
      "step": 306
    },
    {
      "epoch": 1.228,
      "grad_norm": 0.1926119029521942,
      "learning_rate": 0.00017665995412765285,
      "loss": 2.7055,
      "step": 307
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.21092726290225983,
      "learning_rate": 0.00017649325315746478,
      "loss": 2.9376,
      "step": 308
    },
    {
      "epoch": 1.236,
      "grad_norm": 0.18494576215744019,
      "learning_rate": 0.00017632603824139085,
      "loss": 2.6358,
      "step": 309
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.20392940938472748,
      "learning_rate": 0.0001761583105029213,
      "loss": 2.6406,
      "step": 310
    },
    {
      "epoch": 1.244,
      "grad_norm": 0.15542443096637726,
      "learning_rate": 0.0001759900710689918,
      "loss": 2.5291,
      "step": 311
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.15850897133350372,
      "learning_rate": 0.00017582132106997616,
      "loss": 2.9654,
      "step": 312
    },
    {
      "epoch": 1.252,
      "grad_norm": 0.20047743618488312,
      "learning_rate": 0.00017565206163967846,
      "loss": 2.7453,
      "step": 313
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.15150131285190582,
      "learning_rate": 0.00017548229391532572,
      "loss": 2.652,
      "step": 314
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.23210768401622772,
      "learning_rate": 0.00017531201903755994,
      "loss": 2.8997,
      "step": 315
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.1742788553237915,
      "learning_rate": 0.00017514123815043074,
      "loss": 2.6056,
      "step": 316
    },
    {
      "epoch": 1.268,
      "grad_norm": 0.18891814351081848,
      "learning_rate": 0.00017496995240138744,
      "loss": 3.0309,
      "step": 317
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.16621069610118866,
      "learning_rate": 0.00017479816294127152,
      "loss": 2.7102,
      "step": 318
    },
    {
      "epoch": 1.276,
      "grad_norm": 0.1687590479850769,
      "learning_rate": 0.00017462587092430875,
      "loss": 2.645,
      "step": 319
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.17968977987766266,
      "learning_rate": 0.0001744530775081015,
      "loss": 2.7527,
      "step": 320
    },
    {
      "epoch": 1.284,
      "grad_norm": 0.19607365131378174,
      "learning_rate": 0.00017427978385362112,
      "loss": 2.7763,
      "step": 321
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.1578807681798935,
      "learning_rate": 0.0001741059911251997,
      "loss": 2.8599,
      "step": 322
    },
    {
      "epoch": 1.292,
      "grad_norm": 0.18493254482746124,
      "learning_rate": 0.0001739317004905227,
      "loss": 2.7425,
      "step": 323
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.18266692757606506,
      "learning_rate": 0.000173756913120621,
      "loss": 2.6171,
      "step": 324
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.2345346212387085,
      "learning_rate": 0.00017358163018986282,
      "loss": 2.6997,
      "step": 325
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.26040685176849365,
      "learning_rate": 0.00017340585287594604,
      "loss": 2.8777,
      "step": 326
    },
    {
      "epoch": 1.308,
      "grad_norm": 0.17406710982322693,
      "learning_rate": 0.00017322958235989016,
      "loss": 2.7128,
      "step": 327
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.17198453843593597,
      "learning_rate": 0.0001730528198260285,
      "loss": 2.498,
      "step": 328
    },
    {
      "epoch": 1.316,
      "grad_norm": 0.18165162205696106,
      "learning_rate": 0.00017287556646200018,
      "loss": 2.6286,
      "step": 329
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.19203831255435944,
      "learning_rate": 0.00017269782345874203,
      "loss": 2.725,
      "step": 330
    },
    {
      "epoch": 1.324,
      "grad_norm": 0.1644928753376007,
      "learning_rate": 0.00017251959201048083,
      "loss": 2.7227,
      "step": 331
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.18849171698093414,
      "learning_rate": 0.00017234087331472497,
      "loss": 2.8887,
      "step": 332
    },
    {
      "epoch": 1.332,
      "grad_norm": 0.1754441112279892,
      "learning_rate": 0.00017216166857225674,
      "loss": 2.6456,
      "step": 333
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.20156200230121613,
      "learning_rate": 0.00017198197898712404,
      "loss": 2.7547,
      "step": 334
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.22001998126506805,
      "learning_rate": 0.00017180180576663228,
      "loss": 2.5826,
      "step": 335
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.18154068291187286,
      "learning_rate": 0.00017162115012133643,
      "loss": 2.8103,
      "step": 336
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 0.1705700010061264,
      "learning_rate": 0.00017144001326503273,
      "loss": 2.6606,
      "step": 337
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.16810184717178345,
      "learning_rate": 0.00017125839641475072,
      "loss": 2.6679,
      "step": 338
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 0.17922335863113403,
      "learning_rate": 0.00017107630079074478,
      "loss": 2.7809,
      "step": 339
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.16561754047870636,
      "learning_rate": 0.00017089372761648616,
      "loss": 2.5435,
      "step": 340
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 0.19870486855506897,
      "learning_rate": 0.00017071067811865476,
      "loss": 2.8348,
      "step": 341
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.14896561205387115,
      "learning_rate": 0.00017052715352713075,
      "loss": 2.5294,
      "step": 342
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 0.2236500382423401,
      "learning_rate": 0.00017034315507498635,
      "loss": 2.6732,
      "step": 343
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.16938893496990204,
      "learning_rate": 0.00017015868399847768,
      "loss": 2.6032,
      "step": 344
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.16113606095314026,
      "learning_rate": 0.00016997374153703625,
      "loss": 2.6475,
      "step": 345
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.18429893255233765,
      "learning_rate": 0.00016978832893326074,
      "loss": 2.8936,
      "step": 346
    },
    {
      "epoch": 1.388,
      "grad_norm": 0.18665137887001038,
      "learning_rate": 0.00016960244743290868,
      "loss": 2.6118,
      "step": 347
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.17777474224567413,
      "learning_rate": 0.00016941609828488807,
      "loss": 2.7429,
      "step": 348
    },
    {
      "epoch": 1.396,
      "grad_norm": 0.1939142942428589,
      "learning_rate": 0.00016922928274124886,
      "loss": 2.5654,
      "step": 349
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.195732980966568,
      "learning_rate": 0.0001690420020571747,
      "loss": 2.5581,
      "step": 350
    },
    {
      "epoch": 1.404,
      "grad_norm": 0.1694985181093216,
      "learning_rate": 0.00016885425749097444,
      "loss": 2.5014,
      "step": 351
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.16092626750469208,
      "learning_rate": 0.0001686660503040737,
      "loss": 2.4344,
      "step": 352
    },
    {
      "epoch": 1.412,
      "grad_norm": 0.21173854172229767,
      "learning_rate": 0.00016847738176100632,
      "loss": 2.4334,
      "step": 353
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.14105521142482758,
      "learning_rate": 0.00016828825312940592,
      "loss": 2.6748,
      "step": 354
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.18185226619243622,
      "learning_rate": 0.0001680986656799975,
      "loss": 2.5518,
      "step": 355
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.23500433564186096,
      "learning_rate": 0.0001679086206865886,
      "loss": 2.5854,
      "step": 356
    },
    {
      "epoch": 1.428,
      "grad_norm": 0.1710791438817978,
      "learning_rate": 0.00016771811942606108,
      "loss": 2.565,
      "step": 357
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.15893039107322693,
      "learning_rate": 0.00016752716317836229,
      "loss": 2.4846,
      "step": 358
    },
    {
      "epoch": 1.436,
      "grad_norm": 0.24022246897220612,
      "learning_rate": 0.00016733575322649657,
      "loss": 2.5496,
      "step": 359
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.17566943168640137,
      "learning_rate": 0.0001671438908565167,
      "loss": 2.6377,
      "step": 360
    },
    {
      "epoch": 1.444,
      "grad_norm": 0.17920133471488953,
      "learning_rate": 0.00016695157735751513,
      "loss": 2.7653,
      "step": 361
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.24256287515163422,
      "learning_rate": 0.00016675881402161536,
      "loss": 2.6774,
      "step": 362
    },
    {
      "epoch": 1.452,
      "grad_norm": 0.1592651754617691,
      "learning_rate": 0.0001665656021439633,
      "loss": 2.3627,
      "step": 363
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.18511445820331573,
      "learning_rate": 0.0001663719430227186,
      "loss": 2.6889,
      "step": 364
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.19486095011234283,
      "learning_rate": 0.00016617783795904565,
      "loss": 2.4212,
      "step": 365
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.22086837887763977,
      "learning_rate": 0.00016598328825710533,
      "loss": 2.5615,
      "step": 366
    },
    {
      "epoch": 1.468,
      "grad_norm": 0.14962030947208405,
      "learning_rate": 0.00016578829522404583,
      "loss": 2.7334,
      "step": 367
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.20832780003547668,
      "learning_rate": 0.000165592860169994,
      "loss": 2.2951,
      "step": 368
    },
    {
      "epoch": 1.476,
      "grad_norm": 0.19668641686439514,
      "learning_rate": 0.00016539698440804661,
      "loss": 2.6852,
      "step": 369
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.1707521378993988,
      "learning_rate": 0.00016520066925426144,
      "loss": 2.5572,
      "step": 370
    },
    {
      "epoch": 1.484,
      "grad_norm": 0.20181632041931152,
      "learning_rate": 0.0001650039160276485,
      "loss": 2.5316,
      "step": 371
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.1736031174659729,
      "learning_rate": 0.0001648067260501611,
      "loss": 2.3791,
      "step": 372
    },
    {
      "epoch": 1.492,
      "grad_norm": 0.18900729715824127,
      "learning_rate": 0.0001646091006466871,
      "loss": 2.6888,
      "step": 373
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.24509203433990479,
      "learning_rate": 0.0001644110411450398,
      "loss": 2.4835,
      "step": 374
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.19061371684074402,
      "learning_rate": 0.00016421254887594917,
      "loss": 2.5431,
      "step": 375
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.1850917488336563,
      "learning_rate": 0.00016401362517305296,
      "loss": 2.6409,
      "step": 376
    },
    {
      "epoch": 1.508,
      "grad_norm": 0.1680152267217636,
      "learning_rate": 0.00016381427137288754,
      "loss": 2.7768,
      "step": 377
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.19064804911613464,
      "learning_rate": 0.00016361448881487914,
      "loss": 2.6862,
      "step": 378
    },
    {
      "epoch": 1.516,
      "grad_norm": 0.1788070648908615,
      "learning_rate": 0.0001634142788413346,
      "loss": 2.7704,
      "step": 379
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.1952916383743286,
      "learning_rate": 0.00016321364279743266,
      "loss": 2.5225,
      "step": 380
    },
    {
      "epoch": 1.524,
      "grad_norm": 0.15985770523548126,
      "learning_rate": 0.00016301258203121462,
      "loss": 2.456,
      "step": 381
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.16746337711811066,
      "learning_rate": 0.0001628110978935756,
      "loss": 2.5038,
      "step": 382
    },
    {
      "epoch": 1.532,
      "grad_norm": 0.20668505132198334,
      "learning_rate": 0.00016260919173825508,
      "loss": 2.7181,
      "step": 383
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.18437303602695465,
      "learning_rate": 0.00016240686492182804,
      "loss": 2.4956,
      "step": 384
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.2472464144229889,
      "learning_rate": 0.00016220411880369601,
      "loss": 2.4499,
      "step": 385
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.20210042595863342,
      "learning_rate": 0.00016200095474607753,
      "loss": 2.2603,
      "step": 386
    },
    {
      "epoch": 1.548,
      "grad_norm": 0.1679307371377945,
      "learning_rate": 0.00016179737411399926,
      "loss": 2.4872,
      "step": 387
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.20563571155071259,
      "learning_rate": 0.00016159337827528685,
      "loss": 2.5692,
      "step": 388
    },
    {
      "epoch": 1.556,
      "grad_norm": 0.19437505304813385,
      "learning_rate": 0.00016138896860055555,
      "loss": 2.3431,
      "step": 389
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.16625766456127167,
      "learning_rate": 0.0001611841464632011,
      "loss": 2.2359,
      "step": 390
    },
    {
      "epoch": 1.564,
      "grad_norm": 0.18835023045539856,
      "learning_rate": 0.00016097891323939062,
      "loss": 2.4588,
      "step": 391
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.16678737103939056,
      "learning_rate": 0.0001607732703080532,
      "loss": 2.4295,
      "step": 392
    },
    {
      "epoch": 1.572,
      "grad_norm": 0.15912556648254395,
      "learning_rate": 0.00016056721905087056,
      "loss": 2.294,
      "step": 393
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.22998102009296417,
      "learning_rate": 0.00016036076085226814,
      "loss": 2.4911,
      "step": 394
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.17910678684711456,
      "learning_rate": 0.00016015389709940538,
      "loss": 2.5071,
      "step": 395
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.22010616958141327,
      "learning_rate": 0.0001599466291821666,
      "loss": 2.2842,
      "step": 396
    },
    {
      "epoch": 1.588,
      "grad_norm": 0.17226716876029968,
      "learning_rate": 0.0001597389584931517,
      "loss": 2.5852,
      "step": 397
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.22156870365142822,
      "learning_rate": 0.0001595308864276666,
      "loss": 2.8931,
      "step": 398
    },
    {
      "epoch": 1.596,
      "grad_norm": 0.34022679924964905,
      "learning_rate": 0.0001593224143837142,
      "loss": 2.5516,
      "step": 399
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.18445925414562225,
      "learning_rate": 0.0001591135437619847,
      "loss": 2.4852,
      "step": 400
    },
    {
      "epoch": 1.604,
      "grad_norm": 0.16197091341018677,
      "learning_rate": 0.00015890427596584617,
      "loss": 2.3297,
      "step": 401
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.1524742841720581,
      "learning_rate": 0.0001586946124013354,
      "loss": 2.3842,
      "step": 402
    },
    {
      "epoch": 1.612,
      "grad_norm": 0.17333006858825684,
      "learning_rate": 0.00015848455447714822,
      "loss": 2.2001,
      "step": 403
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.18723492324352264,
      "learning_rate": 0.0001582741036046301,
      "loss": 2.566,
      "step": 404
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.18579038977622986,
      "learning_rate": 0.00015806326119776663,
      "loss": 2.4629,
      "step": 405
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.17910632491111755,
      "learning_rate": 0.00015785202867317407,
      "loss": 2.5345,
      "step": 406
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 0.17997631430625916,
      "learning_rate": 0.00015764040745008988,
      "loss": 2.4699,
      "step": 407
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.24215340614318848,
      "learning_rate": 0.00015742839895036305,
      "loss": 2.4891,
      "step": 408
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 0.19676744937896729,
      "learning_rate": 0.00015721600459844468,
      "loss": 2.3032,
      "step": 409
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.17996621131896973,
      "learning_rate": 0.00015700322582137827,
      "loss": 2.2155,
      "step": 410
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 0.24135266244411469,
      "learning_rate": 0.00015679006404879033,
      "loss": 2.4179,
      "step": 411
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.1824471354484558,
      "learning_rate": 0.0001565765207128805,
      "loss": 2.2686,
      "step": 412
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 0.1967494785785675,
      "learning_rate": 0.00015636259724841222,
      "loss": 2.3984,
      "step": 413
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.188935324549675,
      "learning_rate": 0.0001561482950927029,
      "loss": 2.4666,
      "step": 414
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.19695217907428741,
      "learning_rate": 0.00015593361568561428,
      "loss": 2.2996,
      "step": 415
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.18019133806228638,
      "learning_rate": 0.00015571856046954285,
      "loss": 2.2186,
      "step": 416
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 0.22328829765319824,
      "learning_rate": 0.0001555031308894101,
      "loss": 2.4031,
      "step": 417
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.23357196152210236,
      "learning_rate": 0.00015528732839265272,
      "loss": 2.4954,
      "step": 418
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 0.19573460519313812,
      "learning_rate": 0.0001550711544292131,
      "loss": 2.3325,
      "step": 419
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.16497112810611725,
      "learning_rate": 0.0001548546104515294,
      "loss": 2.3749,
      "step": 420
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 0.2917661964893341,
      "learning_rate": 0.00015463769791452574,
      "loss": 2.6517,
      "step": 421
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.24156291782855988,
      "learning_rate": 0.00015442041827560274,
      "loss": 2.2875,
      "step": 422
    },
    {
      "epoch": 1.692,
      "grad_norm": 0.18187840282917023,
      "learning_rate": 0.00015420277299462736,
      "loss": 2.5685,
      "step": 423
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.2046596109867096,
      "learning_rate": 0.00015398476353392323,
      "loss": 2.22,
      "step": 424
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.1914888322353363,
      "learning_rate": 0.00015376639135826107,
      "loss": 2.27,
      "step": 425
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.18382759392261505,
      "learning_rate": 0.00015354765793484834,
      "loss": 2.4968,
      "step": 426
    },
    {
      "epoch": 1.708,
      "grad_norm": 0.23019248247146606,
      "learning_rate": 0.00015332856473331978,
      "loss": 2.2832,
      "step": 427
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.19204913079738617,
      "learning_rate": 0.00015310911322572753,
      "loss": 2.241,
      "step": 428
    },
    {
      "epoch": 1.716,
      "grad_norm": 0.17317421734333038,
      "learning_rate": 0.00015288930488653094,
      "loss": 2.4007,
      "step": 429
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.2256227284669876,
      "learning_rate": 0.000152669141192587,
      "loss": 2.2302,
      "step": 430
    },
    {
      "epoch": 1.724,
      "grad_norm": 0.21414019167423248,
      "learning_rate": 0.0001524486236231402,
      "loss": 2.3774,
      "step": 431
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.22046643495559692,
      "learning_rate": 0.00015222775365981273,
      "loss": 2.4043,
      "step": 432
    },
    {
      "epoch": 1.732,
      "grad_norm": 0.17292536795139313,
      "learning_rate": 0.00015200653278659432,
      "loss": 2.3182,
      "step": 433
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.16933369636535645,
      "learning_rate": 0.00015178496248983254,
      "loss": 2.3434,
      "step": 434
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.19948233664035797,
      "learning_rate": 0.00015156304425822267,
      "loss": 2.2258,
      "step": 435
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.17962366342544556,
      "learning_rate": 0.00015134077958279765,
      "loss": 2.3616,
      "step": 436
    },
    {
      "epoch": 1.748,
      "grad_norm": 0.21868257224559784,
      "learning_rate": 0.00015111816995691809,
      "loss": 2.2991,
      "step": 437
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.20290447771549225,
      "learning_rate": 0.00015089521687626243,
      "loss": 2.441,
      "step": 438
    },
    {
      "epoch": 1.756,
      "grad_norm": 0.22686225175857544,
      "learning_rate": 0.00015067192183881658,
      "loss": 2.5948,
      "step": 439
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.27408501505851746,
      "learning_rate": 0.000150448286344864,
      "loss": 2.1497,
      "step": 440
    },
    {
      "epoch": 1.764,
      "grad_norm": 0.2716353237628937,
      "learning_rate": 0.00015022431189697568,
      "loss": 2.4596,
      "step": 441
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.257307231426239,
      "learning_rate": 0.00015000000000000001,
      "loss": 2.2735,
      "step": 442
    },
    {
      "epoch": 1.772,
      "grad_norm": 0.2330036163330078,
      "learning_rate": 0.0001497753521610526,
      "loss": 2.2313,
      "step": 443
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.2573404312133789,
      "learning_rate": 0.00014955036988950618,
      "loss": 2.369,
      "step": 444
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.20655156672000885,
      "learning_rate": 0.00014932505469698052,
      "loss": 2.5703,
      "step": 445
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.1829787790775299,
      "learning_rate": 0.00014909940809733222,
      "loss": 2.1102,
      "step": 446
    },
    {
      "epoch": 1.788,
      "grad_norm": 0.18688388168811798,
      "learning_rate": 0.0001488734316066446,
      "loss": 2.3978,
      "step": 447
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.26589077711105347,
      "learning_rate": 0.00014864712674321734,
      "loss": 2.171,
      "step": 448
    },
    {
      "epoch": 1.796,
      "grad_norm": 0.23303888738155365,
      "learning_rate": 0.0001484204950275565,
      "loss": 2.2617,
      "step": 449
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.23106397688388824,
      "learning_rate": 0.00014819353798236427,
      "loss": 2.0797,
      "step": 450
    },
    {
      "epoch": 1.804,
      "grad_norm": 0.20299752056598663,
      "learning_rate": 0.00014796625713252848,
      "loss": 2.1213,
      "step": 451
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.16940052807331085,
      "learning_rate": 0.00014773865400511272,
      "loss": 1.9216,
      "step": 452
    },
    {
      "epoch": 1.812,
      "grad_norm": 0.25728839635849,
      "learning_rate": 0.00014751073012934587,
      "loss": 2.285,
      "step": 453
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.1868535280227661,
      "learning_rate": 0.00014728248703661182,
      "loss": 2.4596,
      "step": 454
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.29915738105773926,
      "learning_rate": 0.0001470539262604393,
      "loss": 2.405,
      "step": 455
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.1725856214761734,
      "learning_rate": 0.00014682504933649144,
      "loss": 2.0576,
      "step": 456
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 0.19717518985271454,
      "learning_rate": 0.00014659585780255556,
      "loss": 1.8678,
      "step": 457
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.17078836262226105,
      "learning_rate": 0.00014636635319853275,
      "loss": 2.1845,
      "step": 458
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 0.17009608447551727,
      "learning_rate": 0.0001461365370664276,
      "loss": 2.3326,
      "step": 459
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.29109078645706177,
      "learning_rate": 0.00014590641095033787,
      "loss": 2.3823,
      "step": 460
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 0.18609720468521118,
      "learning_rate": 0.00014567597639644387,
      "loss": 2.0575,
      "step": 461
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.2412686049938202,
      "learning_rate": 0.00014544523495299842,
      "loss": 2.0678,
      "step": 462
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 0.20197857916355133,
      "learning_rate": 0.00014521418817031628,
      "loss": 2.3095,
      "step": 463
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.17876283824443817,
      "learning_rate": 0.0001449828376007636,
      "loss": 2.3318,
      "step": 464
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.17931701242923737,
      "learning_rate": 0.00014475118479874774,
      "loss": 2.2074,
      "step": 465
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.16827575862407684,
      "learning_rate": 0.0001445192313207067,
      "loss": 2.5512,
      "step": 466
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 0.20803618431091309,
      "learning_rate": 0.0001442869787250987,
      "loss": 2.3044,
      "step": 467
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.20197102427482605,
      "learning_rate": 0.0001440544285723915,
      "loss": 2.1675,
      "step": 468
    },
    {
      "epoch": 1.876,
      "grad_norm": 0.2958400249481201,
      "learning_rate": 0.00014382158242505234,
      "loss": 2.272,
      "step": 469
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.20997442305088043,
      "learning_rate": 0.00014358844184753712,
      "loss": 2.0162,
      "step": 470
    },
    {
      "epoch": 1.884,
      "grad_norm": 0.1865428239107132,
      "learning_rate": 0.00014335500840627986,
      "loss": 2.1981,
      "step": 471
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.20231419801712036,
      "learning_rate": 0.00014312128366968243,
      "loss": 2.3383,
      "step": 472
    },
    {
      "epoch": 1.892,
      "grad_norm": 0.17440253496170044,
      "learning_rate": 0.0001428872692081038,
      "loss": 2.5005,
      "step": 473
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.2900272607803345,
      "learning_rate": 0.00014265296659384956,
      "loss": 2.4718,
      "step": 474
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.23251210153102875,
      "learning_rate": 0.00014241837740116132,
      "loss": 2.0861,
      "step": 475
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.3362973630428314,
      "learning_rate": 0.00014218350320620624,
      "loss": 2.6548,
      "step": 476
    },
    {
      "epoch": 1.908,
      "grad_norm": 0.2431890219449997,
      "learning_rate": 0.00014194834558706632,
      "loss": 2.685,
      "step": 477
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.1677597165107727,
      "learning_rate": 0.0001417129061237278,
      "loss": 2.0594,
      "step": 478
    },
    {
      "epoch": 1.916,
      "grad_norm": 0.2346009463071823,
      "learning_rate": 0.0001414771863980707,
      "loss": 2.1055,
      "step": 479
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.2606126368045807,
      "learning_rate": 0.00014124118799385796,
      "loss": 2.4047,
      "step": 480
    },
    {
      "epoch": 1.924,
      "grad_norm": 0.21545985341072083,
      "learning_rate": 0.00014100491249672498,
      "loss": 2.3048,
      "step": 481
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.2643473744392395,
      "learning_rate": 0.00014076836149416887,
      "loss": 2.2152,
      "step": 482
    },
    {
      "epoch": 1.932,
      "grad_norm": 0.23589491844177246,
      "learning_rate": 0.0001405315365755379,
      "loss": 2.3446,
      "step": 483
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.17589299380779266,
      "learning_rate": 0.0001402944393320206,
      "loss": 2.0756,
      "step": 484
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.2525233328342438,
      "learning_rate": 0.00014005707135663527,
      "loss": 2.6994,
      "step": 485
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.25443917512893677,
      "learning_rate": 0.00013981943424421932,
      "loss": 2.4758,
      "step": 486
    },
    {
      "epoch": 1.948,
      "grad_norm": 0.2590820789337158,
      "learning_rate": 0.00013958152959141825,
      "loss": 2.0539,
      "step": 487
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.17872028052806854,
      "learning_rate": 0.00013934335899667527,
      "loss": 2.2332,
      "step": 488
    },
    {
      "epoch": 1.956,
      "grad_norm": 0.24020805954933167,
      "learning_rate": 0.00013910492406022033,
      "loss": 2.1271,
      "step": 489
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.21120630204677582,
      "learning_rate": 0.00013886622638405952,
      "loss": 2.2215,
      "step": 490
    },
    {
      "epoch": 1.964,
      "grad_norm": 0.2111382931470871,
      "learning_rate": 0.0001386272675719642,
      "loss": 2.3864,
      "step": 491
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.17840349674224854,
      "learning_rate": 0.00013838804922946027,
      "loss": 2.1905,
      "step": 492
    },
    {
      "epoch": 1.972,
      "grad_norm": 0.18323981761932373,
      "learning_rate": 0.00013814857296381728,
      "loss": 2.2331,
      "step": 493
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.21629758179187775,
      "learning_rate": 0.00013790884038403795,
      "loss": 2.0413,
      "step": 494
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.16951102018356323,
      "learning_rate": 0.00013766885310084688,
      "loss": 2.1128,
      "step": 495
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.20552338659763336,
      "learning_rate": 0.00013742861272668012,
      "loss": 2.3394,
      "step": 496
    },
    {
      "epoch": 1.988,
      "grad_norm": 0.17408131062984467,
      "learning_rate": 0.00013718812087567414,
      "loss": 2.1852,
      "step": 497
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.23815959692001343,
      "learning_rate": 0.00013694737916365517,
      "loss": 2.2799,
      "step": 498
    },
    {
      "epoch": 1.996,
      "grad_norm": 0.19214096665382385,
      "learning_rate": 0.000136706389208128,
      "loss": 2.2231,
      "step": 499
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.24942433834075928,
      "learning_rate": 0.00013646515262826552,
      "loss": 2.1883,
      "step": 500
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.2349820137023926,
      "eval_runtime": 23.761,
      "eval_samples_per_second": 21.043,
      "eval_steps_per_second": 2.651,
      "step": 500
    },
    {
      "epoch": 2.004,
      "grad_norm": 0.2045227736234665,
      "learning_rate": 0.00013622367104489756,
      "loss": 2.067,
      "step": 501
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.25126564502716064,
      "learning_rate": 0.0001359819460805001,
      "loss": 2.1963,
      "step": 502
    },
    {
      "epoch": 2.012,
      "grad_norm": 0.15164078772068024,
      "learning_rate": 0.0001357399793591844,
      "loss": 2.1282,
      "step": 503
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.163617342710495,
      "learning_rate": 0.0001354977725066859,
      "loss": 2.1365,
      "step": 504
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.18248052895069122,
      "learning_rate": 0.00013525532715035366,
      "loss": 1.9952,
      "step": 505
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.1918688416481018,
      "learning_rate": 0.00013501264491913906,
      "loss": 2.307,
      "step": 506
    },
    {
      "epoch": 2.028,
      "grad_norm": 0.18775244057178497,
      "learning_rate": 0.00013476972744358507,
      "loss": 2.1416,
      "step": 507
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.23231719434261322,
      "learning_rate": 0.0001345265763558152,
      "loss": 2.2077,
      "step": 508
    },
    {
      "epoch": 2.036,
      "grad_norm": 0.21832697093486786,
      "learning_rate": 0.00013428319328952253,
      "loss": 2.0469,
      "step": 509
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.16841080784797668,
      "learning_rate": 0.00013403957987995882,
      "loss": 2.0261,
      "step": 510
    },
    {
      "epoch": 2.044,
      "grad_norm": 0.29230329394340515,
      "learning_rate": 0.0001337957377639235,
      "loss": 2.2589,
      "step": 511
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.18653948605060577,
      "learning_rate": 0.0001335516685797525,
      "loss": 2.262,
      "step": 512
    },
    {
      "epoch": 2.052,
      "grad_norm": 0.3481246829032898,
      "learning_rate": 0.0001333073739673076,
      "loss": 2.3131,
      "step": 513
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.2878885269165039,
      "learning_rate": 0.00013306285556796495,
      "loss": 2.7005,
      "step": 514
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.24504698812961578,
      "learning_rate": 0.0001328181150246045,
      "loss": 2.1877,
      "step": 515
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.18866559863090515,
      "learning_rate": 0.00013257315398159864,
      "loss": 2.0319,
      "step": 516
    },
    {
      "epoch": 2.068,
      "grad_norm": 0.19440270960330963,
      "learning_rate": 0.00013232797408480127,
      "loss": 2.0927,
      "step": 517
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.210863396525383,
      "learning_rate": 0.00013208257698153677,
      "loss": 2.2488,
      "step": 518
    },
    {
      "epoch": 2.076,
      "grad_norm": 0.1637328714132309,
      "learning_rate": 0.00013183696432058888,
      "loss": 2.3139,
      "step": 519
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.19948065280914307,
      "learning_rate": 0.00013159113775218964,
      "loss": 2.3835,
      "step": 520
    },
    {
      "epoch": 2.084,
      "grad_norm": 0.21904189884662628,
      "learning_rate": 0.00013134509892800822,
      "loss": 2.4376,
      "step": 521
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.19126202166080475,
      "learning_rate": 0.00013109884950114007,
      "loss": 2.1826,
      "step": 522
    },
    {
      "epoch": 2.092,
      "grad_norm": 0.2532016932964325,
      "learning_rate": 0.00013085239112609547,
      "loss": 2.2474,
      "step": 523
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.2045210748910904,
      "learning_rate": 0.00013060572545878875,
      "loss": 2.0526,
      "step": 524
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.19045324623584747,
      "learning_rate": 0.00013035885415652685,
      "loss": 2.1227,
      "step": 525
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.2151007056236267,
      "learning_rate": 0.00013011177887799845,
      "loss": 2.2571,
      "step": 526
    },
    {
      "epoch": 2.108,
      "grad_norm": 0.1654035449028015,
      "learning_rate": 0.00012986450128326266,
      "loss": 2.2681,
      "step": 527
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.2270631045103073,
      "learning_rate": 0.00012961702303373795,
      "loss": 1.9358,
      "step": 528
    },
    {
      "epoch": 2.116,
      "grad_norm": 0.22204148769378662,
      "learning_rate": 0.00012936934579219094,
      "loss": 2.2369,
      "step": 529
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.15701240301132202,
      "learning_rate": 0.00012912147122272523,
      "loss": 2.0941,
      "step": 530
    },
    {
      "epoch": 2.124,
      "grad_norm": 0.20666079223155975,
      "learning_rate": 0.00012887340099077024,
      "loss": 2.1482,
      "step": 531
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.18200227618217468,
      "learning_rate": 0.00012862513676307008,
      "loss": 2.1604,
      "step": 532
    },
    {
      "epoch": 2.132,
      "grad_norm": 0.17886865139007568,
      "learning_rate": 0.0001283766802076722,
      "loss": 2.2975,
      "step": 533
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.1786285936832428,
      "learning_rate": 0.00012812803299391628,
      "loss": 2.036,
      "step": 534
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.21830686926841736,
      "learning_rate": 0.00012787919679242306,
      "loss": 2.0183,
      "step": 535
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.2340046465396881,
      "learning_rate": 0.00012763017327508305,
      "loss": 1.991,
      "step": 536
    },
    {
      "epoch": 2.148,
      "grad_norm": 0.22991491854190826,
      "learning_rate": 0.00012738096411504522,
      "loss": 2.0861,
      "step": 537
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.20468463003635406,
      "learning_rate": 0.0001271315709867059,
      "loss": 2.0776,
      "step": 538
    },
    {
      "epoch": 2.156,
      "grad_norm": 0.2529546320438385,
      "learning_rate": 0.00012688199556569753,
      "loss": 2.4214,
      "step": 539
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.22978870570659637,
      "learning_rate": 0.00012663223952887723,
      "loss": 2.019,
      "step": 540
    },
    {
      "epoch": 2.164,
      "grad_norm": 0.18388356268405914,
      "learning_rate": 0.0001263823045543158,
      "loss": 2.3398,
      "step": 541
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.2467704564332962,
      "learning_rate": 0.00012613219232128608,
      "loss": 1.9649,
      "step": 542
    },
    {
      "epoch": 2.172,
      "grad_norm": 0.17378175258636475,
      "learning_rate": 0.00012588190451025207,
      "loss": 2.2152,
      "step": 543
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.17413683235645294,
      "learning_rate": 0.00012563144280285741,
      "loss": 2.3107,
      "step": 544
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.29551419615745544,
      "learning_rate": 0.00012538080888191408,
      "loss": 1.8734,
      "step": 545
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.19811345636844635,
      "learning_rate": 0.00012513000443139112,
      "loss": 2.052,
      "step": 546
    },
    {
      "epoch": 2.188,
      "grad_norm": 0.176791712641716,
      "learning_rate": 0.00012487903113640337,
      "loss": 1.9852,
      "step": 547
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.15029460191726685,
      "learning_rate": 0.00012462789068320017,
      "loss": 2.0253,
      "step": 548
    },
    {
      "epoch": 2.196,
      "grad_norm": 0.1961965709924698,
      "learning_rate": 0.00012437658475915377,
      "loss": 2.373,
      "step": 549
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.27635782957077026,
      "learning_rate": 0.00012412511505274844,
      "loss": 2.418,
      "step": 550
    },
    {
      "epoch": 2.204,
      "grad_norm": 0.1972801685333252,
      "learning_rate": 0.00012387348325356874,
      "loss": 2.0946,
      "step": 551
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.1895841509103775,
      "learning_rate": 0.00012362169105228826,
      "loss": 2.0603,
      "step": 552
    },
    {
      "epoch": 2.212,
      "grad_norm": 0.1802162230014801,
      "learning_rate": 0.00012336974014065844,
      "loss": 2.1058,
      "step": 553
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.1406419724225998,
      "learning_rate": 0.000123117632211497,
      "loss": 1.952,
      "step": 554
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.2951201796531677,
      "learning_rate": 0.00012286536895867654,
      "loss": 2.2788,
      "step": 555
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.2049516886472702,
      "learning_rate": 0.00012261295207711346,
      "loss": 1.9317,
      "step": 556
    },
    {
      "epoch": 2.228,
      "grad_norm": 0.22501994669437408,
      "learning_rate": 0.00012236038326275626,
      "loss": 2.2184,
      "step": 557
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.22014661133289337,
      "learning_rate": 0.0001221076642125742,
      "loss": 2.5066,
      "step": 558
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 0.1810944676399231,
      "learning_rate": 0.00012185479662454595,
      "loss": 2.0995,
      "step": 559
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.25683358311653137,
      "learning_rate": 0.00012160178219764837,
      "loss": 2.1225,
      "step": 560
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 0.25220510363578796,
      "learning_rate": 0.00012134862263184467,
      "loss": 2.0077,
      "step": 561
    },
    {
      "epoch": 2.248,
      "grad_norm": 0.16244342923164368,
      "learning_rate": 0.00012109531962807332,
      "loss": 2.2287,
      "step": 562
    },
    {
      "epoch": 2.252,
      "grad_norm": 0.21872717142105103,
      "learning_rate": 0.00012084187488823657,
      "loss": 2.141,
      "step": 563
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.20502915978431702,
      "learning_rate": 0.00012058829011518896,
      "loss": 2.2432,
      "step": 564
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.21433058381080627,
      "learning_rate": 0.00012033456701272576,
      "loss": 2.2096,
      "step": 565
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.17969895899295807,
      "learning_rate": 0.00012008070728557186,
      "loss": 1.9935,
      "step": 566
    },
    {
      "epoch": 2.268,
      "grad_norm": 0.2115897387266159,
      "learning_rate": 0.00011982671263936995,
      "loss": 2.1159,
      "step": 567
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.18867698311805725,
      "learning_rate": 0.00011957258478066931,
      "loss": 1.9283,
      "step": 568
    },
    {
      "epoch": 2.276,
      "grad_norm": 0.3277583718299866,
      "learning_rate": 0.00011931832541691418,
      "loss": 2.4427,
      "step": 569
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.16848647594451904,
      "learning_rate": 0.00011906393625643244,
      "loss": 2.0919,
      "step": 570
    },
    {
      "epoch": 2.284,
      "grad_norm": 0.19793321192264557,
      "learning_rate": 0.00011880941900842397,
      "loss": 2.0405,
      "step": 571
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.22856798768043518,
      "learning_rate": 0.00011855477538294935,
      "loss": 2.2992,
      "step": 572
    },
    {
      "epoch": 2.292,
      "grad_norm": 0.16188204288482666,
      "learning_rate": 0.00011830000709091815,
      "loss": 1.8037,
      "step": 573
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.23483048379421234,
      "learning_rate": 0.00011804511584407763,
      "loss": 2.0626,
      "step": 574
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.1890602707862854,
      "learning_rate": 0.0001177901033550012,
      "loss": 2.1222,
      "step": 575
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.183222234249115,
      "learning_rate": 0.00011753497133707679,
      "loss": 2.4173,
      "step": 576
    },
    {
      "epoch": 2.308,
      "grad_norm": 0.139686718583107,
      "learning_rate": 0.00011727972150449544,
      "loss": 1.9295,
      "step": 577
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.17446854710578918,
      "learning_rate": 0.00011702435557223987,
      "loss": 2.0084,
      "step": 578
    },
    {
      "epoch": 2.316,
      "grad_norm": 0.16803738474845886,
      "learning_rate": 0.00011676887525607271,
      "loss": 2.0993,
      "step": 579
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.19748535752296448,
      "learning_rate": 0.00011651328227252517,
      "loss": 1.9961,
      "step": 580
    },
    {
      "epoch": 2.324,
      "grad_norm": 0.1528916358947754,
      "learning_rate": 0.00011625757833888551,
      "loss": 2.0652,
      "step": 581
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.2572670876979828,
      "learning_rate": 0.00011600176517318741,
      "loss": 2.1398,
      "step": 582
    },
    {
      "epoch": 2.332,
      "grad_norm": 0.28067469596862793,
      "learning_rate": 0.0001157458444941984,
      "loss": 2.1746,
      "step": 583
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.19753672182559967,
      "learning_rate": 0.00011548981802140848,
      "loss": 2.2,
      "step": 584
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.1476505845785141,
      "learning_rate": 0.00011523368747501839,
      "loss": 2.1189,
      "step": 585
    },
    {
      "epoch": 2.344,
      "grad_norm": 0.18562138080596924,
      "learning_rate": 0.00011497745457592816,
      "loss": 2.1452,
      "step": 586
    },
    {
      "epoch": 2.348,
      "grad_norm": 0.21142861247062683,
      "learning_rate": 0.00011472112104572547,
      "loss": 1.938,
      "step": 587
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.1823478490114212,
      "learning_rate": 0.00011446468860667421,
      "loss": 2.0482,
      "step": 588
    },
    {
      "epoch": 2.356,
      "grad_norm": 0.18604394793510437,
      "learning_rate": 0.0001142081589817027,
      "loss": 2.1004,
      "step": 589
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.19793963432312012,
      "learning_rate": 0.00011395153389439233,
      "loss": 2.028,
      "step": 590
    },
    {
      "epoch": 2.364,
      "grad_norm": 0.1618792712688446,
      "learning_rate": 0.00011369481506896582,
      "loss": 1.8852,
      "step": 591
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.17950376868247986,
      "learning_rate": 0.00011343800423027582,
      "loss": 1.9679,
      "step": 592
    },
    {
      "epoch": 2.372,
      "grad_norm": 0.18492065370082855,
      "learning_rate": 0.00011318110310379301,
      "loss": 2.1147,
      "step": 593
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.19769540429115295,
      "learning_rate": 0.0001129241134155949,
      "loss": 2.3209,
      "step": 594
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.18148808181285858,
      "learning_rate": 0.00011266703689235394,
      "loss": 1.9597,
      "step": 595
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.27210623025894165,
      "learning_rate": 0.00011240987526132594,
      "loss": 2.1206,
      "step": 596
    },
    {
      "epoch": 2.388,
      "grad_norm": 0.14450953900814056,
      "learning_rate": 0.00011215263025033869,
      "loss": 1.7278,
      "step": 597
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.29406508803367615,
      "learning_rate": 0.00011189530358778005,
      "loss": 1.8148,
      "step": 598
    },
    {
      "epoch": 2.396,
      "grad_norm": 0.22594888508319855,
      "learning_rate": 0.00011163789700258655,
      "loss": 2.1626,
      "step": 599
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.18433333933353424,
      "learning_rate": 0.00011138041222423177,
      "loss": 2.3308,
      "step": 600
    },
    {
      "epoch": 2.404,
      "grad_norm": 0.1620372235774994,
      "learning_rate": 0.00011112285098271451,
      "loss": 2.0092,
      "step": 601
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.16127929091453552,
      "learning_rate": 0.00011086521500854745,
      "loss": 1.977,
      "step": 602
    },
    {
      "epoch": 2.412,
      "grad_norm": 0.22987830638885498,
      "learning_rate": 0.00011060750603274535,
      "loss": 2.2146,
      "step": 603
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.18060705065727234,
      "learning_rate": 0.00011034972578681338,
      "loss": 2.036,
      "step": 604
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.16269712150096893,
      "learning_rate": 0.00011009187600273566,
      "loss": 1.9233,
      "step": 605
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.15734319388866425,
      "learning_rate": 0.00010983395841296348,
      "loss": 2.5637,
      "step": 606
    },
    {
      "epoch": 2.428,
      "grad_norm": 0.2742651402950287,
      "learning_rate": 0.00010957597475040373,
      "loss": 2.1306,
      "step": 607
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.1357511579990387,
      "learning_rate": 0.00010931792674840718,
      "loss": 2.2603,
      "step": 608
    },
    {
      "epoch": 2.436,
      "grad_norm": 0.17030450701713562,
      "learning_rate": 0.00010905981614075693,
      "loss": 1.97,
      "step": 609
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.24511027336120605,
      "learning_rate": 0.00010880164466165674,
      "loss": 1.9373,
      "step": 610
    },
    {
      "epoch": 2.444,
      "grad_norm": 0.2153739482164383,
      "learning_rate": 0.00010854341404571928,
      "loss": 2.1208,
      "step": 611
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.15565064549446106,
      "learning_rate": 0.00010828512602795462,
      "loss": 2.0333,
      "step": 612
    },
    {
      "epoch": 2.452,
      "grad_norm": 0.2044702172279358,
      "learning_rate": 0.00010802678234375851,
      "loss": 1.8064,
      "step": 613
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.1499081552028656,
      "learning_rate": 0.00010776838472890065,
      "loss": 1.8572,
      "step": 614
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.19229094684123993,
      "learning_rate": 0.0001075099349195131,
      "loss": 2.4403,
      "step": 615
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.21295735239982605,
      "learning_rate": 0.00010725143465207867,
      "loss": 2.0545,
      "step": 616
    },
    {
      "epoch": 2.468,
      "grad_norm": 0.1537831872701645,
      "learning_rate": 0.00010699288566341914,
      "loss": 1.9869,
      "step": 617
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.15025602281093597,
      "learning_rate": 0.00010673428969068364,
      "loss": 1.8364,
      "step": 618
    },
    {
      "epoch": 2.476,
      "grad_norm": 0.16458475589752197,
      "learning_rate": 0.000106475648471337,
      "loss": 2.123,
      "step": 619
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.20051711797714233,
      "learning_rate": 0.00010621696374314807,
      "loss": 1.9288,
      "step": 620
    },
    {
      "epoch": 2.484,
      "grad_norm": 0.19618502259254456,
      "learning_rate": 0.00010595823724417795,
      "loss": 2.0872,
      "step": 621
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.15890859067440033,
      "learning_rate": 0.00010569947071276847,
      "loss": 1.8292,
      "step": 622
    },
    {
      "epoch": 2.492,
      "grad_norm": 0.1967117339372635,
      "learning_rate": 0.00010544066588753044,
      "loss": 2.0165,
      "step": 623
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.1616954505443573,
      "learning_rate": 0.00010518182450733186,
      "loss": 1.9291,
      "step": 624
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.17152057588100433,
      "learning_rate": 0.00010492294831128641,
      "loss": 2.0695,
      "step": 625
    },
    {
      "epoch": 2.504,
      "grad_norm": 0.14590948820114136,
      "learning_rate": 0.00010466403903874176,
      "loss": 1.8343,
      "step": 626
    },
    {
      "epoch": 2.508,
      "grad_norm": 0.16265377402305603,
      "learning_rate": 0.00010440509842926767,
      "loss": 1.9904,
      "step": 627
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.12479811161756516,
      "learning_rate": 0.00010414612822264455,
      "loss": 2.1444,
      "step": 628
    },
    {
      "epoch": 2.516,
      "grad_norm": 0.2951153516769409,
      "learning_rate": 0.00010388713015885161,
      "loss": 2.4103,
      "step": 629
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.14703083038330078,
      "learning_rate": 0.00010362810597805526,
      "loss": 1.9599,
      "step": 630
    },
    {
      "epoch": 2.524,
      "grad_norm": 0.22389915585517883,
      "learning_rate": 0.00010336905742059742,
      "loss": 2.2641,
      "step": 631
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.2583003044128418,
      "learning_rate": 0.0001031099862269837,
      "loss": 2.1202,
      "step": 632
    },
    {
      "epoch": 2.532,
      "grad_norm": 0.18293912708759308,
      "learning_rate": 0.0001028508941378719,
      "loss": 2.0505,
      "step": 633
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.2322653830051422,
      "learning_rate": 0.00010259178289406011,
      "loss": 2.1892,
      "step": 634
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.18160414695739746,
      "learning_rate": 0.00010233265423647523,
      "loss": 2.1207,
      "step": 635
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.1556388884782791,
      "learning_rate": 0.00010207350990616107,
      "loss": 1.8495,
      "step": 636
    },
    {
      "epoch": 2.548,
      "grad_norm": 0.17538346350193024,
      "learning_rate": 0.00010181435164426676,
      "loss": 2.0246,
      "step": 637
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.19769003987312317,
      "learning_rate": 0.0001015551811920351,
      "loss": 2.1966,
      "step": 638
    },
    {
      "epoch": 2.556,
      "grad_norm": 0.41468051075935364,
      "learning_rate": 0.00010129600029079072,
      "loss": 1.8968,
      "step": 639
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.244182750582695,
      "learning_rate": 0.00010103681068192845,
      "loss": 2.0539,
      "step": 640
    },
    {
      "epoch": 2.564,
      "grad_norm": 0.18310318887233734,
      "learning_rate": 0.00010077761410690172,
      "loss": 1.9814,
      "step": 641
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.1874096840620041,
      "learning_rate": 0.00010051841230721065,
      "loss": 2.0184,
      "step": 642
    },
    {
      "epoch": 2.572,
      "grad_norm": 0.1842314600944519,
      "learning_rate": 0.00010025920702439051,
      "loss": 2.1099,
      "step": 643
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.23102633655071259,
      "learning_rate": 0.0001,
      "loss": 1.8497,
      "step": 644
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.22248736023902893,
      "learning_rate": 9.97407929756095e-05,
      "loss": 1.684,
      "step": 645
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.17979896068572998,
      "learning_rate": 9.948158769278939e-05,
      "loss": 1.996,
      "step": 646
    },
    {
      "epoch": 2.588,
      "grad_norm": 0.15631046891212463,
      "learning_rate": 9.92223858930983e-05,
      "loss": 1.8105,
      "step": 647
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.2246106117963791,
      "learning_rate": 9.896318931807155e-05,
      "loss": 1.9914,
      "step": 648
    },
    {
      "epoch": 2.596,
      "grad_norm": 0.15404613316059113,
      "learning_rate": 9.870399970920932e-05,
      "loss": 2.0027,
      "step": 649
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.20971478521823883,
      "learning_rate": 9.844481880796491e-05,
      "loss": 2.062,
      "step": 650
    },
    {
      "epoch": 2.604,
      "grad_norm": 0.18851901590824127,
      "learning_rate": 9.818564835573323e-05,
      "loss": 1.7959,
      "step": 651
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.16200150549411774,
      "learning_rate": 9.792649009383899e-05,
      "loss": 1.7786,
      "step": 652
    },
    {
      "epoch": 2.612,
      "grad_norm": 0.21786220371723175,
      "learning_rate": 9.766734576352478e-05,
      "loss": 1.8845,
      "step": 653
    },
    {
      "epoch": 2.616,
      "grad_norm": 0.1756245195865631,
      "learning_rate": 9.740821710593989e-05,
      "loss": 1.9905,
      "step": 654
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.15581092238426208,
      "learning_rate": 9.714910586212816e-05,
      "loss": 2.0474,
      "step": 655
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.15878917276859283,
      "learning_rate": 9.689001377301633e-05,
      "loss": 2.2744,
      "step": 656
    },
    {
      "epoch": 2.628,
      "grad_norm": 0.17826415598392487,
      "learning_rate": 9.663094257940258e-05,
      "loss": 1.8469,
      "step": 657
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.1503579169511795,
      "learning_rate": 9.637189402194476e-05,
      "loss": 1.86,
      "step": 658
    },
    {
      "epoch": 2.636,
      "grad_norm": 0.14635200798511505,
      "learning_rate": 9.611286984114841e-05,
      "loss": 1.92,
      "step": 659
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.13543826341629028,
      "learning_rate": 9.585387177735547e-05,
      "loss": 1.8554,
      "step": 660
    },
    {
      "epoch": 2.644,
      "grad_norm": 0.15403532981872559,
      "learning_rate": 9.559490157073236e-05,
      "loss": 1.9836,
      "step": 661
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.1389348953962326,
      "learning_rate": 9.533596096125825e-05,
      "loss": 1.8247,
      "step": 662
    },
    {
      "epoch": 2.652,
      "grad_norm": 0.16044658422470093,
      "learning_rate": 9.507705168871358e-05,
      "loss": 2.1065,
      "step": 663
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.2710728049278259,
      "learning_rate": 9.481817549266817e-05,
      "loss": 1.7826,
      "step": 664
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.16373810172080994,
      "learning_rate": 9.455933411246958e-05,
      "loss": 1.8716,
      "step": 665
    },
    {
      "epoch": 2.664,
      "grad_norm": 0.2470797598361969,
      "learning_rate": 9.430052928723153e-05,
      "loss": 1.9077,
      "step": 666
    },
    {
      "epoch": 2.668,
      "grad_norm": 0.13708074390888214,
      "learning_rate": 9.404176275582208e-05,
      "loss": 2.0555,
      "step": 667
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.14753413200378418,
      "learning_rate": 9.378303625685195e-05,
      "loss": 1.9522,
      "step": 668
    },
    {
      "epoch": 2.676,
      "grad_norm": 0.18609081208705902,
      "learning_rate": 9.352435152866298e-05,
      "loss": 1.7716,
      "step": 669
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.16802561283111572,
      "learning_rate": 9.326571030931637e-05,
      "loss": 2.2276,
      "step": 670
    },
    {
      "epoch": 2.684,
      "grad_norm": 0.17084893584251404,
      "learning_rate": 9.300711433658087e-05,
      "loss": 1.8985,
      "step": 671
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.1510791778564453,
      "learning_rate": 9.274856534792138e-05,
      "loss": 1.9951,
      "step": 672
    },
    {
      "epoch": 2.692,
      "grad_norm": 0.13875995576381683,
      "learning_rate": 9.249006508048694e-05,
      "loss": 1.7061,
      "step": 673
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.1901439130306244,
      "learning_rate": 9.223161527109937e-05,
      "loss": 1.7938,
      "step": 674
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.17847593128681183,
      "learning_rate": 9.197321765624152e-05,
      "loss": 1.8443,
      "step": 675
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.16448496282100677,
      "learning_rate": 9.171487397204539e-05,
      "loss": 1.6034,
      "step": 676
    },
    {
      "epoch": 2.708,
      "grad_norm": 0.1370270550251007,
      "learning_rate": 9.145658595428074e-05,
      "loss": 1.9547,
      "step": 677
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.1491595208644867,
      "learning_rate": 9.119835533834331e-05,
      "loss": 1.817,
      "step": 678
    },
    {
      "epoch": 2.716,
      "grad_norm": 0.2423771321773529,
      "learning_rate": 9.09401838592431e-05,
      "loss": 1.9272,
      "step": 679
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.15361756086349487,
      "learning_rate": 9.068207325159284e-05,
      "loss": 1.8609,
      "step": 680
    },
    {
      "epoch": 2.724,
      "grad_norm": 0.16426873207092285,
      "learning_rate": 9.04240252495963e-05,
      "loss": 1.5616,
      "step": 681
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.1415938138961792,
      "learning_rate": 9.016604158703654e-05,
      "loss": 2.1733,
      "step": 682
    },
    {
      "epoch": 2.732,
      "grad_norm": 0.2685019075870514,
      "learning_rate": 8.990812399726435e-05,
      "loss": 1.884,
      "step": 683
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.15310733020305634,
      "learning_rate": 8.965027421318665e-05,
      "loss": 1.8362,
      "step": 684
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.15076956152915955,
      "learning_rate": 8.939249396725467e-05,
      "loss": 1.6272,
      "step": 685
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.1618519127368927,
      "learning_rate": 8.913478499145254e-05,
      "loss": 1.9735,
      "step": 686
    },
    {
      "epoch": 2.748,
      "grad_norm": 0.19540993869304657,
      "learning_rate": 8.887714901728551e-05,
      "loss": 1.8568,
      "step": 687
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.17696402966976166,
      "learning_rate": 8.861958777576827e-05,
      "loss": 1.7977,
      "step": 688
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 0.17330212891101837,
      "learning_rate": 8.836210299741346e-05,
      "loss": 1.8128,
      "step": 689
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.2995413541793823,
      "learning_rate": 8.810469641222001e-05,
      "loss": 2.0634,
      "step": 690
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 0.22277355194091797,
      "learning_rate": 8.784736974966135e-05,
      "loss": 1.7826,
      "step": 691
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.18263229727745056,
      "learning_rate": 8.759012473867407e-05,
      "loss": 2.3785,
      "step": 692
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 0.2195274829864502,
      "learning_rate": 8.733296310764611e-05,
      "loss": 2.1618,
      "step": 693
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.16705581545829773,
      "learning_rate": 8.707588658440511e-05,
      "loss": 2.0607,
      "step": 694
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 0.14317256212234497,
      "learning_rate": 8.6818896896207e-05,
      "loss": 1.9644,
      "step": 695
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.25592413544654846,
      "learning_rate": 8.656199576972423e-05,
      "loss": 2.3467,
      "step": 696
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 0.18913403153419495,
      "learning_rate": 8.63051849310342e-05,
      "loss": 2.0022,
      "step": 697
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.1497451215982437,
      "learning_rate": 8.604846610560771e-05,
      "loss": 1.9212,
      "step": 698
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 0.1749730259180069,
      "learning_rate": 8.579184101829734e-05,
      "loss": 1.8596,
      "step": 699
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.15654607117176056,
      "learning_rate": 8.553531139332582e-05,
      "loss": 1.8289,
      "step": 700
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 0.18694479763507843,
      "learning_rate": 8.527887895427454e-05,
      "loss": 1.6319,
      "step": 701
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.13817040622234344,
      "learning_rate": 8.502254542407186e-05,
      "loss": 1.8937,
      "step": 702
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 0.15409019589424133,
      "learning_rate": 8.476631252498162e-05,
      "loss": 2.1491,
      "step": 703
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.14858001470565796,
      "learning_rate": 8.451018197859153e-05,
      "loss": 1.8035,
      "step": 704
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.18732406198978424,
      "learning_rate": 8.425415550580162e-05,
      "loss": 1.9305,
      "step": 705
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.14656221866607666,
      "learning_rate": 8.399823482681262e-05,
      "loss": 1.8639,
      "step": 706
    },
    {
      "epoch": 2.828,
      "grad_norm": 0.20689214766025543,
      "learning_rate": 8.374242166111448e-05,
      "loss": 1.8736,
      "step": 707
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.26563897728919983,
      "learning_rate": 8.348671772747487e-05,
      "loss": 2.3962,
      "step": 708
    },
    {
      "epoch": 2.836,
      "grad_norm": 0.13878609240055084,
      "learning_rate": 8.323112474392731e-05,
      "loss": 1.7364,
      "step": 709
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.21830011904239655,
      "learning_rate": 8.297564442776014e-05,
      "loss": 1.8907,
      "step": 710
    },
    {
      "epoch": 2.844,
      "grad_norm": 0.1388748735189438,
      "learning_rate": 8.272027849550457e-05,
      "loss": 1.8351,
      "step": 711
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.18855561316013336,
      "learning_rate": 8.246502866292324e-05,
      "loss": 2.1214,
      "step": 712
    },
    {
      "epoch": 2.852,
      "grad_norm": 0.20257987082004547,
      "learning_rate": 8.220989664499878e-05,
      "loss": 1.8997,
      "step": 713
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.23046156764030457,
      "learning_rate": 8.195488415592238e-05,
      "loss": 1.771,
      "step": 714
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.15141865611076355,
      "learning_rate": 8.169999290908188e-05,
      "loss": 1.6185,
      "step": 715
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.1794278472661972,
      "learning_rate": 8.144522461705067e-05,
      "loss": 1.9861,
      "step": 716
    },
    {
      "epoch": 2.868,
      "grad_norm": 0.15025009214878082,
      "learning_rate": 8.119058099157604e-05,
      "loss": 1.9571,
      "step": 717
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.16221442818641663,
      "learning_rate": 8.093606374356759e-05,
      "loss": 1.9609,
      "step": 718
    },
    {
      "epoch": 2.876,
      "grad_norm": 0.14964789152145386,
      "learning_rate": 8.068167458308582e-05,
      "loss": 2.0662,
      "step": 719
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.2767201364040375,
      "learning_rate": 8.042741521933071e-05,
      "loss": 1.9643,
      "step": 720
    },
    {
      "epoch": 2.884,
      "grad_norm": 0.23399552702903748,
      "learning_rate": 8.017328736063006e-05,
      "loss": 2.1019,
      "step": 721
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.15427672863006592,
      "learning_rate": 7.991929271442817e-05,
      "loss": 1.7895,
      "step": 722
    },
    {
      "epoch": 2.892,
      "grad_norm": 0.1621965616941452,
      "learning_rate": 7.966543298727425e-05,
      "loss": 1.9684,
      "step": 723
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.3451474606990814,
      "learning_rate": 7.941170988481108e-05,
      "loss": 2.0822,
      "step": 724
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.1831127107143402,
      "learning_rate": 7.915812511176347e-05,
      "loss": 1.9204,
      "step": 725
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.1613176465034485,
      "learning_rate": 7.89046803719267e-05,
      "loss": 1.7971,
      "step": 726
    },
    {
      "epoch": 2.908,
      "grad_norm": 0.1715327948331833,
      "learning_rate": 7.865137736815535e-05,
      "loss": 2.1245,
      "step": 727
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.17193591594696045,
      "learning_rate": 7.839821780235168e-05,
      "loss": 1.824,
      "step": 728
    },
    {
      "epoch": 2.916,
      "grad_norm": 0.1586778461933136,
      "learning_rate": 7.814520337545406e-05,
      "loss": 1.7614,
      "step": 729
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.2161073088645935,
      "learning_rate": 7.789233578742582e-05,
      "loss": 1.8626,
      "step": 730
    },
    {
      "epoch": 2.924,
      "grad_norm": 0.2184544801712036,
      "learning_rate": 7.763961673724379e-05,
      "loss": 1.7929,
      "step": 731
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.22426220774650574,
      "learning_rate": 7.738704792288655e-05,
      "loss": 1.8551,
      "step": 732
    },
    {
      "epoch": 2.932,
      "grad_norm": 0.20324860513210297,
      "learning_rate": 7.713463104132345e-05,
      "loss": 1.8551,
      "step": 733
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.1811738908290863,
      "learning_rate": 7.688236778850306e-05,
      "loss": 1.7324,
      "step": 734
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.19647830724716187,
      "learning_rate": 7.663025985934158e-05,
      "loss": 2.0676,
      "step": 735
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.21182474493980408,
      "learning_rate": 7.637830894771175e-05,
      "loss": 1.7351,
      "step": 736
    },
    {
      "epoch": 2.948,
      "grad_norm": 0.14565347135066986,
      "learning_rate": 7.61265167464313e-05,
      "loss": 1.901,
      "step": 737
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.1601180136203766,
      "learning_rate": 7.587488494725157e-05,
      "loss": 1.9537,
      "step": 738
    },
    {
      "epoch": 2.956,
      "grad_norm": 0.27412593364715576,
      "learning_rate": 7.562341524084623e-05,
      "loss": 1.9204,
      "step": 739
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.15422390401363373,
      "learning_rate": 7.537210931679987e-05,
      "loss": 2.2244,
      "step": 740
    },
    {
      "epoch": 2.964,
      "grad_norm": 0.13262850046157837,
      "learning_rate": 7.512096886359664e-05,
      "loss": 1.9955,
      "step": 741
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.3681770861148834,
      "learning_rate": 7.48699955686089e-05,
      "loss": 2.0325,
      "step": 742
    },
    {
      "epoch": 2.972,
      "grad_norm": 0.15433210134506226,
      "learning_rate": 7.461919111808595e-05,
      "loss": 1.9011,
      "step": 743
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.15942245721817017,
      "learning_rate": 7.43685571971426e-05,
      "loss": 1.7252,
      "step": 744
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.20460253953933716,
      "learning_rate": 7.411809548974792e-05,
      "loss": 1.8014,
      "step": 745
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.19481869041919708,
      "learning_rate": 7.386780767871397e-05,
      "loss": 1.8996,
      "step": 746
    },
    {
      "epoch": 2.988,
      "grad_norm": 0.17041510343551636,
      "learning_rate": 7.361769544568425e-05,
      "loss": 2.063,
      "step": 747
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.1859513521194458,
      "learning_rate": 7.336776047112276e-05,
      "loss": 1.6568,
      "step": 748
    },
    {
      "epoch": 2.996,
      "grad_norm": 0.14082273840904236,
      "learning_rate": 7.311800443430251e-05,
      "loss": 1.6851,
      "step": 749
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.16044463217258453,
      "learning_rate": 7.286842901329412e-05,
      "loss": 2.1589,
      "step": 750
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.9331860542297363,
      "eval_runtime": 23.8118,
      "eval_samples_per_second": 20.998,
      "eval_steps_per_second": 2.646,
      "step": 750
    }
  ],
  "logging_steps": 1,
  "max_steps": 1250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 250,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.855505568025805e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
