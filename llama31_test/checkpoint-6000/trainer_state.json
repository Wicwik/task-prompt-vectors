{
  "best_metric": 1.03153395652771,
  "best_model_checkpoint": "llama31_test/checkpoint-6000",
  "epoch": 8.0,
  "eval_steps": 1500,
  "global_step": 6000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0013333333333333333,
      "grad_norm": 0.3555504083633423,
      "learning_rate": 8.888888888888888e-06,
      "loss": 6.939,
      "step": 1
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 0.2820369005203247,
      "learning_rate": 1.7777777777777777e-05,
      "loss": 7.1212,
      "step": 2
    },
    {
      "epoch": 0.004,
      "grad_norm": 0.3040475845336914,
      "learning_rate": 2.666666666666667e-05,
      "loss": 7.2469,
      "step": 3
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 0.3130744695663452,
      "learning_rate": 3.555555555555555e-05,
      "loss": 7.1404,
      "step": 4
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 0.3134247660636902,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 7.2064,
      "step": 5
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.33323732018470764,
      "learning_rate": 5.333333333333334e-05,
      "loss": 6.659,
      "step": 6
    },
    {
      "epoch": 0.009333333333333334,
      "grad_norm": 0.3007800579071045,
      "learning_rate": 6.222222222222222e-05,
      "loss": 6.9849,
      "step": 7
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.2976188063621521,
      "learning_rate": 7.11111111111111e-05,
      "loss": 7.0471,
      "step": 8
    },
    {
      "epoch": 0.012,
      "grad_norm": 0.32053157687187195,
      "learning_rate": 8e-05,
      "loss": 6.9614,
      "step": 9
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.328104704618454,
      "learning_rate": 8.888888888888889e-05,
      "loss": 6.9047,
      "step": 10
    },
    {
      "epoch": 0.014666666666666666,
      "grad_norm": 0.3572377562522888,
      "learning_rate": 9.777777777777778e-05,
      "loss": 7.0393,
      "step": 11
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.3350681662559509,
      "learning_rate": 0.00010666666666666668,
      "loss": 6.9802,
      "step": 12
    },
    {
      "epoch": 0.017333333333333333,
      "grad_norm": 0.3303508162498474,
      "learning_rate": 0.00011555555555555555,
      "loss": 6.9706,
      "step": 13
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.32809674739837646,
      "learning_rate": 0.00012444444444444444,
      "loss": 6.9325,
      "step": 14
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.3702910840511322,
      "learning_rate": 0.00013333333333333334,
      "loss": 6.7135,
      "step": 15
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.32899269461631775,
      "learning_rate": 0.0001422222222222222,
      "loss": 6.8394,
      "step": 16
    },
    {
      "epoch": 0.02266666666666667,
      "grad_norm": 0.30374035239219666,
      "learning_rate": 0.0001511111111111111,
      "loss": 6.8322,
      "step": 17
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.366574764251709,
      "learning_rate": 0.00016,
      "loss": 6.7342,
      "step": 18
    },
    {
      "epoch": 0.025333333333333333,
      "grad_norm": 0.34597158432006836,
      "learning_rate": 0.00016888888888888889,
      "loss": 7.1006,
      "step": 19
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.3574230968952179,
      "learning_rate": 0.00017777777777777779,
      "loss": 7.0246,
      "step": 20
    },
    {
      "epoch": 0.028,
      "grad_norm": 0.32099682092666626,
      "learning_rate": 0.0001866666666666667,
      "loss": 6.9404,
      "step": 21
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 0.3596108555793762,
      "learning_rate": 0.00019555555555555556,
      "loss": 6.9141,
      "step": 22
    },
    {
      "epoch": 0.030666666666666665,
      "grad_norm": 0.323851078748703,
      "learning_rate": 0.00020444444444444446,
      "loss": 6.7382,
      "step": 23
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.290274977684021,
      "learning_rate": 0.00021333333333333336,
      "loss": 6.8497,
      "step": 24
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.3392907679080963,
      "learning_rate": 0.0002222222222222222,
      "loss": 6.7182,
      "step": 25
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 0.3558824360370636,
      "learning_rate": 0.0002311111111111111,
      "loss": 6.5068,
      "step": 26
    },
    {
      "epoch": 0.036,
      "grad_norm": 0.3710341453552246,
      "learning_rate": 0.00024,
      "loss": 6.4784,
      "step": 27
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 0.3727741241455078,
      "learning_rate": 0.0002488888888888889,
      "loss": 6.5286,
      "step": 28
    },
    {
      "epoch": 0.03866666666666667,
      "grad_norm": 0.3603370189666748,
      "learning_rate": 0.0002577777777777778,
      "loss": 6.6161,
      "step": 29
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.38295966386795044,
      "learning_rate": 0.0002666666666666667,
      "loss": 6.5721,
      "step": 30
    },
    {
      "epoch": 0.04133333333333333,
      "grad_norm": 0.41009706258773804,
      "learning_rate": 0.0002755555555555556,
      "loss": 6.6032,
      "step": 31
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.30817705392837524,
      "learning_rate": 0.0002844444444444444,
      "loss": 6.6088,
      "step": 32
    },
    {
      "epoch": 0.044,
      "grad_norm": 0.4053432047367096,
      "learning_rate": 0.0002933333333333333,
      "loss": 6.3758,
      "step": 33
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 0.3585493266582489,
      "learning_rate": 0.0003022222222222222,
      "loss": 6.5408,
      "step": 34
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.3294336497783661,
      "learning_rate": 0.0003111111111111111,
      "loss": 6.4788,
      "step": 35
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.38775748014450073,
      "learning_rate": 0.00032,
      "loss": 5.8985,
      "step": 36
    },
    {
      "epoch": 0.04933333333333333,
      "grad_norm": 0.5096060037612915,
      "learning_rate": 0.0003288888888888889,
      "loss": 5.8477,
      "step": 37
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.30484312772750854,
      "learning_rate": 0.00033777777777777777,
      "loss": 6.2385,
      "step": 38
    },
    {
      "epoch": 0.052,
      "grad_norm": 0.36814066767692566,
      "learning_rate": 0.00034666666666666667,
      "loss": 6.1029,
      "step": 39
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.35196802020072937,
      "learning_rate": 0.00035555555555555557,
      "loss": 6.3254,
      "step": 40
    },
    {
      "epoch": 0.05466666666666667,
      "grad_norm": 0.36300069093704224,
      "learning_rate": 0.00036444444444444447,
      "loss": 5.6745,
      "step": 41
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.3681320250034332,
      "learning_rate": 0.0003733333333333334,
      "loss": 5.7915,
      "step": 42
    },
    {
      "epoch": 0.05733333333333333,
      "grad_norm": 0.40314555168151855,
      "learning_rate": 0.0003822222222222223,
      "loss": 5.9162,
      "step": 43
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.39633047580718994,
      "learning_rate": 0.0003911111111111111,
      "loss": 5.7487,
      "step": 44
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.39122119545936584,
      "learning_rate": 0.0004,
      "loss": 6.0053,
      "step": 45
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.3738775849342346,
      "learning_rate": 0.0004088888888888889,
      "loss": 5.8295,
      "step": 46
    },
    {
      "epoch": 0.06266666666666666,
      "grad_norm": 0.4502626359462738,
      "learning_rate": 0.0004177777777777778,
      "loss": 5.602,
      "step": 47
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.41406819224357605,
      "learning_rate": 0.0004266666666666667,
      "loss": 5.5885,
      "step": 48
    },
    {
      "epoch": 0.06533333333333333,
      "grad_norm": 0.3728400766849518,
      "learning_rate": 0.0004355555555555555,
      "loss": 5.3816,
      "step": 49
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.43621519207954407,
      "learning_rate": 0.0004444444444444444,
      "loss": 5.5194,
      "step": 50
    },
    {
      "epoch": 0.068,
      "grad_norm": 0.40215903520584106,
      "learning_rate": 0.0004533333333333333,
      "loss": 5.3399,
      "step": 51
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.35502320528030396,
      "learning_rate": 0.0004622222222222222,
      "loss": 5.5855,
      "step": 52
    },
    {
      "epoch": 0.07066666666666667,
      "grad_norm": 0.3777894079685211,
      "learning_rate": 0.0004711111111111111,
      "loss": 5.02,
      "step": 53
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.40208253264427185,
      "learning_rate": 0.00048,
      "loss": 5.239,
      "step": 54
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.4758330285549164,
      "learning_rate": 0.0004888888888888889,
      "loss": 4.995,
      "step": 55
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.4006824195384979,
      "learning_rate": 0.0004977777777777778,
      "loss": 4.9496,
      "step": 56
    },
    {
      "epoch": 0.076,
      "grad_norm": 0.34312161803245544,
      "learning_rate": 0.0005066666666666668,
      "loss": 4.6876,
      "step": 57
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.390020489692688,
      "learning_rate": 0.0005155555555555556,
      "loss": 4.8512,
      "step": 58
    },
    {
      "epoch": 0.07866666666666666,
      "grad_norm": 0.3633885085582733,
      "learning_rate": 0.0005244444444444445,
      "loss": 5.0354,
      "step": 59
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.33825743198394775,
      "learning_rate": 0.0005333333333333334,
      "loss": 5.0089,
      "step": 60
    },
    {
      "epoch": 0.08133333333333333,
      "grad_norm": 0.27596643567085266,
      "learning_rate": 0.0005422222222222223,
      "loss": 4.5417,
      "step": 61
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.2744942903518677,
      "learning_rate": 0.0005511111111111112,
      "loss": 4.5155,
      "step": 62
    },
    {
      "epoch": 0.084,
      "grad_norm": 0.2609971761703491,
      "learning_rate": 0.0005600000000000001,
      "loss": 4.3623,
      "step": 63
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.25846701860427856,
      "learning_rate": 0.0005688888888888889,
      "loss": 4.539,
      "step": 64
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.24574969708919525,
      "learning_rate": 0.0005777777777777778,
      "loss": 4.2871,
      "step": 65
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.2544957995414734,
      "learning_rate": 0.0005866666666666667,
      "loss": 4.4581,
      "step": 66
    },
    {
      "epoch": 0.08933333333333333,
      "grad_norm": 0.2602943480014801,
      "learning_rate": 0.0005955555555555556,
      "loss": 4.2923,
      "step": 67
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.21910735964775085,
      "learning_rate": 0.0006044444444444445,
      "loss": 4.001,
      "step": 68
    },
    {
      "epoch": 0.092,
      "grad_norm": 0.2860652506351471,
      "learning_rate": 0.0006133333333333334,
      "loss": 4.2471,
      "step": 69
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.23105306923389435,
      "learning_rate": 0.0006222222222222223,
      "loss": 4.1282,
      "step": 70
    },
    {
      "epoch": 0.09466666666666666,
      "grad_norm": 0.2389366626739502,
      "learning_rate": 0.000631111111111111,
      "loss": 3.8047,
      "step": 71
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.21928375959396362,
      "learning_rate": 0.00064,
      "loss": 4.0391,
      "step": 72
    },
    {
      "epoch": 0.09733333333333333,
      "grad_norm": 0.24404151737689972,
      "learning_rate": 0.0006488888888888888,
      "loss": 3.9932,
      "step": 73
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.2796590328216553,
      "learning_rate": 0.0006577777777777779,
      "loss": 4.1413,
      "step": 74
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.29691430926322937,
      "learning_rate": 0.0006666666666666666,
      "loss": 4.0533,
      "step": 75
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.2289622575044632,
      "learning_rate": 0.0006755555555555555,
      "loss": 3.9604,
      "step": 76
    },
    {
      "epoch": 0.10266666666666667,
      "grad_norm": 0.25216829776763916,
      "learning_rate": 0.0006844444444444444,
      "loss": 4.0024,
      "step": 77
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.18387772142887115,
      "learning_rate": 0.0006933333333333333,
      "loss": 3.7311,
      "step": 78
    },
    {
      "epoch": 0.10533333333333333,
      "grad_norm": 0.2159779965877533,
      "learning_rate": 0.0007022222222222222,
      "loss": 4.0338,
      "step": 79
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.23165352642536163,
      "learning_rate": 0.0007111111111111111,
      "loss": 3.6967,
      "step": 80
    },
    {
      "epoch": 0.108,
      "grad_norm": 0.25378698110580444,
      "learning_rate": 0.0007199999999999999,
      "loss": 3.5939,
      "step": 81
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.20034092664718628,
      "learning_rate": 0.0007288888888888889,
      "loss": 3.8341,
      "step": 82
    },
    {
      "epoch": 0.11066666666666666,
      "grad_norm": 0.17457611858844757,
      "learning_rate": 0.0007377777777777777,
      "loss": 3.5018,
      "step": 83
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.2083585560321808,
      "learning_rate": 0.0007466666666666667,
      "loss": 3.81,
      "step": 84
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.2119496464729309,
      "learning_rate": 0.0007555555555555555,
      "loss": 3.8609,
      "step": 85
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.22008116543293,
      "learning_rate": 0.0007644444444444445,
      "loss": 3.6746,
      "step": 86
    },
    {
      "epoch": 0.116,
      "grad_norm": 0.21630693972110748,
      "learning_rate": 0.0007733333333333333,
      "loss": 3.5259,
      "step": 87
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.18918000161647797,
      "learning_rate": 0.0007822222222222222,
      "loss": 3.417,
      "step": 88
    },
    {
      "epoch": 0.11866666666666667,
      "grad_norm": 0.24622909724712372,
      "learning_rate": 0.0007911111111111111,
      "loss": 3.504,
      "step": 89
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.2659607231616974,
      "learning_rate": 0.0008,
      "loss": 3.6211,
      "step": 90
    },
    {
      "epoch": 0.12133333333333333,
      "grad_norm": 0.29008498787879944,
      "learning_rate": 0.0008088888888888889,
      "loss": 3.4069,
      "step": 91
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.2224079668521881,
      "learning_rate": 0.0008177777777777778,
      "loss": 3.2265,
      "step": 92
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.19913670420646667,
      "learning_rate": 0.0008266666666666666,
      "loss": 3.1952,
      "step": 93
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.2076563686132431,
      "learning_rate": 0.0008355555555555556,
      "loss": 3.2571,
      "step": 94
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.1831824779510498,
      "learning_rate": 0.0008444444444444444,
      "loss": 3.2619,
      "step": 95
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.23425628244876862,
      "learning_rate": 0.0008533333333333334,
      "loss": 3.0934,
      "step": 96
    },
    {
      "epoch": 0.12933333333333333,
      "grad_norm": 0.25713077187538147,
      "learning_rate": 0.0008622222222222222,
      "loss": 3.4453,
      "step": 97
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.19652040302753448,
      "learning_rate": 0.000871111111111111,
      "loss": 3.0896,
      "step": 98
    },
    {
      "epoch": 0.132,
      "grad_norm": 0.1827600747346878,
      "learning_rate": 0.00088,
      "loss": 2.8709,
      "step": 99
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.20682170987129211,
      "learning_rate": 0.0008888888888888888,
      "loss": 3.0675,
      "step": 100
    },
    {
      "epoch": 0.13466666666666666,
      "grad_norm": 0.1917726695537567,
      "learning_rate": 0.0008977777777777778,
      "loss": 3.0282,
      "step": 101
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.18286015093326569,
      "learning_rate": 0.0009066666666666666,
      "loss": 3.3707,
      "step": 102
    },
    {
      "epoch": 0.13733333333333334,
      "grad_norm": 0.18966540694236755,
      "learning_rate": 0.0009155555555555556,
      "loss": 3.1299,
      "step": 103
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.18103714287281036,
      "learning_rate": 0.0009244444444444444,
      "loss": 2.9937,
      "step": 104
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.17885956168174744,
      "learning_rate": 0.0009333333333333333,
      "loss": 3.2555,
      "step": 105
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.35943517088890076,
      "learning_rate": 0.0009422222222222222,
      "loss": 3.0403,
      "step": 106
    },
    {
      "epoch": 0.14266666666666666,
      "grad_norm": 0.20058420300483704,
      "learning_rate": 0.0009511111111111111,
      "loss": 2.9642,
      "step": 107
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.2611333727836609,
      "learning_rate": 0.00096,
      "loss": 3.096,
      "step": 108
    },
    {
      "epoch": 0.14533333333333334,
      "grad_norm": 0.3253711760044098,
      "learning_rate": 0.0009688888888888889,
      "loss": 3.0214,
      "step": 109
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.2833825945854187,
      "learning_rate": 0.0009777777777777777,
      "loss": 3.0148,
      "step": 110
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.15768861770629883,
      "learning_rate": 0.0009866666666666667,
      "loss": 3.2658,
      "step": 111
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.19665218889713287,
      "learning_rate": 0.0009955555555555555,
      "loss": 2.9948,
      "step": 112
    },
    {
      "epoch": 0.15066666666666667,
      "grad_norm": 0.22823019325733185,
      "learning_rate": 0.0010044444444444445,
      "loss": 2.7265,
      "step": 113
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.15567077696323395,
      "learning_rate": 0.0010133333333333335,
      "loss": 2.8005,
      "step": 114
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.2153978943824768,
      "learning_rate": 0.0010222222222222221,
      "loss": 2.7751,
      "step": 115
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.43959227204322815,
      "learning_rate": 0.0010311111111111111,
      "loss": 2.7895,
      "step": 116
    },
    {
      "epoch": 0.156,
      "grad_norm": 0.19013641774654388,
      "learning_rate": 0.0010400000000000001,
      "loss": 2.774,
      "step": 117
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.21319551765918732,
      "learning_rate": 0.001048888888888889,
      "loss": 2.9346,
      "step": 118
    },
    {
      "epoch": 0.15866666666666668,
      "grad_norm": 0.3194159269332886,
      "learning_rate": 0.0010577777777777777,
      "loss": 2.753,
      "step": 119
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.24583186209201813,
      "learning_rate": 0.0010666666666666667,
      "loss": 3.0352,
      "step": 120
    },
    {
      "epoch": 0.16133333333333333,
      "grad_norm": 0.2755632996559143,
      "learning_rate": 0.0010755555555555557,
      "loss": 2.6773,
      "step": 121
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.1570361852645874,
      "learning_rate": 0.0010844444444444445,
      "loss": 2.895,
      "step": 122
    },
    {
      "epoch": 0.164,
      "grad_norm": 0.31439799070358276,
      "learning_rate": 0.0010933333333333333,
      "loss": 2.8218,
      "step": 123
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.1796349436044693,
      "learning_rate": 0.0011022222222222223,
      "loss": 2.6005,
      "step": 124
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.1848248541355133,
      "learning_rate": 0.0011111111111111111,
      "loss": 2.715,
      "step": 125
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.19730344414710999,
      "learning_rate": 0.0011200000000000001,
      "loss": 2.7502,
      "step": 126
    },
    {
      "epoch": 0.16933333333333334,
      "grad_norm": 0.23708689212799072,
      "learning_rate": 0.001128888888888889,
      "loss": 2.4887,
      "step": 127
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.1518843024969101,
      "learning_rate": 0.0011377777777777777,
      "loss": 2.9067,
      "step": 128
    },
    {
      "epoch": 0.172,
      "grad_norm": 0.23372532427310944,
      "learning_rate": 0.0011466666666666667,
      "loss": 2.5531,
      "step": 129
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.19489368796348572,
      "learning_rate": 0.0011555555555555555,
      "loss": 2.5173,
      "step": 130
    },
    {
      "epoch": 0.17466666666666666,
      "grad_norm": 0.17375832796096802,
      "learning_rate": 0.0011644444444444445,
      "loss": 2.7289,
      "step": 131
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.17223307490348816,
      "learning_rate": 0.0011733333333333333,
      "loss": 2.3688,
      "step": 132
    },
    {
      "epoch": 0.17733333333333334,
      "grad_norm": 0.1609000563621521,
      "learning_rate": 0.0011822222222222223,
      "loss": 2.6588,
      "step": 133
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.2193676382303238,
      "learning_rate": 0.001191111111111111,
      "loss": 2.407,
      "step": 134
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.2354118674993515,
      "learning_rate": 0.0012,
      "loss": 2.5228,
      "step": 135
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.2277151644229889,
      "learning_rate": 0.001208888888888889,
      "loss": 2.5359,
      "step": 136
    },
    {
      "epoch": 0.18266666666666667,
      "grad_norm": 0.2764875888824463,
      "learning_rate": 0.001217777777777778,
      "loss": 2.8204,
      "step": 137
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.18017089366912842,
      "learning_rate": 0.0012266666666666667,
      "loss": 2.3479,
      "step": 138
    },
    {
      "epoch": 0.18533333333333332,
      "grad_norm": 0.2732390761375427,
      "learning_rate": 0.0012355555555555555,
      "loss": 2.6582,
      "step": 139
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.24082329869270325,
      "learning_rate": 0.0012444444444444445,
      "loss": 2.1702,
      "step": 140
    },
    {
      "epoch": 0.188,
      "grad_norm": 0.23642857372760773,
      "learning_rate": 0.0012533333333333335,
      "loss": 2.6012,
      "step": 141
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.22302553057670593,
      "learning_rate": 0.001262222222222222,
      "loss": 2.5584,
      "step": 142
    },
    {
      "epoch": 0.19066666666666668,
      "grad_norm": 0.21631298959255219,
      "learning_rate": 0.001271111111111111,
      "loss": 2.4059,
      "step": 143
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.23782074451446533,
      "learning_rate": 0.00128,
      "loss": 2.4692,
      "step": 144
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.1809307038784027,
      "learning_rate": 0.001288888888888889,
      "loss": 2.442,
      "step": 145
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.22575511038303375,
      "learning_rate": 0.0012977777777777777,
      "loss": 2.3062,
      "step": 146
    },
    {
      "epoch": 0.196,
      "grad_norm": 0.22840678691864014,
      "learning_rate": 0.0013066666666666667,
      "loss": 2.4491,
      "step": 147
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.2556595504283905,
      "learning_rate": 0.0013155555555555557,
      "loss": 2.309,
      "step": 148
    },
    {
      "epoch": 0.19866666666666666,
      "grad_norm": 0.25059255957603455,
      "learning_rate": 0.0013244444444444445,
      "loss": 2.2907,
      "step": 149
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.2540203332901001,
      "learning_rate": 0.0013333333333333333,
      "loss": 2.4229,
      "step": 150
    },
    {
      "epoch": 0.20133333333333334,
      "grad_norm": 0.3189737796783447,
      "learning_rate": 0.0013422222222222223,
      "loss": 2.3595,
      "step": 151
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.2814345061779022,
      "learning_rate": 0.001351111111111111,
      "loss": 2.3265,
      "step": 152
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.23742927610874176,
      "learning_rate": 0.00136,
      "loss": 2.2254,
      "step": 153
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.4587046205997467,
      "learning_rate": 0.0013688888888888889,
      "loss": 2.1579,
      "step": 154
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.30864301323890686,
      "learning_rate": 0.001377777777777778,
      "loss": 2.0703,
      "step": 155
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.22583816945552826,
      "learning_rate": 0.0013866666666666667,
      "loss": 2.3132,
      "step": 156
    },
    {
      "epoch": 0.20933333333333334,
      "grad_norm": 0.22097212076187134,
      "learning_rate": 0.0013955555555555557,
      "loss": 2.3189,
      "step": 157
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.5596842765808105,
      "learning_rate": 0.0014044444444444445,
      "loss": 2.3481,
      "step": 158
    },
    {
      "epoch": 0.212,
      "grad_norm": 0.24860230088233948,
      "learning_rate": 0.0014133333333333333,
      "loss": 2.2695,
      "step": 159
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.5702952742576599,
      "learning_rate": 0.0014222222222222223,
      "loss": 2.2506,
      "step": 160
    },
    {
      "epoch": 0.21466666666666667,
      "grad_norm": 0.2053312063217163,
      "learning_rate": 0.001431111111111111,
      "loss": 2.2972,
      "step": 161
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.26408371329307556,
      "learning_rate": 0.0014399999999999999,
      "loss": 2.1839,
      "step": 162
    },
    {
      "epoch": 0.21733333333333332,
      "grad_norm": 0.21351370215415955,
      "learning_rate": 0.0014488888888888889,
      "loss": 2.1841,
      "step": 163
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.25986039638519287,
      "learning_rate": 0.0014577777777777779,
      "loss": 1.887,
      "step": 164
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.2630249559879303,
      "learning_rate": 0.0014666666666666667,
      "loss": 2.1449,
      "step": 165
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.2603936493396759,
      "learning_rate": 0.0014755555555555555,
      "loss": 1.8729,
      "step": 166
    },
    {
      "epoch": 0.22266666666666668,
      "grad_norm": 0.22913362085819244,
      "learning_rate": 0.0014844444444444445,
      "loss": 2.0419,
      "step": 167
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.2890835106372833,
      "learning_rate": 0.0014933333333333335,
      "loss": 2.0237,
      "step": 168
    },
    {
      "epoch": 0.22533333333333333,
      "grad_norm": 0.24949640035629272,
      "learning_rate": 0.001502222222222222,
      "loss": 2.1428,
      "step": 169
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.2959970235824585,
      "learning_rate": 0.001511111111111111,
      "loss": 2.3783,
      "step": 170
    },
    {
      "epoch": 0.228,
      "grad_norm": 0.3019074499607086,
      "learning_rate": 0.00152,
      "loss": 2.5129,
      "step": 171
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.37460583448410034,
      "learning_rate": 0.001528888888888889,
      "loss": 1.7151,
      "step": 172
    },
    {
      "epoch": 0.23066666666666666,
      "grad_norm": 0.3367548882961273,
      "learning_rate": 0.0015377777777777777,
      "loss": 1.77,
      "step": 173
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.27657562494277954,
      "learning_rate": 0.0015466666666666667,
      "loss": 2.1734,
      "step": 174
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.25605666637420654,
      "learning_rate": 0.0015555555555555557,
      "loss": 1.8297,
      "step": 175
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.32427042722702026,
      "learning_rate": 0.0015644444444444445,
      "loss": 1.9176,
      "step": 176
    },
    {
      "epoch": 0.236,
      "grad_norm": 0.30497390031814575,
      "learning_rate": 0.0015733333333333333,
      "loss": 1.9386,
      "step": 177
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.24644477665424347,
      "learning_rate": 0.0015822222222222223,
      "loss": 1.7461,
      "step": 178
    },
    {
      "epoch": 0.23866666666666667,
      "grad_norm": 0.26502078771591187,
      "learning_rate": 0.001591111111111111,
      "loss": 1.8744,
      "step": 179
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.4013954699039459,
      "learning_rate": 0.0016,
      "loss": 1.7072,
      "step": 180
    },
    {
      "epoch": 0.24133333333333334,
      "grad_norm": 0.26181158423423767,
      "learning_rate": 0.0016088888888888889,
      "loss": 1.9893,
      "step": 181
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.38677167892456055,
      "learning_rate": 0.0016177777777777779,
      "loss": 1.7762,
      "step": 182
    },
    {
      "epoch": 0.244,
      "grad_norm": 0.2490021288394928,
      "learning_rate": 0.0016266666666666667,
      "loss": 2.2498,
      "step": 183
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.3483845293521881,
      "learning_rate": 0.0016355555555555557,
      "loss": 1.7619,
      "step": 184
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.21299581229686737,
      "learning_rate": 0.0016444444444444445,
      "loss": 1.8468,
      "step": 185
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.2674828767776489,
      "learning_rate": 0.0016533333333333333,
      "loss": 2.0769,
      "step": 186
    },
    {
      "epoch": 0.24933333333333332,
      "grad_norm": 0.18275614082813263,
      "learning_rate": 0.0016622222222222223,
      "loss": 1.816,
      "step": 187
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.357870489358902,
      "learning_rate": 0.0016711111111111113,
      "loss": 1.9828,
      "step": 188
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.22154709696769714,
      "learning_rate": 0.00168,
      "loss": 1.327,
      "step": 189
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.1553424596786499,
      "learning_rate": 0.0016888888888888889,
      "loss": 1.7697,
      "step": 190
    },
    {
      "epoch": 0.25466666666666665,
      "grad_norm": 0.35465845465660095,
      "learning_rate": 0.0016977777777777779,
      "loss": 2.1192,
      "step": 191
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.16351741552352905,
      "learning_rate": 0.0017066666666666669,
      "loss": 1.5972,
      "step": 192
    },
    {
      "epoch": 0.25733333333333336,
      "grad_norm": 0.16911204159259796,
      "learning_rate": 0.0017155555555555555,
      "loss": 1.4392,
      "step": 193
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 0.3353917896747589,
      "learning_rate": 0.0017244444444444445,
      "loss": 1.453,
      "step": 194
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.18812192976474762,
      "learning_rate": 0.0017333333333333335,
      "loss": 1.6527,
      "step": 195
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.13612323999404907,
      "learning_rate": 0.001742222222222222,
      "loss": 1.7609,
      "step": 196
    },
    {
      "epoch": 0.26266666666666666,
      "grad_norm": 0.18409249186515808,
      "learning_rate": 0.001751111111111111,
      "loss": 1.731,
      "step": 197
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.20915021002292633,
      "learning_rate": 0.00176,
      "loss": 1.3184,
      "step": 198
    },
    {
      "epoch": 0.2653333333333333,
      "grad_norm": 0.14540532231330872,
      "learning_rate": 0.001768888888888889,
      "loss": 1.7068,
      "step": 199
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.1447267383337021,
      "learning_rate": 0.0017777777777777776,
      "loss": 1.4373,
      "step": 200
    },
    {
      "epoch": 0.268,
      "grad_norm": 0.15977557003498077,
      "learning_rate": 0.0017866666666666667,
      "loss": 1.5344,
      "step": 201
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.23101578652858734,
      "learning_rate": 0.0017955555555555557,
      "loss": 1.8091,
      "step": 202
    },
    {
      "epoch": 0.27066666666666667,
      "grad_norm": 0.11719998717308044,
      "learning_rate": 0.0018044444444444445,
      "loss": 1.7493,
      "step": 203
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.14386355876922607,
      "learning_rate": 0.0018133333333333332,
      "loss": 1.6227,
      "step": 204
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.29026785492897034,
      "learning_rate": 0.0018222222222222223,
      "loss": 1.7728,
      "step": 205
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.12349116802215576,
      "learning_rate": 0.0018311111111111113,
      "loss": 1.5605,
      "step": 206
    },
    {
      "epoch": 0.276,
      "grad_norm": 0.2713688611984253,
      "learning_rate": 0.00184,
      "loss": 1.6615,
      "step": 207
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.14579862356185913,
      "learning_rate": 0.0018488888888888888,
      "loss": 1.4276,
      "step": 208
    },
    {
      "epoch": 0.2786666666666667,
      "grad_norm": 0.15123353898525238,
      "learning_rate": 0.0018577777777777779,
      "loss": 1.2556,
      "step": 209
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.2123190015554428,
      "learning_rate": 0.0018666666666666666,
      "loss": 1.4817,
      "step": 210
    },
    {
      "epoch": 0.2813333333333333,
      "grad_norm": 0.09837790578603745,
      "learning_rate": 0.0018755555555555557,
      "loss": 1.509,
      "step": 211
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.13006064295768738,
      "learning_rate": 0.0018844444444444444,
      "loss": 1.4932,
      "step": 212
    },
    {
      "epoch": 0.284,
      "grad_norm": 0.1544894278049469,
      "learning_rate": 0.0018933333333333335,
      "loss": 1.8282,
      "step": 213
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.1085115373134613,
      "learning_rate": 0.0019022222222222222,
      "loss": 1.364,
      "step": 214
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.2004687786102295,
      "learning_rate": 0.0019111111111111113,
      "loss": 1.8523,
      "step": 215
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.1133243516087532,
      "learning_rate": 0.00192,
      "loss": 1.1968,
      "step": 216
    },
    {
      "epoch": 0.28933333333333333,
      "grad_norm": 0.14336636662483215,
      "learning_rate": 0.0019288888888888888,
      "loss": 1.2636,
      "step": 217
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.10528384894132614,
      "learning_rate": 0.0019377777777777778,
      "loss": 1.2506,
      "step": 218
    },
    {
      "epoch": 0.292,
      "grad_norm": 0.11885785311460495,
      "learning_rate": 0.0019466666666666669,
      "loss": 1.528,
      "step": 219
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.10311006009578705,
      "learning_rate": 0.0019555555555555554,
      "loss": 1.4176,
      "step": 220
    },
    {
      "epoch": 0.2946666666666667,
      "grad_norm": 0.08208838850259781,
      "learning_rate": 0.0019644444444444444,
      "loss": 1.3646,
      "step": 221
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.0704248771071434,
      "learning_rate": 0.0019733333333333334,
      "loss": 1.3744,
      "step": 222
    },
    {
      "epoch": 0.29733333333333334,
      "grad_norm": 0.17308077216148376,
      "learning_rate": 0.0019822222222222225,
      "loss": 1.6596,
      "step": 223
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.09888805449008942,
      "learning_rate": 0.001991111111111111,
      "loss": 1.2529,
      "step": 224
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.11442390829324722,
      "learning_rate": 0.002,
      "loss": 1.59,
      "step": 225
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.08426883816719055,
      "learning_rate": 0.001999999906759682,
      "loss": 1.3271,
      "step": 226
    },
    {
      "epoch": 0.30266666666666664,
      "grad_norm": 0.4455508589744568,
      "learning_rate": 0.001999999627038744,
      "loss": 1.9235,
      "step": 227
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.23622238636016846,
      "learning_rate": 0.001999999160837239,
      "loss": 1.5459,
      "step": 228
    },
    {
      "epoch": 0.30533333333333335,
      "grad_norm": 0.15035676956176758,
      "learning_rate": 0.001999998508155254,
      "loss": 1.5617,
      "step": 229
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.1432712823152542,
      "learning_rate": 0.001999997668992911,
      "loss": 1.3752,
      "step": 230
    },
    {
      "epoch": 0.308,
      "grad_norm": 0.19954468309879303,
      "learning_rate": 0.001999996643350365,
      "loss": 1.2875,
      "step": 231
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.2520114779472351,
      "learning_rate": 0.0019999954312278087,
      "loss": 1.7282,
      "step": 232
    },
    {
      "epoch": 0.31066666666666665,
      "grad_norm": 0.15023937821388245,
      "learning_rate": 0.0019999940326254676,
      "loss": 1.3115,
      "step": 233
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.1070207878947258,
      "learning_rate": 0.001999992447543603,
      "loss": 1.1875,
      "step": 234
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.17872443795204163,
      "learning_rate": 0.0019999906759825097,
      "loss": 1.3943,
      "step": 235
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.11692877113819122,
      "learning_rate": 0.0019999887179425187,
      "loss": 1.5984,
      "step": 236
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.10689838975667953,
      "learning_rate": 0.0019999865734239945,
      "loss": 1.4206,
      "step": 237
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.09283257275819778,
      "learning_rate": 0.001999984242427338,
      "loss": 1.3056,
      "step": 238
    },
    {
      "epoch": 0.31866666666666665,
      "grad_norm": 0.20318013429641724,
      "learning_rate": 0.0019999817249529827,
      "loss": 1.4931,
      "step": 239
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.09454657882452011,
      "learning_rate": 0.001999979021001399,
      "loss": 1.3915,
      "step": 240
    },
    {
      "epoch": 0.32133333333333336,
      "grad_norm": 0.0849783793091774,
      "learning_rate": 0.00199997613057309,
      "loss": 1.5463,
      "step": 241
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.19206766784191132,
      "learning_rate": 0.0019999730536685964,
      "loss": 1.7484,
      "step": 242
    },
    {
      "epoch": 0.324,
      "grad_norm": 0.10975363105535507,
      "learning_rate": 0.0019999697902884908,
      "loss": 1.5411,
      "step": 243
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.1025867611169815,
      "learning_rate": 0.001999966340433382,
      "loss": 1.5015,
      "step": 244
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.07621404528617859,
      "learning_rate": 0.0019999627041039133,
      "loss": 1.4361,
      "step": 245
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.11332632601261139,
      "learning_rate": 0.0019999588813007633,
      "loss": 1.5896,
      "step": 246
    },
    {
      "epoch": 0.3293333333333333,
      "grad_norm": 0.08630890399217606,
      "learning_rate": 0.001999954872024644,
      "loss": 1.1898,
      "step": 247
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.10983987897634506,
      "learning_rate": 0.0019999506762763035,
      "loss": 1.4508,
      "step": 248
    },
    {
      "epoch": 0.332,
      "grad_norm": 0.06788527965545654,
      "learning_rate": 0.001999946294056524,
      "loss": 1.1999,
      "step": 249
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.5273227691650391,
      "learning_rate": 0.0019999417253661234,
      "loss": 1.8123,
      "step": 250
    },
    {
      "epoch": 0.33466666666666667,
      "grad_norm": 0.08027791976928711,
      "learning_rate": 0.0019999369702059532,
      "loss": 1.443,
      "step": 251
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.1597403585910797,
      "learning_rate": 0.0019999320285769,
      "loss": 1.5052,
      "step": 252
    },
    {
      "epoch": 0.3373333333333333,
      "grad_norm": 0.11737503856420517,
      "learning_rate": 0.001999926900479885,
      "loss": 1.5635,
      "step": 253
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.14986412227153778,
      "learning_rate": 0.0019999215859158657,
      "loss": 1.4726,
      "step": 254
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.17985229194164276,
      "learning_rate": 0.001999916084885832,
      "loss": 1.5511,
      "step": 255
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.13349486887454987,
      "learning_rate": 0.00199991039739081,
      "loss": 1.1446,
      "step": 256
    },
    {
      "epoch": 0.3426666666666667,
      "grad_norm": 0.26438799500465393,
      "learning_rate": 0.0019999045234318606,
      "loss": 1.4797,
      "step": 257
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.09764478355646133,
      "learning_rate": 0.001999898463010079,
      "loss": 1.1688,
      "step": 258
    },
    {
      "epoch": 0.3453333333333333,
      "grad_norm": 0.07612636685371399,
      "learning_rate": 0.001999892216126596,
      "loss": 1.4874,
      "step": 259
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.14608581364154816,
      "learning_rate": 0.001999885782782575,
      "loss": 1.393,
      "step": 260
    },
    {
      "epoch": 0.348,
      "grad_norm": 0.21489006280899048,
      "learning_rate": 0.001999879162979217,
      "loss": 1.3894,
      "step": 261
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 0.06065966188907623,
      "learning_rate": 0.0019998723567177562,
      "loss": 1.2482,
      "step": 262
    },
    {
      "epoch": 0.3506666666666667,
      "grad_norm": 0.0650487095117569,
      "learning_rate": 0.001999865363999461,
      "loss": 1.432,
      "step": 263
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.05423147231340408,
      "learning_rate": 0.0019998581848256368,
      "loss": 1.3507,
      "step": 264
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.07804792374372482,
      "learning_rate": 0.0019998508191976217,
      "loss": 1.3087,
      "step": 265
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.09763214737176895,
      "learning_rate": 0.001999843267116789,
      "loss": 1.2827,
      "step": 266
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.0960591584444046,
      "learning_rate": 0.0019998355285845474,
      "loss": 1.531,
      "step": 267
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.13922002911567688,
      "learning_rate": 0.0019998276036023396,
      "loss": 1.5412,
      "step": 268
    },
    {
      "epoch": 0.3586666666666667,
      "grad_norm": 0.05023603141307831,
      "learning_rate": 0.001999819492171644,
      "loss": 1.4064,
      "step": 269
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.10569024085998535,
      "learning_rate": 0.0019998111942939726,
      "loss": 1.3646,
      "step": 270
    },
    {
      "epoch": 0.36133333333333334,
      "grad_norm": 0.11148208379745483,
      "learning_rate": 0.0019998027099708735,
      "loss": 1.293,
      "step": 271
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.10627737641334534,
      "learning_rate": 0.001999794039203928,
      "loss": 1.4043,
      "step": 272
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.08687829971313477,
      "learning_rate": 0.0019997851819947535,
      "loss": 1.4107,
      "step": 273
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.07988125085830688,
      "learning_rate": 0.001999776138345002,
      "loss": 1.259,
      "step": 274
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.1479395031929016,
      "learning_rate": 0.0019997669082563595,
      "loss": 1.595,
      "step": 275
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.05802853778004646,
      "learning_rate": 0.001999757491730548,
      "loss": 1.4499,
      "step": 276
    },
    {
      "epoch": 0.36933333333333335,
      "grad_norm": 0.06045597046613693,
      "learning_rate": 0.001999747888769322,
      "loss": 1.5376,
      "step": 277
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.07695658504962921,
      "learning_rate": 0.001999738099374474,
      "loss": 1.3521,
      "step": 278
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.08334402740001678,
      "learning_rate": 0.001999728123547828,
      "loss": 1.3981,
      "step": 279
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.06757953017950058,
      "learning_rate": 0.001999717961291245,
      "loss": 1.3287,
      "step": 280
    },
    {
      "epoch": 0.37466666666666665,
      "grad_norm": 0.04772596433758736,
      "learning_rate": 0.0019997076126066207,
      "loss": 1.3087,
      "step": 281
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.05686959996819496,
      "learning_rate": 0.0019996970774958838,
      "loss": 1.2932,
      "step": 282
    },
    {
      "epoch": 0.37733333333333335,
      "grad_norm": 0.05310272052884102,
      "learning_rate": 0.001999686355960999,
      "loss": 1.5018,
      "step": 283
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.05681496486067772,
      "learning_rate": 0.0019996754480039666,
      "loss": 1.234,
      "step": 284
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.05258401110768318,
      "learning_rate": 0.00199966435362682,
      "loss": 1.3291,
      "step": 285
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.07269256561994553,
      "learning_rate": 0.001999653072831629,
      "loss": 1.2992,
      "step": 286
    },
    {
      "epoch": 0.38266666666666665,
      "grad_norm": 0.04860762506723404,
      "learning_rate": 0.0019996416056204955,
      "loss": 1.4232,
      "step": 287
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.044220492243766785,
      "learning_rate": 0.001999629951995559,
      "loss": 1.4881,
      "step": 288
    },
    {
      "epoch": 0.38533333333333336,
      "grad_norm": 0.14887695014476776,
      "learning_rate": 0.0019996181119589927,
      "loss": 1.0483,
      "step": 289
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.09845087677240372,
      "learning_rate": 0.001999606085513005,
      "loss": 1.713,
      "step": 290
    },
    {
      "epoch": 0.388,
      "grad_norm": 0.07102132588624954,
      "learning_rate": 0.0019995938726598372,
      "loss": 1.4657,
      "step": 291
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 0.06169552728533745,
      "learning_rate": 0.001999581473401768,
      "loss": 1.3369,
      "step": 292
    },
    {
      "epoch": 0.39066666666666666,
      "grad_norm": 0.052081745117902756,
      "learning_rate": 0.001999568887741109,
      "loss": 1.3564,
      "step": 293
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.05409561097621918,
      "learning_rate": 0.0019995561156802076,
      "loss": 1.3551,
      "step": 294
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.05737118050456047,
      "learning_rate": 0.0019995431572214454,
      "loss": 1.4825,
      "step": 295
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.12405510246753693,
      "learning_rate": 0.001999530012367239,
      "loss": 1.2869,
      "step": 296
    },
    {
      "epoch": 0.396,
      "grad_norm": 0.04846273735165596,
      "learning_rate": 0.001999516681120039,
      "loss": 1.5018,
      "step": 297
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.07069501280784607,
      "learning_rate": 0.001999503163482332,
      "loss": 1.5463,
      "step": 298
    },
    {
      "epoch": 0.39866666666666667,
      "grad_norm": 0.07002240419387817,
      "learning_rate": 0.001999489459456639,
      "loss": 1.1883,
      "step": 299
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.036256637424230576,
      "learning_rate": 0.0019994755690455153,
      "loss": 1.2057,
      "step": 300
    },
    {
      "epoch": 0.4013333333333333,
      "grad_norm": 0.07686804234981537,
      "learning_rate": 0.0019994614922515504,
      "loss": 1.2535,
      "step": 301
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.06085726246237755,
      "learning_rate": 0.0019994472290773705,
      "loss": 1.3783,
      "step": 302
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.04615631699562073,
      "learning_rate": 0.0019994327795256352,
      "loss": 1.2315,
      "step": 303
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.07021823525428772,
      "learning_rate": 0.0019994181435990382,
      "loss": 1.5601,
      "step": 304
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.06546615064144135,
      "learning_rate": 0.0019994033213003104,
      "loss": 1.2778,
      "step": 305
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.053704507648944855,
      "learning_rate": 0.001999388312632214,
      "loss": 1.3663,
      "step": 306
    },
    {
      "epoch": 0.4093333333333333,
      "grad_norm": 0.045590389519929886,
      "learning_rate": 0.0019993731175975494,
      "loss": 1.1747,
      "step": 307
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.08280221372842789,
      "learning_rate": 0.001999357736199149,
      "loss": 1.2012,
      "step": 308
    },
    {
      "epoch": 0.412,
      "grad_norm": 0.03712066262960434,
      "learning_rate": 0.001999342168439882,
      "loss": 1.4598,
      "step": 309
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.09471578896045685,
      "learning_rate": 0.0019993264143226513,
      "loss": 1.3591,
      "step": 310
    },
    {
      "epoch": 0.4146666666666667,
      "grad_norm": 0.2632363736629486,
      "learning_rate": 0.0019993104738503945,
      "loss": 1.2627,
      "step": 311
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.06657509505748749,
      "learning_rate": 0.001999294347026084,
      "loss": 1.3266,
      "step": 312
    },
    {
      "epoch": 0.41733333333333333,
      "grad_norm": 0.06597900390625,
      "learning_rate": 0.0019992780338527276,
      "loss": 1.4617,
      "step": 313
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.0662798061966896,
      "learning_rate": 0.0019992615343333675,
      "loss": 1.1541,
      "step": 314
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.09077456593513489,
      "learning_rate": 0.0019992448484710797,
      "loss": 1.1156,
      "step": 315
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.049744147807359695,
      "learning_rate": 0.001999227976268977,
      "loss": 1.2986,
      "step": 316
    },
    {
      "epoch": 0.4226666666666667,
      "grad_norm": 0.07648691534996033,
      "learning_rate": 0.0019992109177302043,
      "loss": 1.3802,
      "step": 317
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.30873656272888184,
      "learning_rate": 0.0019991936728579436,
      "loss": 1.7303,
      "step": 318
    },
    {
      "epoch": 0.42533333333333334,
      "grad_norm": 0.10793206095695496,
      "learning_rate": 0.001999176241655411,
      "loss": 1.2319,
      "step": 319
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.04792880639433861,
      "learning_rate": 0.0019991586241258565,
      "loss": 1.2385,
      "step": 320
    },
    {
      "epoch": 0.428,
      "grad_norm": 0.10951211303472519,
      "learning_rate": 0.0019991408202725655,
      "loss": 1.4573,
      "step": 321
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.0815192237496376,
      "learning_rate": 0.0019991228300988585,
      "loss": 1.1636,
      "step": 322
    },
    {
      "epoch": 0.43066666666666664,
      "grad_norm": 0.09811433404684067,
      "learning_rate": 0.0019991046536080898,
      "loss": 1.4158,
      "step": 323
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.07855795323848724,
      "learning_rate": 0.0019990862908036487,
      "loss": 1.4849,
      "step": 324
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.06553905457258224,
      "learning_rate": 0.0019990677416889605,
      "loss": 1.2736,
      "step": 325
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.05087639391422272,
      "learning_rate": 0.001999049006267484,
      "loss": 1.406,
      "step": 326
    },
    {
      "epoch": 0.436,
      "grad_norm": 0.06218847632408142,
      "learning_rate": 0.0019990300845427124,
      "loss": 1.5146,
      "step": 327
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.06691920012235641,
      "learning_rate": 0.0019990109765181743,
      "loss": 1.4756,
      "step": 328
    },
    {
      "epoch": 0.43866666666666665,
      "grad_norm": 0.05301818996667862,
      "learning_rate": 0.0019989916821974334,
      "loss": 1.5027,
      "step": 329
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.05837976187467575,
      "learning_rate": 0.001998972201584088,
      "loss": 1.2203,
      "step": 330
    },
    {
      "epoch": 0.44133333333333336,
      "grad_norm": 0.05512513220310211,
      "learning_rate": 0.00199895253468177,
      "loss": 1.294,
      "step": 331
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.07397747039794922,
      "learning_rate": 0.0019989326814941473,
      "loss": 1.3471,
      "step": 332
    },
    {
      "epoch": 0.444,
      "grad_norm": 0.06325335800647736,
      "learning_rate": 0.0019989126420249218,
      "loss": 1.3156,
      "step": 333
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.05998796969652176,
      "learning_rate": 0.001998892416277831,
      "loss": 1.1595,
      "step": 334
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.10149934887886047,
      "learning_rate": 0.0019988720042566467,
      "loss": 1.3689,
      "step": 335
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.05460279434919357,
      "learning_rate": 0.001998851405965175,
      "loss": 1.6546,
      "step": 336
    },
    {
      "epoch": 0.4493333333333333,
      "grad_norm": 0.054483331739902496,
      "learning_rate": 0.0019988306214072573,
      "loss": 1.2249,
      "step": 337
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.08378994464874268,
      "learning_rate": 0.001998809650586769,
      "loss": 1.8498,
      "step": 338
    },
    {
      "epoch": 0.452,
      "grad_norm": 0.09979964047670364,
      "learning_rate": 0.001998788493507621,
      "loss": 1.2252,
      "step": 339
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.04588304087519646,
      "learning_rate": 0.001998767150173759,
      "loss": 1.5296,
      "step": 340
    },
    {
      "epoch": 0.45466666666666666,
      "grad_norm": 0.0771678239107132,
      "learning_rate": 0.001998745620589163,
      "loss": 1.3201,
      "step": 341
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.057599280029535294,
      "learning_rate": 0.001998723904757848,
      "loss": 1.2344,
      "step": 342
    },
    {
      "epoch": 0.4573333333333333,
      "grad_norm": 0.04583011195063591,
      "learning_rate": 0.0019987020026838633,
      "loss": 1.3051,
      "step": 343
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.03659690544009209,
      "learning_rate": 0.001998679914371293,
      "loss": 1.1183,
      "step": 344
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.0475444570183754,
      "learning_rate": 0.0019986576398242565,
      "loss": 1.1643,
      "step": 345
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.09210855513811111,
      "learning_rate": 0.0019986351790469074,
      "loss": 1.1875,
      "step": 346
    },
    {
      "epoch": 0.46266666666666667,
      "grad_norm": 0.050942420959472656,
      "learning_rate": 0.0019986125320434344,
      "loss": 1.2574,
      "step": 347
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.08606415241956711,
      "learning_rate": 0.0019985896988180605,
      "loss": 1.652,
      "step": 348
    },
    {
      "epoch": 0.4653333333333333,
      "grad_norm": 0.08358950912952423,
      "learning_rate": 0.001998566679375044,
      "loss": 1.562,
      "step": 349
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.05088535696268082,
      "learning_rate": 0.001998543473718677,
      "loss": 1.456,
      "step": 350
    },
    {
      "epoch": 0.468,
      "grad_norm": 0.0559992715716362,
      "learning_rate": 0.0019985200818532873,
      "loss": 1.5776,
      "step": 351
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.05187581852078438,
      "learning_rate": 0.001998496503783237,
      "loss": 1.3837,
      "step": 352
    },
    {
      "epoch": 0.4706666666666667,
      "grad_norm": 0.03263884037733078,
      "learning_rate": 0.0019984727395129234,
      "loss": 1.2556,
      "step": 353
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.04913047328591347,
      "learning_rate": 0.001998448789046777,
      "loss": 1.3417,
      "step": 354
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.05219447240233421,
      "learning_rate": 0.001998424652389265,
      "loss": 1.4745,
      "step": 355
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.057174138724803925,
      "learning_rate": 0.001998400329544888,
      "loss": 1.2246,
      "step": 356
    },
    {
      "epoch": 0.476,
      "grad_norm": 0.041574083268642426,
      "learning_rate": 0.001998375820518182,
      "loss": 1.3928,
      "step": 357
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.047312237322330475,
      "learning_rate": 0.001998351125313717,
      "loss": 1.065,
      "step": 358
    },
    {
      "epoch": 0.4786666666666667,
      "grad_norm": 0.03565726429224014,
      "learning_rate": 0.001998326243936099,
      "loss": 1.1635,
      "step": 359
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.09561602771282196,
      "learning_rate": 0.0019983011763899674,
      "loss": 1.3387,
      "step": 360
    },
    {
      "epoch": 0.48133333333333334,
      "grad_norm": 0.04501534253358841,
      "learning_rate": 0.0019982759226799965,
      "loss": 1.2516,
      "step": 361
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.05453461408615112,
      "learning_rate": 0.001998250482810896,
      "loss": 1.5275,
      "step": 362
    },
    {
      "epoch": 0.484,
      "grad_norm": 0.038464803248643875,
      "learning_rate": 0.0019982248567874095,
      "loss": 1.2146,
      "step": 363
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.04511315003037453,
      "learning_rate": 0.001998199044614317,
      "loss": 1.4316,
      "step": 364
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.0429406613111496,
      "learning_rate": 0.0019981730462964305,
      "loss": 1.4252,
      "step": 365
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.03547189384698868,
      "learning_rate": 0.001998146861838599,
      "loss": 1.4203,
      "step": 366
    },
    {
      "epoch": 0.48933333333333334,
      "grad_norm": 0.07850974053144455,
      "learning_rate": 0.0019981204912457046,
      "loss": 1.2955,
      "step": 367
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.04389988258481026,
      "learning_rate": 0.001998093934522666,
      "loss": 1.2423,
      "step": 368
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.046507686376571655,
      "learning_rate": 0.001998067191674435,
      "loss": 1.0064,
      "step": 369
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.051280561834573746,
      "learning_rate": 0.001998040262705999,
      "loss": 1.4747,
      "step": 370
    },
    {
      "epoch": 0.49466666666666664,
      "grad_norm": 0.04559962823987007,
      "learning_rate": 0.001998013147622379,
      "loss": 1.4199,
      "step": 371
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.04297352582216263,
      "learning_rate": 0.0019979858464286315,
      "loss": 1.1705,
      "step": 372
    },
    {
      "epoch": 0.49733333333333335,
      "grad_norm": 0.04963921383023262,
      "learning_rate": 0.0019979583591298485,
      "loss": 1.1705,
      "step": 373
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.18203385174274445,
      "learning_rate": 0.0019979306857311548,
      "loss": 1.5593,
      "step": 374
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.05946153402328491,
      "learning_rate": 0.0019979028262377117,
      "loss": 1.4933,
      "step": 375
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.04380752146244049,
      "learning_rate": 0.0019978747806547142,
      "loss": 1.206,
      "step": 376
    },
    {
      "epoch": 0.5026666666666667,
      "grad_norm": 0.04074501991271973,
      "learning_rate": 0.001997846548987392,
      "loss": 1.2303,
      "step": 377
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.04231489077210426,
      "learning_rate": 0.0019978181312410104,
      "loss": 1.2513,
      "step": 378
    },
    {
      "epoch": 0.5053333333333333,
      "grad_norm": 0.05422556772828102,
      "learning_rate": 0.001997789527420868,
      "loss": 1.3631,
      "step": 379
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.0528741180896759,
      "learning_rate": 0.0019977607375323,
      "loss": 1.2854,
      "step": 380
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.044870343059301376,
      "learning_rate": 0.0019977317615806735,
      "loss": 1.3378,
      "step": 381
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.044257208704948425,
      "learning_rate": 0.001997702599571393,
      "loss": 1.1967,
      "step": 382
    },
    {
      "epoch": 0.5106666666666667,
      "grad_norm": 0.04860832914710045,
      "learning_rate": 0.001997673251509897,
      "loss": 1.3478,
      "step": 383
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.045245107263326645,
      "learning_rate": 0.001997643717401657,
      "loss": 1.285,
      "step": 384
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.04363589733839035,
      "learning_rate": 0.001997613997252182,
      "loss": 1.0023,
      "step": 385
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.05787007510662079,
      "learning_rate": 0.001997584091067014,
      "loss": 1.4132,
      "step": 386
    },
    {
      "epoch": 0.516,
      "grad_norm": 0.03477425128221512,
      "learning_rate": 0.0019975539988517288,
      "loss": 1.1194,
      "step": 387
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.05705839768052101,
      "learning_rate": 0.001997523720611939,
      "loss": 1.2487,
      "step": 388
    },
    {
      "epoch": 0.5186666666666667,
      "grad_norm": 0.05525148659944534,
      "learning_rate": 0.0019974932563532905,
      "loss": 0.8382,
      "step": 389
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.05530788376927376,
      "learning_rate": 0.0019974626060814647,
      "loss": 1.3131,
      "step": 390
    },
    {
      "epoch": 0.5213333333333333,
      "grad_norm": 0.05661793425679207,
      "learning_rate": 0.001997431769802177,
      "loss": 1.0936,
      "step": 391
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.05634351447224617,
      "learning_rate": 0.0019974007475211776,
      "loss": 1.3485,
      "step": 392
    },
    {
      "epoch": 0.524,
      "grad_norm": 0.04908985644578934,
      "learning_rate": 0.001997369539244252,
      "loss": 1.1945,
      "step": 393
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.14249204099178314,
      "learning_rate": 0.0019973381449772194,
      "loss": 1.6922,
      "step": 394
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.04738152399659157,
      "learning_rate": 0.0019973065647259348,
      "loss": 0.9895,
      "step": 395
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.0476241409778595,
      "learning_rate": 0.0019972747984962867,
      "loss": 1.2069,
      "step": 396
    },
    {
      "epoch": 0.5293333333333333,
      "grad_norm": 0.15888147056102753,
      "learning_rate": 0.0019972428462941994,
      "loss": 1.3894,
      "step": 397
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.046857211738824844,
      "learning_rate": 0.0019972107081256316,
      "loss": 1.4062,
      "step": 398
    },
    {
      "epoch": 0.532,
      "grad_norm": 0.034172605723142624,
      "learning_rate": 0.0019971783839965755,
      "loss": 1.3365,
      "step": 399
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.041829273104667664,
      "learning_rate": 0.0019971458739130596,
      "loss": 0.9306,
      "step": 400
    },
    {
      "epoch": 0.5346666666666666,
      "grad_norm": 0.14157475531101227,
      "learning_rate": 0.0019971131778811465,
      "loss": 1.2573,
      "step": 401
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.05568951368331909,
      "learning_rate": 0.0019970802959069327,
      "loss": 1.3749,
      "step": 402
    },
    {
      "epoch": 0.5373333333333333,
      "grad_norm": 0.05192892253398895,
      "learning_rate": 0.0019970472279965505,
      "loss": 1.3734,
      "step": 403
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.05479362979531288,
      "learning_rate": 0.001997013974156167,
      "loss": 1.3693,
      "step": 404
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.04695793241262436,
      "learning_rate": 0.001996980534391982,
      "loss": 1.4303,
      "step": 405
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 0.049125052988529205,
      "learning_rate": 0.0019969469087102324,
      "loss": 1.3034,
      "step": 406
    },
    {
      "epoch": 0.5426666666666666,
      "grad_norm": 0.04361307993531227,
      "learning_rate": 0.0019969130971171888,
      "loss": 1.1984,
      "step": 407
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.050563666969537735,
      "learning_rate": 0.001996879099619156,
      "loss": 1.2095,
      "step": 408
    },
    {
      "epoch": 0.5453333333333333,
      "grad_norm": 0.05336029827594757,
      "learning_rate": 0.001996844916222474,
      "loss": 1.4589,
      "step": 409
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.04923446476459503,
      "learning_rate": 0.001996810546933517,
      "loss": 1.0218,
      "step": 410
    },
    {
      "epoch": 0.548,
      "grad_norm": 0.4374910891056061,
      "learning_rate": 0.0019967759917586952,
      "loss": 1.3898,
      "step": 411
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.0457625612616539,
      "learning_rate": 0.001996741250704451,
      "loss": 1.2638,
      "step": 412
    },
    {
      "epoch": 0.5506666666666666,
      "grad_norm": 0.07485470920801163,
      "learning_rate": 0.001996706323777264,
      "loss": 1.1842,
      "step": 413
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.16584523022174835,
      "learning_rate": 0.0019966712109836474,
      "loss": 1.2966,
      "step": 414
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.24083121120929718,
      "learning_rate": 0.001996635912330149,
      "loss": 1.1396,
      "step": 415
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.04897521436214447,
      "learning_rate": 0.001996600427823351,
      "loss": 1.1033,
      "step": 416
    },
    {
      "epoch": 0.556,
      "grad_norm": 0.04386454448103905,
      "learning_rate": 0.0019965647574698704,
      "loss": 1.3412,
      "step": 417
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.17673587799072266,
      "learning_rate": 0.0019965289012763596,
      "loss": 1.2012,
      "step": 418
    },
    {
      "epoch": 0.5586666666666666,
      "grad_norm": 0.09123574942350388,
      "learning_rate": 0.0019964928592495043,
      "loss": 1.3724,
      "step": 419
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.30419105291366577,
      "learning_rate": 0.0019964566313960264,
      "loss": 1.2557,
      "step": 420
    },
    {
      "epoch": 0.5613333333333334,
      "grad_norm": 0.06432117521762848,
      "learning_rate": 0.001996420217722682,
      "loss": 1.3414,
      "step": 421
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 0.11227492988109589,
      "learning_rate": 0.0019963836182362604,
      "loss": 1.2183,
      "step": 422
    },
    {
      "epoch": 0.564,
      "grad_norm": 0.2097117155790329,
      "learning_rate": 0.001996346832943587,
      "loss": 1.5035,
      "step": 423
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.06525377184152603,
      "learning_rate": 0.0019963098618515224,
      "loss": 1.2455,
      "step": 424
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.11905170977115631,
      "learning_rate": 0.00199627270496696,
      "loss": 1.4152,
      "step": 425
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.08599259704351425,
      "learning_rate": 0.0019962353622968295,
      "loss": 1.4766,
      "step": 426
    },
    {
      "epoch": 0.5693333333333334,
      "grad_norm": 0.05159797891974449,
      "learning_rate": 0.001996197833848094,
      "loss": 1.4331,
      "step": 427
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.05167139321565628,
      "learning_rate": 0.0019961601196277524,
      "loss": 1.0232,
      "step": 428
    },
    {
      "epoch": 0.572,
      "grad_norm": 0.06547737866640091,
      "learning_rate": 0.0019961222196428377,
      "loss": 1.3607,
      "step": 429
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.04885708540678024,
      "learning_rate": 0.001996084133900417,
      "loss": 1.3514,
      "step": 430
    },
    {
      "epoch": 0.5746666666666667,
      "grad_norm": 0.06885898113250732,
      "learning_rate": 0.001996045862407593,
      "loss": 1.2983,
      "step": 431
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.11762596666812897,
      "learning_rate": 0.001996007405171502,
      "loss": 1.4115,
      "step": 432
    },
    {
      "epoch": 0.5773333333333334,
      "grad_norm": 0.045557085424661636,
      "learning_rate": 0.001995968762199316,
      "loss": 1.2843,
      "step": 433
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.0447065569460392,
      "learning_rate": 0.0019959299334982414,
      "loss": 1.126,
      "step": 434
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.06316620856523514,
      "learning_rate": 0.0019958909190755185,
      "loss": 1.2184,
      "step": 435
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.17869415879249573,
      "learning_rate": 0.001995851718938423,
      "loss": 1.335,
      "step": 436
    },
    {
      "epoch": 0.5826666666666667,
      "grad_norm": 0.041679441928863525,
      "learning_rate": 0.001995812333094265,
      "loss": 1.2453,
      "step": 437
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.06389506906270981,
      "learning_rate": 0.0019957727615503885,
      "loss": 1.4884,
      "step": 438
    },
    {
      "epoch": 0.5853333333333334,
      "grad_norm": 0.05041440948843956,
      "learning_rate": 0.001995733004314174,
      "loss": 1.2995,
      "step": 439
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.039798665791749954,
      "learning_rate": 0.001995693061393035,
      "loss": 1.2662,
      "step": 440
    },
    {
      "epoch": 0.588,
      "grad_norm": 0.05408851057291031,
      "learning_rate": 0.00199565293279442,
      "loss": 1.3753,
      "step": 441
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.039038997143507004,
      "learning_rate": 0.0019956126185258116,
      "loss": 1.2177,
      "step": 442
    },
    {
      "epoch": 0.5906666666666667,
      "grad_norm": 0.04923113062977791,
      "learning_rate": 0.0019955721185947284,
      "loss": 1.2576,
      "step": 443
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.05296563357114792,
      "learning_rate": 0.0019955314330087222,
      "loss": 1.2748,
      "step": 444
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.04187644645571709,
      "learning_rate": 0.0019954905617753814,
      "loss": 1.2104,
      "step": 445
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.12507914006710052,
      "learning_rate": 0.0019954495049023264,
      "loss": 1.2816,
      "step": 446
    },
    {
      "epoch": 0.596,
      "grad_norm": 0.03879881650209427,
      "learning_rate": 0.001995408262397214,
      "loss": 1.3522,
      "step": 447
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.03624623641371727,
      "learning_rate": 0.001995366834267735,
      "loss": 1.1835,
      "step": 448
    },
    {
      "epoch": 0.5986666666666667,
      "grad_norm": 0.08685246109962463,
      "learning_rate": 0.001995325220521615,
      "loss": 1.1703,
      "step": 449
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.05578622221946716,
      "learning_rate": 0.0019952834211666138,
      "loss": 1.3275,
      "step": 450
    },
    {
      "epoch": 0.6013333333333334,
      "grad_norm": 0.042471833527088165,
      "learning_rate": 0.001995241436210527,
      "loss": 1.2373,
      "step": 451
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 0.05162429064512253,
      "learning_rate": 0.0019951992656611836,
      "loss": 1.178,
      "step": 452
    },
    {
      "epoch": 0.604,
      "grad_norm": 0.05223008617758751,
      "learning_rate": 0.0019951569095264473,
      "loss": 1.15,
      "step": 453
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 0.03955993056297302,
      "learning_rate": 0.0019951143678142167,
      "loss": 1.3227,
      "step": 454
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.05032728984951973,
      "learning_rate": 0.0019950716405324254,
      "loss": 1.3173,
      "step": 455
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.0402766689658165,
      "learning_rate": 0.0019950287276890412,
      "loss": 1.067,
      "step": 456
    },
    {
      "epoch": 0.6093333333333333,
      "grad_norm": 0.053399451076984406,
      "learning_rate": 0.001994985629292066,
      "loss": 1.3012,
      "step": 457
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.06857909262180328,
      "learning_rate": 0.001994942345349537,
      "loss": 1.129,
      "step": 458
    },
    {
      "epoch": 0.612,
      "grad_norm": 0.03352987393736839,
      "learning_rate": 0.0019948988758695264,
      "loss": 1.2819,
      "step": 459
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.07027451694011688,
      "learning_rate": 0.00199485522086014,
      "loss": 1.6333,
      "step": 460
    },
    {
      "epoch": 0.6146666666666667,
      "grad_norm": 0.05211083963513374,
      "learning_rate": 0.001994811380329518,
      "loss": 1.5878,
      "step": 461
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.040701575577259064,
      "learning_rate": 0.0019947673542858365,
      "loss": 1.1966,
      "step": 462
    },
    {
      "epoch": 0.6173333333333333,
      "grad_norm": 0.039525751024484634,
      "learning_rate": 0.0019947231427373062,
      "loss": 1.5503,
      "step": 463
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.04832125082612038,
      "learning_rate": 0.00199467874569217,
      "loss": 1.2978,
      "step": 464
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.04553849995136261,
      "learning_rate": 0.0019946341631587087,
      "loss": 1.2427,
      "step": 465
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.32125380635261536,
      "learning_rate": 0.001994589395145235,
      "loss": 1.4024,
      "step": 466
    },
    {
      "epoch": 0.6226666666666667,
      "grad_norm": 0.040016330778598785,
      "learning_rate": 0.001994544441660098,
      "loss": 1.2246,
      "step": 467
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.10011456906795502,
      "learning_rate": 0.0019944993027116798,
      "loss": 1.4486,
      "step": 468
    },
    {
      "epoch": 0.6253333333333333,
      "grad_norm": 0.03368661180138588,
      "learning_rate": 0.0019944539783083985,
      "loss": 1.279,
      "step": 469
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.04721704125404358,
      "learning_rate": 0.001994408468458706,
      "loss": 1.0057,
      "step": 470
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.14364473521709442,
      "learning_rate": 0.0019943627731710896,
      "loss": 1.7496,
      "step": 471
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.03712965548038483,
      "learning_rate": 0.00199431689245407,
      "loss": 0.95,
      "step": 472
    },
    {
      "epoch": 0.6306666666666667,
      "grad_norm": 0.06311935931444168,
      "learning_rate": 0.001994270826316203,
      "loss": 1.3062,
      "step": 473
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.15321768820285797,
      "learning_rate": 0.0019942245747660795,
      "loss": 1.422,
      "step": 474
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.0938134640455246,
      "learning_rate": 0.001994178137812324,
      "loss": 1.3303,
      "step": 475
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.04027548059821129,
      "learning_rate": 0.0019941315154635973,
      "loss": 1.4462,
      "step": 476
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.043631475418806076,
      "learning_rate": 0.0019940847077285916,
      "loss": 1.2526,
      "step": 477
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.07198604196310043,
      "learning_rate": 0.001994037714616037,
      "loss": 1.0239,
      "step": 478
    },
    {
      "epoch": 0.6386666666666667,
      "grad_norm": 0.07211945205926895,
      "learning_rate": 0.0019939905361346963,
      "loss": 1.4472,
      "step": 479
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.04374998062849045,
      "learning_rate": 0.0019939431722933677,
      "loss": 1.3215,
      "step": 480
    },
    {
      "epoch": 0.6413333333333333,
      "grad_norm": 0.04757560417056084,
      "learning_rate": 0.0019938956231008837,
      "loss": 1.3205,
      "step": 481
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.037524498999118805,
      "learning_rate": 0.001993847888566111,
      "loss": 1.3772,
      "step": 482
    },
    {
      "epoch": 0.644,
      "grad_norm": 0.03711952269077301,
      "learning_rate": 0.001993799968697951,
      "loss": 1.3041,
      "step": 483
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 0.05570259690284729,
      "learning_rate": 0.0019937518635053404,
      "loss": 1.5562,
      "step": 484
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.04483720660209656,
      "learning_rate": 0.0019937035729972494,
      "loss": 1.0846,
      "step": 485
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.05575212463736534,
      "learning_rate": 0.0019936550971826833,
      "loss": 1.6606,
      "step": 486
    },
    {
      "epoch": 0.6493333333333333,
      "grad_norm": 0.07995740324258804,
      "learning_rate": 0.001993606436070682,
      "loss": 1.205,
      "step": 487
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.05766502395272255,
      "learning_rate": 0.0019935575896703204,
      "loss": 1.5778,
      "step": 488
    },
    {
      "epoch": 0.652,
      "grad_norm": 0.16146396100521088,
      "learning_rate": 0.0019935085579907063,
      "loss": 1.3767,
      "step": 489
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.05064406991004944,
      "learning_rate": 0.001993459341040984,
      "loss": 1.3276,
      "step": 490
    },
    {
      "epoch": 0.6546666666666666,
      "grad_norm": 0.05369731783866882,
      "learning_rate": 0.001993409938830331,
      "loss": 1.4342,
      "step": 491
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.03992399945855141,
      "learning_rate": 0.0019933603513679603,
      "loss": 1.2062,
      "step": 492
    },
    {
      "epoch": 0.6573333333333333,
      "grad_norm": 0.032426804304122925,
      "learning_rate": 0.001993310578663119,
      "loss": 1.4297,
      "step": 493
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.04238004609942436,
      "learning_rate": 0.0019932606207250883,
      "loss": 1.0547,
      "step": 494
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.04586634784936905,
      "learning_rate": 0.0019932104775631843,
      "loss": 1.0463,
      "step": 495
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.047575145959854126,
      "learning_rate": 0.0019931601491867588,
      "loss": 1.4064,
      "step": 496
    },
    {
      "epoch": 0.6626666666666666,
      "grad_norm": 0.05401758849620819,
      "learning_rate": 0.001993109635605196,
      "loss": 1.2379,
      "step": 497
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.07208560407161713,
      "learning_rate": 0.001993058936827916,
      "loss": 1.2841,
      "step": 498
    },
    {
      "epoch": 0.6653333333333333,
      "grad_norm": 0.03680965304374695,
      "learning_rate": 0.001993008052864373,
      "loss": 1.195,
      "step": 499
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.03922390192747116,
      "learning_rate": 0.0019929569837240564,
      "loss": 1.4405,
      "step": 500
    },
    {
      "epoch": 0.668,
      "grad_norm": 0.11053036898374557,
      "learning_rate": 0.001992905729416489,
      "loss": 1.2263,
      "step": 501
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.32040610909461975,
      "learning_rate": 0.0019928542899512293,
      "loss": 1.4134,
      "step": 502
    },
    {
      "epoch": 0.6706666666666666,
      "grad_norm": 0.04465348646044731,
      "learning_rate": 0.001992802665337869,
      "loss": 1.2449,
      "step": 503
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.05368734896183014,
      "learning_rate": 0.001992750855586036,
      "loss": 1.3804,
      "step": 504
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.13195988535881042,
      "learning_rate": 0.0019926988607053913,
      "loss": 1.3817,
      "step": 505
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.10302291810512543,
      "learning_rate": 0.0019926466807056306,
      "loss": 1.221,
      "step": 506
    },
    {
      "epoch": 0.676,
      "grad_norm": 0.05810856074094772,
      "learning_rate": 0.0019925943155964855,
      "loss": 1.1166,
      "step": 507
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 0.035961881279945374,
      "learning_rate": 0.0019925417653877202,
      "loss": 1.2398,
      "step": 508
    },
    {
      "epoch": 0.6786666666666666,
      "grad_norm": 0.15068960189819336,
      "learning_rate": 0.0019924890300891344,
      "loss": 1.2033,
      "step": 509
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.15369467437267303,
      "learning_rate": 0.0019924361097105625,
      "loss": 1.5533,
      "step": 510
    },
    {
      "epoch": 0.6813333333333333,
      "grad_norm": 0.03356549143791199,
      "learning_rate": 0.001992383004261873,
      "loss": 1.3505,
      "step": 511
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.060285672545433044,
      "learning_rate": 0.0019923297137529688,
      "loss": 1.2064,
      "step": 512
    },
    {
      "epoch": 0.684,
      "grad_norm": 0.03869315981864929,
      "learning_rate": 0.001992276238193788,
      "loss": 1.1933,
      "step": 513
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 0.0386049710214138,
      "learning_rate": 0.0019922225775943023,
      "loss": 1.0469,
      "step": 514
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.10605252534151077,
      "learning_rate": 0.0019921687319645184,
      "loss": 1.1665,
      "step": 515
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.05392318591475487,
      "learning_rate": 0.001992114701314478,
      "loss": 0.9972,
      "step": 516
    },
    {
      "epoch": 0.6893333333333334,
      "grad_norm": 0.049436092376708984,
      "learning_rate": 0.0019920604856542563,
      "loss": 1.3673,
      "step": 517
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 0.10797630995512009,
      "learning_rate": 0.0019920060849939634,
      "loss": 1.3112,
      "step": 518
    },
    {
      "epoch": 0.692,
      "grad_norm": 0.2239043116569519,
      "learning_rate": 0.0019919514993437444,
      "loss": 1.6899,
      "step": 519
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.09764652699232101,
      "learning_rate": 0.0019918967287137785,
      "loss": 1.4775,
      "step": 520
    },
    {
      "epoch": 0.6946666666666667,
      "grad_norm": 0.07788337022066116,
      "learning_rate": 0.0019918417731142786,
      "loss": 1.3581,
      "step": 521
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.05877821892499924,
      "learning_rate": 0.001991786632555494,
      "loss": 1.2085,
      "step": 522
    },
    {
      "epoch": 0.6973333333333334,
      "grad_norm": 0.03842935338616371,
      "learning_rate": 0.0019917313070477055,
      "loss": 1.1956,
      "step": 523
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.08199071139097214,
      "learning_rate": 0.0019916757966012325,
      "loss": 1.178,
      "step": 524
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.04760304465889931,
      "learning_rate": 0.001991620101226425,
      "loss": 1.2226,
      "step": 525
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.039904844015836716,
      "learning_rate": 0.00199156422093367,
      "loss": 1.1394,
      "step": 526
    },
    {
      "epoch": 0.7026666666666667,
      "grad_norm": 0.0473652258515358,
      "learning_rate": 0.0019915081557333875,
      "loss": 1.2372,
      "step": 527
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.04281729459762573,
      "learning_rate": 0.001991451905636033,
      "loss": 1.191,
      "step": 528
    },
    {
      "epoch": 0.7053333333333334,
      "grad_norm": 0.05050241947174072,
      "learning_rate": 0.001991395470652096,
      "loss": 1.1128,
      "step": 529
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.2780245244503021,
      "learning_rate": 0.0019913388507921,
      "loss": 1.4138,
      "step": 530
    },
    {
      "epoch": 0.708,
      "grad_norm": 0.06240695342421532,
      "learning_rate": 0.0019912820460666046,
      "loss": 1.379,
      "step": 531
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 0.07397565245628357,
      "learning_rate": 0.0019912250564862017,
      "loss": 1.3243,
      "step": 532
    },
    {
      "epoch": 0.7106666666666667,
      "grad_norm": 0.06275905668735504,
      "learning_rate": 0.0019911678820615187,
      "loss": 1.2253,
      "step": 533
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.07825533300638199,
      "learning_rate": 0.0019911105228032186,
      "loss": 1.266,
      "step": 534
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.0673811212182045,
      "learning_rate": 0.0019910529787219968,
      "loss": 1.2804,
      "step": 535
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.04868144541978836,
      "learning_rate": 0.0019909952498285846,
      "loss": 1.3076,
      "step": 536
    },
    {
      "epoch": 0.716,
      "grad_norm": 0.06706177443265915,
      "learning_rate": 0.0019909373361337475,
      "loss": 1.399,
      "step": 537
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 0.055421821773052216,
      "learning_rate": 0.001990879237648285,
      "loss": 1.1078,
      "step": 538
    },
    {
      "epoch": 0.7186666666666667,
      "grad_norm": 0.05965227261185646,
      "learning_rate": 0.0019908209543830313,
      "loss": 1.263,
      "step": 539
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.035955995321273804,
      "learning_rate": 0.001990762486348855,
      "loss": 0.9878,
      "step": 540
    },
    {
      "epoch": 0.7213333333333334,
      "grad_norm": 0.03754599392414093,
      "learning_rate": 0.0019907038335566594,
      "loss": 1.2353,
      "step": 541
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.05093695968389511,
      "learning_rate": 0.001990644996017382,
      "loss": 1.2934,
      "step": 542
    },
    {
      "epoch": 0.724,
      "grad_norm": 0.04218876734375954,
      "learning_rate": 0.0019905859737419955,
      "loss": 1.2224,
      "step": 543
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.04036647453904152,
      "learning_rate": 0.0019905267667415056,
      "loss": 1.1762,
      "step": 544
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.07038184255361557,
      "learning_rate": 0.0019904673750269536,
      "loss": 1.1477,
      "step": 545
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.03646077215671539,
      "learning_rate": 0.001990407798609415,
      "loss": 1.0942,
      "step": 546
    },
    {
      "epoch": 0.7293333333333333,
      "grad_norm": 0.05545599013566971,
      "learning_rate": 0.0019903480374999995,
      "loss": 1.147,
      "step": 547
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.04763709753751755,
      "learning_rate": 0.0019902880917098513,
      "loss": 1.1576,
      "step": 548
    },
    {
      "epoch": 0.732,
      "grad_norm": 0.04583637788891792,
      "learning_rate": 0.001990227961250149,
      "loss": 1.0715,
      "step": 549
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.04283996298909187,
      "learning_rate": 0.0019901676461321067,
      "loss": 1.195,
      "step": 550
    },
    {
      "epoch": 0.7346666666666667,
      "grad_norm": 0.030183978378772736,
      "learning_rate": 0.001990107146366971,
      "loss": 1.2644,
      "step": 551
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.03771253675222397,
      "learning_rate": 0.001990046461966024,
      "loss": 1.105,
      "step": 552
    },
    {
      "epoch": 0.7373333333333333,
      "grad_norm": 0.03785443305969238,
      "learning_rate": 0.0019899855929405826,
      "loss": 1.2575,
      "step": 553
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 0.17935176193714142,
      "learning_rate": 0.0019899245393019977,
      "loss": 1.5944,
      "step": 554
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.05186612159013748,
      "learning_rate": 0.001989863301061654,
      "loss": 1.3438,
      "step": 555
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.04307100921869278,
      "learning_rate": 0.001989801878230972,
      "loss": 1.3061,
      "step": 556
    },
    {
      "epoch": 0.7426666666666667,
      "grad_norm": 0.03372262418270111,
      "learning_rate": 0.0019897402708214055,
      "loss": 1.0965,
      "step": 557
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.04772927612066269,
      "learning_rate": 0.0019896784788444428,
      "loss": 1.1738,
      "step": 558
    },
    {
      "epoch": 0.7453333333333333,
      "grad_norm": 0.044674813747406006,
      "learning_rate": 0.0019896165023116077,
      "loss": 1.1048,
      "step": 559
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.05813051760196686,
      "learning_rate": 0.0019895543412344566,
      "loss": 1.2387,
      "step": 560
    },
    {
      "epoch": 0.748,
      "grad_norm": 0.10255257040262222,
      "learning_rate": 0.0019894919956245827,
      "loss": 1.369,
      "step": 561
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.038473162800073624,
      "learning_rate": 0.0019894294654936107,
      "loss": 1.279,
      "step": 562
    },
    {
      "epoch": 0.7506666666666667,
      "grad_norm": 0.0671471357345581,
      "learning_rate": 0.0019893667508532023,
      "loss": 1.4185,
      "step": 563
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.04327027499675751,
      "learning_rate": 0.001989303851715052,
      "loss": 0.8439,
      "step": 564
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.04634663090109825,
      "learning_rate": 0.00198924076809089,
      "loss": 1.1487,
      "step": 565
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 0.03611162677407265,
      "learning_rate": 0.0019891774999924797,
      "loss": 1.0885,
      "step": 566
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.04367527738213539,
      "learning_rate": 0.0019891140474316196,
      "loss": 1.5282,
      "step": 567
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.04957663640379906,
      "learning_rate": 0.0019890504104201415,
      "loss": 1.2081,
      "step": 568
    },
    {
      "epoch": 0.7586666666666667,
      "grad_norm": 0.04294877499341965,
      "learning_rate": 0.0019889865889699135,
      "loss": 1.066,
      "step": 569
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.04970566928386688,
      "learning_rate": 0.0019889225830928363,
      "loss": 1.3773,
      "step": 570
    },
    {
      "epoch": 0.7613333333333333,
      "grad_norm": 0.04009956121444702,
      "learning_rate": 0.0019888583928008466,
      "loss": 1.4071,
      "step": 571
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.05686815828084946,
      "learning_rate": 0.0019887940181059142,
      "loss": 1.2829,
      "step": 572
    },
    {
      "epoch": 0.764,
      "grad_norm": 0.042250555008649826,
      "learning_rate": 0.0019887294590200436,
      "loss": 1.3649,
      "step": 573
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.04666374251246452,
      "learning_rate": 0.001988664715555274,
      "loss": 1.3635,
      "step": 574
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.03796734660863876,
      "learning_rate": 0.0019885997877236786,
      "loss": 1.2567,
      "step": 575
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.042796675115823746,
      "learning_rate": 0.0019885346755373658,
      "loss": 1.202,
      "step": 576
    },
    {
      "epoch": 0.7693333333333333,
      "grad_norm": 0.038811858743429184,
      "learning_rate": 0.001988469379008477,
      "loss": 1.1965,
      "step": 577
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.04945554956793785,
      "learning_rate": 0.001988403898149189,
      "loss": 1.2736,
      "step": 578
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.04024607688188553,
      "learning_rate": 0.0019883382329717128,
      "loss": 1.0842,
      "step": 579
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.09316089749336243,
      "learning_rate": 0.0019882723834882933,
      "loss": 1.3453,
      "step": 580
    },
    {
      "epoch": 0.7746666666666666,
      "grad_norm": 0.0477818064391613,
      "learning_rate": 0.001988206349711211,
      "loss": 1.2358,
      "step": 581
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.0430501364171505,
      "learning_rate": 0.0019881401316527796,
      "loss": 1.2793,
      "step": 582
    },
    {
      "epoch": 0.7773333333333333,
      "grad_norm": 0.04995362088084221,
      "learning_rate": 0.001988073729325347,
      "loss": 1.1499,
      "step": 583
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.0571594201028347,
      "learning_rate": 0.0019880071427412957,
      "loss": 1.3315,
      "step": 584
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.04272359237074852,
      "learning_rate": 0.001987940371913044,
      "loss": 1.1035,
      "step": 585
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 0.09237006306648254,
      "learning_rate": 0.0019878734168530428,
      "loss": 1.171,
      "step": 586
    },
    {
      "epoch": 0.7826666666666666,
      "grad_norm": 0.06027572974562645,
      "learning_rate": 0.0019878062775737777,
      "loss": 1.2347,
      "step": 587
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.07006745785474777,
      "learning_rate": 0.0019877389540877686,
      "loss": 1.2659,
      "step": 588
    },
    {
      "epoch": 0.7853333333333333,
      "grad_norm": 0.034680936485528946,
      "learning_rate": 0.001987671446407571,
      "loss": 1.31,
      "step": 589
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.03421400114893913,
      "learning_rate": 0.001987603754545773,
      "loss": 1.0262,
      "step": 590
    },
    {
      "epoch": 0.788,
      "grad_norm": 0.035117678344249725,
      "learning_rate": 0.001987535878514998,
      "loss": 1.1246,
      "step": 591
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.0343889556825161,
      "learning_rate": 0.001987467818327904,
      "loss": 1.1339,
      "step": 592
    },
    {
      "epoch": 0.7906666666666666,
      "grad_norm": 0.0352587066590786,
      "learning_rate": 0.0019873995739971818,
      "loss": 1.199,
      "step": 593
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.048659373074769974,
      "learning_rate": 0.001987331145535559,
      "loss": 1.277,
      "step": 594
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.05149490758776665,
      "learning_rate": 0.0019872625329557954,
      "loss": 1.4934,
      "step": 595
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.21961577236652374,
      "learning_rate": 0.001987193736270686,
      "loss": 1.3801,
      "step": 596
    },
    {
      "epoch": 0.796,
      "grad_norm": 0.061457935720682144,
      "learning_rate": 0.00198712475549306,
      "loss": 1.2059,
      "step": 597
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 0.07134959101676941,
      "learning_rate": 0.001987055590635781,
      "loss": 1.1302,
      "step": 598
    },
    {
      "epoch": 0.7986666666666666,
      "grad_norm": 0.04010030999779701,
      "learning_rate": 0.0019869862417117475,
      "loss": 1.0476,
      "step": 599
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.057877711951732635,
      "learning_rate": 0.0019869167087338906,
      "loss": 1.2614,
      "step": 600
    },
    {
      "epoch": 0.8013333333333333,
      "grad_norm": 0.0511474646627903,
      "learning_rate": 0.001986846991715178,
      "loss": 1.0175,
      "step": 601
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 0.07443512976169586,
      "learning_rate": 0.0019867770906686097,
      "loss": 0.989,
      "step": 602
    },
    {
      "epoch": 0.804,
      "grad_norm": 0.050948869436979294,
      "learning_rate": 0.0019867070056072216,
      "loss": 1.1243,
      "step": 603
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.08675958216190338,
      "learning_rate": 0.0019866367365440826,
      "loss": 1.3732,
      "step": 604
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.05105762556195259,
      "learning_rate": 0.001986566283492297,
      "loss": 1.456,
      "step": 605
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.03602202981710434,
      "learning_rate": 0.0019864956464650026,
      "loss": 1.0652,
      "step": 606
    },
    {
      "epoch": 0.8093333333333333,
      "grad_norm": 0.042252231389284134,
      "learning_rate": 0.0019864248254753717,
      "loss": 1.549,
      "step": 607
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.038269586861133575,
      "learning_rate": 0.0019863538205366115,
      "loss": 1.2007,
      "step": 608
    },
    {
      "epoch": 0.812,
      "grad_norm": 0.041908953338861465,
      "learning_rate": 0.001986282631661963,
      "loss": 1.5537,
      "step": 609
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.04403570666909218,
      "learning_rate": 0.0019862112588647013,
      "loss": 1.1792,
      "step": 610
    },
    {
      "epoch": 0.8146666666666667,
      "grad_norm": 0.048691533505916595,
      "learning_rate": 0.001986139702158136,
      "loss": 1.3752,
      "step": 611
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.03814046084880829,
      "learning_rate": 0.001986067961555611,
      "loss": 1.337,
      "step": 612
    },
    {
      "epoch": 0.8173333333333334,
      "grad_norm": 0.04022346809506416,
      "learning_rate": 0.001985996037070505,
      "loss": 1.1274,
      "step": 613
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.047556325793266296,
      "learning_rate": 0.00198592392871623,
      "loss": 1.3994,
      "step": 614
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.03753335401415825,
      "learning_rate": 0.0019858516365062334,
      "loss": 1.0339,
      "step": 615
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.04924526438117027,
      "learning_rate": 0.0019857791604539956,
      "loss": 1.238,
      "step": 616
    },
    {
      "epoch": 0.8226666666666667,
      "grad_norm": 0.0611661821603775,
      "learning_rate": 0.0019857065005730325,
      "loss": 1.3735,
      "step": 617
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.06356005370616913,
      "learning_rate": 0.0019856336568768933,
      "loss": 1.2529,
      "step": 618
    },
    {
      "epoch": 0.8253333333333334,
      "grad_norm": 0.040762778371572495,
      "learning_rate": 0.0019855606293791624,
      "loss": 1.0775,
      "step": 619
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.04204663261771202,
      "learning_rate": 0.001985487418093458,
      "loss": 1.2484,
      "step": 620
    },
    {
      "epoch": 0.828,
      "grad_norm": 0.043720610439777374,
      "learning_rate": 0.001985414023033432,
      "loss": 1.351,
      "step": 621
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 0.04349161311984062,
      "learning_rate": 0.001985340444212772,
      "loss": 1.2079,
      "step": 622
    },
    {
      "epoch": 0.8306666666666667,
      "grad_norm": 0.04948198050260544,
      "learning_rate": 0.0019852666816451985,
      "loss": 1.4174,
      "step": 623
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.03907691314816475,
      "learning_rate": 0.001985192735344467,
      "loss": 1.2594,
      "step": 624
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.036606643348932266,
      "learning_rate": 0.0019851186053243667,
      "loss": 1.3409,
      "step": 625
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.03999806568026543,
      "learning_rate": 0.0019850442915987213,
      "loss": 1.3813,
      "step": 626
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.05849708616733551,
      "learning_rate": 0.00198496979418139,
      "loss": 1.3855,
      "step": 627
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.030042242258787155,
      "learning_rate": 0.001984895113086264,
      "loss": 1.2365,
      "step": 628
    },
    {
      "epoch": 0.8386666666666667,
      "grad_norm": 0.0543988011777401,
      "learning_rate": 0.0019848202483272698,
      "loss": 1.3074,
      "step": 629
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.03862643614411354,
      "learning_rate": 0.001984745199918369,
      "loss": 1.3165,
      "step": 630
    },
    {
      "epoch": 0.8413333333333334,
      "grad_norm": 0.033345337957143784,
      "learning_rate": 0.0019846699678735566,
      "loss": 1.0515,
      "step": 631
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.04215315356850624,
      "learning_rate": 0.0019845945522068615,
      "loss": 1.1388,
      "step": 632
    },
    {
      "epoch": 0.844,
      "grad_norm": 0.04496956616640091,
      "learning_rate": 0.0019845189529323474,
      "loss": 1.2424,
      "step": 633
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 0.05219142884016037,
      "learning_rate": 0.001984443170064112,
      "loss": 1.2956,
      "step": 634
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 0.035296279937028885,
      "learning_rate": 0.001984367203616288,
      "loss": 1.159,
      "step": 635
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.039884909987449646,
      "learning_rate": 0.00198429105360304,
      "loss": 1.286,
      "step": 636
    },
    {
      "epoch": 0.8493333333333334,
      "grad_norm": 0.034350570291280746,
      "learning_rate": 0.0019842147200385706,
      "loss": 1.1479,
      "step": 637
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 0.03566160053014755,
      "learning_rate": 0.001984138202937113,
      "loss": 1.4377,
      "step": 638
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.04362429305911064,
      "learning_rate": 0.001984061502312937,
      "loss": 1.2657,
      "step": 639
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.04142385348677635,
      "learning_rate": 0.0019839846181803457,
      "loss": 1.1858,
      "step": 640
    },
    {
      "epoch": 0.8546666666666667,
      "grad_norm": 0.03652351722121239,
      "learning_rate": 0.001983907550553676,
      "loss": 0.834,
      "step": 641
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.040226809680461884,
      "learning_rate": 0.0019838302994472997,
      "loss": 0.955,
      "step": 642
    },
    {
      "epoch": 0.8573333333333333,
      "grad_norm": 0.030205493792891502,
      "learning_rate": 0.001983752864875623,
      "loss": 1.1313,
      "step": 643
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.042249441146850586,
      "learning_rate": 0.0019836752468530856,
      "loss": 1.1725,
      "step": 644
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.07298334687948227,
      "learning_rate": 0.0019835974453941622,
      "loss": 1.2121,
      "step": 645
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.046084094792604446,
      "learning_rate": 0.00198351946051336,
      "loss": 1.3234,
      "step": 646
    },
    {
      "epoch": 0.8626666666666667,
      "grad_norm": 0.0505850613117218,
      "learning_rate": 0.0019834412922252235,
      "loss": 1.4608,
      "step": 647
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.03963499888777733,
      "learning_rate": 0.0019833629405443284,
      "loss": 1.2604,
      "step": 648
    },
    {
      "epoch": 0.8653333333333333,
      "grad_norm": 0.0329366996884346,
      "learning_rate": 0.001983284405485286,
      "loss": 0.9693,
      "step": 649
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.04918372631072998,
      "learning_rate": 0.001983205687062742,
      "loss": 1.3107,
      "step": 650
    },
    {
      "epoch": 0.868,
      "grad_norm": 0.1054145097732544,
      "learning_rate": 0.001983126785291375,
      "loss": 1.3209,
      "step": 651
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.04114145413041115,
      "learning_rate": 0.001983047700185899,
      "loss": 0.9805,
      "step": 652
    },
    {
      "epoch": 0.8706666666666667,
      "grad_norm": 0.06032363697886467,
      "learning_rate": 0.001982968431761062,
      "loss": 1.2847,
      "step": 653
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.030844679102301598,
      "learning_rate": 0.0019828889800316465,
      "loss": 1.055,
      "step": 654
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 0.04572083055973053,
      "learning_rate": 0.0019828093450124677,
      "loss": 1.459,
      "step": 655
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.05414131283760071,
      "learning_rate": 0.0019827295267183767,
      "loss": 1.3006,
      "step": 656
    },
    {
      "epoch": 0.876,
      "grad_norm": 0.03188403695821762,
      "learning_rate": 0.001982649525164258,
      "loss": 1.1935,
      "step": 657
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.047899000346660614,
      "learning_rate": 0.00198256934036503,
      "loss": 1.357,
      "step": 658
    },
    {
      "epoch": 0.8786666666666667,
      "grad_norm": 0.03193974867463112,
      "learning_rate": 0.0019824889723356457,
      "loss": 1.2908,
      "step": 659
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.05039690062403679,
      "learning_rate": 0.0019824084210910923,
      "loss": 1.2561,
      "step": 660
    },
    {
      "epoch": 0.8813333333333333,
      "grad_norm": 0.15111958980560303,
      "learning_rate": 0.001982327686646391,
      "loss": 1.4664,
      "step": 661
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.039017800241708755,
      "learning_rate": 0.0019822467690165974,
      "loss": 0.9607,
      "step": 662
    },
    {
      "epoch": 0.884,
      "grad_norm": 0.04465925693511963,
      "learning_rate": 0.001982165668216801,
      "loss": 1.3703,
      "step": 663
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.09993806481361389,
      "learning_rate": 0.001982084384262126,
      "loss": 1.3284,
      "step": 664
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 0.04884537309408188,
      "learning_rate": 0.0019820029171677286,
      "loss": 1.3242,
      "step": 665
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.034920789301395416,
      "learning_rate": 0.0019819212669488027,
      "loss": 1.2328,
      "step": 666
    },
    {
      "epoch": 0.8893333333333333,
      "grad_norm": 0.04159178584814072,
      "learning_rate": 0.001981839433620573,
      "loss": 1.3912,
      "step": 667
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.054168060421943665,
      "learning_rate": 0.0019817574171983013,
      "loss": 1.2546,
      "step": 668
    },
    {
      "epoch": 0.892,
      "grad_norm": 0.035854071378707886,
      "learning_rate": 0.0019816752176972812,
      "loss": 1.0169,
      "step": 669
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.05354318767786026,
      "learning_rate": 0.0019815928351328413,
      "loss": 1.3071,
      "step": 670
    },
    {
      "epoch": 0.8946666666666667,
      "grad_norm": 0.03891652822494507,
      "learning_rate": 0.001981510269520345,
      "loss": 1.423,
      "step": 671
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.058840181678533554,
      "learning_rate": 0.001981427520875188,
      "loss": 1.2855,
      "step": 672
    },
    {
      "epoch": 0.8973333333333333,
      "grad_norm": 0.043568678200244904,
      "learning_rate": 0.0019813445892128026,
      "loss": 1.058,
      "step": 673
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.03873732313513756,
      "learning_rate": 0.001981261474548653,
      "loss": 1.0552,
      "step": 674
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.043549519032239914,
      "learning_rate": 0.0019811781768982392,
      "loss": 1.5924,
      "step": 675
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.04407823830842972,
      "learning_rate": 0.001981094696277094,
      "loss": 1.1208,
      "step": 676
    },
    {
      "epoch": 0.9026666666666666,
      "grad_norm": 0.040066592395305634,
      "learning_rate": 0.001981011032700785,
      "loss": 1.1359,
      "step": 677
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.042482078075408936,
      "learning_rate": 0.0019809271861849147,
      "loss": 1.2618,
      "step": 678
    },
    {
      "epoch": 0.9053333333333333,
      "grad_norm": 0.04104184731841087,
      "learning_rate": 0.0019808431567451177,
      "loss": 0.9655,
      "step": 679
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.043194301426410675,
      "learning_rate": 0.001980758944397064,
      "loss": 1.0895,
      "step": 680
    },
    {
      "epoch": 0.908,
      "grad_norm": 0.04229630529880524,
      "learning_rate": 0.0019806745491564586,
      "loss": 0.9725,
      "step": 681
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.05327345058321953,
      "learning_rate": 0.0019805899710390383,
      "loss": 1.3442,
      "step": 682
    },
    {
      "epoch": 0.9106666666666666,
      "grad_norm": 0.03620363771915436,
      "learning_rate": 0.001980505210060576,
      "loss": 1.3453,
      "step": 683
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.04740983992815018,
      "learning_rate": 0.001980420266236878,
      "loss": 1.1521,
      "step": 684
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 0.03394995257258415,
      "learning_rate": 0.001980335139583785,
      "loss": 1.2801,
      "step": 685
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.03685692697763443,
      "learning_rate": 0.00198024983011717,
      "loss": 1.1639,
      "step": 686
    },
    {
      "epoch": 0.916,
      "grad_norm": 0.038006071001291275,
      "learning_rate": 0.001980164337852943,
      "loss": 1.3461,
      "step": 687
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.03821340203285217,
      "learning_rate": 0.0019800786628070464,
      "loss": 1.2299,
      "step": 688
    },
    {
      "epoch": 0.9186666666666666,
      "grad_norm": 0.028552325442433357,
      "learning_rate": 0.0019799928049954564,
      "loss": 1.1825,
      "step": 689
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.03532811999320984,
      "learning_rate": 0.0019799067644341844,
      "loss": 1.5049,
      "step": 690
    },
    {
      "epoch": 0.9213333333333333,
      "grad_norm": 0.1984838843345642,
      "learning_rate": 0.001979820541139275,
      "loss": 1.0872,
      "step": 691
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.05993962287902832,
      "learning_rate": 0.0019797341351268072,
      "loss": 1.2646,
      "step": 692
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.0827043280005455,
      "learning_rate": 0.001979647546412894,
      "loss": 1.415,
      "step": 693
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.03462410345673561,
      "learning_rate": 0.001979560775013683,
      "loss": 1.1991,
      "step": 694
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 0.04694166034460068,
      "learning_rate": 0.0019794738209453545,
      "loss": 1.0894,
      "step": 695
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.10221926867961884,
      "learning_rate": 0.0019793866842241245,
      "loss": 1.3328,
      "step": 696
    },
    {
      "epoch": 0.9293333333333333,
      "grad_norm": 0.040791817009449005,
      "learning_rate": 0.0019792993648662417,
      "loss": 1.0221,
      "step": 697
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 0.04611221328377724,
      "learning_rate": 0.0019792118628879905,
      "loss": 1.0564,
      "step": 698
    },
    {
      "epoch": 0.932,
      "grad_norm": 0.0477466955780983,
      "learning_rate": 0.001979124178305687,
      "loss": 1.1124,
      "step": 699
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.04596417024731636,
      "learning_rate": 0.0019790363111356836,
      "loss": 1.2323,
      "step": 700
    },
    {
      "epoch": 0.9346666666666666,
      "grad_norm": 0.029860856011509895,
      "learning_rate": 0.0019789482613943657,
      "loss": 0.8308,
      "step": 701
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.039923761039972305,
      "learning_rate": 0.0019788600290981525,
      "loss": 1.1521,
      "step": 702
    },
    {
      "epoch": 0.9373333333333334,
      "grad_norm": 0.04907752200961113,
      "learning_rate": 0.001978771614263498,
      "loss": 0.9647,
      "step": 703
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.036198344081640244,
      "learning_rate": 0.0019786830169068893,
      "loss": 1.5854,
      "step": 704
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.051110878586769104,
      "learning_rate": 0.0019785942370448488,
      "loss": 1.2361,
      "step": 705
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.050387296825647354,
      "learning_rate": 0.001978505274693932,
      "loss": 1.2785,
      "step": 706
    },
    {
      "epoch": 0.9426666666666667,
      "grad_norm": 0.03628532588481903,
      "learning_rate": 0.001978416129870728,
      "loss": 1.0033,
      "step": 707
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.04510756582021713,
      "learning_rate": 0.0019783268025918618,
      "loss": 1.1307,
      "step": 708
    },
    {
      "epoch": 0.9453333333333334,
      "grad_norm": 0.05902193859219551,
      "learning_rate": 0.0019782372928739906,
      "loss": 1.4857,
      "step": 709
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.02775685489177704,
      "learning_rate": 0.0019781476007338056,
      "loss": 1.1896,
      "step": 710
    },
    {
      "epoch": 0.948,
      "grad_norm": 0.049294859170913696,
      "learning_rate": 0.0019780577261880334,
      "loss": 1.1861,
      "step": 711
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.04747086763381958,
      "learning_rate": 0.001977967669253434,
      "loss": 1.446,
      "step": 712
    },
    {
      "epoch": 0.9506666666666667,
      "grad_norm": 0.040599673986434937,
      "learning_rate": 0.0019778774299468007,
      "loss": 1.3227,
      "step": 713
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.061495207250118256,
      "learning_rate": 0.0019777870082849616,
      "loss": 1.276,
      "step": 714
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 0.04132350534200668,
      "learning_rate": 0.001977696404284779,
      "loss": 1.0075,
      "step": 715
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.0538661926984787,
      "learning_rate": 0.0019776056179631484,
      "loss": 1.2371,
      "step": 716
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.04239802062511444,
      "learning_rate": 0.001977514649336999,
      "loss": 1.3505,
      "step": 717
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.03769887238740921,
      "learning_rate": 0.0019774234984232964,
      "loss": 1.1395,
      "step": 718
    },
    {
      "epoch": 0.9586666666666667,
      "grad_norm": 0.05681364983320236,
      "learning_rate": 0.001977332165239037,
      "loss": 1.5656,
      "step": 719
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.05129548907279968,
      "learning_rate": 0.001977240649801253,
      "loss": 1.6277,
      "step": 720
    },
    {
      "epoch": 0.9613333333333334,
      "grad_norm": 0.2924656569957733,
      "learning_rate": 0.0019771489521270107,
      "loss": 1.407,
      "step": 721
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 0.0376729890704155,
      "learning_rate": 0.0019770570722334093,
      "loss": 1.4534,
      "step": 722
    },
    {
      "epoch": 0.964,
      "grad_norm": 0.05404624715447426,
      "learning_rate": 0.0019769650101375837,
      "loss": 0.9892,
      "step": 723
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 0.16851240396499634,
      "learning_rate": 0.0019768727658567003,
      "loss": 1.3257,
      "step": 724
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.06689499318599701,
      "learning_rate": 0.0019767803394079614,
      "loss": 1.1741,
      "step": 725
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.060719843953847885,
      "learning_rate": 0.0019766877308086037,
      "loss": 1.0683,
      "step": 726
    },
    {
      "epoch": 0.9693333333333334,
      "grad_norm": 0.048348911106586456,
      "learning_rate": 0.0019765949400758955,
      "loss": 1.225,
      "step": 727
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.17444469034671783,
      "learning_rate": 0.0019765019672271416,
      "loss": 1.3784,
      "step": 728
    },
    {
      "epoch": 0.972,
      "grad_norm": 0.04657571762800217,
      "learning_rate": 0.0019764088122796782,
      "loss": 1.2849,
      "step": 729
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.05217059329152107,
      "learning_rate": 0.0019763154752508783,
      "loss": 1.2152,
      "step": 730
    },
    {
      "epoch": 0.9746666666666667,
      "grad_norm": 0.09551148116588593,
      "learning_rate": 0.0019762219561581472,
      "loss": 0.9818,
      "step": 731
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.042320579290390015,
      "learning_rate": 0.001976128255018924,
      "loss": 1.5143,
      "step": 732
    },
    {
      "epoch": 0.9773333333333334,
      "grad_norm": 0.043850623071193695,
      "learning_rate": 0.001976034371850682,
      "loss": 1.0459,
      "step": 733
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.06457269191741943,
      "learning_rate": 0.0019759403066709294,
      "loss": 1.1412,
      "step": 734
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.05029945820569992,
      "learning_rate": 0.001975846059497207,
      "loss": 1.2979,
      "step": 735
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.036278240382671356,
      "learning_rate": 0.0019757516303470896,
      "loss": 1.3369,
      "step": 736
    },
    {
      "epoch": 0.9826666666666667,
      "grad_norm": 0.050299856811761856,
      "learning_rate": 0.0019756570192381875,
      "loss": 1.1386,
      "step": 737
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.057707980275154114,
      "learning_rate": 0.0019755622261881426,
      "loss": 1.0933,
      "step": 738
    },
    {
      "epoch": 0.9853333333333333,
      "grad_norm": 0.05284956470131874,
      "learning_rate": 0.001975467251214633,
      "loss": 1.2255,
      "step": 739
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.030747197568416595,
      "learning_rate": 0.0019753720943353694,
      "loss": 1.1293,
      "step": 740
    },
    {
      "epoch": 0.988,
      "grad_norm": 0.038681916892528534,
      "learning_rate": 0.0019752767555680966,
      "loss": 1.2917,
      "step": 741
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 0.03715824335813522,
      "learning_rate": 0.001975181234930593,
      "loss": 1.1178,
      "step": 742
    },
    {
      "epoch": 0.9906666666666667,
      "grad_norm": 0.04513728618621826,
      "learning_rate": 0.0019750855324406724,
      "loss": 1.1234,
      "step": 743
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.05069280043244362,
      "learning_rate": 0.001974989648116181,
      "loss": 1.4125,
      "step": 744
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 0.03868097439408302,
      "learning_rate": 0.0019748935819749987,
      "loss": 1.2644,
      "step": 745
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 0.05064034461975098,
      "learning_rate": 0.001974797334035041,
      "loss": 1.2755,
      "step": 746
    },
    {
      "epoch": 0.996,
      "grad_norm": 0.052486125379800797,
      "learning_rate": 0.0019747009043142552,
      "loss": 1.2067,
      "step": 747
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.0350789949297905,
      "learning_rate": 0.0019746042928306244,
      "loss": 1.1133,
      "step": 748
    },
    {
      "epoch": 0.9986666666666667,
      "grad_norm": 0.061461661010980606,
      "learning_rate": 0.0019745074996021647,
      "loss": 1.1749,
      "step": 749
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.04764922335743904,
      "learning_rate": 0.001974410524646926,
      "loss": 1.0886,
      "step": 750
    },
    {
      "epoch": 1.0013333333333334,
      "grad_norm": 0.05492081493139267,
      "learning_rate": 0.0019743133679829923,
      "loss": 1.1155,
      "step": 751
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 0.028692753985524178,
      "learning_rate": 0.001974216029628481,
      "loss": 1.0918,
      "step": 752
    },
    {
      "epoch": 1.004,
      "grad_norm": 0.045952919870615005,
      "learning_rate": 0.001974118509601545,
      "loss": 1.3218,
      "step": 753
    },
    {
      "epoch": 1.0053333333333334,
      "grad_norm": 0.03732681646943092,
      "learning_rate": 0.001974020807920368,
      "loss": 1.2281,
      "step": 754
    },
    {
      "epoch": 1.0066666666666666,
      "grad_norm": 0.039693862199783325,
      "learning_rate": 0.0019739229246031717,
      "loss": 0.9007,
      "step": 755
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.03235549479722977,
      "learning_rate": 0.0019738248596682076,
      "loss": 0.9669,
      "step": 756
    },
    {
      "epoch": 1.0093333333333334,
      "grad_norm": 0.028382059186697006,
      "learning_rate": 0.0019737266131337637,
      "loss": 1.3491,
      "step": 757
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.04543567821383476,
      "learning_rate": 0.0019736281850181612,
      "loss": 1.3708,
      "step": 758
    },
    {
      "epoch": 1.012,
      "grad_norm": 0.035272035747766495,
      "learning_rate": 0.001973529575339755,
      "loss": 1.0629,
      "step": 759
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.032112959772348404,
      "learning_rate": 0.0019734307841169337,
      "loss": 1.1489,
      "step": 760
    },
    {
      "epoch": 1.0146666666666666,
      "grad_norm": 0.04895009100437164,
      "learning_rate": 0.00197333181136812,
      "loss": 1.1928,
      "step": 761
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.033490847796201706,
      "learning_rate": 0.00197323265711177,
      "loss": 1.2498,
      "step": 762
    },
    {
      "epoch": 1.0173333333333334,
      "grad_norm": 0.03367787227034569,
      "learning_rate": 0.0019731333213663747,
      "loss": 1.2191,
      "step": 763
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 0.030910920351743698,
      "learning_rate": 0.0019730338041504584,
      "loss": 1.2605,
      "step": 764
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.055230170488357544,
      "learning_rate": 0.0019729341054825784,
      "loss": 1.3101,
      "step": 765
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.04118988662958145,
      "learning_rate": 0.0019728342253813266,
      "loss": 1.0732,
      "step": 766
    },
    {
      "epoch": 1.0226666666666666,
      "grad_norm": 0.0421115942299366,
      "learning_rate": 0.0019727341638653296,
      "loss": 1.4456,
      "step": 767
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.044679395854473114,
      "learning_rate": 0.001972633920953246,
      "loss": 1.351,
      "step": 768
    },
    {
      "epoch": 1.0253333333333334,
      "grad_norm": 0.04334598407149315,
      "learning_rate": 0.00197253349666377,
      "loss": 1.3485,
      "step": 769
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.0557689443230629,
      "learning_rate": 0.001972432891015628,
      "loss": 1.1754,
      "step": 770
    },
    {
      "epoch": 1.028,
      "grad_norm": 0.054303061217069626,
      "learning_rate": 0.0019723321040275815,
      "loss": 1.1276,
      "step": 771
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 0.04731139540672302,
      "learning_rate": 0.001972231135718425,
      "loss": 1.0513,
      "step": 772
    },
    {
      "epoch": 1.0306666666666666,
      "grad_norm": 0.03875308111310005,
      "learning_rate": 0.0019721299861069873,
      "loss": 1.301,
      "step": 773
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.03325248509645462,
      "learning_rate": 0.001972028655212131,
      "loss": 1.0212,
      "step": 774
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.19474375247955322,
      "learning_rate": 0.001971927143052752,
      "loss": 1.3438,
      "step": 775
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 0.04760857671499252,
      "learning_rate": 0.0019718254496477803,
      "loss": 1.2521,
      "step": 776
    },
    {
      "epoch": 1.036,
      "grad_norm": 0.04879898205399513,
      "learning_rate": 0.001971723575016181,
      "loss": 1.0142,
      "step": 777
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 0.04724428802728653,
      "learning_rate": 0.00197162151917695,
      "loss": 1.3684,
      "step": 778
    },
    {
      "epoch": 1.0386666666666666,
      "grad_norm": 0.03401247039437294,
      "learning_rate": 0.001971519282149119,
      "loss": 1.1758,
      "step": 779
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.08395465463399887,
      "learning_rate": 0.001971416863951754,
      "loss": 1.4199,
      "step": 780
    },
    {
      "epoch": 1.0413333333333332,
      "grad_norm": 0.06653392314910889,
      "learning_rate": 0.0019713142646039543,
      "loss": 1.147,
      "step": 781
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.043024055659770966,
      "learning_rate": 0.001971211484124852,
      "loss": 1.143,
      "step": 782
    },
    {
      "epoch": 1.044,
      "grad_norm": 0.12867620587348938,
      "learning_rate": 0.0019711085225336132,
      "loss": 1.3113,
      "step": 783
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 0.05742860957980156,
      "learning_rate": 0.001971005379849439,
      "loss": 1.1678,
      "step": 784
    },
    {
      "epoch": 1.0466666666666666,
      "grad_norm": 0.043350595980882645,
      "learning_rate": 0.0019709020560915638,
      "loss": 1.4979,
      "step": 785
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.06436274945735931,
      "learning_rate": 0.001970798551279254,
      "loss": 1.2417,
      "step": 786
    },
    {
      "epoch": 1.0493333333333332,
      "grad_norm": 0.0517854206264019,
      "learning_rate": 0.0019706948654318133,
      "loss": 1.0394,
      "step": 787
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 0.0455281138420105,
      "learning_rate": 0.0019705909985685754,
      "loss": 1.17,
      "step": 788
    },
    {
      "epoch": 1.052,
      "grad_norm": 0.03233114257454872,
      "learning_rate": 0.0019704869507089105,
      "loss": 1.2047,
      "step": 789
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.11199016124010086,
      "learning_rate": 0.001970382721872221,
      "loss": 1.0844,
      "step": 790
    },
    {
      "epoch": 1.0546666666666666,
      "grad_norm": 0.046638764441013336,
      "learning_rate": 0.0019702783120779436,
      "loss": 1.3331,
      "step": 791
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.06628764420747757,
      "learning_rate": 0.0019701737213455492,
      "loss": 1.1167,
      "step": 792
    },
    {
      "epoch": 1.0573333333333332,
      "grad_norm": 0.04928062856197357,
      "learning_rate": 0.0019700689496945406,
      "loss": 1.2975,
      "step": 793
    },
    {
      "epoch": 1.0586666666666666,
      "grad_norm": 0.03600689396262169,
      "learning_rate": 0.0019699639971444576,
      "loss": 1.2342,
      "step": 794
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.044804465025663376,
      "learning_rate": 0.00196985886371487,
      "loss": 1.2783,
      "step": 795
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 0.046508245170116425,
      "learning_rate": 0.001969753549425385,
      "loss": 1.1335,
      "step": 796
    },
    {
      "epoch": 1.0626666666666666,
      "grad_norm": 0.04667240008711815,
      "learning_rate": 0.0019696480542956397,
      "loss": 1.0865,
      "step": 797
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.06626936048269272,
      "learning_rate": 0.0019695423783453085,
      "loss": 1.221,
      "step": 798
    },
    {
      "epoch": 1.0653333333333332,
      "grad_norm": 0.037824466824531555,
      "learning_rate": 0.0019694365215940967,
      "loss": 1.3444,
      "step": 799
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.08075495809316635,
      "learning_rate": 0.0019693304840617456,
      "loss": 1.5071,
      "step": 800
    },
    {
      "epoch": 1.068,
      "grad_norm": 0.041821498423814774,
      "learning_rate": 0.0019692242657680286,
      "loss": 1.2076,
      "step": 801
    },
    {
      "epoch": 1.0693333333333332,
      "grad_norm": 0.042703695595264435,
      "learning_rate": 0.0019691178667327535,
      "loss": 1.2563,
      "step": 802
    },
    {
      "epoch": 1.0706666666666667,
      "grad_norm": 0.04531577229499817,
      "learning_rate": 0.001969011286975761,
      "loss": 1.3219,
      "step": 803
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.05538894236087799,
      "learning_rate": 0.0019689045265169273,
      "loss": 1.1357,
      "step": 804
    },
    {
      "epoch": 1.0733333333333333,
      "grad_norm": 0.033032774925231934,
      "learning_rate": 0.0019687975853761603,
      "loss": 1.2057,
      "step": 805
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.03953256085515022,
      "learning_rate": 0.0019686904635734027,
      "loss": 1.1293,
      "step": 806
    },
    {
      "epoch": 1.076,
      "grad_norm": 0.03795613721013069,
      "learning_rate": 0.001968583161128631,
      "loss": 1.1765,
      "step": 807
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 0.04781962186098099,
      "learning_rate": 0.001968475678061855,
      "loss": 1.1039,
      "step": 808
    },
    {
      "epoch": 1.0786666666666667,
      "grad_norm": 0.04316357895731926,
      "learning_rate": 0.001968368014393117,
      "loss": 1.2861,
      "step": 809
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.029855255037546158,
      "learning_rate": 0.001968260170142496,
      "loss": 1.2248,
      "step": 810
    },
    {
      "epoch": 1.0813333333333333,
      "grad_norm": 0.033655691891908646,
      "learning_rate": 0.0019681521453301016,
      "loss": 1.2381,
      "step": 811
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 0.035517774522304535,
      "learning_rate": 0.0019680439399760784,
      "loss": 1.0373,
      "step": 812
    },
    {
      "epoch": 1.084,
      "grad_norm": 0.04572390392422676,
      "learning_rate": 0.0019679355541006053,
      "loss": 0.9502,
      "step": 813
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.04740644246339798,
      "learning_rate": 0.0019678269877238938,
      "loss": 0.9651,
      "step": 814
    },
    {
      "epoch": 1.0866666666666667,
      "grad_norm": 0.040754277259111404,
      "learning_rate": 0.0019677182408661892,
      "loss": 1.3105,
      "step": 815
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.034917715936899185,
      "learning_rate": 0.0019676093135477715,
      "loss": 1.3288,
      "step": 816
    },
    {
      "epoch": 1.0893333333333333,
      "grad_norm": 0.03089931793510914,
      "learning_rate": 0.0019675002057889523,
      "loss": 1.3395,
      "step": 817
    },
    {
      "epoch": 1.0906666666666667,
      "grad_norm": 0.031500186771154404,
      "learning_rate": 0.0019673909176100785,
      "loss": 1.0815,
      "step": 818
    },
    {
      "epoch": 1.092,
      "grad_norm": 0.041324492543935776,
      "learning_rate": 0.001967281449031531,
      "loss": 1.3037,
      "step": 819
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.03816927224397659,
      "learning_rate": 0.0019671718000737227,
      "loss": 1.4001,
      "step": 820
    },
    {
      "epoch": 1.0946666666666667,
      "grad_norm": 0.0363711453974247,
      "learning_rate": 0.0019670619707571014,
      "loss": 1.304,
      "step": 821
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.03685825318098068,
      "learning_rate": 0.0019669519611021486,
      "loss": 1.2014,
      "step": 822
    },
    {
      "epoch": 1.0973333333333333,
      "grad_norm": 0.05055256560444832,
      "learning_rate": 0.001966841771129378,
      "loss": 1.1765,
      "step": 823
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 0.062012407928705215,
      "learning_rate": 0.001966731400859338,
      "loss": 1.0428,
      "step": 824
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.04109656810760498,
      "learning_rate": 0.001966620850312611,
      "loss": 1.3648,
      "step": 825
    },
    {
      "epoch": 1.1013333333333333,
      "grad_norm": 0.08261837810277939,
      "learning_rate": 0.001966510119509813,
      "loss": 1.3917,
      "step": 826
    },
    {
      "epoch": 1.1026666666666667,
      "grad_norm": 0.04505375772714615,
      "learning_rate": 0.0019663992084715917,
      "loss": 1.5472,
      "step": 827
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.03917964547872543,
      "learning_rate": 0.001966288117218631,
      "loss": 1.1403,
      "step": 828
    },
    {
      "epoch": 1.1053333333333333,
      "grad_norm": 0.054943814873695374,
      "learning_rate": 0.001966176845771647,
      "loss": 1.3214,
      "step": 829
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.03743620216846466,
      "learning_rate": 0.00196606539415139,
      "loss": 1.124,
      "step": 830
    },
    {
      "epoch": 1.108,
      "grad_norm": 0.03882941976189613,
      "learning_rate": 0.0019659537623786427,
      "loss": 1.0724,
      "step": 831
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 0.0449087917804718,
      "learning_rate": 0.001965841950474223,
      "loss": 1.1441,
      "step": 832
    },
    {
      "epoch": 1.1106666666666667,
      "grad_norm": 0.03611493483185768,
      "learning_rate": 0.0019657299584589817,
      "loss": 1.2189,
      "step": 833
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.21207331120967865,
      "learning_rate": 0.0019656177863538026,
      "loss": 1.076,
      "step": 834
    },
    {
      "epoch": 1.1133333333333333,
      "grad_norm": 0.0416511632502079,
      "learning_rate": 0.0019655054341796035,
      "loss": 1.3412,
      "step": 835
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 0.046583060175180435,
      "learning_rate": 0.0019653929019573368,
      "loss": 1.3777,
      "step": 836
    },
    {
      "epoch": 1.116,
      "grad_norm": 0.03714112937450409,
      "learning_rate": 0.001965280189707987,
      "loss": 1.0972,
      "step": 837
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 0.04214714094996452,
      "learning_rate": 0.0019651672974525724,
      "loss": 1.0115,
      "step": 838
    },
    {
      "epoch": 1.1186666666666667,
      "grad_norm": 0.04557332769036293,
      "learning_rate": 0.001965054225212146,
      "loss": 1.1312,
      "step": 839
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.05759608373045921,
      "learning_rate": 0.0019649409730077933,
      "loss": 1.1488,
      "step": 840
    },
    {
      "epoch": 1.1213333333333333,
      "grad_norm": 0.040716059505939484,
      "learning_rate": 0.0019648275408606338,
      "loss": 0.9685,
      "step": 841
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 0.15011951327323914,
      "learning_rate": 0.00196471392879182,
      "loss": 1.3555,
      "step": 842
    },
    {
      "epoch": 1.124,
      "grad_norm": 0.029683878645300865,
      "learning_rate": 0.001964600136822538,
      "loss": 1.1814,
      "step": 843
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 0.033196765929460526,
      "learning_rate": 0.0019644861649740085,
      "loss": 1.3367,
      "step": 844
    },
    {
      "epoch": 1.1266666666666667,
      "grad_norm": 0.04839568957686424,
      "learning_rate": 0.0019643720132674855,
      "loss": 1.2071,
      "step": 845
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.04349754750728607,
      "learning_rate": 0.001964257681724255,
      "loss": 1.0548,
      "step": 846
    },
    {
      "epoch": 1.1293333333333333,
      "grad_norm": 0.038985058665275574,
      "learning_rate": 0.001964143170365638,
      "loss": 1.1327,
      "step": 847
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 0.04016222059726715,
      "learning_rate": 0.0019640284792129888,
      "loss": 1.2203,
      "step": 848
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 0.0388663075864315,
      "learning_rate": 0.0019639136082876952,
      "loss": 1.0207,
      "step": 849
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.05538376420736313,
      "learning_rate": 0.001963798557611178,
      "loss": 1.2075,
      "step": 850
    },
    {
      "epoch": 1.1346666666666667,
      "grad_norm": 0.05111375451087952,
      "learning_rate": 0.001963683327204892,
      "loss": 1.2315,
      "step": 851
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.07650972157716751,
      "learning_rate": 0.001963567917090326,
      "loss": 1.5605,
      "step": 852
    },
    {
      "epoch": 1.1373333333333333,
      "grad_norm": 0.034140679985284805,
      "learning_rate": 0.001963452327289001,
      "loss": 0.9406,
      "step": 853
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.06674176454544067,
      "learning_rate": 0.0019633365578224727,
      "loss": 1.097,
      "step": 854
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.043116141110658646,
      "learning_rate": 0.0019632206087123296,
      "loss": 1.2446,
      "step": 855
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 0.042417071759700775,
      "learning_rate": 0.0019631044799801943,
      "loss": 1.3021,
      "step": 856
    },
    {
      "epoch": 1.1426666666666667,
      "grad_norm": 0.29406219720840454,
      "learning_rate": 0.0019629881716477222,
      "loss": 1.6659,
      "step": 857
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.040381766855716705,
      "learning_rate": 0.0019628716837366026,
      "loss": 1.0983,
      "step": 858
    },
    {
      "epoch": 1.1453333333333333,
      "grad_norm": 0.0932982787489891,
      "learning_rate": 0.0019627550162685586,
      "loss": 1.1384,
      "step": 859
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.1289646178483963,
      "learning_rate": 0.001962638169265346,
      "loss": 1.2801,
      "step": 860
    },
    {
      "epoch": 1.148,
      "grad_norm": 0.06896897405385971,
      "learning_rate": 0.0019625211427487547,
      "loss": 1.2081,
      "step": 861
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 0.04520745202898979,
      "learning_rate": 0.001962403936740608,
      "loss": 1.3,
      "step": 862
    },
    {
      "epoch": 1.1506666666666667,
      "grad_norm": 0.057265505194664,
      "learning_rate": 0.001962286551262762,
      "loss": 1.1457,
      "step": 863
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.06256968528032303,
      "learning_rate": 0.001962168986337108,
      "loss": 1.2759,
      "step": 864
    },
    {
      "epoch": 1.1533333333333333,
      "grad_norm": 0.23656372725963593,
      "learning_rate": 0.001962051241985568,
      "loss": 1.2796,
      "step": 865
    },
    {
      "epoch": 1.1546666666666667,
      "grad_norm": 0.03691737353801727,
      "learning_rate": 0.0019619333182301006,
      "loss": 1.1457,
      "step": 866
    },
    {
      "epoch": 1.156,
      "grad_norm": 0.04464427009224892,
      "learning_rate": 0.0019618152150926954,
      "loss": 1.2173,
      "step": 867
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 0.2768774926662445,
      "learning_rate": 0.0019616969325953765,
      "loss": 1.1512,
      "step": 868
    },
    {
      "epoch": 1.1586666666666667,
      "grad_norm": 0.12546859681606293,
      "learning_rate": 0.001961578470760201,
      "loss": 1.3206,
      "step": 869
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.04840635880827904,
      "learning_rate": 0.0019614598296092602,
      "loss": 1.0681,
      "step": 870
    },
    {
      "epoch": 1.1613333333333333,
      "grad_norm": 0.03620665520429611,
      "learning_rate": 0.001961341009164678,
      "loss": 0.9532,
      "step": 871
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 0.050271548330783844,
      "learning_rate": 0.0019612220094486123,
      "loss": 1.1742,
      "step": 872
    },
    {
      "epoch": 1.164,
      "grad_norm": 0.05934023857116699,
      "learning_rate": 0.0019611028304832544,
      "loss": 1.145,
      "step": 873
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 0.11515268683433533,
      "learning_rate": 0.001960983472290829,
      "loss": 1.4425,
      "step": 874
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.07152783125638962,
      "learning_rate": 0.0019608639348935937,
      "loss": 1.2687,
      "step": 875
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.041697029024362564,
      "learning_rate": 0.00196074421831384,
      "loss": 1.3197,
      "step": 876
    },
    {
      "epoch": 1.1693333333333333,
      "grad_norm": 0.03429299220442772,
      "learning_rate": 0.0019606243225738927,
      "loss": 1.1547,
      "step": 877
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.044424984604120255,
      "learning_rate": 0.00196050424769611,
      "loss": 1.1934,
      "step": 878
    },
    {
      "epoch": 1.172,
      "grad_norm": 0.37057796120643616,
      "learning_rate": 0.001960383993702884,
      "loss": 1.076,
      "step": 879
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.037166111171245575,
      "learning_rate": 0.001960263560616639,
      "loss": 1.2215,
      "step": 880
    },
    {
      "epoch": 1.1746666666666667,
      "grad_norm": 0.11643952131271362,
      "learning_rate": 0.001960142948459834,
      "loss": 1.2489,
      "step": 881
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.338459849357605,
      "learning_rate": 0.0019600221572549604,
      "loss": 1.1654,
      "step": 882
    },
    {
      "epoch": 1.1773333333333333,
      "grad_norm": 0.04853470250964165,
      "learning_rate": 0.0019599011870245443,
      "loss": 1.1954,
      "step": 883
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 0.04907139390707016,
      "learning_rate": 0.001959780037791143,
      "loss": 1.3119,
      "step": 884
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.041177451610565186,
      "learning_rate": 0.0019596587095773495,
      "loss": 1.1804,
      "step": 885
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 0.07074406743049622,
      "learning_rate": 0.0019595372024057887,
      "loss": 1.1682,
      "step": 886
    },
    {
      "epoch": 1.1826666666666668,
      "grad_norm": 0.21728043258190155,
      "learning_rate": 0.0019594155162991193,
      "loss": 1.0897,
      "step": 887
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.20473513007164001,
      "learning_rate": 0.001959293651280034,
      "loss": 1.1523,
      "step": 888
    },
    {
      "epoch": 1.1853333333333333,
      "grad_norm": 0.07304471731185913,
      "learning_rate": 0.0019591716073712575,
      "loss": 1.0748,
      "step": 889
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.22039680182933807,
      "learning_rate": 0.001959049384595549,
      "loss": 1.0558,
      "step": 890
    },
    {
      "epoch": 1.188,
      "grad_norm": 0.040471937507390976,
      "learning_rate": 0.001958926982975701,
      "loss": 1.1499,
      "step": 891
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 0.0521916039288044,
      "learning_rate": 0.001958804402534538,
      "loss": 1.1655,
      "step": 892
    },
    {
      "epoch": 1.1906666666666668,
      "grad_norm": 0.07169001549482346,
      "learning_rate": 0.00195868164329492,
      "loss": 1.2261,
      "step": 893
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.04304784536361694,
      "learning_rate": 0.0019585587052797387,
      "loss": 1.174,
      "step": 894
    },
    {
      "epoch": 1.1933333333333334,
      "grad_norm": 0.034983307123184204,
      "learning_rate": 0.0019584355885119194,
      "loss": 1.2627,
      "step": 895
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 0.04538155719637871,
      "learning_rate": 0.001958312293014422,
      "loss": 0.977,
      "step": 896
    },
    {
      "epoch": 1.196,
      "grad_norm": 0.038281772285699844,
      "learning_rate": 0.0019581888188102375,
      "loss": 1.1638,
      "step": 897
    },
    {
      "epoch": 1.1973333333333334,
      "grad_norm": 0.04049857705831528,
      "learning_rate": 0.0019580651659223923,
      "loss": 1.1048,
      "step": 898
    },
    {
      "epoch": 1.1986666666666665,
      "grad_norm": 0.05089913681149483,
      "learning_rate": 0.0019579413343739447,
      "loss": 1.1091,
      "step": 899
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.032131582498550415,
      "learning_rate": 0.001957817324187987,
      "loss": 1.1479,
      "step": 900
    },
    {
      "epoch": 1.2013333333333334,
      "grad_norm": 0.04219766706228256,
      "learning_rate": 0.0019576931353876455,
      "loss": 1.0884,
      "step": 901
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.034969180822372437,
      "learning_rate": 0.0019575687679960776,
      "loss": 1.3627,
      "step": 902
    },
    {
      "epoch": 1.204,
      "grad_norm": 0.032337967306375504,
      "learning_rate": 0.0019574442220364765,
      "loss": 0.9053,
      "step": 903
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 0.03637808933854103,
      "learning_rate": 0.001957319497532067,
      "loss": 1.3538,
      "step": 904
    },
    {
      "epoch": 1.2066666666666666,
      "grad_norm": 0.039203744381666183,
      "learning_rate": 0.0019571945945061086,
      "loss": 1.2707,
      "step": 905
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.036725353449583054,
      "learning_rate": 0.0019570695129818927,
      "loss": 1.07,
      "step": 906
    },
    {
      "epoch": 1.2093333333333334,
      "grad_norm": 0.03589681163430214,
      "learning_rate": 0.0019569442529827445,
      "loss": 1.0078,
      "step": 907
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 0.052406907081604004,
      "learning_rate": 0.0019568188145320225,
      "loss": 1.4642,
      "step": 908
    },
    {
      "epoch": 1.212,
      "grad_norm": 0.030685564503073692,
      "learning_rate": 0.001956693197653119,
      "loss": 1.1603,
      "step": 909
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.08401060104370117,
      "learning_rate": 0.0019565674023694587,
      "loss": 1.2702,
      "step": 910
    },
    {
      "epoch": 1.2146666666666666,
      "grad_norm": 0.0396094024181366,
      "learning_rate": 0.0019564414287045005,
      "loss": 1.0741,
      "step": 911
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.053863316774368286,
      "learning_rate": 0.0019563152766817354,
      "loss": 0.7975,
      "step": 912
    },
    {
      "epoch": 1.2173333333333334,
      "grad_norm": 0.043405681848526,
      "learning_rate": 0.0019561889463246885,
      "loss": 1.1493,
      "step": 913
    },
    {
      "epoch": 1.2186666666666666,
      "grad_norm": 0.02939494140446186,
      "learning_rate": 0.0019560624376569187,
      "loss": 1.1243,
      "step": 914
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.043171703815460205,
      "learning_rate": 0.001955935750702016,
      "loss": 1.1387,
      "step": 915
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 0.03859351575374603,
      "learning_rate": 0.0019558088854836064,
      "loss": 0.8999,
      "step": 916
    },
    {
      "epoch": 1.2226666666666666,
      "grad_norm": 0.04007256403565407,
      "learning_rate": 0.001955681842025347,
      "loss": 1.2114,
      "step": 917
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.030193137004971504,
      "learning_rate": 0.0019555546203509295,
      "loss": 1.3066,
      "step": 918
    },
    {
      "epoch": 1.2253333333333334,
      "grad_norm": 0.043922893702983856,
      "learning_rate": 0.0019554272204840776,
      "loss": 0.9868,
      "step": 919
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.028725994750857353,
      "learning_rate": 0.00195529964244855,
      "loss": 1.2071,
      "step": 920
    },
    {
      "epoch": 1.228,
      "grad_norm": 0.037407130002975464,
      "learning_rate": 0.001955171886268136,
      "loss": 1.2809,
      "step": 921
    },
    {
      "epoch": 1.2293333333333334,
      "grad_norm": 0.03820015862584114,
      "learning_rate": 0.001955043951966661,
      "loss": 1.3424,
      "step": 922
    },
    {
      "epoch": 1.2306666666666666,
      "grad_norm": 0.026921585202217102,
      "learning_rate": 0.0019549158395679818,
      "loss": 0.9847,
      "step": 923
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.06583460420370102,
      "learning_rate": 0.0019547875490959882,
      "loss": 1.3747,
      "step": 924
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 0.032380588352680206,
      "learning_rate": 0.0019546590805746052,
      "loss": 0.9501,
      "step": 925
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.058085694909095764,
      "learning_rate": 0.001954530434027789,
      "loss": 1.2671,
      "step": 926
    },
    {
      "epoch": 1.236,
      "grad_norm": 0.025331571698188782,
      "learning_rate": 0.0019544016094795295,
      "loss": 0.9797,
      "step": 927
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 0.034199487417936325,
      "learning_rate": 0.0019542726069538504,
      "loss": 1.1158,
      "step": 928
    },
    {
      "epoch": 1.2386666666666666,
      "grad_norm": 0.03000166453421116,
      "learning_rate": 0.0019541434264748075,
      "loss": 1.1205,
      "step": 929
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.09325883537530899,
      "learning_rate": 0.0019540140680664913,
      "loss": 1.4931,
      "step": 930
    },
    {
      "epoch": 1.2413333333333334,
      "grad_norm": 0.035580094903707504,
      "learning_rate": 0.0019538845317530243,
      "loss": 0.9834,
      "step": 931
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 0.032512106001377106,
      "learning_rate": 0.0019537548175585623,
      "loss": 1.1614,
      "step": 932
    },
    {
      "epoch": 1.244,
      "grad_norm": 0.030432922765612602,
      "learning_rate": 0.0019536249255072947,
      "loss": 1.3405,
      "step": 933
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 0.05341111123561859,
      "learning_rate": 0.001953494855623444,
      "loss": 1.1252,
      "step": 934
    },
    {
      "epoch": 1.2466666666666666,
      "grad_norm": 0.058438364416360855,
      "learning_rate": 0.0019533646079312656,
      "loss": 1.1084,
      "step": 935
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.08664575219154358,
      "learning_rate": 0.001953234182455048,
      "loss": 1.4712,
      "step": 936
    },
    {
      "epoch": 1.2493333333333334,
      "grad_norm": 0.03525920212268829,
      "learning_rate": 0.0019531035792191126,
      "loss": 1.2232,
      "step": 937
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 0.059603240340948105,
      "learning_rate": 0.0019529727982478154,
      "loss": 1.0196,
      "step": 938
    },
    {
      "epoch": 1.252,
      "grad_norm": 0.043205082416534424,
      "learning_rate": 0.0019528418395655441,
      "loss": 1.1563,
      "step": 939
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.040585629642009735,
      "learning_rate": 0.0019527107031967197,
      "loss": 1.179,
      "step": 940
    },
    {
      "epoch": 1.2546666666666666,
      "grad_norm": 0.07096292078495026,
      "learning_rate": 0.001952579389165797,
      "loss": 1.2439,
      "step": 941
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.29361552000045776,
      "learning_rate": 0.001952447897497263,
      "loss": 1.3428,
      "step": 942
    },
    {
      "epoch": 1.2573333333333334,
      "grad_norm": 0.04388132318854332,
      "learning_rate": 0.0019523162282156388,
      "loss": 1.4457,
      "step": 943
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 0.06553230434656143,
      "learning_rate": 0.001952184381345478,
      "loss": 1.2623,
      "step": 944
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.0411115437746048,
      "learning_rate": 0.0019520523569113678,
      "loss": 1.3557,
      "step": 945
    },
    {
      "epoch": 1.2613333333333334,
      "grad_norm": 0.07150384783744812,
      "learning_rate": 0.0019519201549379273,
      "loss": 1.2459,
      "step": 946
    },
    {
      "epoch": 1.2626666666666666,
      "grad_norm": 0.10820503532886505,
      "learning_rate": 0.0019517877754498107,
      "loss": 1.0689,
      "step": 947
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.18272580206394196,
      "learning_rate": 0.0019516552184717037,
      "loss": 1.227,
      "step": 948
    },
    {
      "epoch": 1.2653333333333334,
      "grad_norm": 0.04019171744585037,
      "learning_rate": 0.0019515224840283255,
      "loss": 1.486,
      "step": 949
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.035130444914102554,
      "learning_rate": 0.0019513895721444286,
      "loss": 1.308,
      "step": 950
    },
    {
      "epoch": 1.268,
      "grad_norm": 0.06054501608014107,
      "learning_rate": 0.0019512564828447988,
      "loss": 1.2732,
      "step": 951
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 0.05069601163268089,
      "learning_rate": 0.0019511232161542541,
      "loss": 1.0948,
      "step": 952
    },
    {
      "epoch": 1.2706666666666666,
      "grad_norm": 0.12024369090795517,
      "learning_rate": 0.0019509897720976466,
      "loss": 1.2654,
      "step": 953
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.03756503760814667,
      "learning_rate": 0.001950856150699861,
      "loss": 1.1607,
      "step": 954
    },
    {
      "epoch": 1.2733333333333334,
      "grad_norm": 0.06278540939092636,
      "learning_rate": 0.0019507223519858146,
      "loss": 1.4081,
      "step": 955
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 0.051109179854393005,
      "learning_rate": 0.001950588375980459,
      "loss": 1.1908,
      "step": 956
    },
    {
      "epoch": 1.276,
      "grad_norm": 0.02911866270005703,
      "learning_rate": 0.0019504542227087778,
      "loss": 1.2778,
      "step": 957
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.041811563074588776,
      "learning_rate": 0.001950319892195788,
      "loss": 1.187,
      "step": 958
    },
    {
      "epoch": 1.2786666666666666,
      "grad_norm": 0.05014164373278618,
      "learning_rate": 0.0019501853844665399,
      "loss": 1.1465,
      "step": 959
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.036165930330753326,
      "learning_rate": 0.0019500506995461157,
      "loss": 1.094,
      "step": 960
    },
    {
      "epoch": 1.2813333333333334,
      "grad_norm": 0.033391740173101425,
      "learning_rate": 0.0019499158374596327,
      "loss": 0.868,
      "step": 961
    },
    {
      "epoch": 1.2826666666666666,
      "grad_norm": 0.030474362894892693,
      "learning_rate": 0.0019497807982322393,
      "loss": 1.04,
      "step": 962
    },
    {
      "epoch": 1.284,
      "grad_norm": 0.03946472704410553,
      "learning_rate": 0.0019496455818891181,
      "loss": 1.0843,
      "step": 963
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 0.038722872734069824,
      "learning_rate": 0.0019495101884554839,
      "loss": 1.244,
      "step": 964
    },
    {
      "epoch": 1.2866666666666666,
      "grad_norm": 0.03766161948442459,
      "learning_rate": 0.0019493746179565852,
      "loss": 1.3545,
      "step": 965
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.034053198993206024,
      "learning_rate": 0.0019492388704177035,
      "loss": 0.9666,
      "step": 966
    },
    {
      "epoch": 1.2893333333333334,
      "grad_norm": 0.04746085777878761,
      "learning_rate": 0.0019491029458641527,
      "loss": 1.3431,
      "step": 967
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 0.03471178933978081,
      "learning_rate": 0.0019489668443212805,
      "loss": 1.1371,
      "step": 968
    },
    {
      "epoch": 1.292,
      "grad_norm": 0.03200376406311989,
      "learning_rate": 0.0019488305658144666,
      "loss": 1.2418,
      "step": 969
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.03146272525191307,
      "learning_rate": 0.0019486941103691246,
      "loss": 1.3092,
      "step": 970
    },
    {
      "epoch": 1.2946666666666666,
      "grad_norm": 0.033365488052368164,
      "learning_rate": 0.0019485574780107014,
      "loss": 1.2712,
      "step": 971
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.03075232543051243,
      "learning_rate": 0.0019484206687646753,
      "loss": 1.2443,
      "step": 972
    },
    {
      "epoch": 1.2973333333333334,
      "grad_norm": 0.051488567143678665,
      "learning_rate": 0.0019482836826565594,
      "loss": 1.1158,
      "step": 973
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.030538994818925858,
      "learning_rate": 0.0019481465197118983,
      "loss": 1.1642,
      "step": 974
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.038301169872283936,
      "learning_rate": 0.0019480091799562705,
      "loss": 1.359,
      "step": 975
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 0.06107578054070473,
      "learning_rate": 0.0019478716634152872,
      "loss": 0.9968,
      "step": 976
    },
    {
      "epoch": 1.3026666666666666,
      "grad_norm": 0.0440947525203228,
      "learning_rate": 0.001947733970114593,
      "loss": 1.2118,
      "step": 977
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.02929622121155262,
      "learning_rate": 0.0019475961000798643,
      "loss": 0.9592,
      "step": 978
    },
    {
      "epoch": 1.3053333333333335,
      "grad_norm": 0.049520161002874374,
      "learning_rate": 0.0019474580533368115,
      "loss": 1.2133,
      "step": 979
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.026915408670902252,
      "learning_rate": 0.0019473198299111778,
      "loss": 1.25,
      "step": 980
    },
    {
      "epoch": 1.308,
      "grad_norm": 0.03466947004199028,
      "learning_rate": 0.0019471814298287389,
      "loss": 1.3719,
      "step": 981
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.037364665418863297,
      "learning_rate": 0.0019470428531153038,
      "loss": 1.0986,
      "step": 982
    },
    {
      "epoch": 1.3106666666666666,
      "grad_norm": 0.04226904362440109,
      "learning_rate": 0.001946904099796715,
      "loss": 1.3798,
      "step": 983
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.03864710405468941,
      "learning_rate": 0.0019467651698988461,
      "loss": 1.3063,
      "step": 984
    },
    {
      "epoch": 1.3133333333333335,
      "grad_norm": 0.048180874437093735,
      "learning_rate": 0.001946626063447606,
      "loss": 0.9279,
      "step": 985
    },
    {
      "epoch": 1.3146666666666667,
      "grad_norm": 0.04228246212005615,
      "learning_rate": 0.0019464867804689348,
      "loss": 1.1569,
      "step": 986
    },
    {
      "epoch": 1.316,
      "grad_norm": 0.02623417228460312,
      "learning_rate": 0.001946347320988806,
      "loss": 1.225,
      "step": 987
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 0.047404587268829346,
      "learning_rate": 0.0019462076850332265,
      "loss": 1.1843,
      "step": 988
    },
    {
      "epoch": 1.3186666666666667,
      "grad_norm": 0.03544633090496063,
      "learning_rate": 0.001946067872628235,
      "loss": 1.2119,
      "step": 989
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.05128188803792,
      "learning_rate": 0.0019459278837999046,
      "loss": 1.449,
      "step": 990
    },
    {
      "epoch": 1.3213333333333335,
      "grad_norm": 0.030097145587205887,
      "learning_rate": 0.0019457877185743403,
      "loss": 1.1071,
      "step": 991
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 0.031008301302790642,
      "learning_rate": 0.0019456473769776795,
      "loss": 1.2859,
      "step": 992
    },
    {
      "epoch": 1.324,
      "grad_norm": 0.034020811319351196,
      "learning_rate": 0.0019455068590360943,
      "loss": 1.0749,
      "step": 993
    },
    {
      "epoch": 1.3253333333333333,
      "grad_norm": 0.03134119510650635,
      "learning_rate": 0.0019453661647757877,
      "loss": 1.0457,
      "step": 994
    },
    {
      "epoch": 1.3266666666666667,
      "grad_norm": 0.04797649011015892,
      "learning_rate": 0.001945225294222997,
      "loss": 1.3599,
      "step": 995
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.0311111006885767,
      "learning_rate": 0.0019450842474039913,
      "loss": 1.29,
      "step": 996
    },
    {
      "epoch": 1.3293333333333333,
      "grad_norm": 0.03706010803580284,
      "learning_rate": 0.0019449430243450737,
      "loss": 1.1249,
      "step": 997
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.038652338087558746,
      "learning_rate": 0.0019448016250725791,
      "loss": 0.921,
      "step": 998
    },
    {
      "epoch": 1.332,
      "grad_norm": 0.034926656633615494,
      "learning_rate": 0.0019446600496128758,
      "loss": 1.1532,
      "step": 999
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.027791153639554977,
      "learning_rate": 0.0019445182979923655,
      "loss": 1.1127,
      "step": 1000
    },
    {
      "epoch": 1.3346666666666667,
      "grad_norm": 0.0369306243956089,
      "learning_rate": 0.0019443763702374811,
      "loss": 1.2877,
      "step": 1001
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.03968014568090439,
      "learning_rate": 0.00194423426637469,
      "loss": 1.1902,
      "step": 1002
    },
    {
      "epoch": 1.3373333333333333,
      "grad_norm": 0.041777413338422775,
      "learning_rate": 0.0019440919864304918,
      "loss": 1.1582,
      "step": 1003
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 0.03023817017674446,
      "learning_rate": 0.0019439495304314188,
      "loss": 1.1592,
      "step": 1004
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.03844117000699043,
      "learning_rate": 0.0019438068984040365,
      "loss": 1.0502,
      "step": 1005
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.033100344240665436,
      "learning_rate": 0.0019436640903749427,
      "loss": 1.2267,
      "step": 1006
    },
    {
      "epoch": 1.3426666666666667,
      "grad_norm": 0.03463374078273773,
      "learning_rate": 0.0019435211063707687,
      "loss": 1.0481,
      "step": 1007
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.053173381835222244,
      "learning_rate": 0.0019433779464181778,
      "loss": 1.2908,
      "step": 1008
    },
    {
      "epoch": 1.3453333333333333,
      "grad_norm": 0.03280322253704071,
      "learning_rate": 0.001943234610543867,
      "loss": 0.8925,
      "step": 1009
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.025117285549640656,
      "learning_rate": 0.0019430910987745654,
      "loss": 1.1763,
      "step": 1010
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 0.0350940078496933,
      "learning_rate": 0.001942947411137035,
      "loss": 1.1532,
      "step": 1011
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 0.03816450014710426,
      "learning_rate": 0.0019428035476580713,
      "loss": 1.0659,
      "step": 1012
    },
    {
      "epoch": 1.3506666666666667,
      "grad_norm": 0.031671345233917236,
      "learning_rate": 0.0019426595083645015,
      "loss": 1.4603,
      "step": 1013
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.08946898579597473,
      "learning_rate": 0.001942515293283187,
      "loss": 1.324,
      "step": 1014
    },
    {
      "epoch": 1.3533333333333333,
      "grad_norm": 0.03596218675374985,
      "learning_rate": 0.0019423709024410196,
      "loss": 1.2267,
      "step": 1015
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 0.04250172898173332,
      "learning_rate": 0.0019422263358649268,
      "loss": 1.0925,
      "step": 1016
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 0.06566435098648071,
      "learning_rate": 0.0019420815935818673,
      "loss": 1.4852,
      "step": 1017
    },
    {
      "epoch": 1.3573333333333333,
      "grad_norm": 0.03196272999048233,
      "learning_rate": 0.0019419366756188317,
      "loss": 1.0055,
      "step": 1018
    },
    {
      "epoch": 1.3586666666666667,
      "grad_norm": 0.036296598613262177,
      "learning_rate": 0.0019417915820028457,
      "loss": 0.8954,
      "step": 1019
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.03516979143023491,
      "learning_rate": 0.0019416463127609656,
      "loss": 1.1029,
      "step": 1020
    },
    {
      "epoch": 1.3613333333333333,
      "grad_norm": 0.03265445679426193,
      "learning_rate": 0.0019415008679202815,
      "loss": 1.2619,
      "step": 1021
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.04200386628508568,
      "learning_rate": 0.0019413552475079161,
      "loss": 1.0716,
      "step": 1022
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 0.03689655289053917,
      "learning_rate": 0.0019412094515510248,
      "loss": 1.3979,
      "step": 1023
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 0.03170574828982353,
      "learning_rate": 0.0019410634800767958,
      "loss": 1.1837,
      "step": 1024
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.04890434443950653,
      "learning_rate": 0.0019409173331124499,
      "loss": 0.9876,
      "step": 1025
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.033238139003515244,
      "learning_rate": 0.0019407710106852402,
      "loss": 1.2522,
      "step": 1026
    },
    {
      "epoch": 1.3693333333333333,
      "grad_norm": 0.03388666361570358,
      "learning_rate": 0.001940624512822454,
      "loss": 0.9276,
      "step": 1027
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 0.03537699580192566,
      "learning_rate": 0.0019404778395514094,
      "loss": 1.0985,
      "step": 1028
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 0.04850810021162033,
      "learning_rate": 0.0019403309908994587,
      "loss": 1.1092,
      "step": 1029
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.24810157716274261,
      "learning_rate": 0.001940183966893986,
      "loss": 1.0112,
      "step": 1030
    },
    {
      "epoch": 1.3746666666666667,
      "grad_norm": 0.03380877152085304,
      "learning_rate": 0.0019400367675624084,
      "loss": 0.9848,
      "step": 1031
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.036490194499492645,
      "learning_rate": 0.0019398893929321761,
      "loss": 1.3524,
      "step": 1032
    },
    {
      "epoch": 1.3773333333333333,
      "grad_norm": 0.044986024498939514,
      "learning_rate": 0.0019397418430307713,
      "loss": 1.2282,
      "step": 1033
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 0.10466448217630386,
      "learning_rate": 0.0019395941178857092,
      "loss": 1.2598,
      "step": 1034
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.04024912044405937,
      "learning_rate": 0.001939446217524538,
      "loss": 1.1575,
      "step": 1035
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 0.2530352473258972,
      "learning_rate": 0.0019392981419748376,
      "loss": 1.143,
      "step": 1036
    },
    {
      "epoch": 1.3826666666666667,
      "grad_norm": 0.03773501142859459,
      "learning_rate": 0.001939149891264222,
      "loss": 1.1443,
      "step": 1037
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.0643220916390419,
      "learning_rate": 0.0019390014654203367,
      "loss": 1.1808,
      "step": 1038
    },
    {
      "epoch": 1.3853333333333333,
      "grad_norm": 0.03536034747958183,
      "learning_rate": 0.0019388528644708602,
      "loss": 1.1469,
      "step": 1039
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.06130973622202873,
      "learning_rate": 0.0019387040884435037,
      "loss": 1.109,
      "step": 1040
    },
    {
      "epoch": 1.388,
      "grad_norm": 0.1922924816608429,
      "learning_rate": 0.0019385551373660112,
      "loss": 1.5175,
      "step": 1041
    },
    {
      "epoch": 1.3893333333333333,
      "grad_norm": 0.048888418823480606,
      "learning_rate": 0.001938406011266159,
      "loss": 1.3275,
      "step": 1042
    },
    {
      "epoch": 1.3906666666666667,
      "grad_norm": 0.07075276225805283,
      "learning_rate": 0.0019382567101717564,
      "loss": 1.1408,
      "step": 1043
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.06521464884281158,
      "learning_rate": 0.0019381072341106453,
      "loss": 1.248,
      "step": 1044
    },
    {
      "epoch": 1.3933333333333333,
      "grad_norm": 0.051839422434568405,
      "learning_rate": 0.0019379575831106994,
      "loss": 1.2363,
      "step": 1045
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.042511679232120514,
      "learning_rate": 0.0019378077571998264,
      "loss": 1.2944,
      "step": 1046
    },
    {
      "epoch": 1.396,
      "grad_norm": 0.07609397917985916,
      "learning_rate": 0.001937657756405966,
      "loss": 1.1261,
      "step": 1047
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 0.06251794099807739,
      "learning_rate": 0.00193750758075709,
      "loss": 1.5067,
      "step": 1048
    },
    {
      "epoch": 1.3986666666666667,
      "grad_norm": 0.04129412770271301,
      "learning_rate": 0.0019373572302812034,
      "loss": 1.3242,
      "step": 1049
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.029453624039888382,
      "learning_rate": 0.0019372067050063438,
      "loss": 1.1113,
      "step": 1050
    },
    {
      "epoch": 1.4013333333333333,
      "grad_norm": 0.06899867951869965,
      "learning_rate": 0.001937056004960581,
      "loss": 1.1747,
      "step": 1051
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 0.047641996294260025,
      "learning_rate": 0.0019369051301720177,
      "loss": 1.2007,
      "step": 1052
    },
    {
      "epoch": 1.404,
      "grad_norm": 0.05185546725988388,
      "learning_rate": 0.0019367540806687893,
      "loss": 1.1742,
      "step": 1053
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.031130235642194748,
      "learning_rate": 0.0019366028564790634,
      "loss": 1.207,
      "step": 1054
    },
    {
      "epoch": 1.4066666666666667,
      "grad_norm": 0.03203819692134857,
      "learning_rate": 0.0019364514576310408,
      "loss": 1.2248,
      "step": 1055
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.033899154514074326,
      "learning_rate": 0.001936299884152954,
      "loss": 1.324,
      "step": 1056
    },
    {
      "epoch": 1.4093333333333333,
      "grad_norm": 0.03956972062587738,
      "learning_rate": 0.0019361481360730686,
      "loss": 1.2148,
      "step": 1057
    },
    {
      "epoch": 1.4106666666666667,
      "grad_norm": 0.04931002855300903,
      "learning_rate": 0.0019359962134196827,
      "loss": 1.1771,
      "step": 1058
    },
    {
      "epoch": 1.412,
      "grad_norm": 0.03282837197184563,
      "learning_rate": 0.0019358441162211268,
      "loss": 1.4793,
      "step": 1059
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.02552546001970768,
      "learning_rate": 0.0019356918445057648,
      "loss": 1.3109,
      "step": 1060
    },
    {
      "epoch": 1.4146666666666667,
      "grad_norm": 0.02982131391763687,
      "learning_rate": 0.0019355393983019912,
      "loss": 1.1649,
      "step": 1061
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.034890398383140564,
      "learning_rate": 0.0019353867776382354,
      "loss": 1.1318,
      "step": 1062
    },
    {
      "epoch": 1.4173333333333333,
      "grad_norm": 0.036309607326984406,
      "learning_rate": 0.001935233982542958,
      "loss": 1.254,
      "step": 1063
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 0.027689358219504356,
      "learning_rate": 0.0019350810130446515,
      "loss": 1.1954,
      "step": 1064
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.0335623137652874,
      "learning_rate": 0.0019349278691718427,
      "loss": 1.315,
      "step": 1065
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 0.03918091207742691,
      "learning_rate": 0.0019347745509530894,
      "loss": 1.3466,
      "step": 1066
    },
    {
      "epoch": 1.4226666666666667,
      "grad_norm": 0.04572087153792381,
      "learning_rate": 0.0019346210584169827,
      "loss": 0.8661,
      "step": 1067
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.03770998865365982,
      "learning_rate": 0.0019344673915921461,
      "loss": 1.2371,
      "step": 1068
    },
    {
      "epoch": 1.4253333333333333,
      "grad_norm": 0.03576504811644554,
      "learning_rate": 0.0019343135505072349,
      "loss": 1.3754,
      "step": 1069
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.03012077324092388,
      "learning_rate": 0.0019341595351909384,
      "loss": 1.3323,
      "step": 1070
    },
    {
      "epoch": 1.428,
      "grad_norm": 0.0364651121199131,
      "learning_rate": 0.0019340053456719769,
      "loss": 0.9957,
      "step": 1071
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 0.033979207277297974,
      "learning_rate": 0.0019338509819791037,
      "loss": 0.9219,
      "step": 1072
    },
    {
      "epoch": 1.4306666666666668,
      "grad_norm": 0.05296367406845093,
      "learning_rate": 0.001933696444141105,
      "loss": 1.309,
      "step": 1073
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.029917173087596893,
      "learning_rate": 0.0019335417321867988,
      "loss": 1.3027,
      "step": 1074
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.15252470970153809,
      "learning_rate": 0.0019333868461450358,
      "loss": 1.1495,
      "step": 1075
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 0.04634843021631241,
      "learning_rate": 0.0019332317860446997,
      "loss": 1.3094,
      "step": 1076
    },
    {
      "epoch": 1.436,
      "grad_norm": 0.029510192573070526,
      "learning_rate": 0.0019330765519147057,
      "loss": 0.8929,
      "step": 1077
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 0.03733333945274353,
      "learning_rate": 0.0019329211437840025,
      "loss": 0.9797,
      "step": 1078
    },
    {
      "epoch": 1.4386666666666668,
      "grad_norm": 0.04122123494744301,
      "learning_rate": 0.0019327655616815705,
      "loss": 1.34,
      "step": 1079
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.0380447693169117,
      "learning_rate": 0.0019326098056364222,
      "loss": 1.3913,
      "step": 1080
    },
    {
      "epoch": 1.4413333333333334,
      "grad_norm": 0.03763454034924507,
      "learning_rate": 0.001932453875677604,
      "loss": 1.037,
      "step": 1081
    },
    {
      "epoch": 1.4426666666666668,
      "grad_norm": 0.09115712344646454,
      "learning_rate": 0.0019322977718341933,
      "loss": 1.1692,
      "step": 1082
    },
    {
      "epoch": 1.444,
      "grad_norm": 0.03941146284341812,
      "learning_rate": 0.0019321414941353005,
      "loss": 1.1363,
      "step": 1083
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 0.09977391362190247,
      "learning_rate": 0.001931985042610068,
      "loss": 1.1467,
      "step": 1084
    },
    {
      "epoch": 1.4466666666666668,
      "grad_norm": 0.025791233405470848,
      "learning_rate": 0.0019318284172876719,
      "loss": 1.3123,
      "step": 1085
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.03638454154133797,
      "learning_rate": 0.001931671618197319,
      "loss": 0.8348,
      "step": 1086
    },
    {
      "epoch": 1.4493333333333334,
      "grad_norm": 0.03779686242341995,
      "learning_rate": 0.0019315146453682496,
      "loss": 1.2207,
      "step": 1087
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 0.036933474242687225,
      "learning_rate": 0.0019313574988297358,
      "loss": 1.2993,
      "step": 1088
    },
    {
      "epoch": 1.452,
      "grad_norm": 0.06168394163250923,
      "learning_rate": 0.0019312001786110828,
      "loss": 1.434,
      "step": 1089
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.05353248119354248,
      "learning_rate": 0.0019310426847416275,
      "loss": 1.2748,
      "step": 1090
    },
    {
      "epoch": 1.4546666666666668,
      "grad_norm": 0.06427458673715591,
      "learning_rate": 0.0019308850172507397,
      "loss": 1.3133,
      "step": 1091
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.03762117400765419,
      "learning_rate": 0.0019307271761678214,
      "loss": 1.1852,
      "step": 1092
    },
    {
      "epoch": 1.4573333333333334,
      "grad_norm": 0.039141394197940826,
      "learning_rate": 0.0019305691615223064,
      "loss": 1.3403,
      "step": 1093
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.02479461021721363,
      "learning_rate": 0.0019304109733436616,
      "loss": 1.0433,
      "step": 1094
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.038292624056339264,
      "learning_rate": 0.0019302526116613864,
      "loss": 1.37,
      "step": 1095
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 0.03812011331319809,
      "learning_rate": 0.0019300940765050113,
      "loss": 1.5786,
      "step": 1096
    },
    {
      "epoch": 1.4626666666666668,
      "grad_norm": 0.047531358897686005,
      "learning_rate": 0.001929935367904101,
      "loss": 0.9266,
      "step": 1097
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.06007589399814606,
      "learning_rate": 0.0019297764858882513,
      "loss": 1.0865,
      "step": 1098
    },
    {
      "epoch": 1.4653333333333334,
      "grad_norm": 0.03907125070691109,
      "learning_rate": 0.0019296174304870906,
      "loss": 1.1008,
      "step": 1099
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.03853308781981468,
      "learning_rate": 0.0019294582017302796,
      "loss": 1.3794,
      "step": 1100
    },
    {
      "epoch": 1.468,
      "grad_norm": 0.0294423159211874,
      "learning_rate": 0.001929298799647511,
      "loss": 1.2066,
      "step": 1101
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 0.042536910623311996,
      "learning_rate": 0.0019291392242685107,
      "loss": 0.9692,
      "step": 1102
    },
    {
      "epoch": 1.4706666666666668,
      "grad_norm": 0.06810896843671799,
      "learning_rate": 0.0019289794756230363,
      "loss": 1.6728,
      "step": 1103
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.03829686716198921,
      "learning_rate": 0.0019288195537408778,
      "loss": 1.3638,
      "step": 1104
    },
    {
      "epoch": 1.4733333333333334,
      "grad_norm": 0.03567301854491234,
      "learning_rate": 0.0019286594586518575,
      "loss": 1.3868,
      "step": 1105
    },
    {
      "epoch": 1.4746666666666668,
      "grad_norm": 0.032131630927324295,
      "learning_rate": 0.0019284991903858301,
      "loss": 1.2609,
      "step": 1106
    },
    {
      "epoch": 1.476,
      "grad_norm": 0.03357576206326485,
      "learning_rate": 0.0019283387489726826,
      "loss": 0.8685,
      "step": 1107
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 0.026589877903461456,
      "learning_rate": 0.0019281781344423342,
      "loss": 0.905,
      "step": 1108
    },
    {
      "epoch": 1.4786666666666668,
      "grad_norm": 0.027862684801220894,
      "learning_rate": 0.0019280173468247362,
      "loss": 1.062,
      "step": 1109
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.032053142786026,
      "learning_rate": 0.0019278563861498724,
      "loss": 1.2809,
      "step": 1110
    },
    {
      "epoch": 1.4813333333333334,
      "grad_norm": 0.06968472898006439,
      "learning_rate": 0.0019276952524477592,
      "loss": 1.0491,
      "step": 1111
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 0.030142201110720634,
      "learning_rate": 0.0019275339457484443,
      "loss": 0.964,
      "step": 1112
    },
    {
      "epoch": 1.484,
      "grad_norm": 0.05732380971312523,
      "learning_rate": 0.0019273724660820086,
      "loss": 1.129,
      "step": 1113
    },
    {
      "epoch": 1.4853333333333334,
      "grad_norm": 0.025522222742438316,
      "learning_rate": 0.0019272108134785655,
      "loss": 1.0828,
      "step": 1114
    },
    {
      "epoch": 1.4866666666666668,
      "grad_norm": 0.03420479968190193,
      "learning_rate": 0.001927048987968259,
      "loss": 1.2274,
      "step": 1115
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.03673067316412926,
      "learning_rate": 0.0019268869895812672,
      "loss": 1.3353,
      "step": 1116
    },
    {
      "epoch": 1.4893333333333334,
      "grad_norm": 0.03361811861395836,
      "learning_rate": 0.0019267248183477993,
      "loss": 1.0382,
      "step": 1117
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.036503035575151443,
      "learning_rate": 0.0019265624742980977,
      "loss": 1.2459,
      "step": 1118
    },
    {
      "epoch": 1.492,
      "grad_norm": 0.04614037647843361,
      "learning_rate": 0.0019263999574624356,
      "loss": 1.2337,
      "step": 1119
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.032770320773124695,
      "learning_rate": 0.0019262372678711195,
      "loss": 1.2042,
      "step": 1120
    },
    {
      "epoch": 1.4946666666666666,
      "grad_norm": 0.03707144409418106,
      "learning_rate": 0.001926074405554488,
      "loss": 1.2283,
      "step": 1121
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.03890113905072212,
      "learning_rate": 0.0019259113705429119,
      "loss": 1.1419,
      "step": 1122
    },
    {
      "epoch": 1.4973333333333334,
      "grad_norm": 0.04665500298142433,
      "learning_rate": 0.001925748162866794,
      "loss": 1.3614,
      "step": 1123
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 0.029024560004472733,
      "learning_rate": 0.0019255847825565687,
      "loss": 1.0714,
      "step": 1124
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.027847202494740486,
      "learning_rate": 0.0019254212296427042,
      "loss": 1.066,
      "step": 1125
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.036507509648799896,
      "learning_rate": 0.0019252575041556997,
      "loss": 1.2496,
      "step": 1126
    },
    {
      "epoch": 1.5026666666666668,
      "grad_norm": 0.037839457392692566,
      "learning_rate": 0.0019250936061260865,
      "loss": 1.2916,
      "step": 1127
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.031323038041591644,
      "learning_rate": 0.0019249295355844285,
      "loss": 1.1231,
      "step": 1128
    },
    {
      "epoch": 1.5053333333333332,
      "grad_norm": 0.0476144477725029,
      "learning_rate": 0.0019247652925613221,
      "loss": 1.2372,
      "step": 1129
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.03580286726355553,
      "learning_rate": 0.0019246008770873952,
      "loss": 1.2995,
      "step": 1130
    },
    {
      "epoch": 1.508,
      "grad_norm": 0.053861796855926514,
      "learning_rate": 0.0019244362891933076,
      "loss": 0.9273,
      "step": 1131
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 0.07648592442274094,
      "learning_rate": 0.0019242715289097526,
      "loss": 1.2355,
      "step": 1132
    },
    {
      "epoch": 1.5106666666666668,
      "grad_norm": 0.0389610230922699,
      "learning_rate": 0.0019241065962674541,
      "loss": 1.4008,
      "step": 1133
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.029886845499277115,
      "learning_rate": 0.0019239414912971696,
      "loss": 1.2381,
      "step": 1134
    },
    {
      "epoch": 1.5133333333333332,
      "grad_norm": 0.03296320512890816,
      "learning_rate": 0.0019237762140296873,
      "loss": 1.3392,
      "step": 1135
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 0.043230537325143814,
      "learning_rate": 0.0019236107644958283,
      "loss": 1.5222,
      "step": 1136
    },
    {
      "epoch": 1.516,
      "grad_norm": 0.036820344626903534,
      "learning_rate": 0.0019234451427264459,
      "loss": 1.2157,
      "step": 1137
    },
    {
      "epoch": 1.5173333333333332,
      "grad_norm": 0.038154181092977524,
      "learning_rate": 0.0019232793487524256,
      "loss": 1.2014,
      "step": 1138
    },
    {
      "epoch": 1.5186666666666668,
      "grad_norm": 0.038935672491788864,
      "learning_rate": 0.0019231133826046842,
      "loss": 1.3046,
      "step": 1139
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.08492407947778702,
      "learning_rate": 0.0019229472443141717,
      "loss": 0.9978,
      "step": 1140
    },
    {
      "epoch": 1.5213333333333332,
      "grad_norm": 0.03445436432957649,
      "learning_rate": 0.001922780933911869,
      "loss": 1.2715,
      "step": 1141
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.03141622617840767,
      "learning_rate": 0.0019226144514287906,
      "loss": 1.4054,
      "step": 1142
    },
    {
      "epoch": 1.524,
      "grad_norm": 0.03992561623454094,
      "learning_rate": 0.001922447796895982,
      "loss": 1.1322,
      "step": 1143
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 0.0334891602396965,
      "learning_rate": 0.0019222809703445206,
      "loss": 1.0604,
      "step": 1144
    },
    {
      "epoch": 1.5266666666666666,
      "grad_norm": 0.06263735145330429,
      "learning_rate": 0.001922113971805517,
      "loss": 1.1775,
      "step": 1145
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.08937094360589981,
      "learning_rate": 0.0019219468013101121,
      "loss": 1.0979,
      "step": 1146
    },
    {
      "epoch": 1.5293333333333332,
      "grad_norm": 0.0393209345638752,
      "learning_rate": 0.0019217794588894815,
      "loss": 1.2765,
      "step": 1147
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 0.06908867508172989,
      "learning_rate": 0.0019216119445748302,
      "loss": 1.2652,
      "step": 1148
    },
    {
      "epoch": 1.532,
      "grad_norm": 0.04567311704158783,
      "learning_rate": 0.0019214442583973966,
      "loss": 1.2852,
      "step": 1149
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.03834959492087364,
      "learning_rate": 0.001921276400388451,
      "loss": 1.2297,
      "step": 1150
    },
    {
      "epoch": 1.5346666666666666,
      "grad_norm": 0.06769412755966187,
      "learning_rate": 0.0019211083705792957,
      "loss": 1.2351,
      "step": 1151
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.0361933633685112,
      "learning_rate": 0.0019209401690012651,
      "loss": 1.1698,
      "step": 1152
    },
    {
      "epoch": 1.5373333333333332,
      "grad_norm": 0.039950497448444366,
      "learning_rate": 0.0019207717956857254,
      "loss": 1.3037,
      "step": 1153
    },
    {
      "epoch": 1.5386666666666666,
      "grad_norm": 0.0356692336499691,
      "learning_rate": 0.0019206032506640747,
      "loss": 1.4069,
      "step": 1154
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.030030788853764534,
      "learning_rate": 0.001920434533967744,
      "loss": 1.1806,
      "step": 1155
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 0.04032178595662117,
      "learning_rate": 0.0019202656456281952,
      "loss": 1.2436,
      "step": 1156
    },
    {
      "epoch": 1.5426666666666666,
      "grad_norm": 0.0379839763045311,
      "learning_rate": 0.001920096585676923,
      "loss": 1.1481,
      "step": 1157
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.05142031982541084,
      "learning_rate": 0.0019199273541454537,
      "loss": 1.4854,
      "step": 1158
    },
    {
      "epoch": 1.5453333333333332,
      "grad_norm": 0.04140099883079529,
      "learning_rate": 0.0019197579510653456,
      "loss": 1.2322,
      "step": 1159
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.03596137836575508,
      "learning_rate": 0.0019195883764681892,
      "loss": 1.121,
      "step": 1160
    },
    {
      "epoch": 1.548,
      "grad_norm": 0.029852155596017838,
      "learning_rate": 0.0019194186303856068,
      "loss": 0.9737,
      "step": 1161
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 0.04490175470709801,
      "learning_rate": 0.0019192487128492529,
      "loss": 1.0951,
      "step": 1162
    },
    {
      "epoch": 1.5506666666666666,
      "grad_norm": 0.03802970051765442,
      "learning_rate": 0.0019190786238908136,
      "loss": 1.1863,
      "step": 1163
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.033025212585926056,
      "learning_rate": 0.0019189083635420075,
      "loss": 1.5652,
      "step": 1164
    },
    {
      "epoch": 1.5533333333333332,
      "grad_norm": 0.03308124467730522,
      "learning_rate": 0.0019187379318345845,
      "loss": 1.0581,
      "step": 1165
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.04073697328567505,
      "learning_rate": 0.0019185673288003274,
      "loss": 1.2326,
      "step": 1166
    },
    {
      "epoch": 1.556,
      "grad_norm": 0.028796609491109848,
      "learning_rate": 0.0019183965544710495,
      "loss": 1.1706,
      "step": 1167
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 0.032067958265542984,
      "learning_rate": 0.0019182256088785977,
      "loss": 0.9782,
      "step": 1168
    },
    {
      "epoch": 1.5586666666666666,
      "grad_norm": 0.04135400429368019,
      "learning_rate": 0.0019180544920548495,
      "loss": 1.0871,
      "step": 1169
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.05263848230242729,
      "learning_rate": 0.0019178832040317154,
      "loss": 1.3031,
      "step": 1170
    },
    {
      "epoch": 1.5613333333333332,
      "grad_norm": 0.03904447704553604,
      "learning_rate": 0.0019177117448411366,
      "loss": 1.232,
      "step": 1171
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 0.03162356838583946,
      "learning_rate": 0.0019175401145150874,
      "loss": 1.2634,
      "step": 1172
    },
    {
      "epoch": 1.564,
      "grad_norm": 0.03250851482152939,
      "learning_rate": 0.0019173683130855738,
      "loss": 1.1357,
      "step": 1173
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.02929721400141716,
      "learning_rate": 0.0019171963405846327,
      "loss": 1.1657,
      "step": 1174
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.03873145207762718,
      "learning_rate": 0.0019170241970443342,
      "loss": 1.3201,
      "step": 1175
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.03734424710273743,
      "learning_rate": 0.0019168518824967797,
      "loss": 1.3253,
      "step": 1176
    },
    {
      "epoch": 1.5693333333333332,
      "grad_norm": 0.050296295434236526,
      "learning_rate": 0.0019166793969741024,
      "loss": 1.3498,
      "step": 1177
    },
    {
      "epoch": 1.5706666666666667,
      "grad_norm": 0.05395993962883949,
      "learning_rate": 0.0019165067405084672,
      "loss": 1.3635,
      "step": 1178
    },
    {
      "epoch": 1.572,
      "grad_norm": 0.08466081321239471,
      "learning_rate": 0.0019163339131320716,
      "loss": 1.1913,
      "step": 1179
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.040522653609514236,
      "learning_rate": 0.0019161609148771444,
      "loss": 1.4815,
      "step": 1180
    },
    {
      "epoch": 1.5746666666666667,
      "grad_norm": 0.031109554693102837,
      "learning_rate": 0.0019159877457759467,
      "loss": 0.9617,
      "step": 1181
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.05868156999349594,
      "learning_rate": 0.0019158144058607706,
      "loss": 1.4265,
      "step": 1182
    },
    {
      "epoch": 1.5773333333333333,
      "grad_norm": 0.039943210780620575,
      "learning_rate": 0.0019156408951639414,
      "loss": 1.078,
      "step": 1183
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 0.04538954421877861,
      "learning_rate": 0.0019154672137178148,
      "loss": 1.1227,
      "step": 1184
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.032892413437366486,
      "learning_rate": 0.0019152933615547796,
      "loss": 1.0505,
      "step": 1185
    },
    {
      "epoch": 1.5813333333333333,
      "grad_norm": 0.039090242236852646,
      "learning_rate": 0.0019151193387072555,
      "loss": 0.9823,
      "step": 1186
    },
    {
      "epoch": 1.5826666666666667,
      "grad_norm": 0.03311231732368469,
      "learning_rate": 0.0019149451452076943,
      "loss": 1.0706,
      "step": 1187
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.03480307012796402,
      "learning_rate": 0.0019147707810885798,
      "loss": 1.1343,
      "step": 1188
    },
    {
      "epoch": 1.5853333333333333,
      "grad_norm": 0.03385277837514877,
      "learning_rate": 0.0019145962463824279,
      "loss": 1.1136,
      "step": 1189
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.041126929223537445,
      "learning_rate": 0.001914421541121785,
      "loss": 1.1069,
      "step": 1190
    },
    {
      "epoch": 1.588,
      "grad_norm": 0.040245819836854935,
      "learning_rate": 0.0019142466653392317,
      "loss": 1.1831,
      "step": 1191
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 0.03378815948963165,
      "learning_rate": 0.0019140716190673777,
      "loss": 1.4464,
      "step": 1192
    },
    {
      "epoch": 1.5906666666666667,
      "grad_norm": 0.07238516956567764,
      "learning_rate": 0.0019138964023388662,
      "loss": 1.118,
      "step": 1193
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.03839018568396568,
      "learning_rate": 0.001913721015186372,
      "loss": 1.1486,
      "step": 1194
    },
    {
      "epoch": 1.5933333333333333,
      "grad_norm": 0.02827485091984272,
      "learning_rate": 0.001913545457642601,
      "loss": 0.9859,
      "step": 1195
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 0.03224482014775276,
      "learning_rate": 0.0019133697297402912,
      "loss": 1.1681,
      "step": 1196
    },
    {
      "epoch": 1.596,
      "grad_norm": 0.035218946635723114,
      "learning_rate": 0.0019131938315122131,
      "loss": 1.3026,
      "step": 1197
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.04090793803334236,
      "learning_rate": 0.0019130177629911674,
      "loss": 1.1723,
      "step": 1198
    },
    {
      "epoch": 1.5986666666666667,
      "grad_norm": 0.06373558193445206,
      "learning_rate": 0.0019128415242099881,
      "loss": 1.2005,
      "step": 1199
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.027570605278015137,
      "learning_rate": 0.0019126651152015402,
      "loss": 1.0651,
      "step": 1200
    },
    {
      "epoch": 1.6013333333333333,
      "grad_norm": 0.03094179928302765,
      "learning_rate": 0.0019124885359987206,
      "loss": 1.3301,
      "step": 1201
    },
    {
      "epoch": 1.6026666666666667,
      "grad_norm": 0.039830271154642105,
      "learning_rate": 0.0019123117866344575,
      "loss": 1.3648,
      "step": 1202
    },
    {
      "epoch": 1.604,
      "grad_norm": 0.033579643815755844,
      "learning_rate": 0.0019121348671417117,
      "loss": 1.2406,
      "step": 1203
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 0.036577798426151276,
      "learning_rate": 0.0019119577775534755,
      "loss": 1.1239,
      "step": 1204
    },
    {
      "epoch": 1.6066666666666667,
      "grad_norm": 0.028768688440322876,
      "learning_rate": 0.0019117805179027722,
      "loss": 1.1041,
      "step": 1205
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.03136226907372475,
      "learning_rate": 0.001911603088222657,
      "loss": 1.398,
      "step": 1206
    },
    {
      "epoch": 1.6093333333333333,
      "grad_norm": 0.029745595529675484,
      "learning_rate": 0.0019114254885462176,
      "loss": 1.0693,
      "step": 1207
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 0.0319194495677948,
      "learning_rate": 0.001911247718906573,
      "loss": 1.2434,
      "step": 1208
    },
    {
      "epoch": 1.612,
      "grad_norm": 0.04638802260160446,
      "learning_rate": 0.0019110697793368733,
      "loss": 1.2395,
      "step": 1209
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.029794272035360336,
      "learning_rate": 0.0019108916698703014,
      "loss": 1.1865,
      "step": 1210
    },
    {
      "epoch": 1.6146666666666667,
      "grad_norm": 0.03279958665370941,
      "learning_rate": 0.0019107133905400709,
      "loss": 1.4016,
      "step": 1211
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.031919293105602264,
      "learning_rate": 0.001910534941379427,
      "loss": 1.1496,
      "step": 1212
    },
    {
      "epoch": 1.6173333333333333,
      "grad_norm": 0.25600144267082214,
      "learning_rate": 0.001910356322421648,
      "loss": 1.222,
      "step": 1213
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.05811659246683121,
      "learning_rate": 0.0019101775337000422,
      "loss": 1.2157,
      "step": 1214
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.3031785488128662,
      "learning_rate": 0.0019099985752479506,
      "loss": 1.0924,
      "step": 1215
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 0.028796110302209854,
      "learning_rate": 0.0019098194470987448,
      "loss": 1.1603,
      "step": 1216
    },
    {
      "epoch": 1.6226666666666667,
      "grad_norm": 0.038465362042188644,
      "learning_rate": 0.0019096401492858298,
      "loss": 1.0193,
      "step": 1217
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.07102926820516586,
      "learning_rate": 0.0019094606818426403,
      "loss": 1.1355,
      "step": 1218
    },
    {
      "epoch": 1.6253333333333333,
      "grad_norm": 0.13549208641052246,
      "learning_rate": 0.0019092810448026439,
      "loss": 1.3408,
      "step": 1219
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.03507966175675392,
      "learning_rate": 0.001909101238199339,
      "loss": 1.0847,
      "step": 1220
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 0.041864726692438126,
      "learning_rate": 0.0019089212620662568,
      "loss": 1.2147,
      "step": 1221
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 0.2200983762741089,
      "learning_rate": 0.0019087411164369589,
      "loss": 1.1567,
      "step": 1222
    },
    {
      "epoch": 1.6306666666666667,
      "grad_norm": 0.05468539148569107,
      "learning_rate": 0.0019085608013450388,
      "loss": 1.1195,
      "step": 1223
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.07808142155408859,
      "learning_rate": 0.0019083803168241224,
      "loss": 1.1464,
      "step": 1224
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.058709025382995605,
      "learning_rate": 0.0019081996629078655,
      "loss": 1.2303,
      "step": 1225
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 0.3061031997203827,
      "learning_rate": 0.0019080188396299576,
      "loss": 1.1295,
      "step": 1226
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 0.25929364562034607,
      "learning_rate": 0.0019078378470241183,
      "loss": 1.2193,
      "step": 1227
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 0.03649763762950897,
      "learning_rate": 0.0019076566851240994,
      "loss": 0.8796,
      "step": 1228
    },
    {
      "epoch": 1.6386666666666667,
      "grad_norm": 0.050126027315855026,
      "learning_rate": 0.0019074753539636835,
      "loss": 1.2626,
      "step": 1229
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.09182040393352509,
      "learning_rate": 0.0019072938535766863,
      "loss": 1.034,
      "step": 1230
    },
    {
      "epoch": 1.6413333333333333,
      "grad_norm": 0.16734810173511505,
      "learning_rate": 0.0019071121839969534,
      "loss": 1.0777,
      "step": 1231
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 0.046076979488134384,
      "learning_rate": 0.0019069303452583627,
      "loss": 1.3502,
      "step": 1232
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 0.04674212634563446,
      "learning_rate": 0.0019067483373948243,
      "loss": 1.0992,
      "step": 1233
    },
    {
      "epoch": 1.6453333333333333,
      "grad_norm": 0.04751649498939514,
      "learning_rate": 0.0019065661604402782,
      "loss": 1.0023,
      "step": 1234
    },
    {
      "epoch": 1.6466666666666665,
      "grad_norm": 0.2590172290802002,
      "learning_rate": 0.0019063838144286974,
      "loss": 1.3586,
      "step": 1235
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.03021070919930935,
      "learning_rate": 0.0019062012993940859,
      "loss": 1.0303,
      "step": 1236
    },
    {
      "epoch": 1.6493333333333333,
      "grad_norm": 0.04040015861392021,
      "learning_rate": 0.0019060186153704787,
      "loss": 1.2953,
      "step": 1237
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.032560091465711594,
      "learning_rate": 0.0019058357623919437,
      "loss": 1.1655,
      "step": 1238
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 0.03935252130031586,
      "learning_rate": 0.0019056527404925788,
      "loss": 1.1003,
      "step": 1239
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.0936133936047554,
      "learning_rate": 0.0019054695497065142,
      "loss": 0.8688,
      "step": 1240
    },
    {
      "epoch": 1.6546666666666665,
      "grad_norm": 0.06002434343099594,
      "learning_rate": 0.0019052861900679115,
      "loss": 1.3319,
      "step": 1241
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.03721441701054573,
      "learning_rate": 0.0019051026616109636,
      "loss": 1.2336,
      "step": 1242
    },
    {
      "epoch": 1.6573333333333333,
      "grad_norm": 0.03649011254310608,
      "learning_rate": 0.0019049189643698952,
      "loss": 1.1829,
      "step": 1243
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 0.029769908636808395,
      "learning_rate": 0.0019047350983789622,
      "loss": 1.0471,
      "step": 1244
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.09674405306577682,
      "learning_rate": 0.0019045510636724518,
      "loss": 1.3474,
      "step": 1245
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.03294941037893295,
      "learning_rate": 0.0019043668602846833,
      "loss": 1.1993,
      "step": 1246
    },
    {
      "epoch": 1.6626666666666665,
      "grad_norm": 0.06474320590496063,
      "learning_rate": 0.001904182488250007,
      "loss": 1.0684,
      "step": 1247
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.06734839081764221,
      "learning_rate": 0.001903997947602804,
      "loss": 1.1451,
      "step": 1248
    },
    {
      "epoch": 1.6653333333333333,
      "grad_norm": 0.03365247696638107,
      "learning_rate": 0.0019038132383774888,
      "loss": 1.1108,
      "step": 1249
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.04782658442854881,
      "learning_rate": 0.0019036283606085054,
      "loss": 1.4607,
      "step": 1250
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 0.025025110691785812,
      "learning_rate": 0.0019034433143303299,
      "loss": 0.9268,
      "step": 1251
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 0.04126270115375519,
      "learning_rate": 0.0019032580995774697,
      "loss": 1.172,
      "step": 1252
    },
    {
      "epoch": 1.6706666666666665,
      "grad_norm": 0.05352315679192543,
      "learning_rate": 0.0019030727163844641,
      "loss": 1.2918,
      "step": 1253
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.03063308261334896,
      "learning_rate": 0.0019028871647858833,
      "loss": 1.1123,
      "step": 1254
    },
    {
      "epoch": 1.6733333333333333,
      "grad_norm": 0.078941710293293,
      "learning_rate": 0.0019027014448163295,
      "loss": 1.1963,
      "step": 1255
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 0.028724735602736473,
      "learning_rate": 0.0019025155565104352,
      "loss": 1.3837,
      "step": 1256
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 0.03329942002892494,
      "learning_rate": 0.0019023294999028653,
      "loss": 1.2551,
      "step": 1257
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 0.025968533009290695,
      "learning_rate": 0.001902143275028316,
      "loss": 1.3551,
      "step": 1258
    },
    {
      "epoch": 1.6786666666666665,
      "grad_norm": 0.02595193311572075,
      "learning_rate": 0.001901956881921514,
      "loss": 1.0747,
      "step": 1259
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.03346455469727516,
      "learning_rate": 0.0019017703206172186,
      "loss": 1.0206,
      "step": 1260
    },
    {
      "epoch": 1.6813333333333333,
      "grad_norm": 0.0357394702732563,
      "learning_rate": 0.0019015835911502196,
      "loss": 1.3811,
      "step": 1261
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.03815038874745369,
      "learning_rate": 0.0019013966935553385,
      "loss": 1.0889,
      "step": 1262
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 0.03146841749548912,
      "learning_rate": 0.001901209627867428,
      "loss": 1.0898,
      "step": 1263
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 0.028193963691592216,
      "learning_rate": 0.0019010223941213723,
      "loss": 1.1958,
      "step": 1264
    },
    {
      "epoch": 1.6866666666666665,
      "grad_norm": 0.033551715314388275,
      "learning_rate": 0.001900834992352087,
      "loss": 0.9596,
      "step": 1265
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.04016895219683647,
      "learning_rate": 0.0019006474225945187,
      "loss": 1.1662,
      "step": 1266
    },
    {
      "epoch": 1.6893333333333334,
      "grad_norm": 0.031781382858753204,
      "learning_rate": 0.0019004596848836454,
      "loss": 1.3564,
      "step": 1267
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 0.023324353620409966,
      "learning_rate": 0.001900271779254477,
      "loss": 1.1682,
      "step": 1268
    },
    {
      "epoch": 1.692,
      "grad_norm": 0.03504998981952667,
      "learning_rate": 0.0019000837057420539,
      "loss": 1.3668,
      "step": 1269
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.029398400336503983,
      "learning_rate": 0.0018998954643814484,
      "loss": 1.2265,
      "step": 1270
    },
    {
      "epoch": 1.6946666666666665,
      "grad_norm": 0.04528603330254555,
      "learning_rate": 0.0018997070552077635,
      "loss": 1.2289,
      "step": 1271
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.03444022312760353,
      "learning_rate": 0.0018995184782561343,
      "loss": 0.7848,
      "step": 1272
    },
    {
      "epoch": 1.6973333333333334,
      "grad_norm": 0.026356151327490807,
      "learning_rate": 0.0018993297335617263,
      "loss": 1.129,
      "step": 1273
    },
    {
      "epoch": 1.6986666666666665,
      "grad_norm": 0.039837796241045,
      "learning_rate": 0.0018991408211597371,
      "loss": 1.2663,
      "step": 1274
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.029256027191877365,
      "learning_rate": 0.0018989517410853954,
      "loss": 1.2285,
      "step": 1275
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 0.030302047729492188,
      "learning_rate": 0.0018987624933739604,
      "loss": 0.9946,
      "step": 1276
    },
    {
      "epoch": 1.7026666666666666,
      "grad_norm": 0.03322473168373108,
      "learning_rate": 0.0018985730780607237,
      "loss": 1.2188,
      "step": 1277
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.029829341918230057,
      "learning_rate": 0.0018983834951810069,
      "loss": 1.0724,
      "step": 1278
    },
    {
      "epoch": 1.7053333333333334,
      "grad_norm": 0.030723629519343376,
      "learning_rate": 0.0018981937447701638,
      "loss": 1.0318,
      "step": 1279
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.028177203610539436,
      "learning_rate": 0.0018980038268635794,
      "loss": 1.104,
      "step": 1280
    },
    {
      "epoch": 1.708,
      "grad_norm": 0.027784399688243866,
      "learning_rate": 0.0018978137414966698,
      "loss": 1.2739,
      "step": 1281
    },
    {
      "epoch": 1.7093333333333334,
      "grad_norm": 0.023871077224612236,
      "learning_rate": 0.001897623488704882,
      "loss": 1.2207,
      "step": 1282
    },
    {
      "epoch": 1.7106666666666666,
      "grad_norm": 0.033844079822301865,
      "learning_rate": 0.0018974330685236945,
      "loss": 1.2293,
      "step": 1283
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.052604105323553085,
      "learning_rate": 0.001897242480988617,
      "loss": 1.1445,
      "step": 1284
    },
    {
      "epoch": 1.7133333333333334,
      "grad_norm": 0.03092321753501892,
      "learning_rate": 0.00189705172613519,
      "loss": 1.1699,
      "step": 1285
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.03373512625694275,
      "learning_rate": 0.0018968608039989865,
      "loss": 1.2784,
      "step": 1286
    },
    {
      "epoch": 1.716,
      "grad_norm": 0.03810304030776024,
      "learning_rate": 0.001896669714615609,
      "loss": 1.2912,
      "step": 1287
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 0.035578105598688126,
      "learning_rate": 0.0018964784580206922,
      "loss": 1.0598,
      "step": 1288
    },
    {
      "epoch": 1.7186666666666666,
      "grad_norm": 0.03400229662656784,
      "learning_rate": 0.001896287034249902,
      "loss": 1.2402,
      "step": 1289
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.02596878819167614,
      "learning_rate": 0.0018960954433389346,
      "loss": 0.9581,
      "step": 1290
    },
    {
      "epoch": 1.7213333333333334,
      "grad_norm": 0.03508739173412323,
      "learning_rate": 0.0018959036853235186,
      "loss": 1.2146,
      "step": 1291
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 0.03211157023906708,
      "learning_rate": 0.0018957117602394129,
      "loss": 1.1856,
      "step": 1292
    },
    {
      "epoch": 1.724,
      "grad_norm": 0.031712885946035385,
      "learning_rate": 0.001895519668122408,
      "loss": 1.1832,
      "step": 1293
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.03060387633740902,
      "learning_rate": 0.001895327409008325,
      "loss": 1.1561,
      "step": 1294
    },
    {
      "epoch": 1.7266666666666666,
      "grad_norm": 0.026773180812597275,
      "learning_rate": 0.0018951349829330166,
      "loss": 1.0478,
      "step": 1295
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.03134344145655632,
      "learning_rate": 0.001894942389932367,
      "loss": 1.2289,
      "step": 1296
    },
    {
      "epoch": 1.7293333333333334,
      "grad_norm": 0.03330351412296295,
      "learning_rate": 0.0018947496300422903,
      "loss": 1.1096,
      "step": 1297
    },
    {
      "epoch": 1.7306666666666666,
      "grad_norm": 0.02788815274834633,
      "learning_rate": 0.0018945567032987332,
      "loss": 1.1562,
      "step": 1298
    },
    {
      "epoch": 1.732,
      "grad_norm": 0.03094436414539814,
      "learning_rate": 0.0018943636097376727,
      "loss": 1.1661,
      "step": 1299
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.04924730584025383,
      "learning_rate": 0.0018941703493951163,
      "loss": 1.2369,
      "step": 1300
    },
    {
      "epoch": 1.7346666666666666,
      "grad_norm": 0.034816328436136246,
      "learning_rate": 0.0018939769223071043,
      "loss": 1.0322,
      "step": 1301
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.0391564704477787,
      "learning_rate": 0.0018937833285097066,
      "loss": 1.3479,
      "step": 1302
    },
    {
      "epoch": 1.7373333333333334,
      "grad_norm": 0.032706983387470245,
      "learning_rate": 0.0018935895680390242,
      "loss": 1.0646,
      "step": 1303
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 0.03629806637763977,
      "learning_rate": 0.0018933956409311907,
      "loss": 0.7184,
      "step": 1304
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.036405935883522034,
      "learning_rate": 0.0018932015472223692,
      "loss": 1.0584,
      "step": 1305
    },
    {
      "epoch": 1.7413333333333334,
      "grad_norm": 0.041054125875234604,
      "learning_rate": 0.0018930072869487544,
      "loss": 1.2518,
      "step": 1306
    },
    {
      "epoch": 1.7426666666666666,
      "grad_norm": 0.03304671123623848,
      "learning_rate": 0.0018928128601465723,
      "loss": 1.254,
      "step": 1307
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.03576839715242386,
      "learning_rate": 0.0018926182668520793,
      "loss": 0.8299,
      "step": 1308
    },
    {
      "epoch": 1.7453333333333334,
      "grad_norm": 0.04643280431628227,
      "learning_rate": 0.0018924235071015637,
      "loss": 1.0034,
      "step": 1309
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.03677359223365784,
      "learning_rate": 0.0018922285809313442,
      "loss": 1.409,
      "step": 1310
    },
    {
      "epoch": 1.748,
      "grad_norm": 0.10000596940517426,
      "learning_rate": 0.0018920334883777707,
      "loss": 1.4515,
      "step": 1311
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 0.033641062676906586,
      "learning_rate": 0.0018918382294772246,
      "loss": 0.9019,
      "step": 1312
    },
    {
      "epoch": 1.7506666666666666,
      "grad_norm": 0.02857152372598648,
      "learning_rate": 0.0018916428042661177,
      "loss": 0.9846,
      "step": 1313
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.044123005121946335,
      "learning_rate": 0.0018914472127808928,
      "loss": 1.2375,
      "step": 1314
    },
    {
      "epoch": 1.7533333333333334,
      "grad_norm": 0.03313343971967697,
      "learning_rate": 0.0018912514550580243,
      "loss": 1.3124,
      "step": 1315
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 0.02761625498533249,
      "learning_rate": 0.001891055531134017,
      "loss": 1.2075,
      "step": 1316
    },
    {
      "epoch": 1.756,
      "grad_norm": 0.04096516966819763,
      "learning_rate": 0.0018908594410454067,
      "loss": 1.1835,
      "step": 1317
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.033655934035778046,
      "learning_rate": 0.0018906631848287605,
      "loss": 1.2573,
      "step": 1318
    },
    {
      "epoch": 1.7586666666666666,
      "grad_norm": 0.03689945116639137,
      "learning_rate": 0.0018904667625206767,
      "loss": 1.2539,
      "step": 1319
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.034489117562770844,
      "learning_rate": 0.0018902701741577842,
      "loss": 1.0822,
      "step": 1320
    },
    {
      "epoch": 1.7613333333333334,
      "grad_norm": 0.028308356180787086,
      "learning_rate": 0.0018900734197767424,
      "loss": 1.1201,
      "step": 1321
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 0.031972646713256836,
      "learning_rate": 0.001889876499414243,
      "loss": 1.3334,
      "step": 1322
    },
    {
      "epoch": 1.764,
      "grad_norm": 0.03217318281531334,
      "learning_rate": 0.0018896794131070072,
      "loss": 1.0036,
      "step": 1323
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 0.07792829722166061,
      "learning_rate": 0.001889482160891788,
      "loss": 0.9451,
      "step": 1324
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.04310810565948486,
      "learning_rate": 0.0018892847428053693,
      "loss": 1.1754,
      "step": 1325
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.03274562209844589,
      "learning_rate": 0.001889087158884565,
      "loss": 1.2035,
      "step": 1326
    },
    {
      "epoch": 1.7693333333333334,
      "grad_norm": 0.028197236359119415,
      "learning_rate": 0.0018888894091662217,
      "loss": 0.9115,
      "step": 1327
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 0.03760989382863045,
      "learning_rate": 0.0018886914936872153,
      "loss": 1.227,
      "step": 1328
    },
    {
      "epoch": 1.772,
      "grad_norm": 0.03211091458797455,
      "learning_rate": 0.0018884934124844533,
      "loss": 1.04,
      "step": 1329
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.04761089012026787,
      "learning_rate": 0.001888295165594874,
      "loss": 1.2624,
      "step": 1330
    },
    {
      "epoch": 1.7746666666666666,
      "grad_norm": 0.035953789949417114,
      "learning_rate": 0.001888096753055447,
      "loss": 1.2851,
      "step": 1331
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.12217722833156586,
      "learning_rate": 0.0018878981749031716,
      "loss": 1.2275,
      "step": 1332
    },
    {
      "epoch": 1.7773333333333334,
      "grad_norm": 0.027957692742347717,
      "learning_rate": 0.0018876994311750792,
      "loss": 1.1839,
      "step": 1333
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 0.030590467154979706,
      "learning_rate": 0.001887500521908232,
      "loss": 0.9701,
      "step": 1334
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.04305307939648628,
      "learning_rate": 0.0018873014471397222,
      "loss": 1.0544,
      "step": 1335
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 0.027654387056827545,
      "learning_rate": 0.0018871022069066737,
      "loss": 1.015,
      "step": 1336
    },
    {
      "epoch": 1.7826666666666666,
      "grad_norm": 0.031566549092531204,
      "learning_rate": 0.0018869028012462408,
      "loss": 1.1429,
      "step": 1337
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.03159976750612259,
      "learning_rate": 0.0018867032301956089,
      "loss": 0.9927,
      "step": 1338
    },
    {
      "epoch": 1.7853333333333334,
      "grad_norm": 0.04417363926768303,
      "learning_rate": 0.0018865034937919937,
      "loss": 1.2067,
      "step": 1339
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.027856070548295975,
      "learning_rate": 0.001886303592072643,
      "loss": 1.089,
      "step": 1340
    },
    {
      "epoch": 1.788,
      "grad_norm": 0.040572457015514374,
      "learning_rate": 0.0018861035250748342,
      "loss": 1.3645,
      "step": 1341
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.0314355194568634,
      "learning_rate": 0.0018859032928358755,
      "loss": 1.2355,
      "step": 1342
    },
    {
      "epoch": 1.7906666666666666,
      "grad_norm": 0.039292823523283005,
      "learning_rate": 0.0018857028953931068,
      "loss": 1.2262,
      "step": 1343
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.05921813100576401,
      "learning_rate": 0.0018855023327838984,
      "loss": 1.2787,
      "step": 1344
    },
    {
      "epoch": 1.7933333333333334,
      "grad_norm": 0.10428109019994736,
      "learning_rate": 0.001885301605045651,
      "loss": 0.9102,
      "step": 1345
    },
    {
      "epoch": 1.7946666666666666,
      "grad_norm": 0.029718823730945587,
      "learning_rate": 0.0018851007122157966,
      "loss": 1.0667,
      "step": 1346
    },
    {
      "epoch": 1.796,
      "grad_norm": 0.025827350094914436,
      "learning_rate": 0.001884899654331798,
      "loss": 1.2169,
      "step": 1347
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 0.02552994154393673,
      "learning_rate": 0.0018846984314311484,
      "loss": 0.9912,
      "step": 1348
    },
    {
      "epoch": 1.7986666666666666,
      "grad_norm": 0.040677592158317566,
      "learning_rate": 0.0018844970435513719,
      "loss": 1.076,
      "step": 1349
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.03254977613687515,
      "learning_rate": 0.0018842954907300237,
      "loss": 0.9481,
      "step": 1350
    },
    {
      "epoch": 1.8013333333333335,
      "grad_norm": 0.03555314615368843,
      "learning_rate": 0.0018840937730046892,
      "loss": 1.0393,
      "step": 1351
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 0.03102840669453144,
      "learning_rate": 0.001883891890412985,
      "loss": 1.0495,
      "step": 1352
    },
    {
      "epoch": 1.804,
      "grad_norm": 0.041418544948101044,
      "learning_rate": 0.0018836898429925584,
      "loss": 1.1912,
      "step": 1353
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 0.04032829776406288,
      "learning_rate": 0.0018834876307810876,
      "loss": 1.0502,
      "step": 1354
    },
    {
      "epoch": 1.8066666666666666,
      "grad_norm": 0.03677399829030037,
      "learning_rate": 0.0018832852538162804,
      "loss": 1.1741,
      "step": 1355
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.032436199486255646,
      "learning_rate": 0.0018830827121358769,
      "loss": 0.9785,
      "step": 1356
    },
    {
      "epoch": 1.8093333333333335,
      "grad_norm": 0.03219539299607277,
      "learning_rate": 0.001882880005777647,
      "loss": 1.0005,
      "step": 1357
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 0.05093623325228691,
      "learning_rate": 0.0018826771347793911,
      "loss": 0.9879,
      "step": 1358
    },
    {
      "epoch": 1.812,
      "grad_norm": 0.03988831490278244,
      "learning_rate": 0.0018824740991789416,
      "loss": 1.2959,
      "step": 1359
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.03173240274190903,
      "learning_rate": 0.00188227089901416,
      "loss": 1.1432,
      "step": 1360
    },
    {
      "epoch": 1.8146666666666667,
      "grad_norm": 0.053757261484861374,
      "learning_rate": 0.0018820675343229395,
      "loss": 1.0454,
      "step": 1361
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.031653717160224915,
      "learning_rate": 0.0018818640051432034,
      "loss": 1.076,
      "step": 1362
    },
    {
      "epoch": 1.8173333333333335,
      "grad_norm": 0.034455813467502594,
      "learning_rate": 0.0018816603115129062,
      "loss": 1.0076,
      "step": 1363
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 0.03756426274776459,
      "learning_rate": 0.001881456453470033,
      "loss": 1.3615,
      "step": 1364
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.03341095894575119,
      "learning_rate": 0.001881252431052599,
      "loss": 1.4133,
      "step": 1365
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.050927259027957916,
      "learning_rate": 0.0018810482442986503,
      "loss": 1.1797,
      "step": 1366
    },
    {
      "epoch": 1.8226666666666667,
      "grad_norm": 0.07957785576581955,
      "learning_rate": 0.0018808438932462642,
      "loss": 1.1293,
      "step": 1367
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.03390580043196678,
      "learning_rate": 0.0018806393779335483,
      "loss": 1.1763,
      "step": 1368
    },
    {
      "epoch": 1.8253333333333335,
      "grad_norm": 0.025584766641259193,
      "learning_rate": 0.0018804346983986402,
      "loss": 1.0189,
      "step": 1369
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.05006355792284012,
      "learning_rate": 0.0018802298546797091,
      "loss": 1.4497,
      "step": 1370
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 0.027011804282665253,
      "learning_rate": 0.0018800248468149544,
      "loss": 1.2305,
      "step": 1371
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 0.035984527319669724,
      "learning_rate": 0.0018798196748426056,
      "loss": 1.0747,
      "step": 1372
    },
    {
      "epoch": 1.8306666666666667,
      "grad_norm": 0.04567969590425491,
      "learning_rate": 0.0018796143388009239,
      "loss": 1.3167,
      "step": 1373
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.028441516682505608,
      "learning_rate": 0.0018794088387282,
      "loss": 1.1614,
      "step": 1374
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.029467375949025154,
      "learning_rate": 0.0018792031746627563,
      "loss": 0.9297,
      "step": 1375
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 0.03096092864871025,
      "learning_rate": 0.0018789973466429447,
      "loss": 1.3011,
      "step": 1376
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 0.04032488167285919,
      "learning_rate": 0.0018787913547071483,
      "loss": 1.405,
      "step": 1377
    },
    {
      "epoch": 1.8373333333333335,
      "grad_norm": 0.04002244397997856,
      "learning_rate": 0.0018785851988937802,
      "loss": 1.1646,
      "step": 1378
    },
    {
      "epoch": 1.8386666666666667,
      "grad_norm": 0.03360747918486595,
      "learning_rate": 0.001878378879241285,
      "loss": 1.1413,
      "step": 1379
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.03476371988654137,
      "learning_rate": 0.001878172395788137,
      "loss": 1.1269,
      "step": 1380
    },
    {
      "epoch": 1.8413333333333335,
      "grad_norm": 0.039157234132289886,
      "learning_rate": 0.001877965748572842,
      "loss": 1.2591,
      "step": 1381
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.04895796626806259,
      "learning_rate": 0.001877758937633935,
      "loss": 1.3431,
      "step": 1382
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 0.0792015865445137,
      "learning_rate": 0.001877551963009982,
      "loss": 1.1679,
      "step": 1383
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 0.03211560100317001,
      "learning_rate": 0.0018773448247395806,
      "loss": 1.3012,
      "step": 1384
    },
    {
      "epoch": 1.8466666666666667,
      "grad_norm": 0.034284643828868866,
      "learning_rate": 0.0018771375228613576,
      "loss": 1.2236,
      "step": 1385
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.027183273807168007,
      "learning_rate": 0.0018769300574139709,
      "loss": 0.9811,
      "step": 1386
    },
    {
      "epoch": 1.8493333333333335,
      "grad_norm": 0.030465709045529366,
      "learning_rate": 0.0018767224284361088,
      "loss": 1.3686,
      "step": 1387
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 0.04073521867394447,
      "learning_rate": 0.0018765146359664899,
      "loss": 1.0492,
      "step": 1388
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 0.030783599242568016,
      "learning_rate": 0.0018763066800438636,
      "loss": 1.1797,
      "step": 1389
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.030743855983018875,
      "learning_rate": 0.0018760985607070098,
      "loss": 1.0134,
      "step": 1390
    },
    {
      "epoch": 1.8546666666666667,
      "grad_norm": 0.031264662742614746,
      "learning_rate": 0.0018758902779947384,
      "loss": 0.9404,
      "step": 1391
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.03574087843298912,
      "learning_rate": 0.0018756818319458906,
      "loss": 1.315,
      "step": 1392
    },
    {
      "epoch": 1.8573333333333333,
      "grad_norm": 0.042973555624485016,
      "learning_rate": 0.0018754732225993369,
      "loss": 1.1153,
      "step": 1393
    },
    {
      "epoch": 1.8586666666666667,
      "grad_norm": 0.02912137471139431,
      "learning_rate": 0.0018752644499939795,
      "loss": 0.8428,
      "step": 1394
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.024553248658776283,
      "learning_rate": 0.00187505551416875,
      "loss": 1.2375,
      "step": 1395
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 0.031365782022476196,
      "learning_rate": 0.001874846415162611,
      "loss": 1.0577,
      "step": 1396
    },
    {
      "epoch": 1.8626666666666667,
      "grad_norm": 0.04311109706759453,
      "learning_rate": 0.0018746371530145556,
      "loss": 0.9856,
      "step": 1397
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.0397888720035553,
      "learning_rate": 0.001874427727763607,
      "loss": 1.2438,
      "step": 1398
    },
    {
      "epoch": 1.8653333333333333,
      "grad_norm": 0.03499394282698631,
      "learning_rate": 0.0018742181394488192,
      "loss": 1.1497,
      "step": 1399
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.030373457819223404,
      "learning_rate": 0.0018740083881092758,
      "loss": 1.4715,
      "step": 1400
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 0.030928129330277443,
      "learning_rate": 0.001873798473784092,
      "loss": 1.0604,
      "step": 1401
    },
    {
      "epoch": 1.8693333333333333,
      "grad_norm": 0.06468886137008667,
      "learning_rate": 0.0018735883965124121,
      "loss": 1.2356,
      "step": 1402
    },
    {
      "epoch": 1.8706666666666667,
      "grad_norm": 0.18336279690265656,
      "learning_rate": 0.0018733781563334121,
      "loss": 1.1076,
      "step": 1403
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.038702357560396194,
      "learning_rate": 0.0018731677532862975,
      "loss": 1.0531,
      "step": 1404
    },
    {
      "epoch": 1.8733333333333333,
      "grad_norm": 0.03170780465006828,
      "learning_rate": 0.001872957187410304,
      "loss": 1.1963,
      "step": 1405
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.03813354671001434,
      "learning_rate": 0.0018727464587446985,
      "loss": 1.079,
      "step": 1406
    },
    {
      "epoch": 1.876,
      "grad_norm": 0.03671356663107872,
      "learning_rate": 0.0018725355673287777,
      "loss": 0.8986,
      "step": 1407
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 0.03545207157731056,
      "learning_rate": 0.0018723245132018689,
      "loss": 0.9366,
      "step": 1408
    },
    {
      "epoch": 1.8786666666666667,
      "grad_norm": 0.15159215033054352,
      "learning_rate": 0.0018721132964033293,
      "loss": 1.4992,
      "step": 1409
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.033975109457969666,
      "learning_rate": 0.001871901916972547,
      "loss": 1.1877,
      "step": 1410
    },
    {
      "epoch": 1.8813333333333333,
      "grad_norm": 0.03172123432159424,
      "learning_rate": 0.00187169037494894,
      "loss": 0.9717,
      "step": 1411
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 0.030028928071260452,
      "learning_rate": 0.0018714786703719572,
      "loss": 1.1786,
      "step": 1412
    },
    {
      "epoch": 1.884,
      "grad_norm": 0.03153274953365326,
      "learning_rate": 0.0018712668032810768,
      "loss": 1.0732,
      "step": 1413
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.042484577745199203,
      "learning_rate": 0.001871054773715808,
      "loss": 1.1957,
      "step": 1414
    },
    {
      "epoch": 1.8866666666666667,
      "grad_norm": 0.040974248200654984,
      "learning_rate": 0.001870842581715691,
      "loss": 0.9419,
      "step": 1415
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.08846309781074524,
      "learning_rate": 0.0018706302273202942,
      "loss": 1.1518,
      "step": 1416
    },
    {
      "epoch": 1.8893333333333333,
      "grad_norm": 0.03399694710969925,
      "learning_rate": 0.0018704177105692186,
      "loss": 1.0013,
      "step": 1417
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 0.03126509487628937,
      "learning_rate": 0.0018702050315020941,
      "loss": 1.1241,
      "step": 1418
    },
    {
      "epoch": 1.892,
      "grad_norm": 0.028887374326586723,
      "learning_rate": 0.0018699921901585812,
      "loss": 1.4328,
      "step": 1419
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.038699761033058167,
      "learning_rate": 0.001869779186578371,
      "loss": 1.0091,
      "step": 1420
    },
    {
      "epoch": 1.8946666666666667,
      "grad_norm": 0.04095184803009033,
      "learning_rate": 0.0018695660208011841,
      "loss": 1.2887,
      "step": 1421
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.04157808795571327,
      "learning_rate": 0.0018693526928667721,
      "loss": 1.2746,
      "step": 1422
    },
    {
      "epoch": 1.8973333333333333,
      "grad_norm": 0.10388131439685822,
      "learning_rate": 0.0018691392028149163,
      "loss": 1.3753,
      "step": 1423
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 0.03579718992114067,
      "learning_rate": 0.0018689255506854287,
      "loss": 1.0806,
      "step": 1424
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.026865215972065926,
      "learning_rate": 0.0018687117365181513,
      "loss": 1.0447,
      "step": 1425
    },
    {
      "epoch": 1.9013333333333333,
      "grad_norm": 0.030520541593432426,
      "learning_rate": 0.0018684977603529557,
      "loss": 0.9882,
      "step": 1426
    },
    {
      "epoch": 1.9026666666666667,
      "grad_norm": 0.027295174077153206,
      "learning_rate": 0.001868283622229745,
      "loss": 1.2894,
      "step": 1427
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.029721692204475403,
      "learning_rate": 0.0018680693221884517,
      "loss": 1.016,
      "step": 1428
    },
    {
      "epoch": 1.9053333333333333,
      "grad_norm": 0.02969093807041645,
      "learning_rate": 0.0018678548602690387,
      "loss": 1.0913,
      "step": 1429
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.034553758800029755,
      "learning_rate": 0.0018676402365114982,
      "loss": 1.2696,
      "step": 1430
    },
    {
      "epoch": 1.908,
      "grad_norm": 0.03832172602415085,
      "learning_rate": 0.0018674254509558543,
      "loss": 1.0985,
      "step": 1431
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 0.03448032587766647,
      "learning_rate": 0.00186721050364216,
      "loss": 1.3501,
      "step": 1432
    },
    {
      "epoch": 1.9106666666666667,
      "grad_norm": 0.04191401228308678,
      "learning_rate": 0.001866995394610499,
      "loss": 1.2344,
      "step": 1433
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.03059355355799198,
      "learning_rate": 0.0018667801239009845,
      "loss": 1.274,
      "step": 1434
    },
    {
      "epoch": 1.9133333333333333,
      "grad_norm": 0.03467783331871033,
      "learning_rate": 0.001866564691553761,
      "loss": 0.9425,
      "step": 1435
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 0.027513697743415833,
      "learning_rate": 0.0018663490976090016,
      "loss": 1.0682,
      "step": 1436
    },
    {
      "epoch": 1.916,
      "grad_norm": 0.026467466726899147,
      "learning_rate": 0.001866133342106911,
      "loss": 1.0783,
      "step": 1437
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.047839466482400894,
      "learning_rate": 0.0018659174250877236,
      "loss": 1.4764,
      "step": 1438
    },
    {
      "epoch": 1.9186666666666667,
      "grad_norm": 0.025585174560546875,
      "learning_rate": 0.0018657013465917032,
      "loss": 1.1768,
      "step": 1439
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.04048372432589531,
      "learning_rate": 0.0018654851066591447,
      "loss": 1.221,
      "step": 1440
    },
    {
      "epoch": 1.9213333333333333,
      "grad_norm": 0.029877396300435066,
      "learning_rate": 0.0018652687053303722,
      "loss": 1.2758,
      "step": 1441
    },
    {
      "epoch": 1.9226666666666667,
      "grad_norm": 0.038539037108421326,
      "learning_rate": 0.0018650521426457406,
      "loss": 1.1292,
      "step": 1442
    },
    {
      "epoch": 1.924,
      "grad_norm": 0.03709743171930313,
      "learning_rate": 0.0018648354186456349,
      "loss": 0.914,
      "step": 1443
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 0.033556193113327026,
      "learning_rate": 0.0018646185333704695,
      "loss": 1.018,
      "step": 1444
    },
    {
      "epoch": 1.9266666666666667,
      "grad_norm": 0.03646198660135269,
      "learning_rate": 0.0018644014868606895,
      "loss": 1.1389,
      "step": 1445
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.029426690191030502,
      "learning_rate": 0.00186418427915677,
      "loss": 1.0026,
      "step": 1446
    },
    {
      "epoch": 1.9293333333333333,
      "grad_norm": 0.029403401538729668,
      "learning_rate": 0.0018639669102992159,
      "loss": 1.1398,
      "step": 1447
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 0.040749043226242065,
      "learning_rate": 0.001863749380328562,
      "loss": 1.2134,
      "step": 1448
    },
    {
      "epoch": 1.932,
      "grad_norm": 0.04176736995577812,
      "learning_rate": 0.0018635316892853739,
      "loss": 1.0495,
      "step": 1449
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.03915014490485191,
      "learning_rate": 0.0018633138372102468,
      "loss": 1.4383,
      "step": 1450
    },
    {
      "epoch": 1.9346666666666668,
      "grad_norm": 0.035090718418359756,
      "learning_rate": 0.0018630958241438052,
      "loss": 1.0151,
      "step": 1451
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.04583970457315445,
      "learning_rate": 0.001862877650126705,
      "loss": 1.4779,
      "step": 1452
    },
    {
      "epoch": 1.9373333333333334,
      "grad_norm": 0.03524994105100632,
      "learning_rate": 0.0018626593151996314,
      "loss": 0.9529,
      "step": 1453
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.02554062008857727,
      "learning_rate": 0.0018624408194032993,
      "loss": 1.1822,
      "step": 1454
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.037093088030815125,
      "learning_rate": 0.0018622221627784539,
      "loss": 0.9309,
      "step": 1455
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 0.034735362976789474,
      "learning_rate": 0.001862003345365871,
      "loss": 0.9868,
      "step": 1456
    },
    {
      "epoch": 1.9426666666666668,
      "grad_norm": 0.034348879009485245,
      "learning_rate": 0.0018617843672063552,
      "loss": 0.9333,
      "step": 1457
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.05417470633983612,
      "learning_rate": 0.0018615652283407419,
      "loss": 1.2609,
      "step": 1458
    },
    {
      "epoch": 1.9453333333333334,
      "grad_norm": 0.039587054401636124,
      "learning_rate": 0.0018613459288098961,
      "loss": 1.2004,
      "step": 1459
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.0480683334171772,
      "learning_rate": 0.0018611264686547134,
      "loss": 1.0641,
      "step": 1460
    },
    {
      "epoch": 1.948,
      "grad_norm": 0.07752077281475067,
      "learning_rate": 0.0018609068479161182,
      "loss": 1.2008,
      "step": 1461
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 0.028380312025547028,
      "learning_rate": 0.0018606870666350661,
      "loss": 0.9766,
      "step": 1462
    },
    {
      "epoch": 1.9506666666666668,
      "grad_norm": 0.029873916879296303,
      "learning_rate": 0.0018604671248525417,
      "loss": 0.8542,
      "step": 1463
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.03763354942202568,
      "learning_rate": 0.0018602470226095603,
      "loss": 1.0896,
      "step": 1464
    },
    {
      "epoch": 1.9533333333333334,
      "grad_norm": 0.04448999464511871,
      "learning_rate": 0.001860026759947166,
      "loss": 1.2345,
      "step": 1465
    },
    {
      "epoch": 1.9546666666666668,
      "grad_norm": 0.032382331788539886,
      "learning_rate": 0.0018598063369064343,
      "loss": 0.9043,
      "step": 1466
    },
    {
      "epoch": 1.956,
      "grad_norm": 0.037869833409786224,
      "learning_rate": 0.001859585753528469,
      "loss": 0.9883,
      "step": 1467
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 0.036326002329587936,
      "learning_rate": 0.0018593650098544052,
      "loss": 0.9629,
      "step": 1468
    },
    {
      "epoch": 1.9586666666666668,
      "grad_norm": 0.03744564950466156,
      "learning_rate": 0.0018591441059254074,
      "loss": 1.2657,
      "step": 1469
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.02835443802177906,
      "learning_rate": 0.0018589230417826697,
      "loss": 1.2481,
      "step": 1470
    },
    {
      "epoch": 1.9613333333333334,
      "grad_norm": 0.20039622485637665,
      "learning_rate": 0.0018587018174674164,
      "loss": 1.3225,
      "step": 1471
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 0.03728345036506653,
      "learning_rate": 0.001858480433020901,
      "loss": 1.1595,
      "step": 1472
    },
    {
      "epoch": 1.964,
      "grad_norm": 0.03157305717468262,
      "learning_rate": 0.0018582588884844084,
      "loss": 1.2043,
      "step": 1473
    },
    {
      "epoch": 1.9653333333333334,
      "grad_norm": 0.22094811499118805,
      "learning_rate": 0.0018580371838992514,
      "loss": 1.0541,
      "step": 1474
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.10128224641084671,
      "learning_rate": 0.0018578153193067745,
      "loss": 1.1698,
      "step": 1475
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.1280946284532547,
      "learning_rate": 0.0018575932947483503,
      "loss": 1.2659,
      "step": 1476
    },
    {
      "epoch": 1.9693333333333334,
      "grad_norm": 0.09836617857217789,
      "learning_rate": 0.0018573711102653824,
      "loss": 1.0374,
      "step": 1477
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.031240783631801605,
      "learning_rate": 0.0018571487658993041,
      "loss": 1.0866,
      "step": 1478
    },
    {
      "epoch": 1.972,
      "grad_norm": 0.12695123255252838,
      "learning_rate": 0.0018569262616915782,
      "loss": 1.5457,
      "step": 1479
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.04127512127161026,
      "learning_rate": 0.0018567035976836974,
      "loss": 0.9624,
      "step": 1480
    },
    {
      "epoch": 1.9746666666666668,
      "grad_norm": 0.039520107209682465,
      "learning_rate": 0.0018564807739171842,
      "loss": 1.0489,
      "step": 1481
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.04577441141009331,
      "learning_rate": 0.001856257790433591,
      "loss": 1.0165,
      "step": 1482
    },
    {
      "epoch": 1.9773333333333334,
      "grad_norm": 0.08957981318235397,
      "learning_rate": 0.0018560346472745,
      "loss": 1.4289,
      "step": 1483
    },
    {
      "epoch": 1.9786666666666668,
      "grad_norm": 0.043472450226545334,
      "learning_rate": 0.001855811344481523,
      "loss": 1.1109,
      "step": 1484
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.07138729095458984,
      "learning_rate": 0.0018555878820963013,
      "loss": 0.9984,
      "step": 1485
    },
    {
      "epoch": 1.9813333333333332,
      "grad_norm": 0.045047372579574585,
      "learning_rate": 0.0018553642601605068,
      "loss": 1.1115,
      "step": 1486
    },
    {
      "epoch": 1.9826666666666668,
      "grad_norm": 0.03237010911107063,
      "learning_rate": 0.0018551404787158403,
      "loss": 1.008,
      "step": 1487
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.055632084608078,
      "learning_rate": 0.0018549165378040327,
      "loss": 1.3125,
      "step": 1488
    },
    {
      "epoch": 1.9853333333333332,
      "grad_norm": 0.09588129818439484,
      "learning_rate": 0.001854692437466845,
      "loss": 1.2365,
      "step": 1489
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.03002503141760826,
      "learning_rate": 0.0018544681777460674,
      "loss": 1.1224,
      "step": 1490
    },
    {
      "epoch": 1.988,
      "grad_norm": 0.04386824369430542,
      "learning_rate": 0.00185424375868352,
      "loss": 1.2378,
      "step": 1491
    },
    {
      "epoch": 1.9893333333333332,
      "grad_norm": 0.02813909202814102,
      "learning_rate": 0.001854019180321053,
      "loss": 1.1875,
      "step": 1492
    },
    {
      "epoch": 1.9906666666666668,
      "grad_norm": 0.03873402252793312,
      "learning_rate": 0.0018537944427005448,
      "loss": 1.1552,
      "step": 1493
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.037198666483163834,
      "learning_rate": 0.0018535695458639055,
      "loss": 1.1678,
      "step": 1494
    },
    {
      "epoch": 1.9933333333333332,
      "grad_norm": 0.031215107068419456,
      "learning_rate": 0.001853344489853074,
      "loss": 1.2746,
      "step": 1495
    },
    {
      "epoch": 1.9946666666666668,
      "grad_norm": 0.03172533959150314,
      "learning_rate": 0.0018531192747100185,
      "loss": 1.0674,
      "step": 1496
    },
    {
      "epoch": 1.996,
      "grad_norm": 0.06409084051847458,
      "learning_rate": 0.0018528939004767377,
      "loss": 1.2441,
      "step": 1497
    },
    {
      "epoch": 1.9973333333333332,
      "grad_norm": 0.05110764876008034,
      "learning_rate": 0.001852668367195259,
      "loss": 1.3827,
      "step": 1498
    },
    {
      "epoch": 1.9986666666666668,
      "grad_norm": 0.0384422168135643,
      "learning_rate": 0.0018524426749076406,
      "loss": 1.0968,
      "step": 1499
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.03537039831280708,
      "learning_rate": 0.0018522168236559692,
      "loss": 1.4937,
      "step": 1500
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.1484078168869019,
      "eval_runtime": 24.0374,
      "eval_samples_per_second": 20.801,
      "eval_steps_per_second": 2.621,
      "step": 1500
    },
    {
      "epoch": 2.001333333333333,
      "grad_norm": 0.03206050395965576,
      "learning_rate": 0.0018519908134823622,
      "loss": 1.4192,
      "step": 1501
    },
    {
      "epoch": 2.002666666666667,
      "grad_norm": 0.03889770433306694,
      "learning_rate": 0.0018517646444289656,
      "loss": 1.2112,
      "step": 1502
    },
    {
      "epoch": 2.004,
      "grad_norm": 0.03255879878997803,
      "learning_rate": 0.001851538316537956,
      "loss": 1.244,
      "step": 1503
    },
    {
      "epoch": 2.005333333333333,
      "grad_norm": 0.03734751418232918,
      "learning_rate": 0.0018513118298515386,
      "loss": 1.1316,
      "step": 1504
    },
    {
      "epoch": 2.006666666666667,
      "grad_norm": 0.042906418442726135,
      "learning_rate": 0.0018510851844119494,
      "loss": 1.0432,
      "step": 1505
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.033721111714839935,
      "learning_rate": 0.0018508583802614531,
      "loss": 1.0901,
      "step": 1506
    },
    {
      "epoch": 2.009333333333333,
      "grad_norm": 0.030224928632378578,
      "learning_rate": 0.0018506314174423443,
      "loss": 0.9542,
      "step": 1507
    },
    {
      "epoch": 2.010666666666667,
      "grad_norm": 0.028082944452762604,
      "learning_rate": 0.0018504042959969472,
      "loss": 1.1559,
      "step": 1508
    },
    {
      "epoch": 2.012,
      "grad_norm": 0.04331367835402489,
      "learning_rate": 0.0018501770159676154,
      "loss": 1.121,
      "step": 1509
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.03412313014268875,
      "learning_rate": 0.0018499495773967325,
      "loss": 0.9142,
      "step": 1510
    },
    {
      "epoch": 2.014666666666667,
      "grad_norm": 0.04463547468185425,
      "learning_rate": 0.0018497219803267112,
      "loss": 1.1781,
      "step": 1511
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.04788892716169357,
      "learning_rate": 0.0018494942247999939,
      "loss": 1.2157,
      "step": 1512
    },
    {
      "epoch": 2.017333333333333,
      "grad_norm": 0.04154293239116669,
      "learning_rate": 0.001849266310859053,
      "loss": 1.2934,
      "step": 1513
    },
    {
      "epoch": 2.018666666666667,
      "grad_norm": 0.027358273044228554,
      "learning_rate": 0.0018490382385463894,
      "loss": 1.088,
      "step": 1514
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.025253716856241226,
      "learning_rate": 0.0018488100079045344,
      "loss": 0.9831,
      "step": 1515
    },
    {
      "epoch": 2.021333333333333,
      "grad_norm": 0.033655378967523575,
      "learning_rate": 0.001848581618976049,
      "loss": 1.1799,
      "step": 1516
    },
    {
      "epoch": 2.022666666666667,
      "grad_norm": 0.0278474148362875,
      "learning_rate": 0.0018483530718035228,
      "loss": 0.9627,
      "step": 1517
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.02975114807486534,
      "learning_rate": 0.001848124366429576,
      "loss": 1.4675,
      "step": 1518
    },
    {
      "epoch": 2.025333333333333,
      "grad_norm": 0.030529076233506203,
      "learning_rate": 0.001847895502896857,
      "loss": 1.0013,
      "step": 1519
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.029047828167676926,
      "learning_rate": 0.0018476664812480446,
      "loss": 1.0786,
      "step": 1520
    },
    {
      "epoch": 2.028,
      "grad_norm": 0.03170185908675194,
      "learning_rate": 0.0018474373015258472,
      "loss": 1.2356,
      "step": 1521
    },
    {
      "epoch": 2.029333333333333,
      "grad_norm": 0.041586920619010925,
      "learning_rate": 0.0018472079637730024,
      "loss": 1.1554,
      "step": 1522
    },
    {
      "epoch": 2.030666666666667,
      "grad_norm": 0.042564209550619125,
      "learning_rate": 0.001846978468032277,
      "loss": 1.0688,
      "step": 1523
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.03285310044884682,
      "learning_rate": 0.001846748814346468,
      "loss": 1.1444,
      "step": 1524
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 0.08501674979925156,
      "learning_rate": 0.0018465190027584005,
      "loss": 1.1888,
      "step": 1525
    },
    {
      "epoch": 2.034666666666667,
      "grad_norm": 0.03264405205845833,
      "learning_rate": 0.0018462890333109303,
      "loss": 1.1598,
      "step": 1526
    },
    {
      "epoch": 2.036,
      "grad_norm": 0.04657170549035072,
      "learning_rate": 0.0018460589060469426,
      "loss": 1.0256,
      "step": 1527
    },
    {
      "epoch": 2.037333333333333,
      "grad_norm": 0.03040378727018833,
      "learning_rate": 0.0018458286210093512,
      "loss": 1.1372,
      "step": 1528
    },
    {
      "epoch": 2.038666666666667,
      "grad_norm": 0.030029231682419777,
      "learning_rate": 0.0018455981782411004,
      "loss": 1.1687,
      "step": 1529
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.043281398713588715,
      "learning_rate": 0.0018453675777851627,
      "loss": 0.9563,
      "step": 1530
    },
    {
      "epoch": 2.041333333333333,
      "grad_norm": 0.033876340836286545,
      "learning_rate": 0.0018451368196845408,
      "loss": 1.1639,
      "step": 1531
    },
    {
      "epoch": 2.042666666666667,
      "grad_norm": 0.05470160394906998,
      "learning_rate": 0.0018449059039822666,
      "loss": 1.4458,
      "step": 1532
    },
    {
      "epoch": 2.044,
      "grad_norm": 0.0533413402736187,
      "learning_rate": 0.0018446748307214017,
      "loss": 1.1314,
      "step": 1533
    },
    {
      "epoch": 2.0453333333333332,
      "grad_norm": 0.03326975181698799,
      "learning_rate": 0.0018444435999450364,
      "loss": 1.0742,
      "step": 1534
    },
    {
      "epoch": 2.046666666666667,
      "grad_norm": 0.028921373188495636,
      "learning_rate": 0.001844212211696291,
      "loss": 1.3089,
      "step": 1535
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.035996608436107635,
      "learning_rate": 0.001843980666018315,
      "loss": 1.2341,
      "step": 1536
    },
    {
      "epoch": 2.0493333333333332,
      "grad_norm": 0.04420936852693558,
      "learning_rate": 0.0018437489629542867,
      "loss": 1.1105,
      "step": 1537
    },
    {
      "epoch": 2.050666666666667,
      "grad_norm": 0.04404274746775627,
      "learning_rate": 0.0018435171025474148,
      "loss": 1.3581,
      "step": 1538
    },
    {
      "epoch": 2.052,
      "grad_norm": 0.033280953764915466,
      "learning_rate": 0.0018432850848409364,
      "loss": 1.1615,
      "step": 1539
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.05460378900170326,
      "learning_rate": 0.0018430529098781188,
      "loss": 1.2031,
      "step": 1540
    },
    {
      "epoch": 2.054666666666667,
      "grad_norm": 0.043018683791160583,
      "learning_rate": 0.0018428205777022573,
      "loss": 1.1804,
      "step": 1541
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.046292755752801895,
      "learning_rate": 0.0018425880883566781,
      "loss": 1.1582,
      "step": 1542
    },
    {
      "epoch": 2.0573333333333332,
      "grad_norm": 0.03400758281350136,
      "learning_rate": 0.001842355441884736,
      "loss": 1.1214,
      "step": 1543
    },
    {
      "epoch": 2.058666666666667,
      "grad_norm": 0.026490231975913048,
      "learning_rate": 0.0018421226383298142,
      "loss": 1.1327,
      "step": 1544
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.03161914274096489,
      "learning_rate": 0.001841889677735327,
      "loss": 1.1238,
      "step": 1545
    },
    {
      "epoch": 2.0613333333333332,
      "grad_norm": 0.02715485915541649,
      "learning_rate": 0.0018416565601447165,
      "loss": 1.2562,
      "step": 1546
    },
    {
      "epoch": 2.062666666666667,
      "grad_norm": 0.02938251756131649,
      "learning_rate": 0.001841423285601455,
      "loss": 1.2371,
      "step": 1547
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.036058422178030014,
      "learning_rate": 0.0018411898541490432,
      "loss": 0.9246,
      "step": 1548
    },
    {
      "epoch": 2.0653333333333332,
      "grad_norm": 0.03724882751703262,
      "learning_rate": 0.0018409562658310119,
      "loss": 0.9194,
      "step": 1549
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.034624651074409485,
      "learning_rate": 0.0018407225206909209,
      "loss": 1.1931,
      "step": 1550
    },
    {
      "epoch": 2.068,
      "grad_norm": 0.04064387455582619,
      "learning_rate": 0.0018404886187723588,
      "loss": 1.182,
      "step": 1551
    },
    {
      "epoch": 2.0693333333333332,
      "grad_norm": 0.03459921106696129,
      "learning_rate": 0.001840254560118944,
      "loss": 1.0921,
      "step": 1552
    },
    {
      "epoch": 2.070666666666667,
      "grad_norm": 0.04496217891573906,
      "learning_rate": 0.0018400203447743236,
      "loss": 1.1375,
      "step": 1553
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.02141891047358513,
      "learning_rate": 0.0018397859727821747,
      "loss": 1.0674,
      "step": 1554
    },
    {
      "epoch": 2.0733333333333333,
      "grad_norm": 0.046565279364585876,
      "learning_rate": 0.0018395514441862026,
      "loss": 0.9269,
      "step": 1555
    },
    {
      "epoch": 2.074666666666667,
      "grad_norm": 0.027784891426563263,
      "learning_rate": 0.001839316759030143,
      "loss": 1.0234,
      "step": 1556
    },
    {
      "epoch": 2.076,
      "grad_norm": 0.026486964896321297,
      "learning_rate": 0.0018390819173577599,
      "loss": 0.8707,
      "step": 1557
    },
    {
      "epoch": 2.0773333333333333,
      "grad_norm": 0.03265489265322685,
      "learning_rate": 0.0018388469192128461,
      "loss": 1.2177,
      "step": 1558
    },
    {
      "epoch": 2.078666666666667,
      "grad_norm": 0.04334312304854393,
      "learning_rate": 0.0018386117646392252,
      "loss": 1.0346,
      "step": 1559
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.03962106257677078,
      "learning_rate": 0.0018383764536807484,
      "loss": 0.9807,
      "step": 1560
    },
    {
      "epoch": 2.0813333333333333,
      "grad_norm": 0.03201364725828171,
      "learning_rate": 0.0018381409863812963,
      "loss": 1.2228,
      "step": 1561
    },
    {
      "epoch": 2.0826666666666664,
      "grad_norm": 0.02751343324780464,
      "learning_rate": 0.0018379053627847798,
      "loss": 1.2541,
      "step": 1562
    },
    {
      "epoch": 2.084,
      "grad_norm": 0.03017442300915718,
      "learning_rate": 0.0018376695829351378,
      "loss": 0.9563,
      "step": 1563
    },
    {
      "epoch": 2.0853333333333333,
      "grad_norm": 0.040118440985679626,
      "learning_rate": 0.0018374336468763384,
      "loss": 1.0588,
      "step": 1564
    },
    {
      "epoch": 2.086666666666667,
      "grad_norm": 0.03816062957048416,
      "learning_rate": 0.0018371975546523794,
      "loss": 1.1888,
      "step": 1565
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.03415035456418991,
      "learning_rate": 0.0018369613063072875,
      "loss": 0.9378,
      "step": 1566
    },
    {
      "epoch": 2.0893333333333333,
      "grad_norm": 0.028993602842092514,
      "learning_rate": 0.0018367249018851179,
      "loss": 1.0204,
      "step": 1567
    },
    {
      "epoch": 2.0906666666666665,
      "grad_norm": 0.042814262211322784,
      "learning_rate": 0.0018364883414299564,
      "loss": 1.3079,
      "step": 1568
    },
    {
      "epoch": 2.092,
      "grad_norm": 0.034047871828079224,
      "learning_rate": 0.0018362516249859162,
      "loss": 1.2425,
      "step": 1569
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.04307588189840317,
      "learning_rate": 0.0018360147525971402,
      "loss": 1.1048,
      "step": 1570
    },
    {
      "epoch": 2.0946666666666665,
      "grad_norm": 0.035420097410678864,
      "learning_rate": 0.0018357777243078015,
      "loss": 1.0951,
      "step": 1571
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.030951665714383125,
      "learning_rate": 0.0018355405401621002,
      "loss": 1.0475,
      "step": 1572
    },
    {
      "epoch": 2.0973333333333333,
      "grad_norm": 0.03390814736485481,
      "learning_rate": 0.0018353032002042672,
      "loss": 1.0997,
      "step": 1573
    },
    {
      "epoch": 2.0986666666666665,
      "grad_norm": 0.04838145524263382,
      "learning_rate": 0.0018350657044785613,
      "loss": 1.4146,
      "step": 1574
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.031868401914834976,
      "learning_rate": 0.0018348280530292712,
      "loss": 1.1362,
      "step": 1575
    },
    {
      "epoch": 2.1013333333333333,
      "grad_norm": 0.03241406008601189,
      "learning_rate": 0.0018345902459007144,
      "loss": 1.1548,
      "step": 1576
    },
    {
      "epoch": 2.1026666666666665,
      "grad_norm": 0.031007323414087296,
      "learning_rate": 0.001834352283137237,
      "loss": 0.9932,
      "step": 1577
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.031687408685684204,
      "learning_rate": 0.0018341141647832146,
      "loss": 0.8802,
      "step": 1578
    },
    {
      "epoch": 2.1053333333333333,
      "grad_norm": 0.034072622656822205,
      "learning_rate": 0.001833875890883052,
      "loss": 1.092,
      "step": 1579
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.030009085312485695,
      "learning_rate": 0.001833637461481182,
      "loss": 1.061,
      "step": 1580
    },
    {
      "epoch": 2.108,
      "grad_norm": 0.03078029304742813,
      "learning_rate": 0.0018333988766220674,
      "loss": 1.3165,
      "step": 1581
    },
    {
      "epoch": 2.1093333333333333,
      "grad_norm": 0.03436454012989998,
      "learning_rate": 0.0018331601363502,
      "loss": 1.392,
      "step": 1582
    },
    {
      "epoch": 2.1106666666666665,
      "grad_norm": 0.04147268086671829,
      "learning_rate": 0.0018329212407100994,
      "loss": 0.8812,
      "step": 1583
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.031422559171915054,
      "learning_rate": 0.0018326821897463159,
      "loss": 1.0731,
      "step": 1584
    },
    {
      "epoch": 2.1133333333333333,
      "grad_norm": 0.03252732753753662,
      "learning_rate": 0.0018324429835034275,
      "loss": 1.1624,
      "step": 1585
    },
    {
      "epoch": 2.1146666666666665,
      "grad_norm": 0.046844761818647385,
      "learning_rate": 0.0018322036220260413,
      "loss": 0.8959,
      "step": 1586
    },
    {
      "epoch": 2.116,
      "grad_norm": 0.045018069446086884,
      "learning_rate": 0.001831964105358794,
      "loss": 1.0941,
      "step": 1587
    },
    {
      "epoch": 2.1173333333333333,
      "grad_norm": 0.04557160288095474,
      "learning_rate": 0.0018317244335463504,
      "loss": 0.9714,
      "step": 1588
    },
    {
      "epoch": 2.1186666666666665,
      "grad_norm": 0.03206801041960716,
      "learning_rate": 0.001831484606633405,
      "loss": 0.9409,
      "step": 1589
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.036468785256147385,
      "learning_rate": 0.0018312446246646808,
      "loss": 1.1421,
      "step": 1590
    },
    {
      "epoch": 2.1213333333333333,
      "grad_norm": 0.03275672346353531,
      "learning_rate": 0.0018310044876849293,
      "loss": 0.9643,
      "step": 1591
    },
    {
      "epoch": 2.1226666666666665,
      "grad_norm": 0.026826292276382446,
      "learning_rate": 0.0018307641957389326,
      "loss": 0.9619,
      "step": 1592
    },
    {
      "epoch": 2.124,
      "grad_norm": 0.05341413617134094,
      "learning_rate": 0.0018305237488714994,
      "loss": 1.3254,
      "step": 1593
    },
    {
      "epoch": 2.1253333333333333,
      "grad_norm": 0.031641166657209396,
      "learning_rate": 0.0018302831471274687,
      "loss": 0.9655,
      "step": 1594
    },
    {
      "epoch": 2.1266666666666665,
      "grad_norm": 0.03449874371290207,
      "learning_rate": 0.0018300423905517078,
      "loss": 0.9793,
      "step": 1595
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.034817103296518326,
      "learning_rate": 0.0018298014791891138,
      "loss": 1.319,
      "step": 1596
    },
    {
      "epoch": 2.1293333333333333,
      "grad_norm": 0.040291670709848404,
      "learning_rate": 0.0018295604130846113,
      "loss": 1.2,
      "step": 1597
    },
    {
      "epoch": 2.1306666666666665,
      "grad_norm": 0.04754216596484184,
      "learning_rate": 0.0018293191922831552,
      "loss": 1.1775,
      "step": 1598
    },
    {
      "epoch": 2.132,
      "grad_norm": 0.028905445709824562,
      "learning_rate": 0.0018290778168297277,
      "loss": 1.1431,
      "step": 1599
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.029329994693398476,
      "learning_rate": 0.0018288362867693413,
      "loss": 0.894,
      "step": 1600
    },
    {
      "epoch": 2.1346666666666665,
      "grad_norm": 0.03736545890569687,
      "learning_rate": 0.0018285946021470362,
      "loss": 1.3753,
      "step": 1601
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.19068540632724762,
      "learning_rate": 0.0018283527630078825,
      "loss": 1.3406,
      "step": 1602
    },
    {
      "epoch": 2.1373333333333333,
      "grad_norm": 0.03317524120211601,
      "learning_rate": 0.0018281107693969777,
      "loss": 1.1353,
      "step": 1603
    },
    {
      "epoch": 2.1386666666666665,
      "grad_norm": 0.029504137113690376,
      "learning_rate": 0.0018278686213594499,
      "loss": 1.3085,
      "step": 1604
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.04874732345342636,
      "learning_rate": 0.001827626318940454,
      "loss": 1.1679,
      "step": 1605
    },
    {
      "epoch": 2.1413333333333333,
      "grad_norm": 0.04854116216301918,
      "learning_rate": 0.0018273838621851756,
      "loss": 1.345,
      "step": 1606
    },
    {
      "epoch": 2.1426666666666665,
      "grad_norm": 0.04465069994330406,
      "learning_rate": 0.0018271412511388272,
      "loss": 1.1151,
      "step": 1607
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.07977046072483063,
      "learning_rate": 0.0018268984858466522,
      "loss": 1.2119,
      "step": 1608
    },
    {
      "epoch": 2.1453333333333333,
      "grad_norm": 0.10774002969264984,
      "learning_rate": 0.0018266555663539207,
      "loss": 1.0146,
      "step": 1609
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.05182772874832153,
      "learning_rate": 0.001826412492705933,
      "loss": 1.3048,
      "step": 1610
    },
    {
      "epoch": 2.148,
      "grad_norm": 0.036410458385944366,
      "learning_rate": 0.0018261692649480174,
      "loss": 1.2641,
      "step": 1611
    },
    {
      "epoch": 2.1493333333333333,
      "grad_norm": 0.05257286876440048,
      "learning_rate": 0.0018259258831255316,
      "loss": 0.8887,
      "step": 1612
    },
    {
      "epoch": 2.1506666666666665,
      "grad_norm": 0.05909044295549393,
      "learning_rate": 0.0018256823472838609,
      "loss": 1.23,
      "step": 1613
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.04131048917770386,
      "learning_rate": 0.0018254386574684206,
      "loss": 1.0294,
      "step": 1614
    },
    {
      "epoch": 2.1533333333333333,
      "grad_norm": 0.0457046739757061,
      "learning_rate": 0.0018251948137246537,
      "loss": 1.1295,
      "step": 1615
    },
    {
      "epoch": 2.1546666666666665,
      "grad_norm": 0.12662458419799805,
      "learning_rate": 0.0018249508160980326,
      "loss": 1.2221,
      "step": 1616
    },
    {
      "epoch": 2.156,
      "grad_norm": 0.03341729938983917,
      "learning_rate": 0.001824706664634058,
      "loss": 0.9157,
      "step": 1617
    },
    {
      "epoch": 2.1573333333333333,
      "grad_norm": 0.06334402412176132,
      "learning_rate": 0.0018244623593782596,
      "loss": 1.0748,
      "step": 1618
    },
    {
      "epoch": 2.1586666666666665,
      "grad_norm": 0.07658596336841583,
      "learning_rate": 0.0018242179003761954,
      "loss": 1.1852,
      "step": 1619
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.08540394902229309,
      "learning_rate": 0.0018239732876734526,
      "loss": 1.1042,
      "step": 1620
    },
    {
      "epoch": 2.1613333333333333,
      "grad_norm": 0.16674180328845978,
      "learning_rate": 0.0018237285213156462,
      "loss": 1.1894,
      "step": 1621
    },
    {
      "epoch": 2.1626666666666665,
      "grad_norm": 0.04831162840127945,
      "learning_rate": 0.0018234836013484208,
      "loss": 1.2278,
      "step": 1622
    },
    {
      "epoch": 2.164,
      "grad_norm": 0.02974887564778328,
      "learning_rate": 0.001823238527817449,
      "loss": 1.167,
      "step": 1623
    },
    {
      "epoch": 2.1653333333333333,
      "grad_norm": 0.06757490336894989,
      "learning_rate": 0.0018229933007684325,
      "loss": 1.1919,
      "step": 1624
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 0.03164302185177803,
      "learning_rate": 0.0018227479202471014,
      "loss": 1.1555,
      "step": 1625
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.027667276561260223,
      "learning_rate": 0.0018225023862992141,
      "loss": 1.0211,
      "step": 1626
    },
    {
      "epoch": 2.1693333333333333,
      "grad_norm": 0.12810708582401276,
      "learning_rate": 0.001822256698970558,
      "loss": 1.0947,
      "step": 1627
    },
    {
      "epoch": 2.1706666666666665,
      "grad_norm": 0.033021487295627594,
      "learning_rate": 0.0018220108583069492,
      "loss": 1.3015,
      "step": 1628
    },
    {
      "epoch": 2.172,
      "grad_norm": 0.027670400217175484,
      "learning_rate": 0.0018217648643542323,
      "loss": 1.1677,
      "step": 1629
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.02913912571966648,
      "learning_rate": 0.00182151871715828,
      "loss": 1.221,
      "step": 1630
    },
    {
      "epoch": 2.1746666666666665,
      "grad_norm": 0.0305685605853796,
      "learning_rate": 0.0018212724167649948,
      "loss": 0.9704,
      "step": 1631
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.04029713198542595,
      "learning_rate": 0.001821025963220306,
      "loss": 1.0245,
      "step": 1632
    },
    {
      "epoch": 2.1773333333333333,
      "grad_norm": 0.04308263957500458,
      "learning_rate": 0.001820779356570173,
      "loss": 1.0676,
      "step": 1633
    },
    {
      "epoch": 2.1786666666666665,
      "grad_norm": 0.04760883376002312,
      "learning_rate": 0.001820532596860583,
      "loss": 1.1344,
      "step": 1634
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.10429349541664124,
      "learning_rate": 0.0018202856841375518,
      "loss": 1.0058,
      "step": 1635
    },
    {
      "epoch": 2.1813333333333333,
      "grad_norm": 0.09797783941030502,
      "learning_rate": 0.001820038618447124,
      "loss": 1.048,
      "step": 1636
    },
    {
      "epoch": 2.1826666666666665,
      "grad_norm": 0.03851134702563286,
      "learning_rate": 0.0018197913998353726,
      "loss": 1.2203,
      "step": 1637
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.043880678713321686,
      "learning_rate": 0.001819544028348399,
      "loss": 0.9562,
      "step": 1638
    },
    {
      "epoch": 2.1853333333333333,
      "grad_norm": 0.037051137536764145,
      "learning_rate": 0.001819296504032333,
      "loss": 1.1029,
      "step": 1639
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.04430435225367546,
      "learning_rate": 0.0018190488269333334,
      "loss": 0.9472,
      "step": 1640
    },
    {
      "epoch": 2.188,
      "grad_norm": 0.043403007090091705,
      "learning_rate": 0.001818800997097587,
      "loss": 1.1952,
      "step": 1641
    },
    {
      "epoch": 2.1893333333333334,
      "grad_norm": 0.04232187196612358,
      "learning_rate": 0.0018185530145713093,
      "loss": 1.3875,
      "step": 1642
    },
    {
      "epoch": 2.1906666666666665,
      "grad_norm": 0.03396100178360939,
      "learning_rate": 0.0018183048794007445,
      "loss": 1.1807,
      "step": 1643
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.11884729564189911,
      "learning_rate": 0.0018180565916321645,
      "loss": 1.1825,
      "step": 1644
    },
    {
      "epoch": 2.1933333333333334,
      "grad_norm": 0.038095589727163315,
      "learning_rate": 0.0018178081513118706,
      "loss": 1.1225,
      "step": 1645
    },
    {
      "epoch": 2.1946666666666665,
      "grad_norm": 0.03028082847595215,
      "learning_rate": 0.001817559558486192,
      "loss": 1.0738,
      "step": 1646
    },
    {
      "epoch": 2.196,
      "grad_norm": 0.03438664227724075,
      "learning_rate": 0.0018173108132014861,
      "loss": 1.2055,
      "step": 1647
    },
    {
      "epoch": 2.1973333333333334,
      "grad_norm": 0.04236558452248573,
      "learning_rate": 0.0018170619155041398,
      "loss": 1.0521,
      "step": 1648
    },
    {
      "epoch": 2.1986666666666665,
      "grad_norm": 0.03687522932887077,
      "learning_rate": 0.001816812865440567,
      "loss": 0.9667,
      "step": 1649
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.04468117654323578,
      "learning_rate": 0.001816563663057211,
      "loss": 1.3664,
      "step": 1650
    },
    {
      "epoch": 2.2013333333333334,
      "grad_norm": 0.04563533142209053,
      "learning_rate": 0.001816314308400543,
      "loss": 0.8148,
      "step": 1651
    },
    {
      "epoch": 2.2026666666666666,
      "grad_norm": 0.034806057810783386,
      "learning_rate": 0.0018160648015170633,
      "loss": 0.9305,
      "step": 1652
    },
    {
      "epoch": 2.204,
      "grad_norm": 0.04529105871915817,
      "learning_rate": 0.0018158151424533,
      "loss": 0.9163,
      "step": 1653
    },
    {
      "epoch": 2.2053333333333334,
      "grad_norm": 0.028095286339521408,
      "learning_rate": 0.0018155653312558093,
      "loss": 1.0459,
      "step": 1654
    },
    {
      "epoch": 2.2066666666666666,
      "grad_norm": 0.04845855012536049,
      "learning_rate": 0.0018153153679711762,
      "loss": 1.0904,
      "step": 1655
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.04408740624785423,
      "learning_rate": 0.0018150652526460146,
      "loss": 1.2121,
      "step": 1656
    },
    {
      "epoch": 2.2093333333333334,
      "grad_norm": 0.029395513236522675,
      "learning_rate": 0.0018148149853269653,
      "loss": 1.0822,
      "step": 1657
    },
    {
      "epoch": 2.2106666666666666,
      "grad_norm": 0.04157914221286774,
      "learning_rate": 0.001814564566060699,
      "loss": 1.0859,
      "step": 1658
    },
    {
      "epoch": 2.212,
      "grad_norm": 0.035049669444561005,
      "learning_rate": 0.0018143139948939135,
      "loss": 1.2082,
      "step": 1659
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.03219219669699669,
      "learning_rate": 0.001814063271873336,
      "loss": 1.1309,
      "step": 1660
    },
    {
      "epoch": 2.2146666666666666,
      "grad_norm": 0.035565271973609924,
      "learning_rate": 0.001813812397045721,
      "loss": 0.9219,
      "step": 1661
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.02358989603817463,
      "learning_rate": 0.0018135613704578523,
      "loss": 1.002,
      "step": 1662
    },
    {
      "epoch": 2.2173333333333334,
      "grad_norm": 0.03082539513707161,
      "learning_rate": 0.0018133101921565414,
      "loss": 0.864,
      "step": 1663
    },
    {
      "epoch": 2.2186666666666666,
      "grad_norm": 0.032392315566539764,
      "learning_rate": 0.001813058862188628,
      "loss": 1.0673,
      "step": 1664
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.044918786734342575,
      "learning_rate": 0.00181280738060098,
      "loss": 1.0278,
      "step": 1665
    },
    {
      "epoch": 2.2213333333333334,
      "grad_norm": 0.028875350952148438,
      "learning_rate": 0.001812555747440494,
      "loss": 1.0543,
      "step": 1666
    },
    {
      "epoch": 2.2226666666666666,
      "grad_norm": 0.03857977315783501,
      "learning_rate": 0.0018123039627540953,
      "loss": 1.349,
      "step": 1667
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.03635900467634201,
      "learning_rate": 0.0018120520265887364,
      "loss": 1.2645,
      "step": 1668
    },
    {
      "epoch": 2.2253333333333334,
      "grad_norm": 0.030431773513555527,
      "learning_rate": 0.0018117999389913983,
      "loss": 1.0617,
      "step": 1669
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 0.052355945110321045,
      "learning_rate": 0.0018115477000090908,
      "loss": 1.2363,
      "step": 1670
    },
    {
      "epoch": 2.228,
      "grad_norm": 0.0353848971426487,
      "learning_rate": 0.0018112953096888517,
      "loss": 0.9955,
      "step": 1671
    },
    {
      "epoch": 2.2293333333333334,
      "grad_norm": 0.04861535131931305,
      "learning_rate": 0.0018110427680777463,
      "loss": 1.2152,
      "step": 1672
    },
    {
      "epoch": 2.2306666666666666,
      "grad_norm": 0.031338170170784,
      "learning_rate": 0.0018107900752228694,
      "loss": 1.0966,
      "step": 1673
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.049957167357206345,
      "learning_rate": 0.001810537231171343,
      "loss": 0.9451,
      "step": 1674
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 0.02995944581925869,
      "learning_rate": 0.0018102842359703176,
      "loss": 1.2084,
      "step": 1675
    },
    {
      "epoch": 2.2346666666666666,
      "grad_norm": 0.03569791093468666,
      "learning_rate": 0.0018100310896669721,
      "loss": 1.1391,
      "step": 1676
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 0.027840400114655495,
      "learning_rate": 0.0018097777923085131,
      "loss": 0.7835,
      "step": 1677
    },
    {
      "epoch": 2.2373333333333334,
      "grad_norm": 0.03857380151748657,
      "learning_rate": 0.0018095243439421758,
      "loss": 1.19,
      "step": 1678
    },
    {
      "epoch": 2.2386666666666666,
      "grad_norm": 0.03823813050985336,
      "learning_rate": 0.0018092707446152234,
      "loss": 1.2959,
      "step": 1679
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.029219556599855423,
      "learning_rate": 0.0018090169943749475,
      "loss": 1.1294,
      "step": 1680
    },
    {
      "epoch": 2.2413333333333334,
      "grad_norm": 0.0483291894197464,
      "learning_rate": 0.0018087630932686674,
      "loss": 1.0216,
      "step": 1681
    },
    {
      "epoch": 2.2426666666666666,
      "grad_norm": 0.03905878961086273,
      "learning_rate": 0.0018085090413437303,
      "loss": 1.3077,
      "step": 1682
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 0.04084055498242378,
      "learning_rate": 0.0018082548386475128,
      "loss": 1.1676,
      "step": 1683
    },
    {
      "epoch": 2.2453333333333334,
      "grad_norm": 0.04271857440471649,
      "learning_rate": 0.001808000485227418,
      "loss": 1.1752,
      "step": 1684
    },
    {
      "epoch": 2.2466666666666666,
      "grad_norm": 0.03370962664484978,
      "learning_rate": 0.0018077459811308785,
      "loss": 1.0173,
      "step": 1685
    },
    {
      "epoch": 2.248,
      "grad_norm": 0.04736679047346115,
      "learning_rate": 0.0018074913264053546,
      "loss": 1.1454,
      "step": 1686
    },
    {
      "epoch": 2.2493333333333334,
      "grad_norm": 0.03593824431300163,
      "learning_rate": 0.0018072365210983336,
      "loss": 0.8273,
      "step": 1687
    },
    {
      "epoch": 2.2506666666666666,
      "grad_norm": 0.04554356634616852,
      "learning_rate": 0.001806981565257332,
      "loss": 1.3934,
      "step": 1688
    },
    {
      "epoch": 2.252,
      "grad_norm": 0.02824927493929863,
      "learning_rate": 0.0018067264589298943,
      "loss": 1.0691,
      "step": 1689
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.06588491797447205,
      "learning_rate": 0.0018064712021635932,
      "loss": 1.0669,
      "step": 1690
    },
    {
      "epoch": 2.2546666666666666,
      "grad_norm": 0.03453746438026428,
      "learning_rate": 0.0018062157950060288,
      "loss": 0.9718,
      "step": 1691
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.0420086607336998,
      "learning_rate": 0.0018059602375048295,
      "loss": 0.9937,
      "step": 1692
    },
    {
      "epoch": 2.2573333333333334,
      "grad_norm": 0.03964634984731674,
      "learning_rate": 0.0018057045297076519,
      "loss": 1.1458,
      "step": 1693
    },
    {
      "epoch": 2.2586666666666666,
      "grad_norm": 0.040629856288433075,
      "learning_rate": 0.0018054486716621808,
      "loss": 0.9532,
      "step": 1694
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.031095270067453384,
      "learning_rate": 0.0018051926634161282,
      "loss": 1.0057,
      "step": 1695
    },
    {
      "epoch": 2.2613333333333334,
      "grad_norm": 0.04059898853302002,
      "learning_rate": 0.0018049365050172355,
      "loss": 1.4087,
      "step": 1696
    },
    {
      "epoch": 2.2626666666666666,
      "grad_norm": 0.034647032618522644,
      "learning_rate": 0.0018046801965132706,
      "loss": 1.205,
      "step": 1697
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.026125410571694374,
      "learning_rate": 0.0018044237379520305,
      "loss": 1.0686,
      "step": 1698
    },
    {
      "epoch": 2.2653333333333334,
      "grad_norm": 0.035278454422950745,
      "learning_rate": 0.0018041671293813394,
      "loss": 1.2284,
      "step": 1699
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.04565658047795296,
      "learning_rate": 0.00180391037084905,
      "loss": 1.2434,
      "step": 1700
    },
    {
      "epoch": 2.268,
      "grad_norm": 0.030469771474599838,
      "learning_rate": 0.0018036534624030428,
      "loss": 1.1417,
      "step": 1701
    },
    {
      "epoch": 2.2693333333333334,
      "grad_norm": 0.04211621358990669,
      "learning_rate": 0.0018033964040912265,
      "loss": 1.2422,
      "step": 1702
    },
    {
      "epoch": 2.2706666666666666,
      "grad_norm": 0.03067358396947384,
      "learning_rate": 0.0018031391959615368,
      "loss": 0.7956,
      "step": 1703
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.027364814653992653,
      "learning_rate": 0.0018028818380619387,
      "loss": 1.0465,
      "step": 1704
    },
    {
      "epoch": 2.2733333333333334,
      "grad_norm": 0.0408007949590683,
      "learning_rate": 0.0018026243304404244,
      "loss": 1.1447,
      "step": 1705
    },
    {
      "epoch": 2.2746666666666666,
      "grad_norm": 0.033113993704319,
      "learning_rate": 0.0018023666731450136,
      "loss": 0.9939,
      "step": 1706
    },
    {
      "epoch": 2.276,
      "grad_norm": 0.03716058284044266,
      "learning_rate": 0.001802108866223755,
      "loss": 0.8654,
      "step": 1707
    },
    {
      "epoch": 2.2773333333333334,
      "grad_norm": 0.03138978034257889,
      "learning_rate": 0.0018018509097247244,
      "loss": 1.033,
      "step": 1708
    },
    {
      "epoch": 2.2786666666666666,
      "grad_norm": 0.03387804701924324,
      "learning_rate": 0.0018015928036960254,
      "loss": 1.4128,
      "step": 1709
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.041250452399253845,
      "learning_rate": 0.00180133454818579,
      "loss": 1.156,
      "step": 1710
    },
    {
      "epoch": 2.2813333333333334,
      "grad_norm": 0.03419585898518562,
      "learning_rate": 0.0018010761432421779,
      "loss": 1.085,
      "step": 1711
    },
    {
      "epoch": 2.2826666666666666,
      "grad_norm": 0.03636215254664421,
      "learning_rate": 0.0018008175889133767,
      "loss": 1.1123,
      "step": 1712
    },
    {
      "epoch": 2.284,
      "grad_norm": 0.037826959043741226,
      "learning_rate": 0.0018005588852476016,
      "loss": 1.24,
      "step": 1713
    },
    {
      "epoch": 2.2853333333333334,
      "grad_norm": 0.03214343637228012,
      "learning_rate": 0.0018003000322930956,
      "loss": 1.0178,
      "step": 1714
    },
    {
      "epoch": 2.2866666666666666,
      "grad_norm": 0.05052069202065468,
      "learning_rate": 0.0018000410300981302,
      "loss": 1.1385,
      "step": 1715
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.03094184584915638,
      "learning_rate": 0.0017997818787110042,
      "loss": 1.2783,
      "step": 1716
    },
    {
      "epoch": 2.2893333333333334,
      "grad_norm": 0.0902366116642952,
      "learning_rate": 0.001799522578180044,
      "loss": 0.8201,
      "step": 1717
    },
    {
      "epoch": 2.2906666666666666,
      "grad_norm": 0.03597113862633705,
      "learning_rate": 0.0017992631285536046,
      "loss": 0.9059,
      "step": 1718
    },
    {
      "epoch": 2.292,
      "grad_norm": 0.03699377551674843,
      "learning_rate": 0.001799003529880068,
      "loss": 1.1312,
      "step": 1719
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.038199834525585175,
      "learning_rate": 0.0017987437822078443,
      "loss": 0.9957,
      "step": 1720
    },
    {
      "epoch": 2.2946666666666666,
      "grad_norm": 0.0351998545229435,
      "learning_rate": 0.0017984838855853718,
      "loss": 1.321,
      "step": 1721
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.04645036906003952,
      "learning_rate": 0.001798223840061116,
      "loss": 1.1695,
      "step": 1722
    },
    {
      "epoch": 2.2973333333333334,
      "grad_norm": 0.06070668622851372,
      "learning_rate": 0.00179796364568357,
      "loss": 1.2401,
      "step": 1723
    },
    {
      "epoch": 2.2986666666666666,
      "grad_norm": 0.03369631618261337,
      "learning_rate": 0.0017977033025012553,
      "loss": 1.2651,
      "step": 1724
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.039812520146369934,
      "learning_rate": 0.0017974428105627207,
      "loss": 1.1387,
      "step": 1725
    },
    {
      "epoch": 2.3013333333333335,
      "grad_norm": 0.09986761957406998,
      "learning_rate": 0.0017971821699165433,
      "loss": 1.1701,
      "step": 1726
    },
    {
      "epoch": 2.3026666666666666,
      "grad_norm": 0.07080382853746414,
      "learning_rate": 0.0017969213806113273,
      "loss": 0.8934,
      "step": 1727
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.029702529311180115,
      "learning_rate": 0.0017966604426957048,
      "loss": 1.1291,
      "step": 1728
    },
    {
      "epoch": 2.3053333333333335,
      "grad_norm": 0.028115401044487953,
      "learning_rate": 0.0017963993562183358,
      "loss": 0.8546,
      "step": 1729
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.04317856952548027,
      "learning_rate": 0.0017961381212279078,
      "loss": 1.2439,
      "step": 1730
    },
    {
      "epoch": 2.308,
      "grad_norm": 0.03333313763141632,
      "learning_rate": 0.0017958767377731358,
      "loss": 1.1956,
      "step": 1731
    },
    {
      "epoch": 2.3093333333333335,
      "grad_norm": 0.030578771606087685,
      "learning_rate": 0.0017956152059027631,
      "loss": 1.1173,
      "step": 1732
    },
    {
      "epoch": 2.3106666666666666,
      "grad_norm": 0.19584159553050995,
      "learning_rate": 0.0017953535256655605,
      "loss": 1.0458,
      "step": 1733
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.031134886667132378,
      "learning_rate": 0.0017950916971103258,
      "loss": 1.1581,
      "step": 1734
    },
    {
      "epoch": 2.3133333333333335,
      "grad_norm": 0.034805722534656525,
      "learning_rate": 0.0017948297202858851,
      "loss": 1.1178,
      "step": 1735
    },
    {
      "epoch": 2.3146666666666667,
      "grad_norm": 0.03567242994904518,
      "learning_rate": 0.0017945675952410923,
      "loss": 1.1185,
      "step": 1736
    },
    {
      "epoch": 2.316,
      "grad_norm": 0.031344834715127945,
      "learning_rate": 0.0017943053220248282,
      "loss": 1.1782,
      "step": 1737
    },
    {
      "epoch": 2.3173333333333335,
      "grad_norm": 0.08595309406518936,
      "learning_rate": 0.0017940429006860022,
      "loss": 1.0594,
      "step": 1738
    },
    {
      "epoch": 2.3186666666666667,
      "grad_norm": 0.038143713027238846,
      "learning_rate": 0.0017937803312735503,
      "loss": 1.2497,
      "step": 1739
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.035372767597436905,
      "learning_rate": 0.0017935176138364369,
      "loss": 1.0617,
      "step": 1740
    },
    {
      "epoch": 2.3213333333333335,
      "grad_norm": 0.04239934682846069,
      "learning_rate": 0.0017932547484236538,
      "loss": 0.9344,
      "step": 1741
    },
    {
      "epoch": 2.3226666666666667,
      "grad_norm": 0.09970785677433014,
      "learning_rate": 0.00179299173508422,
      "loss": 1.2976,
      "step": 1742
    },
    {
      "epoch": 2.324,
      "grad_norm": 0.02823290228843689,
      "learning_rate": 0.0017927285738671824,
      "loss": 1.134,
      "step": 1743
    },
    {
      "epoch": 2.3253333333333335,
      "grad_norm": 0.03258182853460312,
      "learning_rate": 0.0017924652648216158,
      "loss": 1.2484,
      "step": 1744
    },
    {
      "epoch": 2.3266666666666667,
      "grad_norm": 0.030590206384658813,
      "learning_rate": 0.001792201807996622,
      "loss": 0.8827,
      "step": 1745
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.15038266777992249,
      "learning_rate": 0.0017919382034413305,
      "loss": 1.0933,
      "step": 1746
    },
    {
      "epoch": 2.3293333333333335,
      "grad_norm": 0.03026830404996872,
      "learning_rate": 0.0017916744512048988,
      "loss": 1.1623,
      "step": 1747
    },
    {
      "epoch": 2.3306666666666667,
      "grad_norm": 0.037652187049388885,
      "learning_rate": 0.0017914105513365114,
      "loss": 1.0428,
      "step": 1748
    },
    {
      "epoch": 2.332,
      "grad_norm": 0.03560274839401245,
      "learning_rate": 0.0017911465038853804,
      "loss": 0.8626,
      "step": 1749
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.05173017829656601,
      "learning_rate": 0.0017908823089007458,
      "loss": 1.1475,
      "step": 1750
    },
    {
      "epoch": 2.3346666666666667,
      "grad_norm": 0.051743168383836746,
      "learning_rate": 0.0017906179664318742,
      "loss": 1.0132,
      "step": 1751
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.0336705818772316,
      "learning_rate": 0.0017903534765280613,
      "loss": 0.9336,
      "step": 1752
    },
    {
      "epoch": 2.3373333333333335,
      "grad_norm": 0.029300078749656677,
      "learning_rate": 0.0017900888392386287,
      "loss": 0.881,
      "step": 1753
    },
    {
      "epoch": 2.3386666666666667,
      "grad_norm": 0.09040197730064392,
      "learning_rate": 0.0017898240546129266,
      "loss": 1.062,
      "step": 1754
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.029729235917329788,
      "learning_rate": 0.0017895591227003315,
      "loss": 1.087,
      "step": 1755
    },
    {
      "epoch": 2.3413333333333335,
      "grad_norm": 0.036320168524980545,
      "learning_rate": 0.0017892940435502488,
      "loss": 1.4125,
      "step": 1756
    },
    {
      "epoch": 2.3426666666666667,
      "grad_norm": 0.23262295126914978,
      "learning_rate": 0.00178902881721211,
      "loss": 1.3967,
      "step": 1757
    },
    {
      "epoch": 2.344,
      "grad_norm": 0.027778787538409233,
      "learning_rate": 0.0017887634437353753,
      "loss": 0.9664,
      "step": 1758
    },
    {
      "epoch": 2.3453333333333335,
      "grad_norm": 0.03037768043577671,
      "learning_rate": 0.001788497923169531,
      "loss": 1.0879,
      "step": 1759
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.059893619269132614,
      "learning_rate": 0.0017882322555640925,
      "loss": 1.2257,
      "step": 1760
    },
    {
      "epoch": 2.348,
      "grad_norm": 0.12460192292928696,
      "learning_rate": 0.0017879664409686008,
      "loss": 1.4975,
      "step": 1761
    },
    {
      "epoch": 2.3493333333333335,
      "grad_norm": 0.04613211005926132,
      "learning_rate": 0.0017877004794326257,
      "loss": 1.0365,
      "step": 1762
    },
    {
      "epoch": 2.3506666666666667,
      "grad_norm": 0.040938276797533035,
      "learning_rate": 0.0017874343710057633,
      "loss": 1.105,
      "step": 1763
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.16651339828968048,
      "learning_rate": 0.0017871681157376382,
      "loss": 1.3314,
      "step": 1764
    },
    {
      "epoch": 2.3533333333333335,
      "grad_norm": 0.060410432517528534,
      "learning_rate": 0.001786901713677902,
      "loss": 1.2257,
      "step": 1765
    },
    {
      "epoch": 2.3546666666666667,
      "grad_norm": 0.052317097783088684,
      "learning_rate": 0.0017866351648762326,
      "loss": 1.1273,
      "step": 1766
    },
    {
      "epoch": 2.356,
      "grad_norm": 0.0341738685965538,
      "learning_rate": 0.0017863684693823375,
      "loss": 1.0144,
      "step": 1767
    },
    {
      "epoch": 2.3573333333333335,
      "grad_norm": 0.052692994475364685,
      "learning_rate": 0.001786101627245949,
      "loss": 0.9721,
      "step": 1768
    },
    {
      "epoch": 2.3586666666666667,
      "grad_norm": 0.03822941705584526,
      "learning_rate": 0.0017858346385168286,
      "loss": 0.9676,
      "step": 1769
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.09847387671470642,
      "learning_rate": 0.0017855675032447647,
      "loss": 0.8639,
      "step": 1770
    },
    {
      "epoch": 2.3613333333333335,
      "grad_norm": 0.043565038591623306,
      "learning_rate": 0.0017853002214795724,
      "loss": 1.0312,
      "step": 1771
    },
    {
      "epoch": 2.3626666666666667,
      "grad_norm": 0.037510454654693604,
      "learning_rate": 0.001785032793271095,
      "loss": 1.2343,
      "step": 1772
    },
    {
      "epoch": 2.364,
      "grad_norm": 0.036151375621557236,
      "learning_rate": 0.0017847652186692025,
      "loss": 1.0979,
      "step": 1773
    },
    {
      "epoch": 2.3653333333333335,
      "grad_norm": 0.05663544684648514,
      "learning_rate": 0.0017844974977237922,
      "loss": 0.9264,
      "step": 1774
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 0.03927619010210037,
      "learning_rate": 0.0017842296304847892,
      "loss": 1.067,
      "step": 1775
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.06601516902446747,
      "learning_rate": 0.001783961617002145,
      "loss": 1.2388,
      "step": 1776
    },
    {
      "epoch": 2.3693333333333335,
      "grad_norm": 0.03471463918685913,
      "learning_rate": 0.0017836934573258398,
      "loss": 1.2929,
      "step": 1777
    },
    {
      "epoch": 2.3706666666666667,
      "grad_norm": 0.03234587982296944,
      "learning_rate": 0.0017834251515058796,
      "loss": 0.9758,
      "step": 1778
    },
    {
      "epoch": 2.372,
      "grad_norm": 0.0383436493575573,
      "learning_rate": 0.0017831566995922983,
      "loss": 1.1954,
      "step": 1779
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.03241103142499924,
      "learning_rate": 0.001782888101635157,
      "loss": 0.7187,
      "step": 1780
    },
    {
      "epoch": 2.3746666666666667,
      "grad_norm": 0.0412507988512516,
      "learning_rate": 0.0017826193576845437,
      "loss": 1.1048,
      "step": 1781
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.036640431731939316,
      "learning_rate": 0.0017823504677905748,
      "loss": 1.2559,
      "step": 1782
    },
    {
      "epoch": 2.3773333333333335,
      "grad_norm": 0.030063489452004433,
      "learning_rate": 0.0017820814320033925,
      "loss": 1.1776,
      "step": 1783
    },
    {
      "epoch": 2.3786666666666667,
      "grad_norm": 0.031771220266819,
      "learning_rate": 0.0017818122503731667,
      "loss": 1.2755,
      "step": 1784
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.03764480724930763,
      "learning_rate": 0.0017815429229500945,
      "loss": 1.0223,
      "step": 1785
    },
    {
      "epoch": 2.3813333333333335,
      "grad_norm": 0.032228320837020874,
      "learning_rate": 0.0017812734497844006,
      "loss": 1.1901,
      "step": 1786
    },
    {
      "epoch": 2.3826666666666667,
      "grad_norm": 0.03026685304939747,
      "learning_rate": 0.0017810038309263362,
      "loss": 1.152,
      "step": 1787
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.03129875287413597,
      "learning_rate": 0.0017807340664261803,
      "loss": 1.0957,
      "step": 1788
    },
    {
      "epoch": 2.3853333333333335,
      "grad_norm": 0.03441300615668297,
      "learning_rate": 0.0017804641563342384,
      "loss": 1.2054,
      "step": 1789
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.03896637260913849,
      "learning_rate": 0.001780194100700844,
      "loss": 1.0362,
      "step": 1790
    },
    {
      "epoch": 2.388,
      "grad_norm": 0.054559703916311264,
      "learning_rate": 0.0017799238995763566,
      "loss": 1.1567,
      "step": 1791
    },
    {
      "epoch": 2.389333333333333,
      "grad_norm": 0.04548582434654236,
      "learning_rate": 0.0017796535530111642,
      "loss": 1.0237,
      "step": 1792
    },
    {
      "epoch": 2.3906666666666667,
      "grad_norm": 0.021943435072898865,
      "learning_rate": 0.0017793830610556807,
      "loss": 1.0219,
      "step": 1793
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.031209014356136322,
      "learning_rate": 0.0017791124237603476,
      "loss": 1.0604,
      "step": 1794
    },
    {
      "epoch": 2.3933333333333335,
      "grad_norm": 0.03615739941596985,
      "learning_rate": 0.0017788416411756338,
      "loss": 1.214,
      "step": 1795
    },
    {
      "epoch": 2.3946666666666667,
      "grad_norm": 0.060399722307920456,
      "learning_rate": 0.0017785707133520345,
      "loss": 0.7537,
      "step": 1796
    },
    {
      "epoch": 2.396,
      "grad_norm": 0.035668402910232544,
      "learning_rate": 0.0017782996403400736,
      "loss": 1.3504,
      "step": 1797
    },
    {
      "epoch": 2.397333333333333,
      "grad_norm": 0.025548497214913368,
      "learning_rate": 0.0017780284221902995,
      "loss": 0.7873,
      "step": 1798
    },
    {
      "epoch": 2.3986666666666667,
      "grad_norm": 0.028521409258246422,
      "learning_rate": 0.0017777570589532902,
      "loss": 1.4767,
      "step": 1799
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.02875196374952793,
      "learning_rate": 0.0017774855506796495,
      "loss": 1.3289,
      "step": 1800
    },
    {
      "epoch": 2.4013333333333335,
      "grad_norm": 0.03630419448018074,
      "learning_rate": 0.001777213897420008,
      "loss": 1.0493,
      "step": 1801
    },
    {
      "epoch": 2.4026666666666667,
      "grad_norm": 0.028192274272441864,
      "learning_rate": 0.0017769420992250244,
      "loss": 1.0367,
      "step": 1802
    },
    {
      "epoch": 2.404,
      "grad_norm": 0.038275036960840225,
      "learning_rate": 0.001776670156145383,
      "loss": 0.971,
      "step": 1803
    },
    {
      "epoch": 2.405333333333333,
      "grad_norm": 0.04102632403373718,
      "learning_rate": 0.0017763980682317965,
      "loss": 0.9339,
      "step": 1804
    },
    {
      "epoch": 2.4066666666666667,
      "grad_norm": 0.034298546612262726,
      "learning_rate": 0.0017761258355350037,
      "loss": 1.1935,
      "step": 1805
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.06290225684642792,
      "learning_rate": 0.0017758534581057716,
      "loss": 1.0231,
      "step": 1806
    },
    {
      "epoch": 2.4093333333333335,
      "grad_norm": 0.0270040575414896,
      "learning_rate": 0.0017755809359948922,
      "loss": 1.0656,
      "step": 1807
    },
    {
      "epoch": 2.4106666666666667,
      "grad_norm": 0.025817429646849632,
      "learning_rate": 0.0017753082692531861,
      "loss": 1.3573,
      "step": 1808
    },
    {
      "epoch": 2.412,
      "grad_norm": 0.03418901190161705,
      "learning_rate": 0.0017750354579315001,
      "loss": 0.8279,
      "step": 1809
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 0.04308672994375229,
      "learning_rate": 0.001774762502080709,
      "loss": 0.7867,
      "step": 1810
    },
    {
      "epoch": 2.4146666666666667,
      "grad_norm": 0.03089023195207119,
      "learning_rate": 0.0017744894017517128,
      "loss": 0.8859,
      "step": 1811
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.026297254487872124,
      "learning_rate": 0.0017742161569954398,
      "loss": 1.1141,
      "step": 1812
    },
    {
      "epoch": 2.4173333333333336,
      "grad_norm": 0.03513956815004349,
      "learning_rate": 0.0017739427678628453,
      "loss": 1.0223,
      "step": 1813
    },
    {
      "epoch": 2.4186666666666667,
      "grad_norm": 0.03486934304237366,
      "learning_rate": 0.00177366923440491,
      "loss": 1.1967,
      "step": 1814
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.03707578405737877,
      "learning_rate": 0.0017733955566726438,
      "loss": 1.3181,
      "step": 1815
    },
    {
      "epoch": 2.421333333333333,
      "grad_norm": 0.02639029547572136,
      "learning_rate": 0.0017731217347170816,
      "loss": 1.0904,
      "step": 1816
    },
    {
      "epoch": 2.4226666666666667,
      "grad_norm": 0.030451657250523567,
      "learning_rate": 0.0017728477685892865,
      "loss": 0.9375,
      "step": 1817
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.029582930728793144,
      "learning_rate": 0.0017725736583403468,
      "loss": 1.0174,
      "step": 1818
    },
    {
      "epoch": 2.4253333333333336,
      "grad_norm": 0.03533751890063286,
      "learning_rate": 0.0017722994040213797,
      "loss": 0.9878,
      "step": 1819
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.02851272001862526,
      "learning_rate": 0.001772025005683528,
      "loss": 0.919,
      "step": 1820
    },
    {
      "epoch": 2.428,
      "grad_norm": 0.038304027169942856,
      "learning_rate": 0.0017717504633779618,
      "loss": 0.9427,
      "step": 1821
    },
    {
      "epoch": 2.429333333333333,
      "grad_norm": 0.02681264840066433,
      "learning_rate": 0.0017714757771558776,
      "loss": 1.0949,
      "step": 1822
    },
    {
      "epoch": 2.4306666666666668,
      "grad_norm": 0.036532603204250336,
      "learning_rate": 0.0017712009470684993,
      "loss": 1.1866,
      "step": 1823
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.061168231070041656,
      "learning_rate": 0.0017709259731670773,
      "loss": 0.9206,
      "step": 1824
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 0.0319136306643486,
      "learning_rate": 0.0017706508555028894,
      "loss": 1.0082,
      "step": 1825
    },
    {
      "epoch": 2.4346666666666668,
      "grad_norm": 0.034186892211437225,
      "learning_rate": 0.0017703755941272388,
      "loss": 1.156,
      "step": 1826
    },
    {
      "epoch": 2.436,
      "grad_norm": 0.03690517321228981,
      "learning_rate": 0.0017701001890914573,
      "loss": 1.2836,
      "step": 1827
    },
    {
      "epoch": 2.437333333333333,
      "grad_norm": 0.035963237285614014,
      "learning_rate": 0.001769824640446902,
      "loss": 1.3316,
      "step": 1828
    },
    {
      "epoch": 2.4386666666666668,
      "grad_norm": 0.03509199619293213,
      "learning_rate": 0.0017695489482449575,
      "loss": 0.9798,
      "step": 1829
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.04518967494368553,
      "learning_rate": 0.0017692731125370353,
      "loss": 1.0611,
      "step": 1830
    },
    {
      "epoch": 2.4413333333333336,
      "grad_norm": 0.03678644821047783,
      "learning_rate": 0.0017689971333745733,
      "loss": 1.22,
      "step": 1831
    },
    {
      "epoch": 2.4426666666666668,
      "grad_norm": 0.03158094361424446,
      "learning_rate": 0.0017687210108090362,
      "loss": 0.9778,
      "step": 1832
    },
    {
      "epoch": 2.444,
      "grad_norm": 0.05484827980399132,
      "learning_rate": 0.0017684447448919154,
      "loss": 1.6631,
      "step": 1833
    },
    {
      "epoch": 2.445333333333333,
      "grad_norm": 0.030619457364082336,
      "learning_rate": 0.0017681683356747296,
      "loss": 1.1729,
      "step": 1834
    },
    {
      "epoch": 2.4466666666666668,
      "grad_norm": 0.0368243083357811,
      "learning_rate": 0.0017678917832090234,
      "loss": 1.0772,
      "step": 1835
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.04346098005771637,
      "learning_rate": 0.0017676150875463685,
      "loss": 1.0152,
      "step": 1836
    },
    {
      "epoch": 2.449333333333333,
      "grad_norm": 0.031813595443964005,
      "learning_rate": 0.0017673382487383633,
      "loss": 1.2632,
      "step": 1837
    },
    {
      "epoch": 2.4506666666666668,
      "grad_norm": 0.030687116086483,
      "learning_rate": 0.0017670612668366328,
      "loss": 1.1283,
      "step": 1838
    },
    {
      "epoch": 2.452,
      "grad_norm": 0.03923732042312622,
      "learning_rate": 0.001766784141892829,
      "loss": 0.9816,
      "step": 1839
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.03170732408761978,
      "learning_rate": 0.0017665068739586304,
      "loss": 0.9982,
      "step": 1840
    },
    {
      "epoch": 2.4546666666666668,
      "grad_norm": 0.03755863383412361,
      "learning_rate": 0.0017662294630857416,
      "loss": 1.1999,
      "step": 1841
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.02729463018476963,
      "learning_rate": 0.001765951909325895,
      "loss": 1.3778,
      "step": 1842
    },
    {
      "epoch": 2.457333333333333,
      "grad_norm": 0.03161436691880226,
      "learning_rate": 0.0017656742127308482,
      "loss": 1.3041,
      "step": 1843
    },
    {
      "epoch": 2.458666666666667,
      "grad_norm": 0.0338800810277462,
      "learning_rate": 0.0017653963733523872,
      "loss": 1.1494,
      "step": 1844
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.03219587728381157,
      "learning_rate": 0.0017651183912423228,
      "loss": 1.1403,
      "step": 1845
    },
    {
      "epoch": 2.461333333333333,
      "grad_norm": 0.031543340533971786,
      "learning_rate": 0.001764840266452494,
      "loss": 1.0327,
      "step": 1846
    },
    {
      "epoch": 2.462666666666667,
      "grad_norm": 0.03982898220419884,
      "learning_rate": 0.001764561999034765,
      "loss": 1.0378,
      "step": 1847
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.03233126178383827,
      "learning_rate": 0.0017642835890410277,
      "loss": 1.2928,
      "step": 1848
    },
    {
      "epoch": 2.465333333333333,
      "grad_norm": 0.030925976112484932,
      "learning_rate": 0.0017640050365232002,
      "loss": 1.1425,
      "step": 1849
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.02965587191283703,
      "learning_rate": 0.001763726341533227,
      "loss": 1.0215,
      "step": 1850
    },
    {
      "epoch": 2.468,
      "grad_norm": 0.03085949644446373,
      "learning_rate": 0.0017634475041230795,
      "loss": 1.2446,
      "step": 1851
    },
    {
      "epoch": 2.469333333333333,
      "grad_norm": 0.03662377968430519,
      "learning_rate": 0.001763168524344755,
      "loss": 1.1081,
      "step": 1852
    },
    {
      "epoch": 2.470666666666667,
      "grad_norm": 0.05134524777531624,
      "learning_rate": 0.0017628894022502783,
      "loss": 1.0003,
      "step": 1853
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.03492407873272896,
      "learning_rate": 0.0017626101378917004,
      "loss": 1.3946,
      "step": 1854
    },
    {
      "epoch": 2.473333333333333,
      "grad_norm": 0.02809261716902256,
      "learning_rate": 0.0017623307313210984,
      "loss": 1.044,
      "step": 1855
    },
    {
      "epoch": 2.474666666666667,
      "grad_norm": 0.03902648016810417,
      "learning_rate": 0.001762051182590576,
      "loss": 1.0221,
      "step": 1856
    },
    {
      "epoch": 2.476,
      "grad_norm": 0.03338392823934555,
      "learning_rate": 0.0017617714917522638,
      "loss": 1.0463,
      "step": 1857
    },
    {
      "epoch": 2.477333333333333,
      "grad_norm": 0.03520733490586281,
      "learning_rate": 0.001761491658858319,
      "loss": 0.8523,
      "step": 1858
    },
    {
      "epoch": 2.478666666666667,
      "grad_norm": 0.04603138938546181,
      "learning_rate": 0.001761211683960925,
      "loss": 1.0081,
      "step": 1859
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.029111968353390694,
      "learning_rate": 0.0017609315671122912,
      "loss": 0.79,
      "step": 1860
    },
    {
      "epoch": 2.481333333333333,
      "grad_norm": 0.02865162491798401,
      "learning_rate": 0.0017606513083646546,
      "loss": 1.1507,
      "step": 1861
    },
    {
      "epoch": 2.482666666666667,
      "grad_norm": 0.03752179816365242,
      "learning_rate": 0.0017603709077702772,
      "loss": 1.1612,
      "step": 1862
    },
    {
      "epoch": 2.484,
      "grad_norm": 0.0258791521191597,
      "learning_rate": 0.001760090365381449,
      "loss": 0.9639,
      "step": 1863
    },
    {
      "epoch": 2.485333333333333,
      "grad_norm": 0.03794706612825394,
      "learning_rate": 0.0017598096812504855,
      "loss": 0.9213,
      "step": 1864
    },
    {
      "epoch": 2.486666666666667,
      "grad_norm": 0.0318702831864357,
      "learning_rate": 0.0017595288554297292,
      "loss": 1.2821,
      "step": 1865
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.030744798481464386,
      "learning_rate": 0.001759247887971548,
      "loss": 0.9946,
      "step": 1866
    },
    {
      "epoch": 2.489333333333333,
      "grad_norm": 0.03663735091686249,
      "learning_rate": 0.0017589667789283375,
      "loss": 1.0183,
      "step": 1867
    },
    {
      "epoch": 2.490666666666667,
      "grad_norm": 0.040438976138830185,
      "learning_rate": 0.0017586855283525185,
      "loss": 1.1402,
      "step": 1868
    },
    {
      "epoch": 2.492,
      "grad_norm": 0.050160542130470276,
      "learning_rate": 0.0017584041362965395,
      "loss": 1.081,
      "step": 1869
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.04451470822095871,
      "learning_rate": 0.001758122602812874,
      "loss": 0.9779,
      "step": 1870
    },
    {
      "epoch": 2.494666666666667,
      "grad_norm": 0.02749297395348549,
      "learning_rate": 0.0017578409279540227,
      "loss": 0.9873,
      "step": 1871
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.04220397770404816,
      "learning_rate": 0.0017575591117725132,
      "loss": 1.0504,
      "step": 1872
    },
    {
      "epoch": 2.497333333333333,
      "grad_norm": 0.03409822657704353,
      "learning_rate": 0.0017572771543208977,
      "loss": 1.1333,
      "step": 1873
    },
    {
      "epoch": 2.498666666666667,
      "grad_norm": 0.03776745870709419,
      "learning_rate": 0.0017569950556517563,
      "loss": 1.0386,
      "step": 1874
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.027868468314409256,
      "learning_rate": 0.0017567128158176952,
      "loss": 1.025,
      "step": 1875
    },
    {
      "epoch": 2.501333333333333,
      "grad_norm": 0.061672892421483994,
      "learning_rate": 0.0017564304348713464,
      "loss": 0.8546,
      "step": 1876
    },
    {
      "epoch": 2.502666666666667,
      "grad_norm": 0.029656067490577698,
      "learning_rate": 0.0017561479128653686,
      "loss": 1.1196,
      "step": 1877
    },
    {
      "epoch": 2.504,
      "grad_norm": 0.0322490856051445,
      "learning_rate": 0.0017558652498524461,
      "loss": 0.863,
      "step": 1878
    },
    {
      "epoch": 2.505333333333333,
      "grad_norm": 0.03603831306099892,
      "learning_rate": 0.0017555824458852908,
      "loss": 1.1456,
      "step": 1879
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.026717795059084892,
      "learning_rate": 0.0017552995010166402,
      "loss": 1.1255,
      "step": 1880
    },
    {
      "epoch": 2.508,
      "grad_norm": 0.03693490847945213,
      "learning_rate": 0.001755016415299257,
      "loss": 1.254,
      "step": 1881
    },
    {
      "epoch": 2.509333333333333,
      "grad_norm": 0.034742843359708786,
      "learning_rate": 0.0017547331887859325,
      "loss": 0.9058,
      "step": 1882
    },
    {
      "epoch": 2.510666666666667,
      "grad_norm": 0.031570591032505035,
      "learning_rate": 0.0017544498215294826,
      "loss": 0.7967,
      "step": 1883
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.03368833661079407,
      "learning_rate": 0.0017541663135827493,
      "loss": 1.0544,
      "step": 1884
    },
    {
      "epoch": 2.513333333333333,
      "grad_norm": 0.03750072419643402,
      "learning_rate": 0.001753882664998602,
      "loss": 1.0748,
      "step": 1885
    },
    {
      "epoch": 2.514666666666667,
      "grad_norm": 0.039020802825689316,
      "learning_rate": 0.001753598875829935,
      "loss": 1.0343,
      "step": 1886
    },
    {
      "epoch": 2.516,
      "grad_norm": 0.029885658994317055,
      "learning_rate": 0.0017533149461296698,
      "loss": 1.0165,
      "step": 1887
    },
    {
      "epoch": 2.517333333333333,
      "grad_norm": 0.033057644963264465,
      "learning_rate": 0.001753030875950754,
      "loss": 1.0208,
      "step": 1888
    },
    {
      "epoch": 2.518666666666667,
      "grad_norm": 0.23908738791942596,
      "learning_rate": 0.0017527466653461609,
      "loss": 1.2208,
      "step": 1889
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.03492164984345436,
      "learning_rate": 0.0017524623143688903,
      "loss": 0.8785,
      "step": 1890
    },
    {
      "epoch": 2.521333333333333,
      "grad_norm": 0.03373975679278374,
      "learning_rate": 0.0017521778230719682,
      "loss": 1.2371,
      "step": 1891
    },
    {
      "epoch": 2.522666666666667,
      "grad_norm": 0.044494688510894775,
      "learning_rate": 0.001751893191508447,
      "loss": 1.2331,
      "step": 1892
    },
    {
      "epoch": 2.524,
      "grad_norm": 0.03650861606001854,
      "learning_rate": 0.0017516084197314043,
      "loss": 1.0097,
      "step": 1893
    },
    {
      "epoch": 2.525333333333333,
      "grad_norm": 0.03320329263806343,
      "learning_rate": 0.0017513235077939453,
      "loss": 1.1105,
      "step": 1894
    },
    {
      "epoch": 2.5266666666666664,
      "grad_norm": 0.03855917602777481,
      "learning_rate": 0.0017510384557492,
      "loss": 0.9187,
      "step": 1895
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.027209850028157234,
      "learning_rate": 0.0017507532636503256,
      "loss": 0.8389,
      "step": 1896
    },
    {
      "epoch": 2.529333333333333,
      "grad_norm": 0.04307852312922478,
      "learning_rate": 0.0017504679315505044,
      "loss": 0.8599,
      "step": 1897
    },
    {
      "epoch": 2.530666666666667,
      "grad_norm": 0.044224899262189865,
      "learning_rate": 0.0017501824595029453,
      "loss": 0.8954,
      "step": 1898
    },
    {
      "epoch": 2.532,
      "grad_norm": 0.03691231086850166,
      "learning_rate": 0.0017498968475608838,
      "loss": 0.993,
      "step": 1899
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.02693348191678524,
      "learning_rate": 0.0017496110957775808,
      "loss": 0.8688,
      "step": 1900
    },
    {
      "epoch": 2.5346666666666664,
      "grad_norm": 0.027915459126234055,
      "learning_rate": 0.001749325204206323,
      "loss": 0.7346,
      "step": 1901
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.1160968542098999,
      "learning_rate": 0.0017490391729004242,
      "loss": 1.0864,
      "step": 1902
    },
    {
      "epoch": 2.537333333333333,
      "grad_norm": 0.03367406874895096,
      "learning_rate": 0.0017487530019132235,
      "loss": 0.9938,
      "step": 1903
    },
    {
      "epoch": 2.538666666666667,
      "grad_norm": 0.03567896783351898,
      "learning_rate": 0.0017484666912980864,
      "loss": 0.9022,
      "step": 1904
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.029370900243520737,
      "learning_rate": 0.001748180241108404,
      "loss": 0.8843,
      "step": 1905
    },
    {
      "epoch": 2.541333333333333,
      "grad_norm": 0.0331161729991436,
      "learning_rate": 0.001747893651397594,
      "loss": 1.029,
      "step": 1906
    },
    {
      "epoch": 2.5426666666666664,
      "grad_norm": 0.03600897267460823,
      "learning_rate": 0.0017476069222190996,
      "loss": 1.4099,
      "step": 1907
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.04450666159391403,
      "learning_rate": 0.0017473200536263906,
      "loss": 1.2551,
      "step": 1908
    },
    {
      "epoch": 2.5453333333333332,
      "grad_norm": 0.030659791082143784,
      "learning_rate": 0.0017470330456729618,
      "loss": 1.1007,
      "step": 1909
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 0.03088466078042984,
      "learning_rate": 0.001746745898412335,
      "loss": 0.9826,
      "step": 1910
    },
    {
      "epoch": 2.548,
      "grad_norm": 0.04555818811058998,
      "learning_rate": 0.0017464586118980578,
      "loss": 1.112,
      "step": 1911
    },
    {
      "epoch": 2.5493333333333332,
      "grad_norm": 0.031047262251377106,
      "learning_rate": 0.0017461711861837036,
      "loss": 1.0188,
      "step": 1912
    },
    {
      "epoch": 2.5506666666666664,
      "grad_norm": 0.04024512693285942,
      "learning_rate": 0.0017458836213228711,
      "loss": 1.0247,
      "step": 1913
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.03365034982562065,
      "learning_rate": 0.0017455959173691862,
      "loss": 0.9796,
      "step": 1914
    },
    {
      "epoch": 2.5533333333333332,
      "grad_norm": 0.03135377913713455,
      "learning_rate": 0.0017453080743763,
      "loss": 1.19,
      "step": 1915
    },
    {
      "epoch": 2.554666666666667,
      "grad_norm": 0.031916435807943344,
      "learning_rate": 0.0017450200923978894,
      "loss": 1.1818,
      "step": 1916
    },
    {
      "epoch": 2.556,
      "grad_norm": 0.037277717143297195,
      "learning_rate": 0.0017447319714876577,
      "loss": 1.045,
      "step": 1917
    },
    {
      "epoch": 2.5573333333333332,
      "grad_norm": 0.028818249702453613,
      "learning_rate": 0.0017444437116993336,
      "loss": 0.9515,
      "step": 1918
    },
    {
      "epoch": 2.5586666666666664,
      "grad_norm": 0.02838919870555401,
      "learning_rate": 0.0017441553130866723,
      "loss": 1.0253,
      "step": 1919
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.03208693861961365,
      "learning_rate": 0.0017438667757034545,
      "loss": 1.1508,
      "step": 1920
    },
    {
      "epoch": 2.5613333333333332,
      "grad_norm": 0.06879857182502747,
      "learning_rate": 0.0017435780996034867,
      "loss": 1.2878,
      "step": 1921
    },
    {
      "epoch": 2.562666666666667,
      "grad_norm": 0.04105928912758827,
      "learning_rate": 0.0017432892848406013,
      "loss": 1.0844,
      "step": 1922
    },
    {
      "epoch": 2.564,
      "grad_norm": 0.035399388521909714,
      "learning_rate": 0.001743000331468657,
      "loss": 1.1193,
      "step": 1923
    },
    {
      "epoch": 2.5653333333333332,
      "grad_norm": 0.04861779883503914,
      "learning_rate": 0.0017427112395415377,
      "loss": 1.1332,
      "step": 1924
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 0.026199307292699814,
      "learning_rate": 0.0017424220091131536,
      "loss": 1.0483,
      "step": 1925
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.03751998022198677,
      "learning_rate": 0.0017421326402374405,
      "loss": 0.9909,
      "step": 1926
    },
    {
      "epoch": 2.5693333333333332,
      "grad_norm": 0.029388923197984695,
      "learning_rate": 0.0017418431329683601,
      "loss": 1.323,
      "step": 1927
    },
    {
      "epoch": 2.570666666666667,
      "grad_norm": 0.03189375251531601,
      "learning_rate": 0.0017415534873599,
      "loss": 0.9134,
      "step": 1928
    },
    {
      "epoch": 2.572,
      "grad_norm": 0.031694378703832626,
      "learning_rate": 0.0017412637034660734,
      "loss": 1.1099,
      "step": 1929
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 0.0507865808904171,
      "learning_rate": 0.0017409737813409195,
      "loss": 1.3134,
      "step": 1930
    },
    {
      "epoch": 2.5746666666666664,
      "grad_norm": 0.02453240193426609,
      "learning_rate": 0.001740683721038503,
      "loss": 1.0524,
      "step": 1931
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.03554791584610939,
      "learning_rate": 0.0017403935226129148,
      "loss": 1.0314,
      "step": 1932
    },
    {
      "epoch": 2.5773333333333333,
      "grad_norm": 0.03371719270944595,
      "learning_rate": 0.0017401031861182708,
      "loss": 0.9404,
      "step": 1933
    },
    {
      "epoch": 2.578666666666667,
      "grad_norm": 0.030676379799842834,
      "learning_rate": 0.0017398127116087134,
      "loss": 1.1914,
      "step": 1934
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.02802792377769947,
      "learning_rate": 0.0017395220991384109,
      "loss": 1.1473,
      "step": 1935
    },
    {
      "epoch": 2.5813333333333333,
      "grad_norm": 0.02333354949951172,
      "learning_rate": 0.0017392313487615562,
      "loss": 0.9428,
      "step": 1936
    },
    {
      "epoch": 2.5826666666666664,
      "grad_norm": 0.02805374376475811,
      "learning_rate": 0.0017389404605323692,
      "loss": 0.9095,
      "step": 1937
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.03820542246103287,
      "learning_rate": 0.0017386494345050943,
      "loss": 1.2131,
      "step": 1938
    },
    {
      "epoch": 2.5853333333333333,
      "grad_norm": 0.03317071124911308,
      "learning_rate": 0.0017383582707340027,
      "loss": 0.9623,
      "step": 1939
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.03358094394207001,
      "learning_rate": 0.0017380669692733904,
      "loss": 0.982,
      "step": 1940
    },
    {
      "epoch": 2.588,
      "grad_norm": 0.02889959327876568,
      "learning_rate": 0.0017377755301775802,
      "loss": 0.8889,
      "step": 1941
    },
    {
      "epoch": 2.5893333333333333,
      "grad_norm": 0.03693389520049095,
      "learning_rate": 0.001737483953500919,
      "loss": 1.21,
      "step": 1942
    },
    {
      "epoch": 2.5906666666666665,
      "grad_norm": 0.028635825961828232,
      "learning_rate": 0.0017371922392977808,
      "loss": 0.9679,
      "step": 1943
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.03758758679032326,
      "learning_rate": 0.0017369003876225642,
      "loss": 1.0779,
      "step": 1944
    },
    {
      "epoch": 2.5933333333333333,
      "grad_norm": 0.02744816616177559,
      "learning_rate": 0.0017366083985296947,
      "loss": 1.1397,
      "step": 1945
    },
    {
      "epoch": 2.594666666666667,
      "grad_norm": 0.03191674128174782,
      "learning_rate": 0.0017363162720736213,
      "loss": 1.1138,
      "step": 1946
    },
    {
      "epoch": 2.596,
      "grad_norm": 0.0489296056330204,
      "learning_rate": 0.0017360240083088212,
      "loss": 1.0352,
      "step": 1947
    },
    {
      "epoch": 2.5973333333333333,
      "grad_norm": 0.03980083391070366,
      "learning_rate": 0.0017357316072897954,
      "loss": 0.8709,
      "step": 1948
    },
    {
      "epoch": 2.5986666666666665,
      "grad_norm": 0.045689892023801804,
      "learning_rate": 0.0017354390690710706,
      "loss": 1.0905,
      "step": 1949
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.030223501846194267,
      "learning_rate": 0.0017351463937072004,
      "loss": 1.1125,
      "step": 1950
    },
    {
      "epoch": 2.6013333333333333,
      "grad_norm": 0.02479376830160618,
      "learning_rate": 0.0017348535812527627,
      "loss": 0.9081,
      "step": 1951
    },
    {
      "epoch": 2.602666666666667,
      "grad_norm": 0.029660390689969063,
      "learning_rate": 0.0017345606317623607,
      "loss": 0.8906,
      "step": 1952
    },
    {
      "epoch": 2.604,
      "grad_norm": 0.027431577444076538,
      "learning_rate": 0.001734267545290625,
      "loss": 0.8534,
      "step": 1953
    },
    {
      "epoch": 2.6053333333333333,
      "grad_norm": 0.026317572221159935,
      "learning_rate": 0.0017339743218922098,
      "loss": 1.0862,
      "step": 1954
    },
    {
      "epoch": 2.6066666666666665,
      "grad_norm": 0.036150652915239334,
      "learning_rate": 0.0017336809616217955,
      "loss": 1.3648,
      "step": 1955
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.03557697683572769,
      "learning_rate": 0.0017333874645340884,
      "loss": 1.2375,
      "step": 1956
    },
    {
      "epoch": 2.6093333333333333,
      "grad_norm": 0.033129189163446426,
      "learning_rate": 0.0017330938306838201,
      "loss": 1.3654,
      "step": 1957
    },
    {
      "epoch": 2.610666666666667,
      "grad_norm": 0.028644705191254616,
      "learning_rate": 0.0017328000601257475,
      "loss": 1.0062,
      "step": 1958
    },
    {
      "epoch": 2.612,
      "grad_norm": 0.02631392516195774,
      "learning_rate": 0.001732506152914653,
      "loss": 1.2656,
      "step": 1959
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.023919207975268364,
      "learning_rate": 0.0017322121091053447,
      "loss": 1.1096,
      "step": 1960
    },
    {
      "epoch": 2.6146666666666665,
      "grad_norm": 0.029948575422167778,
      "learning_rate": 0.0017319179287526563,
      "loss": 0.9483,
      "step": 1961
    },
    {
      "epoch": 2.616,
      "grad_norm": 0.03331578150391579,
      "learning_rate": 0.0017316236119114463,
      "loss": 1.124,
      "step": 1962
    },
    {
      "epoch": 2.6173333333333333,
      "grad_norm": 0.03279265761375427,
      "learning_rate": 0.0017313291586365995,
      "loss": 0.9874,
      "step": 1963
    },
    {
      "epoch": 2.618666666666667,
      "grad_norm": 0.026867352426052094,
      "learning_rate": 0.0017310345689830254,
      "loss": 1.0451,
      "step": 1964
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.032720897346735,
      "learning_rate": 0.0017307398430056594,
      "loss": 1.2785,
      "step": 1965
    },
    {
      "epoch": 2.6213333333333333,
      "grad_norm": 0.03052125871181488,
      "learning_rate": 0.0017304449807594624,
      "loss": 1.1854,
      "step": 1966
    },
    {
      "epoch": 2.6226666666666665,
      "grad_norm": 0.030474716797471046,
      "learning_rate": 0.0017301499822994202,
      "loss": 1.0471,
      "step": 1967
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.04120847210288048,
      "learning_rate": 0.0017298548476805445,
      "loss": 1.1471,
      "step": 1968
    },
    {
      "epoch": 2.6253333333333333,
      "grad_norm": 0.029449958354234695,
      "learning_rate": 0.001729559576957872,
      "loss": 1.0047,
      "step": 1969
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.025842493399977684,
      "learning_rate": 0.001729264170186465,
      "loss": 1.083,
      "step": 1970
    },
    {
      "epoch": 2.628,
      "grad_norm": 0.051873717457056046,
      "learning_rate": 0.0017289686274214115,
      "loss": 1.2096,
      "step": 1971
    },
    {
      "epoch": 2.6293333333333333,
      "grad_norm": 0.028842223808169365,
      "learning_rate": 0.001728672948717824,
      "loss": 0.8898,
      "step": 1972
    },
    {
      "epoch": 2.6306666666666665,
      "grad_norm": 0.03032749891281128,
      "learning_rate": 0.001728377134130841,
      "loss": 0.9749,
      "step": 1973
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.047939151525497437,
      "learning_rate": 0.0017280811837156267,
      "loss": 1.0602,
      "step": 1974
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 0.16585077345371246,
      "learning_rate": 0.0017277850975273696,
      "loss": 1.192,
      "step": 1975
    },
    {
      "epoch": 2.634666666666667,
      "grad_norm": 0.0331089161336422,
      "learning_rate": 0.001727488875621284,
      "loss": 1.0602,
      "step": 1976
    },
    {
      "epoch": 2.636,
      "grad_norm": 0.03391234204173088,
      "learning_rate": 0.0017271925180526093,
      "loss": 1.1445,
      "step": 1977
    },
    {
      "epoch": 2.6373333333333333,
      "grad_norm": 0.024979326874017715,
      "learning_rate": 0.0017268960248766115,
      "loss": 1.0458,
      "step": 1978
    },
    {
      "epoch": 2.6386666666666665,
      "grad_norm": 0.049865689128637314,
      "learning_rate": 0.0017265993961485798,
      "loss": 1.0624,
      "step": 1979
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.2344421148300171,
      "learning_rate": 0.00172630263192383,
      "loss": 0.9843,
      "step": 1980
    },
    {
      "epoch": 2.6413333333333333,
      "grad_norm": 0.04040343314409256,
      "learning_rate": 0.0017260057322577031,
      "loss": 1.3387,
      "step": 1981
    },
    {
      "epoch": 2.642666666666667,
      "grad_norm": 0.03210017457604408,
      "learning_rate": 0.0017257086972055648,
      "loss": 1.38,
      "step": 1982
    },
    {
      "epoch": 2.644,
      "grad_norm": 0.06780732423067093,
      "learning_rate": 0.001725411526822807,
      "loss": 1.0548,
      "step": 1983
    },
    {
      "epoch": 2.6453333333333333,
      "grad_norm": 0.100440613925457,
      "learning_rate": 0.0017251142211648456,
      "loss": 0.9536,
      "step": 1984
    },
    {
      "epoch": 2.6466666666666665,
      "grad_norm": 0.049759045243263245,
      "learning_rate": 0.0017248167802871224,
      "loss": 1.2586,
      "step": 1985
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.056993160396814346,
      "learning_rate": 0.0017245192042451049,
      "loss": 1.0813,
      "step": 1986
    },
    {
      "epoch": 2.6493333333333333,
      "grad_norm": 0.19502931833267212,
      "learning_rate": 0.0017242214930942845,
      "loss": 1.1804,
      "step": 1987
    },
    {
      "epoch": 2.6506666666666665,
      "grad_norm": 0.03506189584732056,
      "learning_rate": 0.0017239236468901788,
      "loss": 1.2308,
      "step": 1988
    },
    {
      "epoch": 2.652,
      "grad_norm": 0.03004157543182373,
      "learning_rate": 0.001723625665688331,
      "loss": 1.0153,
      "step": 1989
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 0.02582607790827751,
      "learning_rate": 0.0017233275495443083,
      "loss": 1.1196,
      "step": 1990
    },
    {
      "epoch": 2.6546666666666665,
      "grad_norm": 0.03473260626196861,
      "learning_rate": 0.0017230292985137032,
      "loss": 1.048,
      "step": 1991
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.03128723055124283,
      "learning_rate": 0.0017227309126521346,
      "loss": 1.2694,
      "step": 1992
    },
    {
      "epoch": 2.6573333333333333,
      "grad_norm": 0.061163391917943954,
      "learning_rate": 0.0017224323920152452,
      "loss": 1.1278,
      "step": 1993
    },
    {
      "epoch": 2.6586666666666665,
      "grad_norm": 0.0666196420788765,
      "learning_rate": 0.0017221337366587028,
      "loss": 1.0615,
      "step": 1994
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.03356173262000084,
      "learning_rate": 0.001721834946638202,
      "loss": 1.201,
      "step": 1995
    },
    {
      "epoch": 2.6613333333333333,
      "grad_norm": 0.034998610615730286,
      "learning_rate": 0.0017215360220094607,
      "loss": 1.0729,
      "step": 1996
    },
    {
      "epoch": 2.6626666666666665,
      "grad_norm": 0.20809976756572723,
      "learning_rate": 0.0017212369628282224,
      "loss": 1.1157,
      "step": 1997
    },
    {
      "epoch": 2.664,
      "grad_norm": 0.2673642635345459,
      "learning_rate": 0.0017209377691502564,
      "loss": 0.8501,
      "step": 1998
    },
    {
      "epoch": 2.6653333333333333,
      "grad_norm": 0.04867381229996681,
      "learning_rate": 0.0017206384410313558,
      "loss": 1.116,
      "step": 1999
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.14093253016471863,
      "learning_rate": 0.00172033897852734,
      "loss": 1.1857,
      "step": 2000
    },
    {
      "epoch": 2.668,
      "grad_norm": 0.04934033378958702,
      "learning_rate": 0.001720039381694053,
      "loss": 0.8924,
      "step": 2001
    },
    {
      "epoch": 2.6693333333333333,
      "grad_norm": 0.09533829987049103,
      "learning_rate": 0.0017197396505873638,
      "loss": 1.0268,
      "step": 2002
    },
    {
      "epoch": 2.6706666666666665,
      "grad_norm": 0.05273430421948433,
      "learning_rate": 0.0017194397852631658,
      "loss": 1.2402,
      "step": 2003
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.030559008941054344,
      "learning_rate": 0.0017191397857773787,
      "loss": 0.8691,
      "step": 2004
    },
    {
      "epoch": 2.6733333333333333,
      "grad_norm": 0.03387072682380676,
      "learning_rate": 0.0017188396521859465,
      "loss": 1.0752,
      "step": 2005
    },
    {
      "epoch": 2.6746666666666665,
      "grad_norm": 0.028690669685602188,
      "learning_rate": 0.0017185393845448385,
      "loss": 1.2717,
      "step": 2006
    },
    {
      "epoch": 2.676,
      "grad_norm": 0.03513453155755997,
      "learning_rate": 0.0017182389829100485,
      "loss": 1.1853,
      "step": 2007
    },
    {
      "epoch": 2.6773333333333333,
      "grad_norm": 0.03785013407468796,
      "learning_rate": 0.0017179384473375956,
      "loss": 0.9724,
      "step": 2008
    },
    {
      "epoch": 2.6786666666666665,
      "grad_norm": 0.0489492192864418,
      "learning_rate": 0.001717637777883524,
      "loss": 1.0388,
      "step": 2009
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.03316517546772957,
      "learning_rate": 0.0017173369746039024,
      "loss": 1.0386,
      "step": 2010
    },
    {
      "epoch": 2.6813333333333333,
      "grad_norm": 0.032570093870162964,
      "learning_rate": 0.0017170360375548253,
      "loss": 1.1129,
      "step": 2011
    },
    {
      "epoch": 2.6826666666666665,
      "grad_norm": 0.04309031739830971,
      "learning_rate": 0.0017167349667924112,
      "loss": 1.0446,
      "step": 2012
    },
    {
      "epoch": 2.684,
      "grad_norm": 0.03181082382798195,
      "learning_rate": 0.0017164337623728043,
      "loss": 1.3203,
      "step": 2013
    },
    {
      "epoch": 2.6853333333333333,
      "grad_norm": 0.02460576593875885,
      "learning_rate": 0.0017161324243521734,
      "loss": 1.0535,
      "step": 2014
    },
    {
      "epoch": 2.6866666666666665,
      "grad_norm": 0.035589586943387985,
      "learning_rate": 0.0017158309527867117,
      "loss": 1.1646,
      "step": 2015
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.04682967811822891,
      "learning_rate": 0.0017155293477326384,
      "loss": 1.0166,
      "step": 2016
    },
    {
      "epoch": 2.6893333333333334,
      "grad_norm": 0.037691835314035416,
      "learning_rate": 0.0017152276092461966,
      "loss": 1.3934,
      "step": 2017
    },
    {
      "epoch": 2.6906666666666665,
      "grad_norm": 0.03317675739526749,
      "learning_rate": 0.0017149257373836552,
      "loss": 1.0606,
      "step": 2018
    },
    {
      "epoch": 2.692,
      "grad_norm": 0.03101968951523304,
      "learning_rate": 0.0017146237322013067,
      "loss": 1.1945,
      "step": 2019
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 0.03179483488202095,
      "learning_rate": 0.0017143215937554697,
      "loss": 1.3917,
      "step": 2020
    },
    {
      "epoch": 2.6946666666666665,
      "grad_norm": 0.03675268962979317,
      "learning_rate": 0.001714019322102487,
      "loss": 0.8715,
      "step": 2021
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.036712389439344406,
      "learning_rate": 0.0017137169172987269,
      "loss": 1.1023,
      "step": 2022
    },
    {
      "epoch": 2.6973333333333334,
      "grad_norm": 0.030668068677186966,
      "learning_rate": 0.0017134143794005814,
      "loss": 1.2298,
      "step": 2023
    },
    {
      "epoch": 2.6986666666666665,
      "grad_norm": 0.038837309926748276,
      "learning_rate": 0.001713111708464468,
      "loss": 1.0209,
      "step": 2024
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.032974373549222946,
      "learning_rate": 0.0017128089045468293,
      "loss": 1.1803,
      "step": 2025
    },
    {
      "epoch": 2.7013333333333334,
      "grad_norm": 0.03275725245475769,
      "learning_rate": 0.0017125059677041321,
      "loss": 1.2616,
      "step": 2026
    },
    {
      "epoch": 2.7026666666666666,
      "grad_norm": 0.03391652554273605,
      "learning_rate": 0.0017122028979928686,
      "loss": 1.1681,
      "step": 2027
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.0403575524687767,
      "learning_rate": 0.0017118996954695552,
      "loss": 0.9883,
      "step": 2028
    },
    {
      "epoch": 2.7053333333333334,
      "grad_norm": 0.03424416854977608,
      "learning_rate": 0.0017115963601907335,
      "loss": 1.0544,
      "step": 2029
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 0.02980235405266285,
      "learning_rate": 0.0017112928922129692,
      "loss": 0.9938,
      "step": 2030
    },
    {
      "epoch": 2.708,
      "grad_norm": 0.025671586394309998,
      "learning_rate": 0.0017109892915928536,
      "loss": 0.8725,
      "step": 2031
    },
    {
      "epoch": 2.7093333333333334,
      "grad_norm": 0.034289754927158356,
      "learning_rate": 0.0017106855583870022,
      "loss": 0.9718,
      "step": 2032
    },
    {
      "epoch": 2.7106666666666666,
      "grad_norm": 0.028375549241900444,
      "learning_rate": 0.0017103816926520553,
      "loss": 1.1043,
      "step": 2033
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.029182562604546547,
      "learning_rate": 0.0017100776944446781,
      "loss": 0.9961,
      "step": 2034
    },
    {
      "epoch": 2.7133333333333334,
      "grad_norm": 0.0296177938580513,
      "learning_rate": 0.0017097735638215603,
      "loss": 0.9604,
      "step": 2035
    },
    {
      "epoch": 2.7146666666666666,
      "grad_norm": 0.02634892426431179,
      "learning_rate": 0.0017094693008394164,
      "loss": 1.0408,
      "step": 2036
    },
    {
      "epoch": 2.716,
      "grad_norm": 0.034140001982450485,
      "learning_rate": 0.0017091649055549856,
      "loss": 1.3482,
      "step": 2037
    },
    {
      "epoch": 2.7173333333333334,
      "grad_norm": 0.02327243983745575,
      "learning_rate": 0.0017088603780250318,
      "loss": 1.1158,
      "step": 2038
    },
    {
      "epoch": 2.7186666666666666,
      "grad_norm": 0.03223568573594093,
      "learning_rate": 0.0017085557183063433,
      "loss": 0.9276,
      "step": 2039
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.03762540966272354,
      "learning_rate": 0.001708250926455733,
      "loss": 1.1997,
      "step": 2040
    },
    {
      "epoch": 2.7213333333333334,
      "grad_norm": 0.03445546329021454,
      "learning_rate": 0.0017079460025300394,
      "loss": 1.2014,
      "step": 2041
    },
    {
      "epoch": 2.7226666666666666,
      "grad_norm": 0.03446699678897858,
      "learning_rate": 0.0017076409465861247,
      "loss": 0.9947,
      "step": 2042
    },
    {
      "epoch": 2.724,
      "grad_norm": 0.03141940012574196,
      "learning_rate": 0.001707335758680875,
      "loss": 1.069,
      "step": 2043
    },
    {
      "epoch": 2.7253333333333334,
      "grad_norm": 0.04520728066563606,
      "learning_rate": 0.0017070304388712035,
      "loss": 1.1207,
      "step": 2044
    },
    {
      "epoch": 2.7266666666666666,
      "grad_norm": 0.027693241834640503,
      "learning_rate": 0.0017067249872140448,
      "loss": 1.1723,
      "step": 2045
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.03063521347939968,
      "learning_rate": 0.0017064194037663609,
      "loss": 1.1132,
      "step": 2046
    },
    {
      "epoch": 2.7293333333333334,
      "grad_norm": 0.031268924474716187,
      "learning_rate": 0.0017061136885851368,
      "loss": 1.111,
      "step": 2047
    },
    {
      "epoch": 2.7306666666666666,
      "grad_norm": 0.027714010328054428,
      "learning_rate": 0.0017058078417273826,
      "loss": 1.1536,
      "step": 2048
    },
    {
      "epoch": 2.732,
      "grad_norm": 0.04158652201294899,
      "learning_rate": 0.0017055018632501325,
      "loss": 1.0998,
      "step": 2049
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.03460021689534187,
      "learning_rate": 0.001705195753210446,
      "loss": 1.1209,
      "step": 2050
    },
    {
      "epoch": 2.7346666666666666,
      "grad_norm": 0.02969503030180931,
      "learning_rate": 0.001704889511665406,
      "loss": 1.0976,
      "step": 2051
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.037438347935676575,
      "learning_rate": 0.0017045831386721213,
      "loss": 0.932,
      "step": 2052
    },
    {
      "epoch": 2.7373333333333334,
      "grad_norm": 0.028125599026679993,
      "learning_rate": 0.0017042766342877244,
      "loss": 1.1059,
      "step": 2053
    },
    {
      "epoch": 2.7386666666666666,
      "grad_norm": 0.037256207317113876,
      "learning_rate": 0.001703969998569372,
      "loss": 1.0108,
      "step": 2054
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.027397317811846733,
      "learning_rate": 0.0017036632315742461,
      "loss": 1.1172,
      "step": 2055
    },
    {
      "epoch": 2.7413333333333334,
      "grad_norm": 0.02697867900133133,
      "learning_rate": 0.0017033563333595531,
      "loss": 1.1107,
      "step": 2056
    },
    {
      "epoch": 2.7426666666666666,
      "grad_norm": 0.03079160302877426,
      "learning_rate": 0.001703049303982523,
      "loss": 0.9345,
      "step": 2057
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.04173128306865692,
      "learning_rate": 0.0017027421435004111,
      "loss": 1.1811,
      "step": 2058
    },
    {
      "epoch": 2.7453333333333334,
      "grad_norm": 0.031500495970249176,
      "learning_rate": 0.001702434851970497,
      "loss": 1.1235,
      "step": 2059
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.026693033054471016,
      "learning_rate": 0.0017021274294500841,
      "loss": 1.0928,
      "step": 2060
    },
    {
      "epoch": 2.748,
      "grad_norm": 0.027827855199575424,
      "learning_rate": 0.0017018198759965015,
      "loss": 1.1029,
      "step": 2061
    },
    {
      "epoch": 2.7493333333333334,
      "grad_norm": 0.033510591834783554,
      "learning_rate": 0.0017015121916671013,
      "loss": 1.1794,
      "step": 2062
    },
    {
      "epoch": 2.7506666666666666,
      "grad_norm": 0.02843673713505268,
      "learning_rate": 0.001701204376519261,
      "loss": 1.0664,
      "step": 2063
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.030112426728010178,
      "learning_rate": 0.0017008964306103823,
      "loss": 0.9972,
      "step": 2064
    },
    {
      "epoch": 2.7533333333333334,
      "grad_norm": 0.047872770577669144,
      "learning_rate": 0.0017005883539978907,
      "loss": 1.3286,
      "step": 2065
    },
    {
      "epoch": 2.7546666666666666,
      "grad_norm": 0.030683279037475586,
      "learning_rate": 0.001700280146739237,
      "loss": 0.9745,
      "step": 2066
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 0.03807344287633896,
      "learning_rate": 0.0016999718088918953,
      "loss": 1.0658,
      "step": 2067
    },
    {
      "epoch": 2.7573333333333334,
      "grad_norm": 0.023799574002623558,
      "learning_rate": 0.0016996633405133655,
      "loss": 1.159,
      "step": 2068
    },
    {
      "epoch": 2.7586666666666666,
      "grad_norm": 0.025288116186857224,
      "learning_rate": 0.0016993547416611703,
      "loss": 0.8737,
      "step": 2069
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.032817404717206955,
      "learning_rate": 0.0016990460123928574,
      "loss": 1.231,
      "step": 2070
    },
    {
      "epoch": 2.7613333333333334,
      "grad_norm": 0.027728982269763947,
      "learning_rate": 0.0016987371527659994,
      "loss": 1.0563,
      "step": 2071
    },
    {
      "epoch": 2.7626666666666666,
      "grad_norm": 0.03325491026043892,
      "learning_rate": 0.0016984281628381918,
      "loss": 1.2032,
      "step": 2072
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 0.033552151173353195,
      "learning_rate": 0.0016981190426670558,
      "loss": 1.0454,
      "step": 2073
    },
    {
      "epoch": 2.7653333333333334,
      "grad_norm": 0.036240726709365845,
      "learning_rate": 0.0016978097923102367,
      "loss": 0.9989,
      "step": 2074
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 0.04960215836763382,
      "learning_rate": 0.001697500411825403,
      "loss": 1.2071,
      "step": 2075
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.02804628200829029,
      "learning_rate": 0.0016971909012702482,
      "loss": 1.0561,
      "step": 2076
    },
    {
      "epoch": 2.7693333333333334,
      "grad_norm": 0.026453105732798576,
      "learning_rate": 0.0016968812607024903,
      "loss": 0.9807,
      "step": 2077
    },
    {
      "epoch": 2.7706666666666666,
      "grad_norm": 0.025326237082481384,
      "learning_rate": 0.0016965714901798713,
      "loss": 1.1016,
      "step": 2078
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 0.023310046643018723,
      "learning_rate": 0.0016962615897601573,
      "loss": 1.1687,
      "step": 2079
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 0.029127145186066628,
      "learning_rate": 0.0016959515595011388,
      "loss": 0.9032,
      "step": 2080
    },
    {
      "epoch": 2.7746666666666666,
      "grad_norm": 0.026633568108081818,
      "learning_rate": 0.00169564139946063,
      "loss": 0.9775,
      "step": 2081
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.10063374042510986,
      "learning_rate": 0.0016953311096964704,
      "loss": 1.1288,
      "step": 2082
    },
    {
      "epoch": 2.7773333333333334,
      "grad_norm": 0.03364305570721626,
      "learning_rate": 0.0016950206902665225,
      "loss": 0.9723,
      "step": 2083
    },
    {
      "epoch": 2.7786666666666666,
      "grad_norm": 0.03733626753091812,
      "learning_rate": 0.0016947101412286743,
      "loss": 0.9299,
      "step": 2084
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 0.033767685294151306,
      "learning_rate": 0.0016943994626408363,
      "loss": 1.0825,
      "step": 2085
    },
    {
      "epoch": 2.7813333333333334,
      "grad_norm": 0.030657511204481125,
      "learning_rate": 0.0016940886545609444,
      "loss": 1.0409,
      "step": 2086
    },
    {
      "epoch": 2.7826666666666666,
      "grad_norm": 0.0309575367718935,
      "learning_rate": 0.001693777717046958,
      "loss": 0.9444,
      "step": 2087
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.028843896463513374,
      "learning_rate": 0.0016934666501568615,
      "loss": 1.0669,
      "step": 2088
    },
    {
      "epoch": 2.7853333333333334,
      "grad_norm": 0.03174426406621933,
      "learning_rate": 0.0016931554539486627,
      "loss": 1.1807,
      "step": 2089
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 0.03522256016731262,
      "learning_rate": 0.0016928441284803934,
      "loss": 1.1396,
      "step": 2090
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 0.0382445827126503,
      "learning_rate": 0.00169253267381011,
      "loss": 1.2009,
      "step": 2091
    },
    {
      "epoch": 2.7893333333333334,
      "grad_norm": 0.021640866994857788,
      "learning_rate": 0.0016922210899958923,
      "loss": 0.9273,
      "step": 2092
    },
    {
      "epoch": 2.7906666666666666,
      "grad_norm": 0.04055514931678772,
      "learning_rate": 0.0016919093770958451,
      "loss": 1.0401,
      "step": 2093
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.03195895627140999,
      "learning_rate": 0.0016915975351680969,
      "loss": 1.3507,
      "step": 2094
    },
    {
      "epoch": 2.7933333333333334,
      "grad_norm": 0.03262104466557503,
      "learning_rate": 0.0016912855642708,
      "loss": 1.1323,
      "step": 2095
    },
    {
      "epoch": 2.7946666666666666,
      "grad_norm": 0.033069856464862823,
      "learning_rate": 0.0016909734644621306,
      "loss": 0.8141,
      "step": 2096
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 0.037650495767593384,
      "learning_rate": 0.0016906612358002899,
      "loss": 1.203,
      "step": 2097
    },
    {
      "epoch": 2.7973333333333334,
      "grad_norm": 0.026346171274781227,
      "learning_rate": 0.0016903488783435023,
      "loss": 1.1525,
      "step": 2098
    },
    {
      "epoch": 2.7986666666666666,
      "grad_norm": 0.025141645222902298,
      "learning_rate": 0.0016900363921500162,
      "loss": 1.0759,
      "step": 2099
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.02343449555337429,
      "learning_rate": 0.0016897237772781045,
      "loss": 1.0777,
      "step": 2100
    },
    {
      "epoch": 2.8013333333333335,
      "grad_norm": 0.02964434213936329,
      "learning_rate": 0.0016894110337860632,
      "loss": 1.1308,
      "step": 2101
    },
    {
      "epoch": 2.8026666666666666,
      "grad_norm": 0.034765392541885376,
      "learning_rate": 0.0016890981617322138,
      "loss": 1.1458,
      "step": 2102
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 0.027675211429595947,
      "learning_rate": 0.0016887851611749004,
      "loss": 0.9139,
      "step": 2103
    },
    {
      "epoch": 2.8053333333333335,
      "grad_norm": 0.045197103172540665,
      "learning_rate": 0.0016884720321724913,
      "loss": 1.0593,
      "step": 2104
    },
    {
      "epoch": 2.8066666666666666,
      "grad_norm": 0.025029940530657768,
      "learning_rate": 0.0016881587747833793,
      "loss": 1.0039,
      "step": 2105
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.03792248293757439,
      "learning_rate": 0.0016878453890659814,
      "loss": 1.1556,
      "step": 2106
    },
    {
      "epoch": 2.8093333333333335,
      "grad_norm": 0.02528727799654007,
      "learning_rate": 0.001687531875078737,
      "loss": 0.9454,
      "step": 2107
    },
    {
      "epoch": 2.8106666666666666,
      "grad_norm": 0.04167123883962631,
      "learning_rate": 0.0016872182328801107,
      "loss": 1.0573,
      "step": 2108
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 0.026222148910164833,
      "learning_rate": 0.0016869044625285907,
      "loss": 1.1156,
      "step": 2109
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 0.05343370512127876,
      "learning_rate": 0.0016865905640826894,
      "loss": 0.7752,
      "step": 2110
    },
    {
      "epoch": 2.8146666666666667,
      "grad_norm": 0.05141665041446686,
      "learning_rate": 0.0016862765376009427,
      "loss": 1.3021,
      "step": 2111
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.029767956584692,
      "learning_rate": 0.00168596238314191,
      "loss": 1.3002,
      "step": 2112
    },
    {
      "epoch": 2.8173333333333335,
      "grad_norm": 0.03189937025308609,
      "learning_rate": 0.0016856481007641755,
      "loss": 1.2978,
      "step": 2113
    },
    {
      "epoch": 2.8186666666666667,
      "grad_norm": 0.030041303485631943,
      "learning_rate": 0.0016853336905263466,
      "loss": 0.8052,
      "step": 2114
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.0308882724493742,
      "learning_rate": 0.0016850191524870547,
      "loss": 1.3346,
      "step": 2115
    },
    {
      "epoch": 2.8213333333333335,
      "grad_norm": 0.040680013597011566,
      "learning_rate": 0.0016847044867049552,
      "loss": 1.1394,
      "step": 2116
    },
    {
      "epoch": 2.8226666666666667,
      "grad_norm": 0.03553124889731407,
      "learning_rate": 0.0016843896932387267,
      "loss": 1.0484,
      "step": 2117
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.03619334101676941,
      "learning_rate": 0.001684074772147073,
      "loss": 0.9638,
      "step": 2118
    },
    {
      "epoch": 2.8253333333333335,
      "grad_norm": 0.03191347047686577,
      "learning_rate": 0.0016837597234887198,
      "loss": 0.9303,
      "step": 2119
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 0.022430045530200005,
      "learning_rate": 0.0016834445473224182,
      "loss": 1.0912,
      "step": 2120
    },
    {
      "epoch": 2.828,
      "grad_norm": 0.04302253574132919,
      "learning_rate": 0.0016831292437069424,
      "loss": 1.2292,
      "step": 2121
    },
    {
      "epoch": 2.8293333333333335,
      "grad_norm": 0.028203662484884262,
      "learning_rate": 0.0016828138127010903,
      "loss": 1.2631,
      "step": 2122
    },
    {
      "epoch": 2.8306666666666667,
      "grad_norm": 0.044472649693489075,
      "learning_rate": 0.0016824982543636833,
      "loss": 1.0203,
      "step": 2123
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.027330053970217705,
      "learning_rate": 0.0016821825687535674,
      "loss": 1.1086,
      "step": 2124
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.04065445438027382,
      "learning_rate": 0.0016818667559296118,
      "loss": 1.048,
      "step": 2125
    },
    {
      "epoch": 2.8346666666666667,
      "grad_norm": 0.03411131352186203,
      "learning_rate": 0.0016815508159507095,
      "loss": 1.2013,
      "step": 2126
    },
    {
      "epoch": 2.836,
      "grad_norm": 0.0335271917283535,
      "learning_rate": 0.0016812347488757773,
      "loss": 1.076,
      "step": 2127
    },
    {
      "epoch": 2.8373333333333335,
      "grad_norm": 0.04863887280225754,
      "learning_rate": 0.0016809185547637551,
      "loss": 0.9435,
      "step": 2128
    },
    {
      "epoch": 2.8386666666666667,
      "grad_norm": 0.03096184879541397,
      "learning_rate": 0.0016806022336736072,
      "loss": 0.9502,
      "step": 2129
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.03180520981550217,
      "learning_rate": 0.0016802857856643215,
      "loss": 1.1657,
      "step": 2130
    },
    {
      "epoch": 2.8413333333333335,
      "grad_norm": 0.026362065225839615,
      "learning_rate": 0.0016799692107949094,
      "loss": 1.0045,
      "step": 2131
    },
    {
      "epoch": 2.8426666666666667,
      "grad_norm": 0.026506422087550163,
      "learning_rate": 0.0016796525091244058,
      "loss": 1.0897,
      "step": 2132
    },
    {
      "epoch": 2.844,
      "grad_norm": 0.021864870563149452,
      "learning_rate": 0.0016793356807118696,
      "loss": 1.0137,
      "step": 2133
    },
    {
      "epoch": 2.8453333333333335,
      "grad_norm": 0.02694203332066536,
      "learning_rate": 0.0016790187256163832,
      "loss": 1.1998,
      "step": 2134
    },
    {
      "epoch": 2.8466666666666667,
      "grad_norm": 0.03404393419623375,
      "learning_rate": 0.0016787016438970525,
      "loss": 1.1206,
      "step": 2135
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.03353532403707504,
      "learning_rate": 0.001678384435613007,
      "loss": 1.5854,
      "step": 2136
    },
    {
      "epoch": 2.8493333333333335,
      "grad_norm": 0.034878987818956375,
      "learning_rate": 0.0016780671008234003,
      "loss": 1.2796,
      "step": 2137
    },
    {
      "epoch": 2.8506666666666667,
      "grad_norm": 0.035059258341789246,
      "learning_rate": 0.0016777496395874087,
      "loss": 1.054,
      "step": 2138
    },
    {
      "epoch": 2.852,
      "grad_norm": 0.02423018403351307,
      "learning_rate": 0.001677432051964233,
      "loss": 1.281,
      "step": 2139
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 0.030760742723941803,
      "learning_rate": 0.0016771143380130966,
      "loss": 1.3064,
      "step": 2140
    },
    {
      "epoch": 2.8546666666666667,
      "grad_norm": 0.03086676076054573,
      "learning_rate": 0.0016767964977932475,
      "loss": 1.0698,
      "step": 2141
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.04736403375864029,
      "learning_rate": 0.0016764785313639569,
      "loss": 1.2101,
      "step": 2142
    },
    {
      "epoch": 2.857333333333333,
      "grad_norm": 0.03809691220521927,
      "learning_rate": 0.0016761604387845186,
      "loss": 1.2404,
      "step": 2143
    },
    {
      "epoch": 2.8586666666666667,
      "grad_norm": 0.030456513166427612,
      "learning_rate": 0.0016758422201142511,
      "loss": 1.1205,
      "step": 2144
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.03305927664041519,
      "learning_rate": 0.0016755238754124963,
      "loss": 1.2568,
      "step": 2145
    },
    {
      "epoch": 2.8613333333333335,
      "grad_norm": 0.0346122570335865,
      "learning_rate": 0.001675205404738619,
      "loss": 0.9099,
      "step": 2146
    },
    {
      "epoch": 2.8626666666666667,
      "grad_norm": 0.029987752437591553,
      "learning_rate": 0.0016748868081520084,
      "loss": 1.5107,
      "step": 2147
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.029430657625198364,
      "learning_rate": 0.0016745680857120754,
      "loss": 1.006,
      "step": 2148
    },
    {
      "epoch": 2.865333333333333,
      "grad_norm": 0.026394207030534744,
      "learning_rate": 0.0016742492374782567,
      "loss": 1.2301,
      "step": 2149
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.030711645260453224,
      "learning_rate": 0.0016739302635100108,
      "loss": 0.9185,
      "step": 2150
    },
    {
      "epoch": 2.868,
      "grad_norm": 0.029481373727321625,
      "learning_rate": 0.0016736111638668203,
      "loss": 1.1503,
      "step": 2151
    },
    {
      "epoch": 2.8693333333333335,
      "grad_norm": 0.03605872020125389,
      "learning_rate": 0.001673291938608191,
      "loss": 1.3511,
      "step": 2152
    },
    {
      "epoch": 2.8706666666666667,
      "grad_norm": 0.06657612323760986,
      "learning_rate": 0.0016729725877936525,
      "loss": 1.2401,
      "step": 2153
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.03753475099802017,
      "learning_rate": 0.0016726531114827571,
      "loss": 1.103,
      "step": 2154
    },
    {
      "epoch": 2.873333333333333,
      "grad_norm": 0.027939658612012863,
      "learning_rate": 0.0016723335097350814,
      "loss": 0.8036,
      "step": 2155
    },
    {
      "epoch": 2.8746666666666667,
      "grad_norm": 0.02446945756673813,
      "learning_rate": 0.0016720137826102246,
      "loss": 0.984,
      "step": 2156
    },
    {
      "epoch": 2.876,
      "grad_norm": 0.030278405174613,
      "learning_rate": 0.0016716939301678099,
      "loss": 0.9672,
      "step": 2157
    },
    {
      "epoch": 2.8773333333333335,
      "grad_norm": 0.030775273218750954,
      "learning_rate": 0.0016713739524674834,
      "loss": 0.9816,
      "step": 2158
    },
    {
      "epoch": 2.8786666666666667,
      "grad_norm": 0.0977621078491211,
      "learning_rate": 0.0016710538495689144,
      "loss": 1.3744,
      "step": 2159
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.03202559053897858,
      "learning_rate": 0.0016707336215317967,
      "loss": 1.1712,
      "step": 2160
    },
    {
      "epoch": 2.881333333333333,
      "grad_norm": 0.03216254338622093,
      "learning_rate": 0.001670413268415846,
      "loss": 0.8732,
      "step": 2161
    },
    {
      "epoch": 2.8826666666666667,
      "grad_norm": 0.025230593979358673,
      "learning_rate": 0.0016700927902808024,
      "loss": 0.7918,
      "step": 2162
    },
    {
      "epoch": 2.884,
      "grad_norm": 0.02734268270432949,
      "learning_rate": 0.0016697721871864284,
      "loss": 0.9456,
      "step": 2163
    },
    {
      "epoch": 2.8853333333333335,
      "grad_norm": 0.032931648194789886,
      "learning_rate": 0.0016694514591925106,
      "loss": 1.252,
      "step": 2164
    },
    {
      "epoch": 2.8866666666666667,
      "grad_norm": 0.031409066170454025,
      "learning_rate": 0.0016691306063588583,
      "loss": 1.2832,
      "step": 2165
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.025559023022651672,
      "learning_rate": 0.0016688096287453046,
      "loss": 1.0691,
      "step": 2166
    },
    {
      "epoch": 2.889333333333333,
      "grad_norm": 0.022565821185708046,
      "learning_rate": 0.0016684885264117053,
      "loss": 1.056,
      "step": 2167
    },
    {
      "epoch": 2.8906666666666667,
      "grad_norm": 0.025213690474629402,
      "learning_rate": 0.0016681672994179402,
      "loss": 0.9716,
      "step": 2168
    },
    {
      "epoch": 2.892,
      "grad_norm": 0.03298121690750122,
      "learning_rate": 0.0016678459478239116,
      "loss": 1.0136,
      "step": 2169
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 0.041112445294857025,
      "learning_rate": 0.0016675244716895454,
      "loss": 0.9623,
      "step": 2170
    },
    {
      "epoch": 2.8946666666666667,
      "grad_norm": 0.02578364685177803,
      "learning_rate": 0.0016672028710747909,
      "loss": 1.0145,
      "step": 2171
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.028488656505942345,
      "learning_rate": 0.0016668811460396201,
      "loss": 0.9108,
      "step": 2172
    },
    {
      "epoch": 2.897333333333333,
      "grad_norm": 0.028791695833206177,
      "learning_rate": 0.0016665592966440283,
      "loss": 1.1392,
      "step": 2173
    },
    {
      "epoch": 2.8986666666666667,
      "grad_norm": 0.025598417967557907,
      "learning_rate": 0.001666237322948035,
      "loss": 1.0611,
      "step": 2174
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.03429477661848068,
      "learning_rate": 0.0016659152250116812,
      "loss": 1.3977,
      "step": 2175
    },
    {
      "epoch": 2.9013333333333335,
      "grad_norm": 0.03685419633984566,
      "learning_rate": 0.001665593002895032,
      "loss": 1.0687,
      "step": 2176
    },
    {
      "epoch": 2.9026666666666667,
      "grad_norm": 0.027557305991649628,
      "learning_rate": 0.0016652706566581763,
      "loss": 0.8656,
      "step": 2177
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.02455511875450611,
      "learning_rate": 0.0016649481863612247,
      "loss": 0.9429,
      "step": 2178
    },
    {
      "epoch": 2.905333333333333,
      "grad_norm": 0.028105491772294044,
      "learning_rate": 0.0016646255920643124,
      "loss": 1.1473,
      "step": 2179
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.026881057769060135,
      "learning_rate": 0.0016643028738275959,
      "loss": 1.0775,
      "step": 2180
    },
    {
      "epoch": 2.908,
      "grad_norm": 0.03895958513021469,
      "learning_rate": 0.0016639800317112567,
      "loss": 0.8634,
      "step": 2181
    },
    {
      "epoch": 2.9093333333333335,
      "grad_norm": 0.028040217235684395,
      "learning_rate": 0.0016636570657754986,
      "loss": 0.9246,
      "step": 2182
    },
    {
      "epoch": 2.9106666666666667,
      "grad_norm": 0.03344236686825752,
      "learning_rate": 0.0016633339760805481,
      "loss": 0.9438,
      "step": 2183
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.03419153764843941,
      "learning_rate": 0.0016630107626866557,
      "loss": 0.7909,
      "step": 2184
    },
    {
      "epoch": 2.913333333333333,
      "grad_norm": 0.03703178092837334,
      "learning_rate": 0.0016626874256540937,
      "loss": 1.2451,
      "step": 2185
    },
    {
      "epoch": 2.9146666666666667,
      "grad_norm": 0.030647223815321922,
      "learning_rate": 0.0016623639650431591,
      "loss": 1.0949,
      "step": 2186
    },
    {
      "epoch": 2.916,
      "grad_norm": 0.04085690900683403,
      "learning_rate": 0.0016620403809141705,
      "loss": 1.0992,
      "step": 2187
    },
    {
      "epoch": 2.9173333333333336,
      "grad_norm": 0.029108494520187378,
      "learning_rate": 0.0016617166733274699,
      "loss": 1.0873,
      "step": 2188
    },
    {
      "epoch": 2.9186666666666667,
      "grad_norm": 0.029377765953540802,
      "learning_rate": 0.001661392842343423,
      "loss": 0.8937,
      "step": 2189
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.0358896367251873,
      "learning_rate": 0.0016610688880224177,
      "loss": 1.2077,
      "step": 2190
    },
    {
      "epoch": 2.921333333333333,
      "grad_norm": 0.03121590055525303,
      "learning_rate": 0.0016607448104248648,
      "loss": 0.944,
      "step": 2191
    },
    {
      "epoch": 2.9226666666666667,
      "grad_norm": 0.03218378126621246,
      "learning_rate": 0.0016604206096111995,
      "loss": 0.9928,
      "step": 2192
    },
    {
      "epoch": 2.924,
      "grad_norm": 0.03160586580634117,
      "learning_rate": 0.0016600962856418782,
      "loss": 0.7924,
      "step": 2193
    },
    {
      "epoch": 2.9253333333333336,
      "grad_norm": 0.04188750684261322,
      "learning_rate": 0.0016597718385773814,
      "loss": 0.9488,
      "step": 2194
    },
    {
      "epoch": 2.9266666666666667,
      "grad_norm": 0.03667214885354042,
      "learning_rate": 0.001659447268478212,
      "loss": 1.4016,
      "step": 2195
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.040883492678403854,
      "learning_rate": 0.0016591225754048961,
      "loss": 1.2023,
      "step": 2196
    },
    {
      "epoch": 2.929333333333333,
      "grad_norm": 0.04800324887037277,
      "learning_rate": 0.0016587977594179825,
      "loss": 1.2528,
      "step": 2197
    },
    {
      "epoch": 2.9306666666666668,
      "grad_norm": 0.03356111794710159,
      "learning_rate": 0.0016584728205780435,
      "loss": 0.9167,
      "step": 2198
    },
    {
      "epoch": 2.932,
      "grad_norm": 0.04765361174941063,
      "learning_rate": 0.0016581477589456735,
      "loss": 1.011,
      "step": 2199
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.031024662777781487,
      "learning_rate": 0.0016578225745814909,
      "loss": 1.1746,
      "step": 2200
    },
    {
      "epoch": 2.9346666666666668,
      "grad_norm": 0.11833731085062027,
      "learning_rate": 0.001657497267546135,
      "loss": 1.0211,
      "step": 2201
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.10052083432674408,
      "learning_rate": 0.0016571718379002705,
      "loss": 1.0527,
      "step": 2202
    },
    {
      "epoch": 2.937333333333333,
      "grad_norm": 0.037267208099365234,
      "learning_rate": 0.001656846285704583,
      "loss": 1.1568,
      "step": 2203
    },
    {
      "epoch": 2.9386666666666668,
      "grad_norm": 0.036114588379859924,
      "learning_rate": 0.001656520611019782,
      "loss": 1.0832,
      "step": 2204
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.035601064562797546,
      "learning_rate": 0.0016561948139065996,
      "loss": 0.944,
      "step": 2205
    },
    {
      "epoch": 2.9413333333333336,
      "grad_norm": 0.02723981998860836,
      "learning_rate": 0.0016558688944257902,
      "loss": 1.2923,
      "step": 2206
    },
    {
      "epoch": 2.9426666666666668,
      "grad_norm": 0.054134175181388855,
      "learning_rate": 0.0016555428526381323,
      "loss": 0.9869,
      "step": 2207
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.04200543835759163,
      "learning_rate": 0.0016552166886044254,
      "loss": 1.1097,
      "step": 2208
    },
    {
      "epoch": 2.945333333333333,
      "grad_norm": 0.03239554539322853,
      "learning_rate": 0.0016548904023854932,
      "loss": 1.2106,
      "step": 2209
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 0.024332625791430473,
      "learning_rate": 0.0016545639940421819,
      "loss": 0.9806,
      "step": 2210
    },
    {
      "epoch": 2.948,
      "grad_norm": 0.03373686224222183,
      "learning_rate": 0.0016542374636353604,
      "loss": 1.0785,
      "step": 2211
    },
    {
      "epoch": 2.9493333333333336,
      "grad_norm": 0.0290727186948061,
      "learning_rate": 0.00165391081122592,
      "loss": 0.8693,
      "step": 2212
    },
    {
      "epoch": 2.9506666666666668,
      "grad_norm": 0.02869122102856636,
      "learning_rate": 0.0016535840368747752,
      "loss": 1.1226,
      "step": 2213
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.02423572912812233,
      "learning_rate": 0.001653257140642863,
      "loss": 0.8516,
      "step": 2214
    },
    {
      "epoch": 2.953333333333333,
      "grad_norm": 0.04644278064370155,
      "learning_rate": 0.0016529301225911431,
      "loss": 0.8499,
      "step": 2215
    },
    {
      "epoch": 2.9546666666666668,
      "grad_norm": 0.02946840599179268,
      "learning_rate": 0.0016526029827805986,
      "loss": 1.0276,
      "step": 2216
    },
    {
      "epoch": 2.956,
      "grad_norm": 0.026463009417057037,
      "learning_rate": 0.0016522757212722344,
      "loss": 0.849,
      "step": 2217
    },
    {
      "epoch": 2.9573333333333336,
      "grad_norm": 0.0431792177259922,
      "learning_rate": 0.0016519483381270779,
      "loss": 1.6877,
      "step": 2218
    },
    {
      "epoch": 2.958666666666667,
      "grad_norm": 0.04342630133032799,
      "learning_rate": 0.0016516208334061806,
      "loss": 0.9664,
      "step": 2219
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.02974679507315159,
      "learning_rate": 0.0016512932071706152,
      "loss": 1.2065,
      "step": 2220
    },
    {
      "epoch": 2.961333333333333,
      "grad_norm": 0.04573775455355644,
      "learning_rate": 0.001650965459481478,
      "loss": 1.2012,
      "step": 2221
    },
    {
      "epoch": 2.962666666666667,
      "grad_norm": 0.029056938365101814,
      "learning_rate": 0.0016506375903998876,
      "loss": 1.0765,
      "step": 2222
    },
    {
      "epoch": 2.964,
      "grad_norm": 0.038847167044878006,
      "learning_rate": 0.0016503095999869848,
      "loss": 0.9019,
      "step": 2223
    },
    {
      "epoch": 2.9653333333333336,
      "grad_norm": 0.033734988421201706,
      "learning_rate": 0.0016499814883039338,
      "loss": 1.0267,
      "step": 2224
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 0.027923185378313065,
      "learning_rate": 0.0016496532554119213,
      "loss": 1.3507,
      "step": 2225
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.03458350896835327,
      "learning_rate": 0.0016493249013721557,
      "loss": 1.3655,
      "step": 2226
    },
    {
      "epoch": 2.969333333333333,
      "grad_norm": 0.030050141736865044,
      "learning_rate": 0.0016489964262458691,
      "loss": 1.0728,
      "step": 2227
    },
    {
      "epoch": 2.970666666666667,
      "grad_norm": 0.03225494176149368,
      "learning_rate": 0.0016486678300943161,
      "loss": 1.0104,
      "step": 2228
    },
    {
      "epoch": 2.972,
      "grad_norm": 0.14536254107952118,
      "learning_rate": 0.0016483391129787726,
      "loss": 0.9894,
      "step": 2229
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 0.03200913593173027,
      "learning_rate": 0.001648010274960539,
      "loss": 1.0865,
      "step": 2230
    },
    {
      "epoch": 2.974666666666667,
      "grad_norm": 0.026379669085144997,
      "learning_rate": 0.0016476813161009365,
      "loss": 1.1653,
      "step": 2231
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.03275229036808014,
      "learning_rate": 0.0016473522364613099,
      "loss": 1.1122,
      "step": 2232
    },
    {
      "epoch": 2.977333333333333,
      "grad_norm": 0.055282022804021835,
      "learning_rate": 0.0016470230361030258,
      "loss": 0.9464,
      "step": 2233
    },
    {
      "epoch": 2.978666666666667,
      "grad_norm": 0.030316080898046494,
      "learning_rate": 0.0016466937150874741,
      "loss": 0.994,
      "step": 2234
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.05141614377498627,
      "learning_rate": 0.0016463642734760669,
      "loss": 1.2082,
      "step": 2235
    },
    {
      "epoch": 2.981333333333333,
      "grad_norm": 0.031790655106306076,
      "learning_rate": 0.0016460347113302382,
      "loss": 1.1428,
      "step": 2236
    },
    {
      "epoch": 2.982666666666667,
      "grad_norm": 0.033395107835531235,
      "learning_rate": 0.0016457050287114452,
      "loss": 1.2728,
      "step": 2237
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.04389146342873573,
      "learning_rate": 0.0016453752256811675,
      "loss": 1.1507,
      "step": 2238
    },
    {
      "epoch": 2.985333333333333,
      "grad_norm": 0.028300154954195023,
      "learning_rate": 0.0016450453023009067,
      "loss": 1.1693,
      "step": 2239
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 0.03334532305598259,
      "learning_rate": 0.001644715258632187,
      "loss": 1.0005,
      "step": 2240
    },
    {
      "epoch": 2.988,
      "grad_norm": 0.04921041056513786,
      "learning_rate": 0.0016443850947365558,
      "loss": 1.5479,
      "step": 2241
    },
    {
      "epoch": 2.989333333333333,
      "grad_norm": 0.0343785397708416,
      "learning_rate": 0.0016440548106755816,
      "loss": 1.1721,
      "step": 2242
    },
    {
      "epoch": 2.990666666666667,
      "grad_norm": 0.03219155967235565,
      "learning_rate": 0.0016437244065108563,
      "loss": 0.876,
      "step": 2243
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.023985540494322777,
      "learning_rate": 0.001643393882303994,
      "loss": 1.2195,
      "step": 2244
    },
    {
      "epoch": 2.993333333333333,
      "grad_norm": 0.029765672981739044,
      "learning_rate": 0.0016430632381166305,
      "loss": 0.9806,
      "step": 2245
    },
    {
      "epoch": 2.994666666666667,
      "grad_norm": 0.023493777960538864,
      "learning_rate": 0.0016427324740104251,
      "loss": 0.7741,
      "step": 2246
    },
    {
      "epoch": 2.996,
      "grad_norm": 0.022160332649946213,
      "learning_rate": 0.0016424015900470589,
      "loss": 0.9733,
      "step": 2247
    },
    {
      "epoch": 2.997333333333333,
      "grad_norm": 0.03327234089374542,
      "learning_rate": 0.0016420705862882348,
      "loss": 1.0625,
      "step": 2248
    },
    {
      "epoch": 2.998666666666667,
      "grad_norm": 0.02747105062007904,
      "learning_rate": 0.0016417394627956793,
      "loss": 1.1508,
      "step": 2249
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.03315483033657074,
      "learning_rate": 0.00164140821963114,
      "loss": 1.1549,
      "step": 2250
    },
    {
      "epoch": 3.001333333333333,
      "grad_norm": 0.02201818861067295,
      "learning_rate": 0.0016410768568563875,
      "loss": 0.9296,
      "step": 2251
    },
    {
      "epoch": 3.002666666666667,
      "grad_norm": 0.03382681682705879,
      "learning_rate": 0.0016407453745332146,
      "loss": 1.2486,
      "step": 2252
    },
    {
      "epoch": 3.004,
      "grad_norm": 0.03193267807364464,
      "learning_rate": 0.0016404137727234365,
      "loss": 0.9823,
      "step": 2253
    },
    {
      "epoch": 3.005333333333333,
      "grad_norm": 0.028958454728126526,
      "learning_rate": 0.0016400820514888901,
      "loss": 1.1274,
      "step": 2254
    },
    {
      "epoch": 3.006666666666667,
      "grad_norm": 0.02540566772222519,
      "learning_rate": 0.0016397502108914353,
      "loss": 0.9422,
      "step": 2255
    },
    {
      "epoch": 3.008,
      "grad_norm": 0.023995349183678627,
      "learning_rate": 0.0016394182509929535,
      "loss": 0.8057,
      "step": 2256
    },
    {
      "epoch": 3.009333333333333,
      "grad_norm": 0.02780074067413807,
      "learning_rate": 0.0016390861718553495,
      "loss": 1.1338,
      "step": 2257
    },
    {
      "epoch": 3.010666666666667,
      "grad_norm": 0.03440787270665169,
      "learning_rate": 0.0016387539735405491,
      "loss": 1.1773,
      "step": 2258
    },
    {
      "epoch": 3.012,
      "grad_norm": 0.03595776855945587,
      "learning_rate": 0.0016384216561105011,
      "loss": 0.9656,
      "step": 2259
    },
    {
      "epoch": 3.013333333333333,
      "grad_norm": 0.02641797624528408,
      "learning_rate": 0.0016380892196271764,
      "loss": 1.21,
      "step": 2260
    },
    {
      "epoch": 3.014666666666667,
      "grad_norm": 0.03101632185280323,
      "learning_rate": 0.0016377566641525673,
      "loss": 0.9221,
      "step": 2261
    },
    {
      "epoch": 3.016,
      "grad_norm": 0.07365258783102036,
      "learning_rate": 0.0016374239897486897,
      "loss": 1.1002,
      "step": 2262
    },
    {
      "epoch": 3.017333333333333,
      "grad_norm": 0.029425906017422676,
      "learning_rate": 0.0016370911964775808,
      "loss": 0.9415,
      "step": 2263
    },
    {
      "epoch": 3.018666666666667,
      "grad_norm": 0.02925816923379898,
      "learning_rate": 0.0016367582844012997,
      "loss": 0.835,
      "step": 2264
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.03603258356451988,
      "learning_rate": 0.0016364252535819282,
      "loss": 1.0834,
      "step": 2265
    },
    {
      "epoch": 3.021333333333333,
      "grad_norm": 0.028746148571372032,
      "learning_rate": 0.0016360921040815704,
      "loss": 1.2157,
      "step": 2266
    },
    {
      "epoch": 3.022666666666667,
      "grad_norm": 0.02923213504254818,
      "learning_rate": 0.0016357588359623518,
      "loss": 1.2099,
      "step": 2267
    },
    {
      "epoch": 3.024,
      "grad_norm": 0.03844751790165901,
      "learning_rate": 0.001635425449286421,
      "loss": 1.2224,
      "step": 2268
    },
    {
      "epoch": 3.025333333333333,
      "grad_norm": 0.04033305123448372,
      "learning_rate": 0.0016350919441159477,
      "loss": 1.0984,
      "step": 2269
    },
    {
      "epoch": 3.026666666666667,
      "grad_norm": 0.03881243243813515,
      "learning_rate": 0.0016347583205131243,
      "loss": 0.9175,
      "step": 2270
    },
    {
      "epoch": 3.028,
      "grad_norm": 0.03453761711716652,
      "learning_rate": 0.0016344245785401652,
      "loss": 1.1304,
      "step": 2271
    },
    {
      "epoch": 3.029333333333333,
      "grad_norm": 0.030447712168097496,
      "learning_rate": 0.0016340907182593066,
      "loss": 1.0367,
      "step": 2272
    },
    {
      "epoch": 3.030666666666667,
      "grad_norm": 0.039583027362823486,
      "learning_rate": 0.0016337567397328072,
      "loss": 1.244,
      "step": 2273
    },
    {
      "epoch": 3.032,
      "grad_norm": 0.031205791980028152,
      "learning_rate": 0.0016334226430229474,
      "loss": 0.9804,
      "step": 2274
    },
    {
      "epoch": 3.033333333333333,
      "grad_norm": 0.03114047273993492,
      "learning_rate": 0.00163308842819203,
      "loss": 0.7762,
      "step": 2275
    },
    {
      "epoch": 3.034666666666667,
      "grad_norm": 0.05856575444340706,
      "learning_rate": 0.0016327540953023792,
      "loss": 1.2586,
      "step": 2276
    },
    {
      "epoch": 3.036,
      "grad_norm": 0.023417966440320015,
      "learning_rate": 0.001632419644416342,
      "loss": 0.6474,
      "step": 2277
    },
    {
      "epoch": 3.037333333333333,
      "grad_norm": 0.16918152570724487,
      "learning_rate": 0.001632085075596287,
      "loss": 1.0804,
      "step": 2278
    },
    {
      "epoch": 3.038666666666667,
      "grad_norm": 0.03254249691963196,
      "learning_rate": 0.0016317503889046044,
      "loss": 0.9352,
      "step": 2279
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.040424417704343796,
      "learning_rate": 0.0016314155844037073,
      "loss": 1.2017,
      "step": 2280
    },
    {
      "epoch": 3.041333333333333,
      "grad_norm": 0.023922564461827278,
      "learning_rate": 0.0016310806621560295,
      "loss": 0.9637,
      "step": 2281
    },
    {
      "epoch": 3.042666666666667,
      "grad_norm": 0.024866243824362755,
      "learning_rate": 0.0016307456222240284,
      "loss": 0.994,
      "step": 2282
    },
    {
      "epoch": 3.044,
      "grad_norm": 0.030618824064731598,
      "learning_rate": 0.0016304104646701818,
      "loss": 1.2371,
      "step": 2283
    },
    {
      "epoch": 3.0453333333333332,
      "grad_norm": 0.146577849984169,
      "learning_rate": 0.0016300751895569907,
      "loss": 1.1071,
      "step": 2284
    },
    {
      "epoch": 3.046666666666667,
      "grad_norm": 0.11519399285316467,
      "learning_rate": 0.0016297397969469772,
      "loss": 0.998,
      "step": 2285
    },
    {
      "epoch": 3.048,
      "grad_norm": 0.020872557535767555,
      "learning_rate": 0.001629404286902685,
      "loss": 0.9712,
      "step": 2286
    },
    {
      "epoch": 3.0493333333333332,
      "grad_norm": 0.025250673294067383,
      "learning_rate": 0.0016290686594866806,
      "loss": 1.0217,
      "step": 2287
    },
    {
      "epoch": 3.050666666666667,
      "grad_norm": 0.04247387498617172,
      "learning_rate": 0.0016287329147615525,
      "loss": 0.8406,
      "step": 2288
    },
    {
      "epoch": 3.052,
      "grad_norm": 0.03769795596599579,
      "learning_rate": 0.00162839705278991,
      "loss": 0.9083,
      "step": 2289
    },
    {
      "epoch": 3.0533333333333332,
      "grad_norm": 0.02173374965786934,
      "learning_rate": 0.0016280610736343847,
      "loss": 0.9662,
      "step": 2290
    },
    {
      "epoch": 3.054666666666667,
      "grad_norm": 0.03408164530992508,
      "learning_rate": 0.0016277249773576307,
      "loss": 1.1005,
      "step": 2291
    },
    {
      "epoch": 3.056,
      "grad_norm": 0.03387819230556488,
      "learning_rate": 0.001627388764022323,
      "loss": 1.222,
      "step": 2292
    },
    {
      "epoch": 3.0573333333333332,
      "grad_norm": 0.044878195971250534,
      "learning_rate": 0.0016270524336911597,
      "loss": 1.1131,
      "step": 2293
    },
    {
      "epoch": 3.058666666666667,
      "grad_norm": 0.028852049261331558,
      "learning_rate": 0.0016267159864268587,
      "loss": 1.0093,
      "step": 2294
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.03263714537024498,
      "learning_rate": 0.001626379422292162,
      "loss": 0.9027,
      "step": 2295
    },
    {
      "epoch": 3.0613333333333332,
      "grad_norm": 0.029844636097550392,
      "learning_rate": 0.0016260427413498319,
      "loss": 1.0352,
      "step": 2296
    },
    {
      "epoch": 3.062666666666667,
      "grad_norm": 0.034320008009672165,
      "learning_rate": 0.0016257059436626522,
      "loss": 1.3137,
      "step": 2297
    },
    {
      "epoch": 3.064,
      "grad_norm": 0.03587692603468895,
      "learning_rate": 0.00162536902929343,
      "loss": 1.2537,
      "step": 2298
    },
    {
      "epoch": 3.0653333333333332,
      "grad_norm": 0.02011966146528721,
      "learning_rate": 0.0016250319983049932,
      "loss": 0.9699,
      "step": 2299
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.028370114043354988,
      "learning_rate": 0.0016246948507601913,
      "loss": 1.1858,
      "step": 2300
    },
    {
      "epoch": 3.068,
      "grad_norm": 0.03497609868645668,
      "learning_rate": 0.0016243575867218958,
      "loss": 1.1321,
      "step": 2301
    },
    {
      "epoch": 3.0693333333333332,
      "grad_norm": 0.027792364358901978,
      "learning_rate": 0.0016240202062530002,
      "loss": 0.9842,
      "step": 2302
    },
    {
      "epoch": 3.070666666666667,
      "grad_norm": 0.02730766125023365,
      "learning_rate": 0.001623682709416419,
      "loss": 1.1692,
      "step": 2303
    },
    {
      "epoch": 3.072,
      "grad_norm": 0.024309365078806877,
      "learning_rate": 0.0016233450962750892,
      "loss": 1.2348,
      "step": 2304
    },
    {
      "epoch": 3.0733333333333333,
      "grad_norm": 0.028886014595627785,
      "learning_rate": 0.0016230073668919692,
      "loss": 1.2561,
      "step": 2305
    },
    {
      "epoch": 3.074666666666667,
      "grad_norm": 0.0318010188639164,
      "learning_rate": 0.0016226695213300381,
      "loss": 1.1774,
      "step": 2306
    },
    {
      "epoch": 3.076,
      "grad_norm": 0.02691299095749855,
      "learning_rate": 0.0016223315596522988,
      "loss": 1.0306,
      "step": 2307
    },
    {
      "epoch": 3.0773333333333333,
      "grad_norm": 0.032502952963113785,
      "learning_rate": 0.0016219934819217737,
      "loss": 1.2317,
      "step": 2308
    },
    {
      "epoch": 3.078666666666667,
      "grad_norm": 0.03170233592391014,
      "learning_rate": 0.001621655288201508,
      "loss": 1.3437,
      "step": 2309
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.027136487886309624,
      "learning_rate": 0.0016213169785545688,
      "loss": 1.0051,
      "step": 2310
    },
    {
      "epoch": 3.0813333333333333,
      "grad_norm": 0.028643356636166573,
      "learning_rate": 0.0016209785530440434,
      "loss": 1.2812,
      "step": 2311
    },
    {
      "epoch": 3.0826666666666664,
      "grad_norm": 0.02991592139005661,
      "learning_rate": 0.0016206400117330425,
      "loss": 1.0872,
      "step": 2312
    },
    {
      "epoch": 3.084,
      "grad_norm": 0.030520278960466385,
      "learning_rate": 0.0016203013546846965,
      "loss": 0.9822,
      "step": 2313
    },
    {
      "epoch": 3.0853333333333333,
      "grad_norm": 0.03446860983967781,
      "learning_rate": 0.001619962581962159,
      "loss": 1.0516,
      "step": 2314
    },
    {
      "epoch": 3.086666666666667,
      "grad_norm": 0.03197808191180229,
      "learning_rate": 0.001619623693628605,
      "loss": 1.0479,
      "step": 2315
    },
    {
      "epoch": 3.088,
      "grad_norm": 0.028749549761414528,
      "learning_rate": 0.0016192846897472297,
      "loss": 1.0989,
      "step": 2316
    },
    {
      "epoch": 3.0893333333333333,
      "grad_norm": 0.0405140146613121,
      "learning_rate": 0.0016189455703812512,
      "loss": 1.0131,
      "step": 2317
    },
    {
      "epoch": 3.0906666666666665,
      "grad_norm": 0.030882062390446663,
      "learning_rate": 0.0016186063355939086,
      "loss": 0.7412,
      "step": 2318
    },
    {
      "epoch": 3.092,
      "grad_norm": 0.039467163383960724,
      "learning_rate": 0.0016182669854484628,
      "loss": 1.0866,
      "step": 2319
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 0.043284397572278976,
      "learning_rate": 0.001617927520008196,
      "loss": 1.3431,
      "step": 2320
    },
    {
      "epoch": 3.0946666666666665,
      "grad_norm": 0.037943463772535324,
      "learning_rate": 0.0016175879393364116,
      "loss": 1.109,
      "step": 2321
    },
    {
      "epoch": 3.096,
      "grad_norm": 0.030784161761403084,
      "learning_rate": 0.0016172482434964352,
      "loss": 0.9644,
      "step": 2322
    },
    {
      "epoch": 3.0973333333333333,
      "grad_norm": 0.038774523884058,
      "learning_rate": 0.0016169084325516133,
      "loss": 1.1237,
      "step": 2323
    },
    {
      "epoch": 3.0986666666666665,
      "grad_norm": 0.03480285778641701,
      "learning_rate": 0.001616568506565314,
      "loss": 0.8635,
      "step": 2324
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.04820620268583298,
      "learning_rate": 0.0016162284656009273,
      "loss": 1.4482,
      "step": 2325
    },
    {
      "epoch": 3.1013333333333333,
      "grad_norm": 0.03230919688940048,
      "learning_rate": 0.001615888309721864,
      "loss": 1.0912,
      "step": 2326
    },
    {
      "epoch": 3.1026666666666665,
      "grad_norm": 0.053422145545482635,
      "learning_rate": 0.0016155480389915561,
      "loss": 0.7465,
      "step": 2327
    },
    {
      "epoch": 3.104,
      "grad_norm": 0.025344572961330414,
      "learning_rate": 0.0016152076534734583,
      "loss": 1.2399,
      "step": 2328
    },
    {
      "epoch": 3.1053333333333333,
      "grad_norm": 0.028936322778463364,
      "learning_rate": 0.0016148671532310456,
      "loss": 1.0431,
      "step": 2329
    },
    {
      "epoch": 3.1066666666666665,
      "grad_norm": 0.026852300390601158,
      "learning_rate": 0.0016145265383278144,
      "loss": 0.9789,
      "step": 2330
    },
    {
      "epoch": 3.108,
      "grad_norm": 0.021957244724035263,
      "learning_rate": 0.0016141858088272835,
      "loss": 0.8924,
      "step": 2331
    },
    {
      "epoch": 3.1093333333333333,
      "grad_norm": 0.03181817755103111,
      "learning_rate": 0.0016138449647929917,
      "loss": 0.9205,
      "step": 2332
    },
    {
      "epoch": 3.1106666666666665,
      "grad_norm": 0.027513276785612106,
      "learning_rate": 0.0016135040062885003,
      "loss": 1.2341,
      "step": 2333
    },
    {
      "epoch": 3.112,
      "grad_norm": 0.0327935554087162,
      "learning_rate": 0.0016131629333773908,
      "loss": 1.2111,
      "step": 2334
    },
    {
      "epoch": 3.1133333333333333,
      "grad_norm": 0.03337598592042923,
      "learning_rate": 0.001612821746123267,
      "loss": 1.1233,
      "step": 2335
    },
    {
      "epoch": 3.1146666666666665,
      "grad_norm": 0.025853391736745834,
      "learning_rate": 0.0016124804445897542,
      "loss": 1.0869,
      "step": 2336
    },
    {
      "epoch": 3.116,
      "grad_norm": 0.04071875661611557,
      "learning_rate": 0.001612139028840498,
      "loss": 0.976,
      "step": 2337
    },
    {
      "epoch": 3.1173333333333333,
      "grad_norm": 0.022651247680187225,
      "learning_rate": 0.0016117974989391657,
      "loss": 1.2395,
      "step": 2338
    },
    {
      "epoch": 3.1186666666666665,
      "grad_norm": 0.03851344808936119,
      "learning_rate": 0.0016114558549494467,
      "loss": 1.1702,
      "step": 2339
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.041822705417871475,
      "learning_rate": 0.0016111140969350501,
      "loss": 1.1929,
      "step": 2340
    },
    {
      "epoch": 3.1213333333333333,
      "grad_norm": 0.03104749694466591,
      "learning_rate": 0.001610772224959708,
      "loss": 1.027,
      "step": 2341
    },
    {
      "epoch": 3.1226666666666665,
      "grad_norm": 0.04554689675569534,
      "learning_rate": 0.0016104302390871723,
      "loss": 1.0279,
      "step": 2342
    },
    {
      "epoch": 3.124,
      "grad_norm": 0.02779427357017994,
      "learning_rate": 0.001610088139381217,
      "loss": 1.0994,
      "step": 2343
    },
    {
      "epoch": 3.1253333333333333,
      "grad_norm": 0.03431810811161995,
      "learning_rate": 0.0016097459259056368,
      "loss": 1.0998,
      "step": 2344
    },
    {
      "epoch": 3.1266666666666665,
      "grad_norm": 0.032478801906108856,
      "learning_rate": 0.0016094035987242483,
      "loss": 1.0492,
      "step": 2345
    },
    {
      "epoch": 3.128,
      "grad_norm": 0.03156214579939842,
      "learning_rate": 0.001609061157900889,
      "loss": 1.1636,
      "step": 2346
    },
    {
      "epoch": 3.1293333333333333,
      "grad_norm": 0.03456081077456474,
      "learning_rate": 0.0016087186034994168,
      "loss": 1.1151,
      "step": 2347
    },
    {
      "epoch": 3.1306666666666665,
      "grad_norm": 0.029416754841804504,
      "learning_rate": 0.001608375935583712,
      "loss": 1.0174,
      "step": 2348
    },
    {
      "epoch": 3.132,
      "grad_norm": 0.03991890326142311,
      "learning_rate": 0.0016080331542176753,
      "loss": 1.0396,
      "step": 2349
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.0238210279494524,
      "learning_rate": 0.001607690259465229,
      "loss": 1.0701,
      "step": 2350
    },
    {
      "epoch": 3.1346666666666665,
      "grad_norm": 0.036639776080846786,
      "learning_rate": 0.001607347251390316,
      "loss": 1.3533,
      "step": 2351
    },
    {
      "epoch": 3.136,
      "grad_norm": 0.041870374232530594,
      "learning_rate": 0.0016070041300569012,
      "loss": 1.1202,
      "step": 2352
    },
    {
      "epoch": 3.1373333333333333,
      "grad_norm": 0.02790934592485428,
      "learning_rate": 0.0016066608955289693,
      "loss": 1.0813,
      "step": 2353
    },
    {
      "epoch": 3.1386666666666665,
      "grad_norm": 0.0314762257039547,
      "learning_rate": 0.0016063175478705277,
      "loss": 0.9794,
      "step": 2354
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.03321545943617821,
      "learning_rate": 0.0016059740871456035,
      "loss": 0.9132,
      "step": 2355
    },
    {
      "epoch": 3.1413333333333333,
      "grad_norm": 0.026199378073215485,
      "learning_rate": 0.0016056305134182459,
      "loss": 1.1801,
      "step": 2356
    },
    {
      "epoch": 3.1426666666666665,
      "grad_norm": 0.042221799492836,
      "learning_rate": 0.0016052868267525246,
      "loss": 1.1024,
      "step": 2357
    },
    {
      "epoch": 3.144,
      "grad_norm": 0.03447648882865906,
      "learning_rate": 0.0016049430272125301,
      "loss": 0.8376,
      "step": 2358
    },
    {
      "epoch": 3.1453333333333333,
      "grad_norm": 0.030333152040839195,
      "learning_rate": 0.001604599114862375,
      "loss": 1.0716,
      "step": 2359
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 0.033913325518369675,
      "learning_rate": 0.0016042550897661918,
      "loss": 0.8823,
      "step": 2360
    },
    {
      "epoch": 3.148,
      "grad_norm": 0.03165041282773018,
      "learning_rate": 0.0016039109519881349,
      "loss": 1.0863,
      "step": 2361
    },
    {
      "epoch": 3.1493333333333333,
      "grad_norm": 0.03663633018732071,
      "learning_rate": 0.001603566701592379,
      "loss": 0.9822,
      "step": 2362
    },
    {
      "epoch": 3.1506666666666665,
      "grad_norm": 0.03279414027929306,
      "learning_rate": 0.0016032223386431206,
      "loss": 1.2889,
      "step": 2363
    },
    {
      "epoch": 3.152,
      "grad_norm": 0.02825789339840412,
      "learning_rate": 0.0016028778632045761,
      "loss": 1.1416,
      "step": 2364
    },
    {
      "epoch": 3.1533333333333333,
      "grad_norm": 0.03851594403386116,
      "learning_rate": 0.0016025332753409841,
      "loss": 1.1684,
      "step": 2365
    },
    {
      "epoch": 3.1546666666666665,
      "grad_norm": 0.025392943993210793,
      "learning_rate": 0.0016021885751166033,
      "loss": 0.8447,
      "step": 2366
    },
    {
      "epoch": 3.156,
      "grad_norm": 0.028521625325083733,
      "learning_rate": 0.0016018437625957135,
      "loss": 0.9073,
      "step": 2367
    },
    {
      "epoch": 3.1573333333333333,
      "grad_norm": 0.037293918430805206,
      "learning_rate": 0.0016014988378426156,
      "loss": 1.3096,
      "step": 2368
    },
    {
      "epoch": 3.1586666666666665,
      "grad_norm": 0.031660426408052444,
      "learning_rate": 0.0016011538009216312,
      "loss": 1.13,
      "step": 2369
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.032650601118803024,
      "learning_rate": 0.0016008086518971036,
      "loss": 1.3917,
      "step": 2370
    },
    {
      "epoch": 3.1613333333333333,
      "grad_norm": 0.030601516366004944,
      "learning_rate": 0.001600463390833396,
      "loss": 1.1389,
      "step": 2371
    },
    {
      "epoch": 3.1626666666666665,
      "grad_norm": 0.04113338887691498,
      "learning_rate": 0.001600118017794893,
      "loss": 1.0637,
      "step": 2372
    },
    {
      "epoch": 3.164,
      "grad_norm": 0.02500620111823082,
      "learning_rate": 0.001599772532846,
      "loss": 0.8134,
      "step": 2373
    },
    {
      "epoch": 3.1653333333333333,
      "grad_norm": 0.02047419548034668,
      "learning_rate": 0.0015994269360511428,
      "loss": 0.9929,
      "step": 2374
    },
    {
      "epoch": 3.1666666666666665,
      "grad_norm": 0.026978135108947754,
      "learning_rate": 0.0015990812274747693,
      "loss": 0.9344,
      "step": 2375
    },
    {
      "epoch": 3.168,
      "grad_norm": 0.02978707104921341,
      "learning_rate": 0.001598735407181347,
      "loss": 0.9699,
      "step": 2376
    },
    {
      "epoch": 3.1693333333333333,
      "grad_norm": 0.026482004672288895,
      "learning_rate": 0.0015983894752353645,
      "loss": 1.0387,
      "step": 2377
    },
    {
      "epoch": 3.1706666666666665,
      "grad_norm": 0.0261706430464983,
      "learning_rate": 0.001598043431701332,
      "loss": 1.1219,
      "step": 2378
    },
    {
      "epoch": 3.172,
      "grad_norm": 0.02744581550359726,
      "learning_rate": 0.0015976972766437794,
      "loss": 1.158,
      "step": 2379
    },
    {
      "epoch": 3.1733333333333333,
      "grad_norm": 0.033714719116687775,
      "learning_rate": 0.001597351010127258,
      "loss": 0.8898,
      "step": 2380
    },
    {
      "epoch": 3.1746666666666665,
      "grad_norm": 0.021076040342450142,
      "learning_rate": 0.0015970046322163402,
      "loss": 0.984,
      "step": 2381
    },
    {
      "epoch": 3.176,
      "grad_norm": 0.025623386725783348,
      "learning_rate": 0.0015966581429756183,
      "loss": 1.2355,
      "step": 2382
    },
    {
      "epoch": 3.1773333333333333,
      "grad_norm": 0.02690540999174118,
      "learning_rate": 0.001596311542469706,
      "loss": 0.8198,
      "step": 2383
    },
    {
      "epoch": 3.1786666666666665,
      "grad_norm": 0.03219997510313988,
      "learning_rate": 0.0015959648307632378,
      "loss": 1.2554,
      "step": 2384
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.031402550637722015,
      "learning_rate": 0.0015956180079208682,
      "loss": 1.2481,
      "step": 2385
    },
    {
      "epoch": 3.1813333333333333,
      "grad_norm": 0.030770208686590195,
      "learning_rate": 0.0015952710740072733,
      "loss": 0.8238,
      "step": 2386
    },
    {
      "epoch": 3.1826666666666665,
      "grad_norm": 0.04094685614109039,
      "learning_rate": 0.0015949240290871495,
      "loss": 0.9654,
      "step": 2387
    },
    {
      "epoch": 3.184,
      "grad_norm": 0.0696566179394722,
      "learning_rate": 0.0015945768732252145,
      "loss": 1.1179,
      "step": 2388
    },
    {
      "epoch": 3.1853333333333333,
      "grad_norm": 0.027888676151633263,
      "learning_rate": 0.001594229606486205,
      "loss": 1.1345,
      "step": 2389
    },
    {
      "epoch": 3.1866666666666665,
      "grad_norm": 0.03430980071425438,
      "learning_rate": 0.0015938822289348802,
      "loss": 0.7379,
      "step": 2390
    },
    {
      "epoch": 3.188,
      "grad_norm": 0.03150368481874466,
      "learning_rate": 0.0015935347406360193,
      "loss": 1.0839,
      "step": 2391
    },
    {
      "epoch": 3.1893333333333334,
      "grad_norm": 0.037834350019693375,
      "learning_rate": 0.0015931871416544222,
      "loss": 1.1257,
      "step": 2392
    },
    {
      "epoch": 3.1906666666666665,
      "grad_norm": 0.04061490669846535,
      "learning_rate": 0.0015928394320549092,
      "loss": 1.1818,
      "step": 2393
    },
    {
      "epoch": 3.192,
      "grad_norm": 0.033057909458875656,
      "learning_rate": 0.0015924916119023213,
      "loss": 1.2485,
      "step": 2394
    },
    {
      "epoch": 3.1933333333333334,
      "grad_norm": 0.0333588682115078,
      "learning_rate": 0.0015921436812615204,
      "loss": 1.0449,
      "step": 2395
    },
    {
      "epoch": 3.1946666666666665,
      "grad_norm": 0.04058796539902687,
      "learning_rate": 0.0015917956401973888,
      "loss": 0.9915,
      "step": 2396
    },
    {
      "epoch": 3.196,
      "grad_norm": 0.03528266400098801,
      "learning_rate": 0.0015914474887748296,
      "loss": 1.2981,
      "step": 2397
    },
    {
      "epoch": 3.1973333333333334,
      "grad_norm": 0.02642429620027542,
      "learning_rate": 0.0015910992270587658,
      "loss": 1.2934,
      "step": 2398
    },
    {
      "epoch": 3.1986666666666665,
      "grad_norm": 0.03836710751056671,
      "learning_rate": 0.0015907508551141422,
      "loss": 1.1517,
      "step": 2399
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.030062664300203323,
      "learning_rate": 0.0015904023730059227,
      "loss": 0.9358,
      "step": 2400
    },
    {
      "epoch": 3.2013333333333334,
      "grad_norm": 0.051980432122945786,
      "learning_rate": 0.0015900537807990928,
      "loss": 0.9354,
      "step": 2401
    },
    {
      "epoch": 3.2026666666666666,
      "grad_norm": 0.023099975660443306,
      "learning_rate": 0.0015897050785586581,
      "loss": 1.0396,
      "step": 2402
    },
    {
      "epoch": 3.204,
      "grad_norm": 0.03210536763072014,
      "learning_rate": 0.0015893562663496451,
      "loss": 1.2459,
      "step": 2403
    },
    {
      "epoch": 3.2053333333333334,
      "grad_norm": 0.02870263159275055,
      "learning_rate": 0.0015890073442371003,
      "loss": 1.2215,
      "step": 2404
    },
    {
      "epoch": 3.2066666666666666,
      "grad_norm": 0.031160004436969757,
      "learning_rate": 0.0015886583122860908,
      "loss": 0.9943,
      "step": 2405
    },
    {
      "epoch": 3.208,
      "grad_norm": 0.03835415840148926,
      "learning_rate": 0.0015883091705617045,
      "loss": 1.031,
      "step": 2406
    },
    {
      "epoch": 3.2093333333333334,
      "grad_norm": 0.024243010208010674,
      "learning_rate": 0.0015879599191290495,
      "loss": 1.2826,
      "step": 2407
    },
    {
      "epoch": 3.2106666666666666,
      "grad_norm": 0.02670104242861271,
      "learning_rate": 0.0015876105580532546,
      "loss": 0.9211,
      "step": 2408
    },
    {
      "epoch": 3.212,
      "grad_norm": 0.021388422697782516,
      "learning_rate": 0.0015872610873994685,
      "loss": 1.1547,
      "step": 2409
    },
    {
      "epoch": 3.2133333333333334,
      "grad_norm": 0.030884861946105957,
      "learning_rate": 0.0015869115072328608,
      "loss": 1.2407,
      "step": 2410
    },
    {
      "epoch": 3.2146666666666666,
      "grad_norm": 0.03482918441295624,
      "learning_rate": 0.001586561817618622,
      "loss": 1.207,
      "step": 2411
    },
    {
      "epoch": 3.216,
      "grad_norm": 0.024487439543008804,
      "learning_rate": 0.0015862120186219612,
      "loss": 1.0289,
      "step": 2412
    },
    {
      "epoch": 3.2173333333333334,
      "grad_norm": 0.036087438464164734,
      "learning_rate": 0.0015858621103081103,
      "loss": 1.373,
      "step": 2413
    },
    {
      "epoch": 3.2186666666666666,
      "grad_norm": 0.033988554030656815,
      "learning_rate": 0.0015855120927423206,
      "loss": 0.8935,
      "step": 2414
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.04793447628617287,
      "learning_rate": 0.0015851619659898624,
      "loss": 1.0997,
      "step": 2415
    },
    {
      "epoch": 3.2213333333333334,
      "grad_norm": 0.0311167873442173,
      "learning_rate": 0.0015848117301160284,
      "loss": 1.0084,
      "step": 2416
    },
    {
      "epoch": 3.2226666666666666,
      "grad_norm": 0.07559961825609207,
      "learning_rate": 0.0015844613851861305,
      "loss": 1.2957,
      "step": 2417
    },
    {
      "epoch": 3.224,
      "grad_norm": 0.03289595991373062,
      "learning_rate": 0.0015841109312655015,
      "loss": 1.1429,
      "step": 2418
    },
    {
      "epoch": 3.2253333333333334,
      "grad_norm": 0.02657989226281643,
      "learning_rate": 0.001583760368419494,
      "loss": 1.1691,
      "step": 2419
    },
    {
      "epoch": 3.2266666666666666,
      "grad_norm": 0.03534427657723427,
      "learning_rate": 0.0015834096967134816,
      "loss": 1.0067,
      "step": 2420
    },
    {
      "epoch": 3.228,
      "grad_norm": 0.03015759587287903,
      "learning_rate": 0.0015830589162128574,
      "loss": 0.9859,
      "step": 2421
    },
    {
      "epoch": 3.2293333333333334,
      "grad_norm": 0.04172728955745697,
      "learning_rate": 0.0015827080269830351,
      "loss": 0.9808,
      "step": 2422
    },
    {
      "epoch": 3.2306666666666666,
      "grad_norm": 0.035861123353242874,
      "learning_rate": 0.001582357029089449,
      "loss": 1.3956,
      "step": 2423
    },
    {
      "epoch": 3.232,
      "grad_norm": 0.04951854795217514,
      "learning_rate": 0.001582005922597553,
      "loss": 1.1465,
      "step": 2424
    },
    {
      "epoch": 3.2333333333333334,
      "grad_norm": 0.025763550773262978,
      "learning_rate": 0.0015816547075728226,
      "loss": 0.9642,
      "step": 2425
    },
    {
      "epoch": 3.2346666666666666,
      "grad_norm": 0.029858488589525223,
      "learning_rate": 0.0015813033840807518,
      "loss": 0.8429,
      "step": 2426
    },
    {
      "epoch": 3.2359999999999998,
      "grad_norm": 0.029611531645059586,
      "learning_rate": 0.0015809519521868558,
      "loss": 1.2328,
      "step": 2427
    },
    {
      "epoch": 3.2373333333333334,
      "grad_norm": 0.041971806436777115,
      "learning_rate": 0.0015806004119566697,
      "loss": 1.0675,
      "step": 2428
    },
    {
      "epoch": 3.2386666666666666,
      "grad_norm": 0.03826865553855896,
      "learning_rate": 0.0015802487634557492,
      "loss": 0.8843,
      "step": 2429
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.04000704362988472,
      "learning_rate": 0.00157989700674967,
      "loss": 1.1849,
      "step": 2430
    },
    {
      "epoch": 3.2413333333333334,
      "grad_norm": 0.04749183729290962,
      "learning_rate": 0.0015795451419040277,
      "loss": 1.3156,
      "step": 2431
    },
    {
      "epoch": 3.2426666666666666,
      "grad_norm": 0.02718333527445793,
      "learning_rate": 0.0015791931689844382,
      "loss": 0.9882,
      "step": 2432
    },
    {
      "epoch": 3.2439999999999998,
      "grad_norm": 0.04511220008134842,
      "learning_rate": 0.0015788410880565379,
      "loss": 0.9692,
      "step": 2433
    },
    {
      "epoch": 3.2453333333333334,
      "grad_norm": 0.07801100611686707,
      "learning_rate": 0.0015784888991859827,
      "loss": 1.2552,
      "step": 2434
    },
    {
      "epoch": 3.2466666666666666,
      "grad_norm": 0.05130777880549431,
      "learning_rate": 0.0015781366024384496,
      "loss": 1.2631,
      "step": 2435
    },
    {
      "epoch": 3.248,
      "grad_norm": 0.030606120824813843,
      "learning_rate": 0.0015777841978796348,
      "loss": 0.9749,
      "step": 2436
    },
    {
      "epoch": 3.2493333333333334,
      "grad_norm": 0.037833765149116516,
      "learning_rate": 0.0015774316855752546,
      "loss": 1.0478,
      "step": 2437
    },
    {
      "epoch": 3.2506666666666666,
      "grad_norm": 0.046919189393520355,
      "learning_rate": 0.0015770790655910462,
      "loss": 0.9603,
      "step": 2438
    },
    {
      "epoch": 3.252,
      "grad_norm": 0.031551793217659,
      "learning_rate": 0.001576726337992766,
      "loss": 1.199,
      "step": 2439
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 0.03881652280688286,
      "learning_rate": 0.0015763735028461914,
      "loss": 0.9861,
      "step": 2440
    },
    {
      "epoch": 3.2546666666666666,
      "grad_norm": 0.03296532481908798,
      "learning_rate": 0.001576020560217119,
      "loss": 0.7305,
      "step": 2441
    },
    {
      "epoch": 3.2560000000000002,
      "grad_norm": 0.030949223786592484,
      "learning_rate": 0.0015756675101713657,
      "loss": 1.0738,
      "step": 2442
    },
    {
      "epoch": 3.2573333333333334,
      "grad_norm": 0.032120369374752045,
      "learning_rate": 0.0015753143527747681,
      "loss": 1.2064,
      "step": 2443
    },
    {
      "epoch": 3.2586666666666666,
      "grad_norm": 0.05047132819890976,
      "learning_rate": 0.0015749610880931844,
      "loss": 0.8185,
      "step": 2444
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.02691749669611454,
      "learning_rate": 0.0015746077161924905,
      "loss": 1.0952,
      "step": 2445
    },
    {
      "epoch": 3.2613333333333334,
      "grad_norm": 0.04127928987145424,
      "learning_rate": 0.0015742542371385841,
      "loss": 1.1455,
      "step": 2446
    },
    {
      "epoch": 3.2626666666666666,
      "grad_norm": 0.029471488669514656,
      "learning_rate": 0.001573900650997382,
      "loss": 0.9453,
      "step": 2447
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 0.03168381378054619,
      "learning_rate": 0.0015735469578348207,
      "loss": 0.9237,
      "step": 2448
    },
    {
      "epoch": 3.2653333333333334,
      "grad_norm": 0.03129103407263756,
      "learning_rate": 0.0015731931577168577,
      "loss": 1.3735,
      "step": 2449
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.02645247057080269,
      "learning_rate": 0.0015728392507094698,
      "loss": 1.006,
      "step": 2450
    },
    {
      "epoch": 3.268,
      "grad_norm": 0.0340455062687397,
      "learning_rate": 0.0015724852368786538,
      "loss": 1.1095,
      "step": 2451
    },
    {
      "epoch": 3.2693333333333334,
      "grad_norm": 0.0373513326048851,
      "learning_rate": 0.001572131116290426,
      "loss": 0.8848,
      "step": 2452
    },
    {
      "epoch": 3.2706666666666666,
      "grad_norm": 0.02731442265212536,
      "learning_rate": 0.0015717768890108235,
      "loss": 1.1297,
      "step": 2453
    },
    {
      "epoch": 3.2720000000000002,
      "grad_norm": 0.02330610901117325,
      "learning_rate": 0.0015714225551059027,
      "loss": 1.0252,
      "step": 2454
    },
    {
      "epoch": 3.2733333333333334,
      "grad_norm": 0.031291525810956955,
      "learning_rate": 0.00157106811464174,
      "loss": 1.0156,
      "step": 2455
    },
    {
      "epoch": 3.2746666666666666,
      "grad_norm": 0.029292384162545204,
      "learning_rate": 0.001570713567684432,
      "loss": 1.0551,
      "step": 2456
    },
    {
      "epoch": 3.276,
      "grad_norm": 0.02778688073158264,
      "learning_rate": 0.001570358914300094,
      "loss": 0.9606,
      "step": 2457
    },
    {
      "epoch": 3.2773333333333334,
      "grad_norm": 0.037094756960868835,
      "learning_rate": 0.0015700041545548627,
      "loss": 1.1873,
      "step": 2458
    },
    {
      "epoch": 3.2786666666666666,
      "grad_norm": 0.03037886507809162,
      "learning_rate": 0.0015696492885148938,
      "loss": 1.157,
      "step": 2459
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.029870638623833656,
      "learning_rate": 0.0015692943162463626,
      "loss": 0.7451,
      "step": 2460
    },
    {
      "epoch": 3.2813333333333334,
      "grad_norm": 0.05937017500400543,
      "learning_rate": 0.0015689392378154653,
      "loss": 1.4976,
      "step": 2461
    },
    {
      "epoch": 3.2826666666666666,
      "grad_norm": 0.029458314180374146,
      "learning_rate": 0.0015685840532884163,
      "loss": 0.8562,
      "step": 2462
    },
    {
      "epoch": 3.284,
      "grad_norm": 0.04551973193883896,
      "learning_rate": 0.0015682287627314514,
      "loss": 1.0387,
      "step": 2463
    },
    {
      "epoch": 3.2853333333333334,
      "grad_norm": 0.0334927998483181,
      "learning_rate": 0.001567873366210825,
      "loss": 1.2356,
      "step": 2464
    },
    {
      "epoch": 3.2866666666666666,
      "grad_norm": 0.029058020561933517,
      "learning_rate": 0.0015675178637928114,
      "loss": 1.1703,
      "step": 2465
    },
    {
      "epoch": 3.288,
      "grad_norm": 0.03343898430466652,
      "learning_rate": 0.0015671622555437053,
      "loss": 1.0158,
      "step": 2466
    },
    {
      "epoch": 3.2893333333333334,
      "grad_norm": 0.0280962772667408,
      "learning_rate": 0.0015668065415298208,
      "loss": 1.3055,
      "step": 2467
    },
    {
      "epoch": 3.2906666666666666,
      "grad_norm": 0.0391867570579052,
      "learning_rate": 0.0015664507218174915,
      "loss": 1.0912,
      "step": 2468
    },
    {
      "epoch": 3.292,
      "grad_norm": 0.028908034786581993,
      "learning_rate": 0.0015660947964730707,
      "loss": 0.9639,
      "step": 2469
    },
    {
      "epoch": 3.2933333333333334,
      "grad_norm": 0.03727992624044418,
      "learning_rate": 0.0015657387655629322,
      "loss": 1.0241,
      "step": 2470
    },
    {
      "epoch": 3.2946666666666666,
      "grad_norm": 0.0337437242269516,
      "learning_rate": 0.0015653826291534684,
      "loss": 1.0286,
      "step": 2471
    },
    {
      "epoch": 3.296,
      "grad_norm": 0.04025668650865555,
      "learning_rate": 0.001565026387311092,
      "loss": 1.4491,
      "step": 2472
    },
    {
      "epoch": 3.2973333333333334,
      "grad_norm": 0.022491183131933212,
      "learning_rate": 0.001564670040102235,
      "loss": 1.0132,
      "step": 2473
    },
    {
      "epoch": 3.2986666666666666,
      "grad_norm": 0.029301991686224937,
      "learning_rate": 0.0015643135875933493,
      "loss": 1.0763,
      "step": 2474
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.031207837164402008,
      "learning_rate": 0.0015639570298509064,
      "loss": 0.9768,
      "step": 2475
    },
    {
      "epoch": 3.3013333333333335,
      "grad_norm": 0.03776101395487785,
      "learning_rate": 0.001563600366941398,
      "loss": 0.7468,
      "step": 2476
    },
    {
      "epoch": 3.3026666666666666,
      "grad_norm": 0.03671630099415779,
      "learning_rate": 0.001563243598931334,
      "loss": 0.8659,
      "step": 2477
    },
    {
      "epoch": 3.304,
      "grad_norm": 0.042604710906744,
      "learning_rate": 0.001562886725887245,
      "loss": 1.1197,
      "step": 2478
    },
    {
      "epoch": 3.3053333333333335,
      "grad_norm": 0.03575126826763153,
      "learning_rate": 0.001562529747875681,
      "loss": 0.878,
      "step": 2479
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 0.03051646798849106,
      "learning_rate": 0.0015621726649632116,
      "loss": 1.163,
      "step": 2480
    },
    {
      "epoch": 3.308,
      "grad_norm": 0.02723105438053608,
      "learning_rate": 0.0015618154772164255,
      "loss": 1.1176,
      "step": 2481
    },
    {
      "epoch": 3.3093333333333335,
      "grad_norm": 0.03136206418275833,
      "learning_rate": 0.0015614581847019316,
      "loss": 1.1611,
      "step": 2482
    },
    {
      "epoch": 3.3106666666666666,
      "grad_norm": 0.030198493972420692,
      "learning_rate": 0.0015611007874863582,
      "loss": 1.1207,
      "step": 2483
    },
    {
      "epoch": 3.312,
      "grad_norm": 0.035837724804878235,
      "learning_rate": 0.0015607432856363523,
      "loss": 1.1507,
      "step": 2484
    },
    {
      "epoch": 3.3133333333333335,
      "grad_norm": 0.024107951670885086,
      "learning_rate": 0.0015603856792185816,
      "loss": 1.1083,
      "step": 2485
    },
    {
      "epoch": 3.3146666666666667,
      "grad_norm": 0.03093414008617401,
      "learning_rate": 0.0015600279682997328,
      "loss": 0.9592,
      "step": 2486
    },
    {
      "epoch": 3.316,
      "grad_norm": 0.022481173276901245,
      "learning_rate": 0.0015596701529465116,
      "loss": 0.9005,
      "step": 2487
    },
    {
      "epoch": 3.3173333333333335,
      "grad_norm": 0.046265240758657455,
      "learning_rate": 0.0015593122332256443,
      "loss": 0.9833,
      "step": 2488
    },
    {
      "epoch": 3.3186666666666667,
      "grad_norm": 0.03052622079849243,
      "learning_rate": 0.0015589542092038751,
      "loss": 0.9426,
      "step": 2489
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.03805726021528244,
      "learning_rate": 0.0015585960809479697,
      "loss": 1.0426,
      "step": 2490
    },
    {
      "epoch": 3.3213333333333335,
      "grad_norm": 0.02625943161547184,
      "learning_rate": 0.001558237848524711,
      "loss": 1.0532,
      "step": 2491
    },
    {
      "epoch": 3.3226666666666667,
      "grad_norm": 0.026508159935474396,
      "learning_rate": 0.001557879512000903,
      "loss": 1.1074,
      "step": 2492
    },
    {
      "epoch": 3.324,
      "grad_norm": 0.03396189957857132,
      "learning_rate": 0.0015575210714433684,
      "loss": 1.0795,
      "step": 2493
    },
    {
      "epoch": 3.3253333333333335,
      "grad_norm": 0.03282664343714714,
      "learning_rate": 0.0015571625269189498,
      "loss": 1.1404,
      "step": 2494
    },
    {
      "epoch": 3.3266666666666667,
      "grad_norm": 0.030214274302124977,
      "learning_rate": 0.0015568038784945079,
      "loss": 1.0356,
      "step": 2495
    },
    {
      "epoch": 3.328,
      "grad_norm": 0.037544384598731995,
      "learning_rate": 0.0015564451262369246,
      "loss": 1.0498,
      "step": 2496
    },
    {
      "epoch": 3.3293333333333335,
      "grad_norm": 0.030074186623096466,
      "learning_rate": 0.0015560862702130997,
      "loss": 1.1749,
      "step": 2497
    },
    {
      "epoch": 3.3306666666666667,
      "grad_norm": 0.022189239040017128,
      "learning_rate": 0.001555727310489953,
      "loss": 1.2424,
      "step": 2498
    },
    {
      "epoch": 3.332,
      "grad_norm": 0.03992554917931557,
      "learning_rate": 0.0015553682471344237,
      "loss": 1.2318,
      "step": 2499
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.027516277506947517,
      "learning_rate": 0.00155500908021347,
      "loss": 0.873,
      "step": 2500
    },
    {
      "epoch": 3.3346666666666667,
      "grad_norm": 0.02751966565847397,
      "learning_rate": 0.0015546498097940697,
      "loss": 0.8206,
      "step": 2501
    },
    {
      "epoch": 3.336,
      "grad_norm": 0.032970257103443146,
      "learning_rate": 0.0015542904359432197,
      "loss": 1.2598,
      "step": 2502
    },
    {
      "epoch": 3.3373333333333335,
      "grad_norm": 0.027595553547143936,
      "learning_rate": 0.0015539309587279365,
      "loss": 1.1091,
      "step": 2503
    },
    {
      "epoch": 3.3386666666666667,
      "grad_norm": 0.02293711155653,
      "learning_rate": 0.0015535713782152552,
      "loss": 1.1512,
      "step": 2504
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.024497611448168755,
      "learning_rate": 0.0015532116944722307,
      "loss": 1.0,
      "step": 2505
    },
    {
      "epoch": 3.3413333333333335,
      "grad_norm": 0.03374919667840004,
      "learning_rate": 0.0015528519075659376,
      "loss": 1.0515,
      "step": 2506
    },
    {
      "epoch": 3.3426666666666667,
      "grad_norm": 0.07426372915506363,
      "learning_rate": 0.0015524920175634684,
      "loss": 1.0105,
      "step": 2507
    },
    {
      "epoch": 3.344,
      "grad_norm": 0.03174735605716705,
      "learning_rate": 0.0015521320245319363,
      "loss": 1.1114,
      "step": 2508
    },
    {
      "epoch": 3.3453333333333335,
      "grad_norm": 0.022116875275969505,
      "learning_rate": 0.0015517719285384724,
      "loss": 1.1248,
      "step": 2509
    },
    {
      "epoch": 3.3466666666666667,
      "grad_norm": 0.0401594415307045,
      "learning_rate": 0.0015514117296502282,
      "loss": 1.0527,
      "step": 2510
    },
    {
      "epoch": 3.348,
      "grad_norm": 0.027611957862973213,
      "learning_rate": 0.0015510514279343734,
      "loss": 0.8824,
      "step": 2511
    },
    {
      "epoch": 3.3493333333333335,
      "grad_norm": 0.025033535435795784,
      "learning_rate": 0.0015506910234580975,
      "loss": 1.1244,
      "step": 2512
    },
    {
      "epoch": 3.3506666666666667,
      "grad_norm": 0.034698694944381714,
      "learning_rate": 0.001550330516288609,
      "loss": 0.9703,
      "step": 2513
    },
    {
      "epoch": 3.352,
      "grad_norm": 0.061758775264024734,
      "learning_rate": 0.0015499699064931353,
      "loss": 0.8947,
      "step": 2514
    },
    {
      "epoch": 3.3533333333333335,
      "grad_norm": 0.0367167703807354,
      "learning_rate": 0.0015496091941389234,
      "loss": 1.021,
      "step": 2515
    },
    {
      "epoch": 3.3546666666666667,
      "grad_norm": 0.04514293000102043,
      "learning_rate": 0.001549248379293239,
      "loss": 1.34,
      "step": 2516
    },
    {
      "epoch": 3.356,
      "grad_norm": 0.02640366181731224,
      "learning_rate": 0.0015488874620233674,
      "loss": 1.0343,
      "step": 2517
    },
    {
      "epoch": 3.3573333333333335,
      "grad_norm": 0.0259980708360672,
      "learning_rate": 0.001548526442396612,
      "loss": 1.0689,
      "step": 2518
    },
    {
      "epoch": 3.3586666666666667,
      "grad_norm": 0.024314777925610542,
      "learning_rate": 0.0015481653204802966,
      "loss": 1.0734,
      "step": 2519
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.034706272184848785,
      "learning_rate": 0.001547804096341763,
      "loss": 1.0274,
      "step": 2520
    },
    {
      "epoch": 3.3613333333333335,
      "grad_norm": 0.22658856213092804,
      "learning_rate": 0.001547442770048373,
      "loss": 1.2501,
      "step": 2521
    },
    {
      "epoch": 3.3626666666666667,
      "grad_norm": 0.023853257298469543,
      "learning_rate": 0.0015470813416675062,
      "loss": 1.1164,
      "step": 2522
    },
    {
      "epoch": 3.364,
      "grad_norm": 0.023285524919629097,
      "learning_rate": 0.001546719811266563,
      "loss": 1.0436,
      "step": 2523
    },
    {
      "epoch": 3.3653333333333335,
      "grad_norm": 0.022638283669948578,
      "learning_rate": 0.0015463581789129613,
      "loss": 0.9166,
      "step": 2524
    },
    {
      "epoch": 3.3666666666666667,
      "grad_norm": 0.02934163808822632,
      "learning_rate": 0.0015459964446741382,
      "loss": 1.1116,
      "step": 2525
    },
    {
      "epoch": 3.368,
      "grad_norm": 0.10960453003644943,
      "learning_rate": 0.0015456346086175508,
      "loss": 0.8396,
      "step": 2526
    },
    {
      "epoch": 3.3693333333333335,
      "grad_norm": 0.06336884200572968,
      "learning_rate": 0.0015452726708106738,
      "loss": 0.885,
      "step": 2527
    },
    {
      "epoch": 3.3706666666666667,
      "grad_norm": 0.024366402998566628,
      "learning_rate": 0.0015449106313210022,
      "loss": 0.9807,
      "step": 2528
    },
    {
      "epoch": 3.372,
      "grad_norm": 0.02573806419968605,
      "learning_rate": 0.0015445484902160491,
      "loss": 0.9541,
      "step": 2529
    },
    {
      "epoch": 3.3733333333333335,
      "grad_norm": 0.02897738479077816,
      "learning_rate": 0.001544186247563347,
      "loss": 1.0493,
      "step": 2530
    },
    {
      "epoch": 3.3746666666666667,
      "grad_norm": 0.03129241615533829,
      "learning_rate": 0.0015438239034304467,
      "loss": 1.1711,
      "step": 2531
    },
    {
      "epoch": 3.376,
      "grad_norm": 0.025472017005085945,
      "learning_rate": 0.0015434614578849187,
      "loss": 1.1078,
      "step": 2532
    },
    {
      "epoch": 3.3773333333333335,
      "grad_norm": 0.034678541123867035,
      "learning_rate": 0.0015430989109943521,
      "loss": 1.0827,
      "step": 2533
    },
    {
      "epoch": 3.3786666666666667,
      "grad_norm": 0.030704062432050705,
      "learning_rate": 0.0015427362628263545,
      "loss": 1.1828,
      "step": 2534
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.020433709025382996,
      "learning_rate": 0.0015423735134485535,
      "loss": 0.8479,
      "step": 2535
    },
    {
      "epoch": 3.3813333333333335,
      "grad_norm": 0.032032184302806854,
      "learning_rate": 0.001542010662928594,
      "loss": 1.0456,
      "step": 2536
    },
    {
      "epoch": 3.3826666666666667,
      "grad_norm": 0.03152830898761749,
      "learning_rate": 0.0015416477113341413,
      "loss": 0.8696,
      "step": 2537
    },
    {
      "epoch": 3.384,
      "grad_norm": 0.02448127046227455,
      "learning_rate": 0.0015412846587328781,
      "loss": 0.8081,
      "step": 2538
    },
    {
      "epoch": 3.3853333333333335,
      "grad_norm": 0.036882489919662476,
      "learning_rate": 0.0015409215051925074,
      "loss": 1.0715,
      "step": 2539
    },
    {
      "epoch": 3.3866666666666667,
      "grad_norm": 0.025204600766301155,
      "learning_rate": 0.0015405582507807504,
      "loss": 1.1502,
      "step": 2540
    },
    {
      "epoch": 3.388,
      "grad_norm": 0.03514653444290161,
      "learning_rate": 0.001540194895565346,
      "loss": 1.1668,
      "step": 2541
    },
    {
      "epoch": 3.389333333333333,
      "grad_norm": 0.030908580869436264,
      "learning_rate": 0.0015398314396140537,
      "loss": 1.1828,
      "step": 2542
    },
    {
      "epoch": 3.3906666666666667,
      "grad_norm": 0.13939185440540314,
      "learning_rate": 0.0015394678829946506,
      "loss": 1.0925,
      "step": 2543
    },
    {
      "epoch": 3.392,
      "grad_norm": 0.02829296514391899,
      "learning_rate": 0.0015391042257749336,
      "loss": 1.2256,
      "step": 2544
    },
    {
      "epoch": 3.3933333333333335,
      "grad_norm": 0.025027265772223473,
      "learning_rate": 0.0015387404680227172,
      "loss": 1.0379,
      "step": 2545
    },
    {
      "epoch": 3.3946666666666667,
      "grad_norm": 0.031607791781425476,
      "learning_rate": 0.0015383766098058353,
      "loss": 1.1257,
      "step": 2546
    },
    {
      "epoch": 3.396,
      "grad_norm": 0.0362633541226387,
      "learning_rate": 0.0015380126511921404,
      "loss": 0.9739,
      "step": 2547
    },
    {
      "epoch": 3.397333333333333,
      "grad_norm": 0.028897542506456375,
      "learning_rate": 0.0015376485922495037,
      "loss": 1.0137,
      "step": 2548
    },
    {
      "epoch": 3.3986666666666667,
      "grad_norm": 0.04516291618347168,
      "learning_rate": 0.0015372844330458152,
      "loss": 0.9501,
      "step": 2549
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.02763497084379196,
      "learning_rate": 0.0015369201736489839,
      "loss": 0.8107,
      "step": 2550
    },
    {
      "epoch": 3.4013333333333335,
      "grad_norm": 0.035611316561698914,
      "learning_rate": 0.0015365558141269364,
      "loss": 1.0528,
      "step": 2551
    },
    {
      "epoch": 3.4026666666666667,
      "grad_norm": 0.048962827771902084,
      "learning_rate": 0.0015361913545476192,
      "loss": 1.1212,
      "step": 2552
    },
    {
      "epoch": 3.404,
      "grad_norm": 0.034978318959474564,
      "learning_rate": 0.0015358267949789966,
      "loss": 1.1161,
      "step": 2553
    },
    {
      "epoch": 3.405333333333333,
      "grad_norm": 0.03474835678935051,
      "learning_rate": 0.0015354621354890523,
      "loss": 1.0733,
      "step": 2554
    },
    {
      "epoch": 3.4066666666666667,
      "grad_norm": 0.03470776602625847,
      "learning_rate": 0.0015350973761457882,
      "loss": 1.105,
      "step": 2555
    },
    {
      "epoch": 3.408,
      "grad_norm": 0.026049938052892685,
      "learning_rate": 0.0015347325170172244,
      "loss": 0.9281,
      "step": 2556
    },
    {
      "epoch": 3.4093333333333335,
      "grad_norm": 0.02864145115017891,
      "learning_rate": 0.0015343675581714002,
      "loss": 1.2602,
      "step": 2557
    },
    {
      "epoch": 3.4106666666666667,
      "grad_norm": 0.031115451827645302,
      "learning_rate": 0.0015340024996763739,
      "loss": 0.9282,
      "step": 2558
    },
    {
      "epoch": 3.412,
      "grad_norm": 0.027052082121372223,
      "learning_rate": 0.0015336373416002212,
      "loss": 1.1182,
      "step": 2559
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 0.02447490021586418,
      "learning_rate": 0.0015332720840110375,
      "loss": 1.2513,
      "step": 2560
    },
    {
      "epoch": 3.4146666666666667,
      "grad_norm": 0.0307502169162035,
      "learning_rate": 0.001532906726976936,
      "loss": 1.2635,
      "step": 2561
    },
    {
      "epoch": 3.416,
      "grad_norm": 0.025227084755897522,
      "learning_rate": 0.0015325412705660488,
      "loss": 0.9812,
      "step": 2562
    },
    {
      "epoch": 3.4173333333333336,
      "grad_norm": 0.031069660559296608,
      "learning_rate": 0.0015321757148465261,
      "loss": 1.1463,
      "step": 2563
    },
    {
      "epoch": 3.4186666666666667,
      "grad_norm": 0.03500524163246155,
      "learning_rate": 0.0015318100598865376,
      "loss": 0.8654,
      "step": 2564
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.02462499588727951,
      "learning_rate": 0.0015314443057542705,
      "loss": 0.8963,
      "step": 2565
    },
    {
      "epoch": 3.421333333333333,
      "grad_norm": 0.03885725885629654,
      "learning_rate": 0.0015310784525179306,
      "loss": 1.2224,
      "step": 2566
    },
    {
      "epoch": 3.4226666666666667,
      "grad_norm": 0.04560307785868645,
      "learning_rate": 0.0015307125002457429,
      "loss": 0.8138,
      "step": 2567
    },
    {
      "epoch": 3.424,
      "grad_norm": 0.026310225948691368,
      "learning_rate": 0.0015303464490059504,
      "loss": 1.0358,
      "step": 2568
    },
    {
      "epoch": 3.4253333333333336,
      "grad_norm": 0.034771308302879333,
      "learning_rate": 0.0015299802988668141,
      "loss": 0.9136,
      "step": 2569
    },
    {
      "epoch": 3.4266666666666667,
      "grad_norm": 0.04426505044102669,
      "learning_rate": 0.0015296140498966145,
      "loss": 1.2255,
      "step": 2570
    },
    {
      "epoch": 3.428,
      "grad_norm": 0.040613871067762375,
      "learning_rate": 0.0015292477021636498,
      "loss": 1.2081,
      "step": 2571
    },
    {
      "epoch": 3.429333333333333,
      "grad_norm": 0.02823302336037159,
      "learning_rate": 0.0015288812557362364,
      "loss": 1.0914,
      "step": 2572
    },
    {
      "epoch": 3.4306666666666668,
      "grad_norm": 0.03431167080998421,
      "learning_rate": 0.0015285147106827094,
      "loss": 1.0734,
      "step": 2573
    },
    {
      "epoch": 3.432,
      "grad_norm": 0.023798825219273567,
      "learning_rate": 0.0015281480670714229,
      "loss": 1.0952,
      "step": 2574
    },
    {
      "epoch": 3.4333333333333336,
      "grad_norm": 0.03730270266532898,
      "learning_rate": 0.0015277813249707486,
      "loss": 1.1841,
      "step": 2575
    },
    {
      "epoch": 3.4346666666666668,
      "grad_norm": 0.0342477411031723,
      "learning_rate": 0.0015274144844490771,
      "loss": 1.081,
      "step": 2576
    },
    {
      "epoch": 3.436,
      "grad_norm": 0.02657194249331951,
      "learning_rate": 0.0015270475455748166,
      "loss": 0.8268,
      "step": 2577
    },
    {
      "epoch": 3.437333333333333,
      "grad_norm": 0.03368576243519783,
      "learning_rate": 0.001526680508416394,
      "loss": 0.8764,
      "step": 2578
    },
    {
      "epoch": 3.4386666666666668,
      "grad_norm": 0.02127956971526146,
      "learning_rate": 0.001526313373042255,
      "loss": 0.8621,
      "step": 2579
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.03725455701351166,
      "learning_rate": 0.0015259461395208628,
      "loss": 1.2843,
      "step": 2580
    },
    {
      "epoch": 3.4413333333333336,
      "grad_norm": 0.039324913173913956,
      "learning_rate": 0.0015255788079206996,
      "loss": 1.0289,
      "step": 2581
    },
    {
      "epoch": 3.4426666666666668,
      "grad_norm": 0.03568699583411217,
      "learning_rate": 0.0015252113783102664,
      "loss": 0.8703,
      "step": 2582
    },
    {
      "epoch": 3.444,
      "grad_norm": 0.022660043090581894,
      "learning_rate": 0.0015248438507580804,
      "loss": 1.099,
      "step": 2583
    },
    {
      "epoch": 3.445333333333333,
      "grad_norm": 0.03190918266773224,
      "learning_rate": 0.001524476225332679,
      "loss": 1.1764,
      "step": 2584
    },
    {
      "epoch": 3.4466666666666668,
      "grad_norm": 0.023582149296998978,
      "learning_rate": 0.001524108502102617,
      "loss": 1.2096,
      "step": 2585
    },
    {
      "epoch": 3.448,
      "grad_norm": 0.049435678869485855,
      "learning_rate": 0.001523740681136468,
      "loss": 1.2135,
      "step": 2586
    },
    {
      "epoch": 3.449333333333333,
      "grad_norm": 0.021071888506412506,
      "learning_rate": 0.0015233727625028235,
      "loss": 0.8288,
      "step": 2587
    },
    {
      "epoch": 3.4506666666666668,
      "grad_norm": 0.020260900259017944,
      "learning_rate": 0.0015230047462702929,
      "loss": 0.954,
      "step": 2588
    },
    {
      "epoch": 3.452,
      "grad_norm": 0.03677899017930031,
      "learning_rate": 0.001522636632507504,
      "loss": 0.9655,
      "step": 2589
    },
    {
      "epoch": 3.453333333333333,
      "grad_norm": 0.040499817579984665,
      "learning_rate": 0.0015222684212831035,
      "loss": 1.073,
      "step": 2590
    },
    {
      "epoch": 3.4546666666666668,
      "grad_norm": 0.03046325221657753,
      "learning_rate": 0.001521900112665755,
      "loss": 1.0951,
      "step": 2591
    },
    {
      "epoch": 3.456,
      "grad_norm": 0.035632211714982986,
      "learning_rate": 0.0015215317067241415,
      "loss": 1.1119,
      "step": 2592
    },
    {
      "epoch": 3.457333333333333,
      "grad_norm": 0.021572867408394814,
      "learning_rate": 0.001521163203526963,
      "loss": 1.1573,
      "step": 2593
    },
    {
      "epoch": 3.458666666666667,
      "grad_norm": 0.0329684317111969,
      "learning_rate": 0.0015207946031429382,
      "loss": 1.1597,
      "step": 2594
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.02423080988228321,
      "learning_rate": 0.0015204259056408045,
      "loss": 1.2825,
      "step": 2595
    },
    {
      "epoch": 3.461333333333333,
      "grad_norm": 0.022977907210588455,
      "learning_rate": 0.0015200571110893166,
      "loss": 1.0494,
      "step": 2596
    },
    {
      "epoch": 3.462666666666667,
      "grad_norm": 0.02543036825954914,
      "learning_rate": 0.0015196882195572478,
      "loss": 0.8409,
      "step": 2597
    },
    {
      "epoch": 3.464,
      "grad_norm": 0.027418948709964752,
      "learning_rate": 0.0015193192311133883,
      "loss": 1.3209,
      "step": 2598
    },
    {
      "epoch": 3.465333333333333,
      "grad_norm": 0.026546886190772057,
      "learning_rate": 0.0015189501458265484,
      "loss": 1.0871,
      "step": 2599
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.02773890271782875,
      "learning_rate": 0.0015185809637655548,
      "loss": 1.0236,
      "step": 2600
    },
    {
      "epoch": 3.468,
      "grad_norm": 0.024350605905056,
      "learning_rate": 0.0015182116849992526,
      "loss": 0.8256,
      "step": 2601
    },
    {
      "epoch": 3.469333333333333,
      "grad_norm": 0.041717130690813065,
      "learning_rate": 0.0015178423095965057,
      "loss": 1.1038,
      "step": 2602
    },
    {
      "epoch": 3.470666666666667,
      "grad_norm": 0.02741559036076069,
      "learning_rate": 0.0015174728376261954,
      "loss": 0.793,
      "step": 2603
    },
    {
      "epoch": 3.472,
      "grad_norm": 0.028016071766614914,
      "learning_rate": 0.0015171032691572207,
      "loss": 1.1204,
      "step": 2604
    },
    {
      "epoch": 3.473333333333333,
      "grad_norm": 0.05290793254971504,
      "learning_rate": 0.0015167336042584988,
      "loss": 1.0694,
      "step": 2605
    },
    {
      "epoch": 3.474666666666667,
      "grad_norm": 0.027287837117910385,
      "learning_rate": 0.001516363842998966,
      "loss": 1.3301,
      "step": 2606
    },
    {
      "epoch": 3.476,
      "grad_norm": 0.2217392772436142,
      "learning_rate": 0.0015159939854475742,
      "loss": 1.1184,
      "step": 2607
    },
    {
      "epoch": 3.477333333333333,
      "grad_norm": 0.03642546385526657,
      "learning_rate": 0.001515624031673296,
      "loss": 1.2972,
      "step": 2608
    },
    {
      "epoch": 3.478666666666667,
      "grad_norm": 0.0277714766561985,
      "learning_rate": 0.00151525398174512,
      "loss": 0.8704,
      "step": 2609
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.03702674061059952,
      "learning_rate": 0.0015148838357320535,
      "loss": 1.1775,
      "step": 2610
    },
    {
      "epoch": 3.481333333333333,
      "grad_norm": 0.3401488661766052,
      "learning_rate": 0.0015145135937031213,
      "loss": 1.0807,
      "step": 2611
    },
    {
      "epoch": 3.482666666666667,
      "grad_norm": 0.06498779356479645,
      "learning_rate": 0.0015141432557273667,
      "loss": 1.0269,
      "step": 2612
    },
    {
      "epoch": 3.484,
      "grad_norm": 0.1089654490351677,
      "learning_rate": 0.0015137728218738503,
      "loss": 0.9474,
      "step": 2613
    },
    {
      "epoch": 3.485333333333333,
      "grad_norm": 0.09508586674928665,
      "learning_rate": 0.001513402292211651,
      "loss": 1.0583,
      "step": 2614
    },
    {
      "epoch": 3.486666666666667,
      "grad_norm": 0.16933989524841309,
      "learning_rate": 0.0015130316668098653,
      "loss": 0.9647,
      "step": 2615
    },
    {
      "epoch": 3.488,
      "grad_norm": 0.10450366139411926,
      "learning_rate": 0.0015126609457376079,
      "loss": 1.0066,
      "step": 2616
    },
    {
      "epoch": 3.489333333333333,
      "grad_norm": 0.12513089179992676,
      "learning_rate": 0.0015122901290640107,
      "loss": 1.041,
      "step": 2617
    },
    {
      "epoch": 3.490666666666667,
      "grad_norm": 0.04251750186085701,
      "learning_rate": 0.0015119192168582245,
      "loss": 1.0019,
      "step": 2618
    },
    {
      "epoch": 3.492,
      "grad_norm": 0.07887353003025055,
      "learning_rate": 0.0015115482091894164,
      "loss": 1.2128,
      "step": 2619
    },
    {
      "epoch": 3.493333333333333,
      "grad_norm": 0.027373600751161575,
      "learning_rate": 0.0015111771061267728,
      "loss": 0.8764,
      "step": 2620
    },
    {
      "epoch": 3.494666666666667,
      "grad_norm": 0.030362196266651154,
      "learning_rate": 0.0015108059077394965,
      "loss": 0.8796,
      "step": 2621
    },
    {
      "epoch": 3.496,
      "grad_norm": 0.026532135903835297,
      "learning_rate": 0.0015104346140968096,
      "loss": 1.1277,
      "step": 2622
    },
    {
      "epoch": 3.497333333333333,
      "grad_norm": 0.031960830092430115,
      "learning_rate": 0.001510063225267951,
      "loss": 1.0707,
      "step": 2623
    },
    {
      "epoch": 3.498666666666667,
      "grad_norm": 0.0385419987142086,
      "learning_rate": 0.0015096917413221771,
      "loss": 1.1094,
      "step": 2624
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.037002794444561005,
      "learning_rate": 0.001509320162328763,
      "loss": 1.3728,
      "step": 2625
    },
    {
      "epoch": 3.501333333333333,
      "grad_norm": 0.03350534290075302,
      "learning_rate": 0.0015089484883570009,
      "loss": 0.952,
      "step": 2626
    },
    {
      "epoch": 3.502666666666667,
      "grad_norm": 0.029442545026540756,
      "learning_rate": 0.0015085767194762002,
      "loss": 0.7674,
      "step": 2627
    },
    {
      "epoch": 3.504,
      "grad_norm": 0.02579192817211151,
      "learning_rate": 0.0015082048557556892,
      "loss": 1.034,
      "step": 2628
    },
    {
      "epoch": 3.505333333333333,
      "grad_norm": 0.043609268963336945,
      "learning_rate": 0.0015078328972648132,
      "loss": 0.9986,
      "step": 2629
    },
    {
      "epoch": 3.506666666666667,
      "grad_norm": 0.02376370131969452,
      "learning_rate": 0.0015074608440729352,
      "loss": 0.9658,
      "step": 2630
    },
    {
      "epoch": 3.508,
      "grad_norm": 0.024407770484685898,
      "learning_rate": 0.0015070886962494359,
      "loss": 0.9332,
      "step": 2631
    },
    {
      "epoch": 3.509333333333333,
      "grad_norm": 0.032607272267341614,
      "learning_rate": 0.0015067164538637137,
      "loss": 1.475,
      "step": 2632
    },
    {
      "epoch": 3.510666666666667,
      "grad_norm": 0.02553672529757023,
      "learning_rate": 0.0015063441169851843,
      "loss": 0.9105,
      "step": 2633
    },
    {
      "epoch": 3.512,
      "grad_norm": 0.04520638659596443,
      "learning_rate": 0.001505971685683282,
      "loss": 0.7777,
      "step": 2634
    },
    {
      "epoch": 3.513333333333333,
      "grad_norm": 0.0288997869938612,
      "learning_rate": 0.0015055991600274573,
      "loss": 0.9255,
      "step": 2635
    },
    {
      "epoch": 3.514666666666667,
      "grad_norm": 0.02700154110789299,
      "learning_rate": 0.0015052265400871793,
      "loss": 1.0392,
      "step": 2636
    },
    {
      "epoch": 3.516,
      "grad_norm": 0.029447391629219055,
      "learning_rate": 0.0015048538259319346,
      "loss": 0.985,
      "step": 2637
    },
    {
      "epoch": 3.517333333333333,
      "grad_norm": 0.045534636825323105,
      "learning_rate": 0.001504481017631227,
      "loss": 1.1505,
      "step": 2638
    },
    {
      "epoch": 3.518666666666667,
      "grad_norm": 0.023737052455544472,
      "learning_rate": 0.001504108115254578,
      "loss": 1.258,
      "step": 2639
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.03951924666762352,
      "learning_rate": 0.0015037351188715265,
      "loss": 1.1325,
      "step": 2640
    },
    {
      "epoch": 3.521333333333333,
      "grad_norm": 0.02568242698907852,
      "learning_rate": 0.0015033620285516294,
      "loss": 0.8359,
      "step": 2641
    },
    {
      "epoch": 3.522666666666667,
      "grad_norm": 0.03653354570269585,
      "learning_rate": 0.0015029888443644608,
      "loss": 0.961,
      "step": 2642
    },
    {
      "epoch": 3.524,
      "grad_norm": 0.029189543798565865,
      "learning_rate": 0.0015026155663796122,
      "loss": 1.0363,
      "step": 2643
    },
    {
      "epoch": 3.525333333333333,
      "grad_norm": 0.023516353219747543,
      "learning_rate": 0.001502242194666693,
      "loss": 1.0919,
      "step": 2644
    },
    {
      "epoch": 3.5266666666666664,
      "grad_norm": 0.027252977713942528,
      "learning_rate": 0.0015018687292953292,
      "loss": 0.991,
      "step": 2645
    },
    {
      "epoch": 3.528,
      "grad_norm": 0.02789846435189247,
      "learning_rate": 0.0015014951703351653,
      "loss": 1.0013,
      "step": 2646
    },
    {
      "epoch": 3.529333333333333,
      "grad_norm": 0.03697671741247177,
      "learning_rate": 0.0015011215178558628,
      "loss": 0.9285,
      "step": 2647
    },
    {
      "epoch": 3.530666666666667,
      "grad_norm": 0.027413450181484222,
      "learning_rate": 0.0015007477719271006,
      "loss": 1.2932,
      "step": 2648
    },
    {
      "epoch": 3.532,
      "grad_norm": 0.02845138870179653,
      "learning_rate": 0.001500373932618575,
      "loss": 0.8689,
      "step": 2649
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.034894756972789764,
      "learning_rate": 0.0015,
      "loss": 0.9474,
      "step": 2650
    },
    {
      "epoch": 3.5346666666666664,
      "grad_norm": 0.028852490708231926,
      "learning_rate": 0.0014996259741411066,
      "loss": 0.9185,
      "step": 2651
    },
    {
      "epoch": 3.536,
      "grad_norm": 0.03075764700770378,
      "learning_rate": 0.0014992518551116434,
      "loss": 0.8864,
      "step": 2652
    },
    {
      "epoch": 3.537333333333333,
      "grad_norm": 0.04611166939139366,
      "learning_rate": 0.0014988776429813765,
      "loss": 1.156,
      "step": 2653
    },
    {
      "epoch": 3.538666666666667,
      "grad_norm": 0.046426475048065186,
      "learning_rate": 0.0014985033378200887,
      "loss": 1.0101,
      "step": 2654
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.029568206518888474,
      "learning_rate": 0.0014981289396975817,
      "loss": 0.939,
      "step": 2655
    },
    {
      "epoch": 3.541333333333333,
      "grad_norm": 0.028134800493717194,
      "learning_rate": 0.0014977544486836725,
      "loss": 1.0501,
      "step": 2656
    },
    {
      "epoch": 3.5426666666666664,
      "grad_norm": 0.029329420998692513,
      "learning_rate": 0.0014973798648481966,
      "loss": 1.1223,
      "step": 2657
    },
    {
      "epoch": 3.544,
      "grad_norm": 0.02560429461300373,
      "learning_rate": 0.001497005188261007,
      "loss": 1.1239,
      "step": 2658
    },
    {
      "epoch": 3.5453333333333332,
      "grad_norm": 0.025953136384487152,
      "learning_rate": 0.0014966304189919738,
      "loss": 0.8423,
      "step": 2659
    },
    {
      "epoch": 3.546666666666667,
      "grad_norm": 0.03728245198726654,
      "learning_rate": 0.0014962555571109836,
      "loss": 1.0215,
      "step": 2660
    },
    {
      "epoch": 3.548,
      "grad_norm": 0.025108549743890762,
      "learning_rate": 0.001495880602687941,
      "loss": 1.1761,
      "step": 2661
    },
    {
      "epoch": 3.5493333333333332,
      "grad_norm": 0.0247303768992424,
      "learning_rate": 0.0014955055557927678,
      "loss": 0.8247,
      "step": 2662
    },
    {
      "epoch": 3.5506666666666664,
      "grad_norm": 0.03704344853758812,
      "learning_rate": 0.0014951304164954032,
      "loss": 0.9531,
      "step": 2663
    },
    {
      "epoch": 3.552,
      "grad_norm": 0.023453280329704285,
      "learning_rate": 0.0014947551848658034,
      "loss": 0.9733,
      "step": 2664
    },
    {
      "epoch": 3.5533333333333332,
      "grad_norm": 0.033672355115413666,
      "learning_rate": 0.0014943798609739418,
      "loss": 1.247,
      "step": 2665
    },
    {
      "epoch": 3.554666666666667,
      "grad_norm": 0.024130815640091896,
      "learning_rate": 0.0014940044448898088,
      "loss": 1.1315,
      "step": 2666
    },
    {
      "epoch": 3.556,
      "grad_norm": 0.05795634537935257,
      "learning_rate": 0.0014936289366834121,
      "loss": 0.9636,
      "step": 2667
    },
    {
      "epoch": 3.5573333333333332,
      "grad_norm": 0.0350647047162056,
      "learning_rate": 0.0014932533364247773,
      "loss": 0.8541,
      "step": 2668
    },
    {
      "epoch": 3.5586666666666664,
      "grad_norm": 0.13287730515003204,
      "learning_rate": 0.0014928776441839463,
      "loss": 0.7253,
      "step": 2669
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.026568235829472542,
      "learning_rate": 0.0014925018600309783,
      "loss": 0.8578,
      "step": 2670
    },
    {
      "epoch": 3.5613333333333332,
      "grad_norm": 0.030157292261719704,
      "learning_rate": 0.00149212598403595,
      "loss": 1.1322,
      "step": 2671
    },
    {
      "epoch": 3.562666666666667,
      "grad_norm": 0.03018285147845745,
      "learning_rate": 0.0014917500162689547,
      "loss": 1.036,
      "step": 2672
    },
    {
      "epoch": 3.564,
      "grad_norm": 0.03393219783902168,
      "learning_rate": 0.0014913739568001032,
      "loss": 1.0344,
      "step": 2673
    },
    {
      "epoch": 3.5653333333333332,
      "grad_norm": 0.03446604311466217,
      "learning_rate": 0.0014909978056995236,
      "loss": 1.1499,
      "step": 2674
    },
    {
      "epoch": 3.5666666666666664,
      "grad_norm": 0.03890344873070717,
      "learning_rate": 0.0014906215630373606,
      "loss": 1.0856,
      "step": 2675
    },
    {
      "epoch": 3.568,
      "grad_norm": 0.040850117802619934,
      "learning_rate": 0.001490245228883776,
      "loss": 1.002,
      "step": 2676
    },
    {
      "epoch": 3.5693333333333332,
      "grad_norm": 0.03146995231509209,
      "learning_rate": 0.0014898688033089492,
      "loss": 1.1613,
      "step": 2677
    },
    {
      "epoch": 3.570666666666667,
      "grad_norm": 0.08061686903238297,
      "learning_rate": 0.0014894922863830755,
      "loss": 0.9757,
      "step": 2678
    },
    {
      "epoch": 3.572,
      "grad_norm": 0.04351629689335823,
      "learning_rate": 0.001489115678176369,
      "loss": 0.8836,
      "step": 2679
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 0.028780026361346245,
      "learning_rate": 0.0014887389787590596,
      "loss": 1.2236,
      "step": 2680
    },
    {
      "epoch": 3.5746666666666664,
      "grad_norm": 0.05139017105102539,
      "learning_rate": 0.001488362188201394,
      "loss": 1.2868,
      "step": 2681
    },
    {
      "epoch": 3.576,
      "grad_norm": 0.02971043810248375,
      "learning_rate": 0.0014879853065736365,
      "loss": 1.1043,
      "step": 2682
    },
    {
      "epoch": 3.5773333333333333,
      "grad_norm": 0.033959951251745224,
      "learning_rate": 0.0014876083339460684,
      "loss": 1.0448,
      "step": 2683
    },
    {
      "epoch": 3.578666666666667,
      "grad_norm": 0.0299295112490654,
      "learning_rate": 0.0014872312703889876,
      "loss": 1.051,
      "step": 2684
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.03797631710767746,
      "learning_rate": 0.0014868541159727096,
      "loss": 0.8981,
      "step": 2685
    },
    {
      "epoch": 3.5813333333333333,
      "grad_norm": 0.025604279711842537,
      "learning_rate": 0.001486476870767566,
      "loss": 1.213,
      "step": 2686
    },
    {
      "epoch": 3.5826666666666664,
      "grad_norm": 0.027425212785601616,
      "learning_rate": 0.001486099534843906,
      "loss": 0.7792,
      "step": 2687
    },
    {
      "epoch": 3.584,
      "grad_norm": 0.02645193785429001,
      "learning_rate": 0.001485722108272095,
      "loss": 1.0424,
      "step": 2688
    },
    {
      "epoch": 3.5853333333333333,
      "grad_norm": 0.027502913028001785,
      "learning_rate": 0.0014853445911225157,
      "loss": 0.9782,
      "step": 2689
    },
    {
      "epoch": 3.586666666666667,
      "grad_norm": 0.026940174400806427,
      "learning_rate": 0.0014849669834655682,
      "loss": 1.0542,
      "step": 2690
    },
    {
      "epoch": 3.588,
      "grad_norm": 0.04200434312224388,
      "learning_rate": 0.0014845892853716691,
      "loss": 0.9287,
      "step": 2691
    },
    {
      "epoch": 3.5893333333333333,
      "grad_norm": 0.024588465690612793,
      "learning_rate": 0.0014842114969112515,
      "loss": 0.9111,
      "step": 2692
    },
    {
      "epoch": 3.5906666666666665,
      "grad_norm": 0.03784032166004181,
      "learning_rate": 0.0014838336181547654,
      "loss": 1.1846,
      "step": 2693
    },
    {
      "epoch": 3.592,
      "grad_norm": 0.024791598320007324,
      "learning_rate": 0.0014834556491726781,
      "loss": 0.9855,
      "step": 2694
    },
    {
      "epoch": 3.5933333333333333,
      "grad_norm": 0.02665967494249344,
      "learning_rate": 0.0014830775900354736,
      "loss": 0.6745,
      "step": 2695
    },
    {
      "epoch": 3.594666666666667,
      "grad_norm": 0.03374791890382767,
      "learning_rate": 0.001482699440813653,
      "loss": 1.02,
      "step": 2696
    },
    {
      "epoch": 3.596,
      "grad_norm": 0.02190694585442543,
      "learning_rate": 0.001482321201577733,
      "loss": 1.0262,
      "step": 2697
    },
    {
      "epoch": 3.5973333333333333,
      "grad_norm": 0.026874126866459846,
      "learning_rate": 0.001481942872398248,
      "loss": 0.9709,
      "step": 2698
    },
    {
      "epoch": 3.5986666666666665,
      "grad_norm": 0.03531080111861229,
      "learning_rate": 0.0014815644533457495,
      "loss": 1.0017,
      "step": 2699
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.028004441410303116,
      "learning_rate": 0.001481185944490805,
      "loss": 1.1243,
      "step": 2700
    },
    {
      "epoch": 3.6013333333333333,
      "grad_norm": 0.025013569742441177,
      "learning_rate": 0.0014808073459039992,
      "loss": 0.9334,
      "step": 2701
    },
    {
      "epoch": 3.602666666666667,
      "grad_norm": 0.027828391641378403,
      "learning_rate": 0.0014804286576559338,
      "loss": 1.1581,
      "step": 2702
    },
    {
      "epoch": 3.604,
      "grad_norm": 0.025029435753822327,
      "learning_rate": 0.0014800498798172262,
      "loss": 0.9739,
      "step": 2703
    },
    {
      "epoch": 3.6053333333333333,
      "grad_norm": 0.023069454357028008,
      "learning_rate": 0.0014796710124585113,
      "loss": 0.9206,
      "step": 2704
    },
    {
      "epoch": 3.6066666666666665,
      "grad_norm": 0.03555361181497574,
      "learning_rate": 0.0014792920556504405,
      "loss": 1.1572,
      "step": 2705
    },
    {
      "epoch": 3.608,
      "grad_norm": 0.03019336797297001,
      "learning_rate": 0.0014789130094636821,
      "loss": 1.0446,
      "step": 2706
    },
    {
      "epoch": 3.6093333333333333,
      "grad_norm": 0.027857037261128426,
      "learning_rate": 0.0014785338739689209,
      "loss": 1.3743,
      "step": 2707
    },
    {
      "epoch": 3.610666666666667,
      "grad_norm": 0.024503404274582863,
      "learning_rate": 0.0014781546492368583,
      "loss": 0.8901,
      "step": 2708
    },
    {
      "epoch": 3.612,
      "grad_norm": 0.0323958694934845,
      "learning_rate": 0.001477775335338212,
      "loss": 1.5475,
      "step": 2709
    },
    {
      "epoch": 3.6133333333333333,
      "grad_norm": 0.024283772334456444,
      "learning_rate": 0.001477395932343717,
      "loss": 1.0242,
      "step": 2710
    },
    {
      "epoch": 3.6146666666666665,
      "grad_norm": 0.023708121851086617,
      "learning_rate": 0.0014770164403241244,
      "loss": 0.9319,
      "step": 2711
    },
    {
      "epoch": 3.616,
      "grad_norm": 0.026696382090449333,
      "learning_rate": 0.0014766368593502027,
      "loss": 1.1006,
      "step": 2712
    },
    {
      "epoch": 3.6173333333333333,
      "grad_norm": 0.022862225770950317,
      "learning_rate": 0.0014762571894927357,
      "loss": 1.1027,
      "step": 2713
    },
    {
      "epoch": 3.618666666666667,
      "grad_norm": 0.04084797948598862,
      "learning_rate": 0.0014758774308225248,
      "loss": 0.994,
      "step": 2714
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.0368378683924675,
      "learning_rate": 0.0014754975834103876,
      "loss": 1.1577,
      "step": 2715
    },
    {
      "epoch": 3.6213333333333333,
      "grad_norm": 0.030806494876742363,
      "learning_rate": 0.0014751176473271584,
      "loss": 1.105,
      "step": 2716
    },
    {
      "epoch": 3.6226666666666665,
      "grad_norm": 0.023765461519360542,
      "learning_rate": 0.0014747376226436877,
      "loss": 0.9846,
      "step": 2717
    },
    {
      "epoch": 3.624,
      "grad_norm": 0.03350101411342621,
      "learning_rate": 0.001474357509430843,
      "loss": 1.1205,
      "step": 2718
    },
    {
      "epoch": 3.6253333333333333,
      "grad_norm": 0.03420400992035866,
      "learning_rate": 0.001473977307759508,
      "loss": 1.0024,
      "step": 2719
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 0.026973959058523178,
      "learning_rate": 0.0014735970177005826,
      "loss": 0.9434,
      "step": 2720
    },
    {
      "epoch": 3.628,
      "grad_norm": 0.029265617951750755,
      "learning_rate": 0.0014732166393249839,
      "loss": 0.9671,
      "step": 2721
    },
    {
      "epoch": 3.6293333333333333,
      "grad_norm": 0.05082523450255394,
      "learning_rate": 0.001472836172703645,
      "loss": 0.8841,
      "step": 2722
    },
    {
      "epoch": 3.6306666666666665,
      "grad_norm": 0.028954751789569855,
      "learning_rate": 0.0014724556179075159,
      "loss": 1.0406,
      "step": 2723
    },
    {
      "epoch": 3.632,
      "grad_norm": 0.025810465216636658,
      "learning_rate": 0.001472074975007562,
      "loss": 1.0434,
      "step": 2724
    },
    {
      "epoch": 3.6333333333333333,
      "grad_norm": 0.02941451407968998,
      "learning_rate": 0.0014716942440747662,
      "loss": 0.9535,
      "step": 2725
    },
    {
      "epoch": 3.634666666666667,
      "grad_norm": 0.03752925619482994,
      "learning_rate": 0.0014713134251801274,
      "loss": 0.8552,
      "step": 2726
    },
    {
      "epoch": 3.636,
      "grad_norm": 0.03051326610147953,
      "learning_rate": 0.001470932518394661,
      "loss": 0.9051,
      "step": 2727
    },
    {
      "epoch": 3.6373333333333333,
      "grad_norm": 0.02990506775677204,
      "learning_rate": 0.0014705515237893989,
      "loss": 0.9257,
      "step": 2728
    },
    {
      "epoch": 3.6386666666666665,
      "grad_norm": 0.042169369757175446,
      "learning_rate": 0.001470170441435389,
      "loss": 0.9231,
      "step": 2729
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.028223607689142227,
      "learning_rate": 0.0014697892714036957,
      "loss": 1.2395,
      "step": 2730
    },
    {
      "epoch": 3.6413333333333333,
      "grad_norm": 0.024999937042593956,
      "learning_rate": 0.0014694080137654,
      "loss": 1.0016,
      "step": 2731
    },
    {
      "epoch": 3.642666666666667,
      "grad_norm": 0.02876865118741989,
      "learning_rate": 0.001469026668591599,
      "loss": 1.1275,
      "step": 2732
    },
    {
      "epoch": 3.644,
      "grad_norm": 0.029911167919635773,
      "learning_rate": 0.0014686452359534065,
      "loss": 0.9685,
      "step": 2733
    },
    {
      "epoch": 3.6453333333333333,
      "grad_norm": 0.028591884300112724,
      "learning_rate": 0.0014682637159219515,
      "loss": 0.9663,
      "step": 2734
    },
    {
      "epoch": 3.6466666666666665,
      "grad_norm": 0.027288606390357018,
      "learning_rate": 0.001467882108568381,
      "loss": 1.0689,
      "step": 2735
    },
    {
      "epoch": 3.648,
      "grad_norm": 0.03460914269089699,
      "learning_rate": 0.001467500413963857,
      "loss": 1.0882,
      "step": 2736
    },
    {
      "epoch": 3.6493333333333333,
      "grad_norm": 0.035340674221515656,
      "learning_rate": 0.001467118632179558,
      "loss": 1.2314,
      "step": 2737
    },
    {
      "epoch": 3.6506666666666665,
      "grad_norm": 0.033759068697690964,
      "learning_rate": 0.0014667367632866788,
      "loss": 1.0389,
      "step": 2738
    },
    {
      "epoch": 3.652,
      "grad_norm": 0.022568589076399803,
      "learning_rate": 0.0014663548073564315,
      "loss": 1.0153,
      "step": 2739
    },
    {
      "epoch": 3.6533333333333333,
      "grad_norm": 0.028117015957832336,
      "learning_rate": 0.0014659727644600422,
      "loss": 1.2227,
      "step": 2740
    },
    {
      "epoch": 3.6546666666666665,
      "grad_norm": 0.03794282674789429,
      "learning_rate": 0.0014655906346687554,
      "loss": 0.7537,
      "step": 2741
    },
    {
      "epoch": 3.656,
      "grad_norm": 0.03535578399896622,
      "learning_rate": 0.0014652084180538302,
      "loss": 0.9344,
      "step": 2742
    },
    {
      "epoch": 3.6573333333333333,
      "grad_norm": 0.024768060073256493,
      "learning_rate": 0.0014648261146865433,
      "loss": 0.7784,
      "step": 2743
    },
    {
      "epoch": 3.6586666666666665,
      "grad_norm": 0.05397210270166397,
      "learning_rate": 0.0014644437246381869,
      "loss": 1.0263,
      "step": 2744
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.03496697172522545,
      "learning_rate": 0.0014640612479800686,
      "loss": 1.2455,
      "step": 2745
    },
    {
      "epoch": 3.6613333333333333,
      "grad_norm": 0.0363704115152359,
      "learning_rate": 0.0014636786847835135,
      "loss": 1.1126,
      "step": 2746
    },
    {
      "epoch": 3.6626666666666665,
      "grad_norm": 0.0311104916036129,
      "learning_rate": 0.0014632960351198619,
      "loss": 0.9661,
      "step": 2747
    },
    {
      "epoch": 3.664,
      "grad_norm": 0.027475086972117424,
      "learning_rate": 0.0014629132990604705,
      "loss": 0.9572,
      "step": 2748
    },
    {
      "epoch": 3.6653333333333333,
      "grad_norm": 0.027844037860631943,
      "learning_rate": 0.001462530476676713,
      "loss": 0.7527,
      "step": 2749
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.0322151780128479,
      "learning_rate": 0.001462147568039977,
      "loss": 1.1066,
      "step": 2750
    },
    {
      "epoch": 3.668,
      "grad_norm": 0.030315663665533066,
      "learning_rate": 0.0014617645732216684,
      "loss": 0.995,
      "step": 2751
    },
    {
      "epoch": 3.6693333333333333,
      "grad_norm": 0.03172527253627777,
      "learning_rate": 0.0014613814922932085,
      "loss": 1.4355,
      "step": 2752
    },
    {
      "epoch": 3.6706666666666665,
      "grad_norm": 0.023575522005558014,
      "learning_rate": 0.001460998325326034,
      "loss": 0.9198,
      "step": 2753
    },
    {
      "epoch": 3.672,
      "grad_norm": 0.0442599281668663,
      "learning_rate": 0.0014606150723915983,
      "loss": 1.0324,
      "step": 2754
    },
    {
      "epoch": 3.6733333333333333,
      "grad_norm": 0.04192838445305824,
      "learning_rate": 0.0014602317335613707,
      "loss": 1.0943,
      "step": 2755
    },
    {
      "epoch": 3.6746666666666665,
      "grad_norm": 0.02252388745546341,
      "learning_rate": 0.0014598483089068362,
      "loss": 1.1833,
      "step": 2756
    },
    {
      "epoch": 3.676,
      "grad_norm": 0.02336704730987549,
      "learning_rate": 0.0014594647984994965,
      "loss": 1.0506,
      "step": 2757
    },
    {
      "epoch": 3.6773333333333333,
      "grad_norm": 0.029484987258911133,
      "learning_rate": 0.0014590812024108683,
      "loss": 1.1492,
      "step": 2758
    },
    {
      "epoch": 3.6786666666666665,
      "grad_norm": 0.028706058859825134,
      "learning_rate": 0.0014586975207124856,
      "loss": 1.0144,
      "step": 2759
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.03972335159778595,
      "learning_rate": 0.0014583137534758968,
      "loss": 0.9103,
      "step": 2760
    },
    {
      "epoch": 3.6813333333333333,
      "grad_norm": 0.03559676185250282,
      "learning_rate": 0.0014579299007726676,
      "loss": 1.2288,
      "step": 2761
    },
    {
      "epoch": 3.6826666666666665,
      "grad_norm": 0.0300539992749691,
      "learning_rate": 0.001457545962674379,
      "loss": 1.1297,
      "step": 2762
    },
    {
      "epoch": 3.684,
      "grad_norm": 0.026957480236887932,
      "learning_rate": 0.001457161939252628,
      "loss": 1.0131,
      "step": 2763
    },
    {
      "epoch": 3.6853333333333333,
      "grad_norm": 0.032016120851039886,
      "learning_rate": 0.001456777830579027,
      "loss": 0.8566,
      "step": 2764
    },
    {
      "epoch": 3.6866666666666665,
      "grad_norm": 0.03578690439462662,
      "learning_rate": 0.0014563936367252063,
      "loss": 1.0326,
      "step": 2765
    },
    {
      "epoch": 3.6879999999999997,
      "grad_norm": 0.03606768697500229,
      "learning_rate": 0.0014560093577628088,
      "loss": 1.2296,
      "step": 2766
    },
    {
      "epoch": 3.6893333333333334,
      "grad_norm": 0.02706056460738182,
      "learning_rate": 0.0014556249937634962,
      "loss": 0.8357,
      "step": 2767
    },
    {
      "epoch": 3.6906666666666665,
      "grad_norm": 0.04262732341885567,
      "learning_rate": 0.0014552405447989447,
      "loss": 1.297,
      "step": 2768
    },
    {
      "epoch": 3.692,
      "grad_norm": 0.032735079526901245,
      "learning_rate": 0.0014548560109408464,
      "loss": 1.2505,
      "step": 2769
    },
    {
      "epoch": 3.6933333333333334,
      "grad_norm": 0.03560561314225197,
      "learning_rate": 0.0014544713922609099,
      "loss": 0.7107,
      "step": 2770
    },
    {
      "epoch": 3.6946666666666665,
      "grad_norm": 0.024613749235868454,
      "learning_rate": 0.0014540866888308583,
      "loss": 1.1823,
      "step": 2771
    },
    {
      "epoch": 3.6959999999999997,
      "grad_norm": 0.02536056563258171,
      "learning_rate": 0.0014537019007224324,
      "loss": 1.0198,
      "step": 2772
    },
    {
      "epoch": 3.6973333333333334,
      "grad_norm": 0.030623432248830795,
      "learning_rate": 0.0014533170280073868,
      "loss": 1.0477,
      "step": 2773
    },
    {
      "epoch": 3.6986666666666665,
      "grad_norm": 0.02865542471408844,
      "learning_rate": 0.0014529320707574933,
      "loss": 0.8892,
      "step": 2774
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.02817879244685173,
      "learning_rate": 0.0014525470290445391,
      "loss": 1.1805,
      "step": 2775
    },
    {
      "epoch": 3.7013333333333334,
      "grad_norm": 0.03903454542160034,
      "learning_rate": 0.0014521619029403264,
      "loss": 0.9745,
      "step": 2776
    },
    {
      "epoch": 3.7026666666666666,
      "grad_norm": 0.02744726836681366,
      "learning_rate": 0.001451776692516674,
      "loss": 0.9936,
      "step": 2777
    },
    {
      "epoch": 3.7039999999999997,
      "grad_norm": 0.03121342882514,
      "learning_rate": 0.0014513913978454168,
      "loss": 1.0479,
      "step": 2778
    },
    {
      "epoch": 3.7053333333333334,
      "grad_norm": 0.03727066144347191,
      "learning_rate": 0.0014510060189984043,
      "loss": 1.0854,
      "step": 2779
    },
    {
      "epoch": 3.7066666666666666,
      "grad_norm": 0.029636893421411514,
      "learning_rate": 0.0014506205560475022,
      "loss": 1.01,
      "step": 2780
    },
    {
      "epoch": 3.708,
      "grad_norm": 0.030298417434096336,
      "learning_rate": 0.0014502350090645918,
      "loss": 1.0504,
      "step": 2781
    },
    {
      "epoch": 3.7093333333333334,
      "grad_norm": 0.026129554957151413,
      "learning_rate": 0.00144984937812157,
      "loss": 1.1572,
      "step": 2782
    },
    {
      "epoch": 3.7106666666666666,
      "grad_norm": 0.032476551830768585,
      "learning_rate": 0.00144946366329035,
      "loss": 1.299,
      "step": 2783
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 0.023621179163455963,
      "learning_rate": 0.00144907786464286,
      "loss": 0.9454,
      "step": 2784
    },
    {
      "epoch": 3.7133333333333334,
      "grad_norm": 0.0596742182970047,
      "learning_rate": 0.0014486919822510437,
      "loss": 1.1052,
      "step": 2785
    },
    {
      "epoch": 3.7146666666666666,
      "grad_norm": 0.030794749036431313,
      "learning_rate": 0.001448306016186861,
      "loss": 0.7735,
      "step": 2786
    },
    {
      "epoch": 3.716,
      "grad_norm": 0.02764621376991272,
      "learning_rate": 0.0014479199665222869,
      "loss": 1.0564,
      "step": 2787
    },
    {
      "epoch": 3.7173333333333334,
      "grad_norm": 0.02544453926384449,
      "learning_rate": 0.0014475338333293122,
      "loss": 1.0569,
      "step": 2788
    },
    {
      "epoch": 3.7186666666666666,
      "grad_norm": 0.03067994862794876,
      "learning_rate": 0.0014471476166799434,
      "loss": 1.1585,
      "step": 2789
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.03241416811943054,
      "learning_rate": 0.0014467613166462023,
      "loss": 0.8737,
      "step": 2790
    },
    {
      "epoch": 3.7213333333333334,
      "grad_norm": 0.031445086002349854,
      "learning_rate": 0.0014463749333001267,
      "loss": 1.057,
      "step": 2791
    },
    {
      "epoch": 3.7226666666666666,
      "grad_norm": 0.028751809149980545,
      "learning_rate": 0.0014459884667137688,
      "loss": 1.1326,
      "step": 2792
    },
    {
      "epoch": 3.724,
      "grad_norm": 0.029487036168575287,
      "learning_rate": 0.001445601916959198,
      "loss": 1.0569,
      "step": 2793
    },
    {
      "epoch": 3.7253333333333334,
      "grad_norm": 0.028473736718297005,
      "learning_rate": 0.0014452152841084977,
      "loss": 1.235,
      "step": 2794
    },
    {
      "epoch": 3.7266666666666666,
      "grad_norm": 0.02992996759712696,
      "learning_rate": 0.0014448285682337682,
      "loss": 0.8398,
      "step": 2795
    },
    {
      "epoch": 3.7279999999999998,
      "grad_norm": 0.03005838580429554,
      "learning_rate": 0.001444441769407124,
      "loss": 0.9326,
      "step": 2796
    },
    {
      "epoch": 3.7293333333333334,
      "grad_norm": 0.026246314868330956,
      "learning_rate": 0.0014440548877006955,
      "loss": 1.0444,
      "step": 2797
    },
    {
      "epoch": 3.7306666666666666,
      "grad_norm": 0.03311502933502197,
      "learning_rate": 0.0014436679231866287,
      "loss": 0.9507,
      "step": 2798
    },
    {
      "epoch": 3.732,
      "grad_norm": 0.03221499174833298,
      "learning_rate": 0.0014432808759370851,
      "loss": 1.1749,
      "step": 2799
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.05810149759054184,
      "learning_rate": 0.0014428937460242417,
      "loss": 0.9207,
      "step": 2800
    },
    {
      "epoch": 3.7346666666666666,
      "grad_norm": 0.030253862962126732,
      "learning_rate": 0.0014425065335202905,
      "loss": 1.1064,
      "step": 2801
    },
    {
      "epoch": 3.7359999999999998,
      "grad_norm": 0.024280721321702003,
      "learning_rate": 0.0014421192384974396,
      "loss": 0.9698,
      "step": 2802
    },
    {
      "epoch": 3.7373333333333334,
      "grad_norm": 0.03633169084787369,
      "learning_rate": 0.0014417318610279108,
      "loss": 1.0055,
      "step": 2803
    },
    {
      "epoch": 3.7386666666666666,
      "grad_norm": 0.0409846194088459,
      "learning_rate": 0.001441344401183944,
      "loss": 0.8952,
      "step": 2804
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.023469528183341026,
      "learning_rate": 0.0014409568590377918,
      "loss": 0.832,
      "step": 2805
    },
    {
      "epoch": 3.7413333333333334,
      "grad_norm": 0.0314524844288826,
      "learning_rate": 0.001440569234661724,
      "loss": 1.1004,
      "step": 2806
    },
    {
      "epoch": 3.7426666666666666,
      "grad_norm": 0.03856069594621658,
      "learning_rate": 0.0014401815281280248,
      "loss": 0.9002,
      "step": 2807
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 0.037458356469869614,
      "learning_rate": 0.001439793739508994,
      "loss": 1.1192,
      "step": 2808
    },
    {
      "epoch": 3.7453333333333334,
      "grad_norm": 0.037933576852083206,
      "learning_rate": 0.0014394058688769462,
      "loss": 1.0488,
      "step": 2809
    },
    {
      "epoch": 3.7466666666666666,
      "grad_norm": 0.03077608346939087,
      "learning_rate": 0.0014390179163042127,
      "loss": 1.0777,
      "step": 2810
    },
    {
      "epoch": 3.748,
      "grad_norm": 0.02886286936700344,
      "learning_rate": 0.0014386298818631388,
      "loss": 1.052,
      "step": 2811
    },
    {
      "epoch": 3.7493333333333334,
      "grad_norm": 0.024394448846578598,
      "learning_rate": 0.001438241765626085,
      "loss": 1.0096,
      "step": 2812
    },
    {
      "epoch": 3.7506666666666666,
      "grad_norm": 0.027189994230866432,
      "learning_rate": 0.0014378535676654275,
      "loss": 1.4139,
      "step": 2813
    },
    {
      "epoch": 3.752,
      "grad_norm": 0.023523354902863503,
      "learning_rate": 0.001437465288053558,
      "loss": 1.1173,
      "step": 2814
    },
    {
      "epoch": 3.7533333333333334,
      "grad_norm": 0.025446990504860878,
      "learning_rate": 0.0014370769268628831,
      "loss": 1.1305,
      "step": 2815
    },
    {
      "epoch": 3.7546666666666666,
      "grad_norm": 0.032425396144390106,
      "learning_rate": 0.0014366884841658246,
      "loss": 1.241,
      "step": 2816
    },
    {
      "epoch": 3.7560000000000002,
      "grad_norm": 0.03546149283647537,
      "learning_rate": 0.0014362999600348197,
      "loss": 0.9426,
      "step": 2817
    },
    {
      "epoch": 3.7573333333333334,
      "grad_norm": 0.04358874633908272,
      "learning_rate": 0.0014359113545423204,
      "loss": 0.9504,
      "step": 2818
    },
    {
      "epoch": 3.7586666666666666,
      "grad_norm": 0.04669652879238129,
      "learning_rate": 0.001435522667760794,
      "loss": 1.2638,
      "step": 2819
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.022312361747026443,
      "learning_rate": 0.0014351338997627232,
      "loss": 0.8551,
      "step": 2820
    },
    {
      "epoch": 3.7613333333333334,
      "grad_norm": 0.03288346901535988,
      "learning_rate": 0.0014347450506206059,
      "loss": 0.897,
      "step": 2821
    },
    {
      "epoch": 3.7626666666666666,
      "grad_norm": 0.024103552103042603,
      "learning_rate": 0.001434356120406955,
      "loss": 0.9446,
      "step": 2822
    },
    {
      "epoch": 3.7640000000000002,
      "grad_norm": 0.02598053403198719,
      "learning_rate": 0.0014339671091942979,
      "loss": 1.0305,
      "step": 2823
    },
    {
      "epoch": 3.7653333333333334,
      "grad_norm": 0.02736038528382778,
      "learning_rate": 0.0014335780170551778,
      "loss": 1.2107,
      "step": 2824
    },
    {
      "epoch": 3.7666666666666666,
      "grad_norm": 0.02991488389670849,
      "learning_rate": 0.0014331888440621532,
      "loss": 1.0434,
      "step": 2825
    },
    {
      "epoch": 3.768,
      "grad_norm": 0.02444799616932869,
      "learning_rate": 0.001432799590287797,
      "loss": 1.2836,
      "step": 2826
    },
    {
      "epoch": 3.7693333333333334,
      "grad_norm": 0.029061418026685715,
      "learning_rate": 0.0014324102558046978,
      "loss": 0.9898,
      "step": 2827
    },
    {
      "epoch": 3.7706666666666666,
      "grad_norm": 0.022267503663897514,
      "learning_rate": 0.001432020840685459,
      "loss": 0.8792,
      "step": 2828
    },
    {
      "epoch": 3.7720000000000002,
      "grad_norm": 0.02409345656633377,
      "learning_rate": 0.0014316313450026986,
      "loss": 1.0105,
      "step": 2829
    },
    {
      "epoch": 3.7733333333333334,
      "grad_norm": 0.042972415685653687,
      "learning_rate": 0.00143124176882905,
      "loss": 0.9696,
      "step": 2830
    },
    {
      "epoch": 3.7746666666666666,
      "grad_norm": 0.025333549827337265,
      "learning_rate": 0.0014308521122371617,
      "loss": 0.9509,
      "step": 2831
    },
    {
      "epoch": 3.776,
      "grad_norm": 0.023711014539003372,
      "learning_rate": 0.0014304623752996973,
      "loss": 0.9569,
      "step": 2832
    },
    {
      "epoch": 3.7773333333333334,
      "grad_norm": 0.02375168539583683,
      "learning_rate": 0.0014300725580893353,
      "loss": 1.0983,
      "step": 2833
    },
    {
      "epoch": 3.7786666666666666,
      "grad_norm": 0.026765450835227966,
      "learning_rate": 0.0014296826606787687,
      "loss": 1.0483,
      "step": 2834
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 0.025843197479844093,
      "learning_rate": 0.001429292683140706,
      "loss": 0.9317,
      "step": 2835
    },
    {
      "epoch": 3.7813333333333334,
      "grad_norm": 0.028770873323082924,
      "learning_rate": 0.0014289026255478704,
      "loss": 1.0646,
      "step": 2836
    },
    {
      "epoch": 3.7826666666666666,
      "grad_norm": 0.02200883813202381,
      "learning_rate": 0.0014285124879729997,
      "loss": 1.0505,
      "step": 2837
    },
    {
      "epoch": 3.784,
      "grad_norm": 0.028123708441853523,
      "learning_rate": 0.0014281222704888479,
      "loss": 1.1583,
      "step": 2838
    },
    {
      "epoch": 3.7853333333333334,
      "grad_norm": 0.029545539990067482,
      "learning_rate": 0.0014277319731681823,
      "loss": 1.1889,
      "step": 2839
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 0.02637980692088604,
      "learning_rate": 0.0014273415960837864,
      "loss": 1.0525,
      "step": 2840
    },
    {
      "epoch": 3.7880000000000003,
      "grad_norm": 0.03301851823925972,
      "learning_rate": 0.001426951139308457,
      "loss": 0.8733,
      "step": 2841
    },
    {
      "epoch": 3.7893333333333334,
      "grad_norm": 0.021629780530929565,
      "learning_rate": 0.0014265606029150075,
      "loss": 1.0962,
      "step": 2842
    },
    {
      "epoch": 3.7906666666666666,
      "grad_norm": 0.028054628521203995,
      "learning_rate": 0.0014261699869762654,
      "loss": 1.0739,
      "step": 2843
    },
    {
      "epoch": 3.792,
      "grad_norm": 0.03427442908287048,
      "learning_rate": 0.0014257792915650727,
      "loss": 1.0667,
      "step": 2844
    },
    {
      "epoch": 3.7933333333333334,
      "grad_norm": 0.03520698472857475,
      "learning_rate": 0.0014253885167542866,
      "loss": 1.0192,
      "step": 2845
    },
    {
      "epoch": 3.7946666666666666,
      "grad_norm": 0.03690023347735405,
      "learning_rate": 0.001424997662616779,
      "loss": 1.3332,
      "step": 2846
    },
    {
      "epoch": 3.7960000000000003,
      "grad_norm": 0.02663618139922619,
      "learning_rate": 0.0014246067292254365,
      "loss": 0.9308,
      "step": 2847
    },
    {
      "epoch": 3.7973333333333334,
      "grad_norm": 0.019507860764861107,
      "learning_rate": 0.001424215716653161,
      "loss": 0.9994,
      "step": 2848
    },
    {
      "epoch": 3.7986666666666666,
      "grad_norm": 0.025148067623376846,
      "learning_rate": 0.0014238246249728686,
      "loss": 1.0536,
      "step": 2849
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.02437569759786129,
      "learning_rate": 0.0014234334542574906,
      "loss": 1.3997,
      "step": 2850
    },
    {
      "epoch": 3.8013333333333335,
      "grad_norm": 0.02610943838953972,
      "learning_rate": 0.001423042204579972,
      "loss": 0.8646,
      "step": 2851
    },
    {
      "epoch": 3.8026666666666666,
      "grad_norm": 0.024848733097314835,
      "learning_rate": 0.0014226508760132736,
      "loss": 0.8758,
      "step": 2852
    },
    {
      "epoch": 3.8040000000000003,
      "grad_norm": 0.02927078865468502,
      "learning_rate": 0.0014222594686303708,
      "loss": 1.2037,
      "step": 2853
    },
    {
      "epoch": 3.8053333333333335,
      "grad_norm": 0.032110195606946945,
      "learning_rate": 0.0014218679825042538,
      "loss": 0.8702,
      "step": 2854
    },
    {
      "epoch": 3.8066666666666666,
      "grad_norm": 0.026564529165625572,
      "learning_rate": 0.0014214764177079265,
      "loss": 1.1801,
      "step": 2855
    },
    {
      "epoch": 3.808,
      "grad_norm": 0.02554307132959366,
      "learning_rate": 0.0014210847743144086,
      "loss": 1.0633,
      "step": 2856
    },
    {
      "epoch": 3.8093333333333335,
      "grad_norm": 0.030500132590532303,
      "learning_rate": 0.0014206930523967337,
      "loss": 0.8669,
      "step": 2857
    },
    {
      "epoch": 3.8106666666666666,
      "grad_norm": 0.03376735374331474,
      "learning_rate": 0.0014203012520279507,
      "loss": 1.1251,
      "step": 2858
    },
    {
      "epoch": 3.8120000000000003,
      "grad_norm": 0.024058109149336815,
      "learning_rate": 0.0014199093732811226,
      "loss": 0.7422,
      "step": 2859
    },
    {
      "epoch": 3.8133333333333335,
      "grad_norm": 0.04014690965414047,
      "learning_rate": 0.0014195174162293272,
      "loss": 1.1854,
      "step": 2860
    },
    {
      "epoch": 3.8146666666666667,
      "grad_norm": 0.028046414256095886,
      "learning_rate": 0.001419125380945657,
      "loss": 1.0043,
      "step": 2861
    },
    {
      "epoch": 3.816,
      "grad_norm": 0.024966608732938766,
      "learning_rate": 0.0014187332675032187,
      "loss": 0.8948,
      "step": 2862
    },
    {
      "epoch": 3.8173333333333335,
      "grad_norm": 0.030467189848423004,
      "learning_rate": 0.0014183410759751342,
      "loss": 1.1929,
      "step": 2863
    },
    {
      "epoch": 3.8186666666666667,
      "grad_norm": 0.030300142243504524,
      "learning_rate": 0.0014179488064345396,
      "loss": 0.7839,
      "step": 2864
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.04391980543732643,
      "learning_rate": 0.0014175564589545852,
      "loss": 1.0627,
      "step": 2865
    },
    {
      "epoch": 3.8213333333333335,
      "grad_norm": 0.026474125683307648,
      "learning_rate": 0.0014171640336084368,
      "loss": 1.1689,
      "step": 2866
    },
    {
      "epoch": 3.8226666666666667,
      "grad_norm": 0.057547785341739655,
      "learning_rate": 0.0014167715304692736,
      "loss": 1.099,
      "step": 2867
    },
    {
      "epoch": 3.824,
      "grad_norm": 0.035072825849056244,
      "learning_rate": 0.0014163789496102902,
      "loss": 0.9904,
      "step": 2868
    },
    {
      "epoch": 3.8253333333333335,
      "grad_norm": 0.028786519542336464,
      "learning_rate": 0.0014159862911046952,
      "loss": 1.0771,
      "step": 2869
    },
    {
      "epoch": 3.8266666666666667,
      "grad_norm": 0.027482429519295692,
      "learning_rate": 0.0014155935550257115,
      "loss": 1.1863,
      "step": 2870
    },
    {
      "epoch": 3.828,
      "grad_norm": 0.031510017812252045,
      "learning_rate": 0.001415200741446577,
      "loss": 1.2065,
      "step": 2871
    },
    {
      "epoch": 3.8293333333333335,
      "grad_norm": 0.04297809302806854,
      "learning_rate": 0.0014148078504405443,
      "loss": 1.0933,
      "step": 2872
    },
    {
      "epoch": 3.8306666666666667,
      "grad_norm": 0.026532214134931564,
      "learning_rate": 0.0014144148820808794,
      "loss": 1.1933,
      "step": 2873
    },
    {
      "epoch": 3.832,
      "grad_norm": 0.03664121404290199,
      "learning_rate": 0.0014140218364408633,
      "loss": 1.0669,
      "step": 2874
    },
    {
      "epoch": 3.8333333333333335,
      "grad_norm": 0.035500943660736084,
      "learning_rate": 0.0014136287135937916,
      "loss": 0.9208,
      "step": 2875
    },
    {
      "epoch": 3.8346666666666667,
      "grad_norm": 0.025860460475087166,
      "learning_rate": 0.0014132355136129736,
      "loss": 0.9283,
      "step": 2876
    },
    {
      "epoch": 3.836,
      "grad_norm": 0.03688691928982735,
      "learning_rate": 0.0014128422365717345,
      "loss": 1.4446,
      "step": 2877
    },
    {
      "epoch": 3.8373333333333335,
      "grad_norm": 0.02830248326063156,
      "learning_rate": 0.001412448882543412,
      "loss": 1.0864,
      "step": 2878
    },
    {
      "epoch": 3.8386666666666667,
      "grad_norm": 0.038166459649801254,
      "learning_rate": 0.0014120554516013596,
      "loss": 1.3745,
      "step": 2879
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.029097585007548332,
      "learning_rate": 0.001411661943818944,
      "loss": 0.8716,
      "step": 2880
    },
    {
      "epoch": 3.8413333333333335,
      "grad_norm": 0.040628302842378616,
      "learning_rate": 0.001411268359269547,
      "loss": 1.1989,
      "step": 2881
    },
    {
      "epoch": 3.8426666666666667,
      "grad_norm": 0.030701041221618652,
      "learning_rate": 0.0014108746980265644,
      "loss": 1.179,
      "step": 2882
    },
    {
      "epoch": 3.844,
      "grad_norm": 0.04437253251671791,
      "learning_rate": 0.0014104809601634067,
      "loss": 0.9472,
      "step": 2883
    },
    {
      "epoch": 3.8453333333333335,
      "grad_norm": 0.03040781244635582,
      "learning_rate": 0.001410087145753498,
      "loss": 0.9455,
      "step": 2884
    },
    {
      "epoch": 3.8466666666666667,
      "grad_norm": 0.028550032526254654,
      "learning_rate": 0.0014096932548702777,
      "loss": 0.9408,
      "step": 2885
    },
    {
      "epoch": 3.848,
      "grad_norm": 0.03133310005068779,
      "learning_rate": 0.001409299287587198,
      "loss": 1.244,
      "step": 2886
    },
    {
      "epoch": 3.8493333333333335,
      "grad_norm": 0.027492988854646683,
      "learning_rate": 0.0014089052439777263,
      "loss": 0.9545,
      "step": 2887
    },
    {
      "epoch": 3.8506666666666667,
      "grad_norm": 0.03247205540537834,
      "learning_rate": 0.0014085111241153448,
      "loss": 1.4562,
      "step": 2888
    },
    {
      "epoch": 3.852,
      "grad_norm": 0.025768816471099854,
      "learning_rate": 0.0014081169280735486,
      "loss": 1.0693,
      "step": 2889
    },
    {
      "epoch": 3.8533333333333335,
      "grad_norm": 0.03161565214395523,
      "learning_rate": 0.001407722655925848,
      "loss": 1.4604,
      "step": 2890
    },
    {
      "epoch": 3.8546666666666667,
      "grad_norm": 0.028620261698961258,
      "learning_rate": 0.0014073283077457666,
      "loss": 1.0197,
      "step": 2891
    },
    {
      "epoch": 3.856,
      "grad_norm": 0.025284118950366974,
      "learning_rate": 0.0014069338836068433,
      "loss": 1.0717,
      "step": 2892
    },
    {
      "epoch": 3.857333333333333,
      "grad_norm": 0.022776255384087563,
      "learning_rate": 0.0014065393835826301,
      "loss": 1.159,
      "step": 2893
    },
    {
      "epoch": 3.8586666666666667,
      "grad_norm": 0.049686819314956665,
      "learning_rate": 0.0014061448077466938,
      "loss": 1.1188,
      "step": 2894
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.02407943643629551,
      "learning_rate": 0.0014057501561726155,
      "loss": 1.0282,
      "step": 2895
    },
    {
      "epoch": 3.8613333333333335,
      "grad_norm": 0.025699738413095474,
      "learning_rate": 0.0014053554289339893,
      "loss": 1.0328,
      "step": 2896
    },
    {
      "epoch": 3.8626666666666667,
      "grad_norm": 0.024676788598299026,
      "learning_rate": 0.0014049606261044248,
      "loss": 0.9082,
      "step": 2897
    },
    {
      "epoch": 3.864,
      "grad_norm": 0.022166617214679718,
      "learning_rate": 0.0014045657477575449,
      "loss": 0.8432,
      "step": 2898
    },
    {
      "epoch": 3.865333333333333,
      "grad_norm": 0.0265134796500206,
      "learning_rate": 0.0014041707939669867,
      "loss": 1.0998,
      "step": 2899
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.0228999275714159,
      "learning_rate": 0.0014037757648064017,
      "loss": 0.9647,
      "step": 2900
    },
    {
      "epoch": 3.868,
      "grad_norm": 0.027951689437031746,
      "learning_rate": 0.001403380660349455,
      "loss": 0.9462,
      "step": 2901
    },
    {
      "epoch": 3.8693333333333335,
      "grad_norm": 0.03290582820773125,
      "learning_rate": 0.0014029854806698256,
      "loss": 0.8403,
      "step": 2902
    },
    {
      "epoch": 3.8706666666666667,
      "grad_norm": 0.02382774092257023,
      "learning_rate": 0.0014025902258412075,
      "loss": 1.1293,
      "step": 2903
    },
    {
      "epoch": 3.872,
      "grad_norm": 0.025104332715272903,
      "learning_rate": 0.0014021948959373075,
      "loss": 1.0567,
      "step": 2904
    },
    {
      "epoch": 3.873333333333333,
      "grad_norm": 0.027715494856238365,
      "learning_rate": 0.0014017994910318474,
      "loss": 1.0719,
      "step": 2905
    },
    {
      "epoch": 3.8746666666666667,
      "grad_norm": 0.030464883893728256,
      "learning_rate": 0.0014014040111985627,
      "loss": 1.2782,
      "step": 2906
    },
    {
      "epoch": 3.876,
      "grad_norm": 0.03135085478425026,
      "learning_rate": 0.0014010084565112017,
      "loss": 0.8926,
      "step": 2907
    },
    {
      "epoch": 3.8773333333333335,
      "grad_norm": 0.026128599420189857,
      "learning_rate": 0.001400612827043529,
      "loss": 0.8622,
      "step": 2908
    },
    {
      "epoch": 3.8786666666666667,
      "grad_norm": 0.028053782880306244,
      "learning_rate": 0.0014002171228693209,
      "loss": 0.9992,
      "step": 2909
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.03819109871983528,
      "learning_rate": 0.001399821344062369,
      "loss": 0.9888,
      "step": 2910
    },
    {
      "epoch": 3.881333333333333,
      "grad_norm": 0.028637899085879326,
      "learning_rate": 0.0013994254906964786,
      "loss": 0.9431,
      "step": 2911
    },
    {
      "epoch": 3.8826666666666667,
      "grad_norm": 0.03652581200003624,
      "learning_rate": 0.0013990295628454681,
      "loss": 1.2598,
      "step": 2912
    },
    {
      "epoch": 3.884,
      "grad_norm": 0.027209248393774033,
      "learning_rate": 0.0013986335605831706,
      "loss": 0.932,
      "step": 2913
    },
    {
      "epoch": 3.8853333333333335,
      "grad_norm": 0.030985858291387558,
      "learning_rate": 0.0013982374839834328,
      "loss": 0.9429,
      "step": 2914
    },
    {
      "epoch": 3.8866666666666667,
      "grad_norm": 0.03402597829699516,
      "learning_rate": 0.0013978413331201156,
      "loss": 1.1373,
      "step": 2915
    },
    {
      "epoch": 3.888,
      "grad_norm": 0.027147766202688217,
      "learning_rate": 0.0013974451080670934,
      "loss": 1.0391,
      "step": 2916
    },
    {
      "epoch": 3.889333333333333,
      "grad_norm": 0.03365808352828026,
      "learning_rate": 0.0013970488088982545,
      "loss": 1.0616,
      "step": 2917
    },
    {
      "epoch": 3.8906666666666667,
      "grad_norm": 0.0384969525039196,
      "learning_rate": 0.0013966524356875006,
      "loss": 0.8975,
      "step": 2918
    },
    {
      "epoch": 3.892,
      "grad_norm": 0.02283892035484314,
      "learning_rate": 0.001396255988508748,
      "loss": 1.1242,
      "step": 2919
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 0.02147103101015091,
      "learning_rate": 0.0013958594674359265,
      "loss": 1.0521,
      "step": 2920
    },
    {
      "epoch": 3.8946666666666667,
      "grad_norm": 0.041412387043237686,
      "learning_rate": 0.0013954628725429794,
      "loss": 0.8445,
      "step": 2921
    },
    {
      "epoch": 3.896,
      "grad_norm": 0.023071283474564552,
      "learning_rate": 0.0013950662039038642,
      "loss": 1.0346,
      "step": 2922
    },
    {
      "epoch": 3.897333333333333,
      "grad_norm": 0.023917181417346,
      "learning_rate": 0.0013946694615925516,
      "loss": 1.1281,
      "step": 2923
    },
    {
      "epoch": 3.8986666666666667,
      "grad_norm": 0.026719313114881516,
      "learning_rate": 0.0013942726456830264,
      "loss": 1.0629,
      "step": 2924
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.033112697303295135,
      "learning_rate": 0.0013938757562492873,
      "loss": 1.0145,
      "step": 2925
    },
    {
      "epoch": 3.9013333333333335,
      "grad_norm": 0.033325403928756714,
      "learning_rate": 0.0013934787933653464,
      "loss": 1.1408,
      "step": 2926
    },
    {
      "epoch": 3.9026666666666667,
      "grad_norm": 0.1430092751979828,
      "learning_rate": 0.0013930817571052296,
      "loss": 1.4047,
      "step": 2927
    },
    {
      "epoch": 3.904,
      "grad_norm": 0.027354784309864044,
      "learning_rate": 0.0013926846475429766,
      "loss": 1.2008,
      "step": 2928
    },
    {
      "epoch": 3.905333333333333,
      "grad_norm": 0.025292841717600822,
      "learning_rate": 0.0013922874647526403,
      "loss": 0.8397,
      "step": 2929
    },
    {
      "epoch": 3.9066666666666667,
      "grad_norm": 0.02794763632118702,
      "learning_rate": 0.0013918902088082877,
      "loss": 0.747,
      "step": 2930
    },
    {
      "epoch": 3.908,
      "grad_norm": 0.03276193141937256,
      "learning_rate": 0.0013914928797839994,
      "loss": 0.9281,
      "step": 2931
    },
    {
      "epoch": 3.9093333333333335,
      "grad_norm": 0.03671855852007866,
      "learning_rate": 0.00139109547775387,
      "loss": 0.9066,
      "step": 2932
    },
    {
      "epoch": 3.9106666666666667,
      "grad_norm": 0.05099052935838699,
      "learning_rate": 0.001390698002792007,
      "loss": 1.0493,
      "step": 2933
    },
    {
      "epoch": 3.912,
      "grad_norm": 0.03391527384519577,
      "learning_rate": 0.0013903004549725311,
      "loss": 0.948,
      "step": 2934
    },
    {
      "epoch": 3.913333333333333,
      "grad_norm": 0.028109708800911903,
      "learning_rate": 0.0013899028343695781,
      "loss": 0.9947,
      "step": 2935
    },
    {
      "epoch": 3.9146666666666667,
      "grad_norm": 0.055167149752378464,
      "learning_rate": 0.0013895051410572963,
      "loss": 1.1125,
      "step": 2936
    },
    {
      "epoch": 3.916,
      "grad_norm": 0.025041639804840088,
      "learning_rate": 0.001389107375109848,
      "loss": 1.0607,
      "step": 2937
    },
    {
      "epoch": 3.9173333333333336,
      "grad_norm": 0.06017831712961197,
      "learning_rate": 0.0013887095366014086,
      "loss": 1.2362,
      "step": 2938
    },
    {
      "epoch": 3.9186666666666667,
      "grad_norm": 0.03311099112033844,
      "learning_rate": 0.0013883116256061673,
      "loss": 0.8488,
      "step": 2939
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.03140242397785187,
      "learning_rate": 0.0013879136421983266,
      "loss": 1.0114,
      "step": 2940
    },
    {
      "epoch": 3.921333333333333,
      "grad_norm": 0.028162464499473572,
      "learning_rate": 0.001387515586452103,
      "loss": 0.8539,
      "step": 2941
    },
    {
      "epoch": 3.9226666666666667,
      "grad_norm": 0.042098518460989,
      "learning_rate": 0.0013871174584417262,
      "loss": 1.2547,
      "step": 2942
    },
    {
      "epoch": 3.924,
      "grad_norm": 0.024709172546863556,
      "learning_rate": 0.001386719258241439,
      "loss": 0.7811,
      "step": 2943
    },
    {
      "epoch": 3.9253333333333336,
      "grad_norm": 0.020634863525629044,
      "learning_rate": 0.001386320985925499,
      "loss": 0.8525,
      "step": 2944
    },
    {
      "epoch": 3.9266666666666667,
      "grad_norm": 0.06294264644384384,
      "learning_rate": 0.001385922641568175,
      "loss": 0.9621,
      "step": 2945
    },
    {
      "epoch": 3.928,
      "grad_norm": 0.0238523930311203,
      "learning_rate": 0.001385524225243751,
      "loss": 0.8795,
      "step": 2946
    },
    {
      "epoch": 3.929333333333333,
      "grad_norm": 0.024240465834736824,
      "learning_rate": 0.0013851257370265243,
      "loss": 1.1705,
      "step": 2947
    },
    {
      "epoch": 3.9306666666666668,
      "grad_norm": 0.03205505385994911,
      "learning_rate": 0.0013847271769908047,
      "loss": 1.2249,
      "step": 2948
    },
    {
      "epoch": 3.932,
      "grad_norm": 0.032892484217882156,
      "learning_rate": 0.0013843285452109166,
      "loss": 1.0197,
      "step": 2949
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 0.022129680961370468,
      "learning_rate": 0.0013839298417611962,
      "loss": 1.1165,
      "step": 2950
    },
    {
      "epoch": 3.9346666666666668,
      "grad_norm": 0.027857843786478043,
      "learning_rate": 0.0013835310667159946,
      "loss": 1.1322,
      "step": 2951
    },
    {
      "epoch": 3.936,
      "grad_norm": 0.030448824167251587,
      "learning_rate": 0.0013831322201496756,
      "loss": 0.9241,
      "step": 2952
    },
    {
      "epoch": 3.937333333333333,
      "grad_norm": 0.028929920867085457,
      "learning_rate": 0.0013827333021366164,
      "loss": 1.2905,
      "step": 2953
    },
    {
      "epoch": 3.9386666666666668,
      "grad_norm": 0.03353249654173851,
      "learning_rate": 0.001382334312751207,
      "loss": 1.0281,
      "step": 2954
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.02495531179010868,
      "learning_rate": 0.0013819352520678518,
      "loss": 0.9456,
      "step": 2955
    },
    {
      "epoch": 3.9413333333333336,
      "grad_norm": 0.02873847633600235,
      "learning_rate": 0.0013815361201609676,
      "loss": 0.9097,
      "step": 2956
    },
    {
      "epoch": 3.9426666666666668,
      "grad_norm": 0.02554921805858612,
      "learning_rate": 0.0013811369171049847,
      "loss": 1.0781,
      "step": 2957
    },
    {
      "epoch": 3.944,
      "grad_norm": 0.03430157154798508,
      "learning_rate": 0.0013807376429743467,
      "loss": 1.0109,
      "step": 2958
    },
    {
      "epoch": 3.945333333333333,
      "grad_norm": 0.03298141807317734,
      "learning_rate": 0.0013803382978435105,
      "loss": 1.0231,
      "step": 2959
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 0.027236148715019226,
      "learning_rate": 0.0013799388817869467,
      "loss": 1.0243,
      "step": 2960
    },
    {
      "epoch": 3.948,
      "grad_norm": 0.021209578961133957,
      "learning_rate": 0.0013795393948791382,
      "loss": 0.9242,
      "step": 2961
    },
    {
      "epoch": 3.9493333333333336,
      "grad_norm": 0.024207210168242455,
      "learning_rate": 0.0013791398371945816,
      "loss": 1.0676,
      "step": 2962
    },
    {
      "epoch": 3.9506666666666668,
      "grad_norm": 0.02760438807308674,
      "learning_rate": 0.001378740208807787,
      "loss": 0.8909,
      "step": 2963
    },
    {
      "epoch": 3.952,
      "grad_norm": 0.03187151253223419,
      "learning_rate": 0.001378340509793277,
      "loss": 1.0745,
      "step": 2964
    },
    {
      "epoch": 3.953333333333333,
      "grad_norm": 0.023799825459718704,
      "learning_rate": 0.0013779407402255879,
      "loss": 1.3522,
      "step": 2965
    },
    {
      "epoch": 3.9546666666666668,
      "grad_norm": 0.030248083174228668,
      "learning_rate": 0.0013775409001792686,
      "loss": 0.9785,
      "step": 2966
    },
    {
      "epoch": 3.956,
      "grad_norm": 0.03491240739822388,
      "learning_rate": 0.0013771409897288822,
      "loss": 1.209,
      "step": 2967
    },
    {
      "epoch": 3.9573333333333336,
      "grad_norm": 0.02713368646800518,
      "learning_rate": 0.0013767410089490038,
      "loss": 0.7809,
      "step": 2968
    },
    {
      "epoch": 3.958666666666667,
      "grad_norm": 0.028289085254073143,
      "learning_rate": 0.001376340957914222,
      "loss": 0.903,
      "step": 2969
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.028389455750584602,
      "learning_rate": 0.001375940836699139,
      "loss": 0.9631,
      "step": 2970
    },
    {
      "epoch": 3.961333333333333,
      "grad_norm": 0.027344994246959686,
      "learning_rate": 0.0013755406453783696,
      "loss": 1.1942,
      "step": 2971
    },
    {
      "epoch": 3.962666666666667,
      "grad_norm": 0.032972611486911774,
      "learning_rate": 0.0013751403840265412,
      "loss": 1.0319,
      "step": 2972
    },
    {
      "epoch": 3.964,
      "grad_norm": 0.02747858129441738,
      "learning_rate": 0.0013747400527182952,
      "loss": 1.18,
      "step": 2973
    },
    {
      "epoch": 3.9653333333333336,
      "grad_norm": 0.0294891856610775,
      "learning_rate": 0.0013743396515282856,
      "loss": 0.9887,
      "step": 2974
    },
    {
      "epoch": 3.966666666666667,
      "grad_norm": 0.027269339188933372,
      "learning_rate": 0.0013739391805311794,
      "loss": 1.119,
      "step": 2975
    },
    {
      "epoch": 3.968,
      "grad_norm": 0.03175204247236252,
      "learning_rate": 0.0013735386398016569,
      "loss": 0.9366,
      "step": 2976
    },
    {
      "epoch": 3.969333333333333,
      "grad_norm": 0.026894399896264076,
      "learning_rate": 0.0013731380294144109,
      "loss": 1.1364,
      "step": 2977
    },
    {
      "epoch": 3.970666666666667,
      "grad_norm": 0.0344371572136879,
      "learning_rate": 0.0013727373494441476,
      "loss": 1.1807,
      "step": 2978
    },
    {
      "epoch": 3.972,
      "grad_norm": 0.02973778359591961,
      "learning_rate": 0.001372336599965586,
      "loss": 0.947,
      "step": 2979
    },
    {
      "epoch": 3.9733333333333336,
      "grad_norm": 0.0271048191934824,
      "learning_rate": 0.0013719357810534581,
      "loss": 0.997,
      "step": 2980
    },
    {
      "epoch": 3.974666666666667,
      "grad_norm": 0.03277235105633736,
      "learning_rate": 0.0013715348927825092,
      "loss": 1.32,
      "step": 2981
    },
    {
      "epoch": 3.976,
      "grad_norm": 0.025291111320257187,
      "learning_rate": 0.0013711339352274966,
      "loss": 1.0587,
      "step": 2982
    },
    {
      "epoch": 3.977333333333333,
      "grad_norm": 0.030764348804950714,
      "learning_rate": 0.0013707329084631915,
      "loss": 1.3051,
      "step": 2983
    },
    {
      "epoch": 3.978666666666667,
      "grad_norm": 0.032761577516794205,
      "learning_rate": 0.0013703318125643782,
      "loss": 0.8557,
      "step": 2984
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.02999434806406498,
      "learning_rate": 0.001369930647605852,
      "loss": 0.8414,
      "step": 2985
    },
    {
      "epoch": 3.981333333333333,
      "grad_norm": 0.03189219534397125,
      "learning_rate": 0.0013695294136624233,
      "loss": 1.2683,
      "step": 2986
    },
    {
      "epoch": 3.982666666666667,
      "grad_norm": 0.022111231461167336,
      "learning_rate": 0.0013691281108089144,
      "loss": 0.9076,
      "step": 2987
    },
    {
      "epoch": 3.984,
      "grad_norm": 0.026869749650359154,
      "learning_rate": 0.0013687267391201603,
      "loss": 1.009,
      "step": 2988
    },
    {
      "epoch": 3.985333333333333,
      "grad_norm": 0.037369295954704285,
      "learning_rate": 0.0013683252986710088,
      "loss": 1.0097,
      "step": 2989
    },
    {
      "epoch": 3.986666666666667,
      "grad_norm": 0.02933996357023716,
      "learning_rate": 0.0013679237895363216,
      "loss": 1.0743,
      "step": 2990
    },
    {
      "epoch": 3.988,
      "grad_norm": 0.028754670172929764,
      "learning_rate": 0.0013675222117909716,
      "loss": 1.1059,
      "step": 2991
    },
    {
      "epoch": 3.989333333333333,
      "grad_norm": 0.03144386038184166,
      "learning_rate": 0.0013671205655098454,
      "loss": 1.059,
      "step": 2992
    },
    {
      "epoch": 3.990666666666667,
      "grad_norm": 0.039157796651124954,
      "learning_rate": 0.001366718850767843,
      "loss": 1.0647,
      "step": 2993
    },
    {
      "epoch": 3.992,
      "grad_norm": 0.025938861072063446,
      "learning_rate": 0.001366317067639875,
      "loss": 1.0592,
      "step": 2994
    },
    {
      "epoch": 3.993333333333333,
      "grad_norm": 0.04146430268883705,
      "learning_rate": 0.0013659152162008676,
      "loss": 1.0874,
      "step": 2995
    },
    {
      "epoch": 3.994666666666667,
      "grad_norm": 0.03025633655488491,
      "learning_rate": 0.0013655132965257576,
      "loss": 1.0152,
      "step": 2996
    },
    {
      "epoch": 3.996,
      "grad_norm": 0.028425440192222595,
      "learning_rate": 0.001365111308689495,
      "loss": 1.1581,
      "step": 2997
    },
    {
      "epoch": 3.997333333333333,
      "grad_norm": 0.02611188031733036,
      "learning_rate": 0.0013647092527670438,
      "loss": 0.9872,
      "step": 2998
    },
    {
      "epoch": 3.998666666666667,
      "grad_norm": 0.02868477627635002,
      "learning_rate": 0.0013643071288333781,
      "loss": 1.2507,
      "step": 2999
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.032342445105314255,
      "learning_rate": 0.0013639049369634877,
      "loss": 1.1493,
      "step": 3000
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.0540913343429565,
      "eval_runtime": 23.7897,
      "eval_samples_per_second": 21.018,
      "eval_steps_per_second": 2.648,
      "step": 3000
    },
    {
      "epoch": 4.001333333333333,
      "grad_norm": 0.02523842826485634,
      "learning_rate": 0.0013635026772323726,
      "loss": 1.0743,
      "step": 3001
    },
    {
      "epoch": 4.002666666666666,
      "grad_norm": 0.024918679147958755,
      "learning_rate": 0.001363100349715047,
      "loss": 1.2178,
      "step": 3002
    },
    {
      "epoch": 4.004,
      "grad_norm": 0.02486097253859043,
      "learning_rate": 0.0013626979544865369,
      "loss": 0.8901,
      "step": 3003
    },
    {
      "epoch": 4.005333333333334,
      "grad_norm": 0.029872875660657883,
      "learning_rate": 0.0013622954916218811,
      "loss": 1.0975,
      "step": 3004
    },
    {
      "epoch": 4.006666666666667,
      "grad_norm": 0.029549453407526016,
      "learning_rate": 0.0013618929611961318,
      "loss": 0.8464,
      "step": 3005
    },
    {
      "epoch": 4.008,
      "grad_norm": 0.02465747483074665,
      "learning_rate": 0.0013614903632843522,
      "loss": 1.052,
      "step": 3006
    },
    {
      "epoch": 4.009333333333333,
      "grad_norm": 0.03251219540834427,
      "learning_rate": 0.00136108769796162,
      "loss": 0.924,
      "step": 3007
    },
    {
      "epoch": 4.010666666666666,
      "grad_norm": 0.02939937263727188,
      "learning_rate": 0.0013606849653030233,
      "loss": 0.9477,
      "step": 3008
    },
    {
      "epoch": 4.012,
      "grad_norm": 0.03256465122103691,
      "learning_rate": 0.0013602821653836653,
      "loss": 0.9012,
      "step": 3009
    },
    {
      "epoch": 4.013333333333334,
      "grad_norm": 0.047743696719408035,
      "learning_rate": 0.0013598792982786594,
      "loss": 1.1214,
      "step": 3010
    },
    {
      "epoch": 4.014666666666667,
      "grad_norm": 0.03132888674736023,
      "learning_rate": 0.0013594763640631328,
      "loss": 1.1967,
      "step": 3011
    },
    {
      "epoch": 4.016,
      "grad_norm": 0.026125939562916756,
      "learning_rate": 0.001359073362812225,
      "loss": 0.9889,
      "step": 3012
    },
    {
      "epoch": 4.017333333333333,
      "grad_norm": 0.028833677992224693,
      "learning_rate": 0.001358670294601088,
      "loss": 0.9883,
      "step": 3013
    },
    {
      "epoch": 4.018666666666666,
      "grad_norm": 0.03218842297792435,
      "learning_rate": 0.001358267159504886,
      "loss": 1.0762,
      "step": 3014
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.03189331665635109,
      "learning_rate": 0.0013578639575987958,
      "loss": 1.2259,
      "step": 3015
    },
    {
      "epoch": 4.021333333333334,
      "grad_norm": 0.029433313757181168,
      "learning_rate": 0.0013574606889580074,
      "loss": 1.0906,
      "step": 3016
    },
    {
      "epoch": 4.022666666666667,
      "grad_norm": 0.04575991630554199,
      "learning_rate": 0.0013570573536577217,
      "loss": 1.0449,
      "step": 3017
    },
    {
      "epoch": 4.024,
      "grad_norm": 0.028437159955501556,
      "learning_rate": 0.0013566539517731535,
      "loss": 0.9816,
      "step": 3018
    },
    {
      "epoch": 4.025333333333333,
      "grad_norm": 0.03484076261520386,
      "learning_rate": 0.0013562504833795294,
      "loss": 1.0932,
      "step": 3019
    },
    {
      "epoch": 4.026666666666666,
      "grad_norm": 0.025773687288165092,
      "learning_rate": 0.0013558469485520882,
      "loss": 1.0804,
      "step": 3020
    },
    {
      "epoch": 4.028,
      "grad_norm": 0.023329364135861397,
      "learning_rate": 0.0013554433473660817,
      "loss": 0.7912,
      "step": 3021
    },
    {
      "epoch": 4.029333333333334,
      "grad_norm": 0.03015492670238018,
      "learning_rate": 0.0013550396798967732,
      "loss": 1.125,
      "step": 3022
    },
    {
      "epoch": 4.030666666666667,
      "grad_norm": 0.027454402297735214,
      "learning_rate": 0.001354635946219439,
      "loss": 1.1011,
      "step": 3023
    },
    {
      "epoch": 4.032,
      "grad_norm": 0.023385820910334587,
      "learning_rate": 0.0013542321464093678,
      "loss": 0.955,
      "step": 3024
    },
    {
      "epoch": 4.033333333333333,
      "grad_norm": 0.027526183053851128,
      "learning_rate": 0.0013538282805418609,
      "loss": 0.7669,
      "step": 3025
    },
    {
      "epoch": 4.034666666666666,
      "grad_norm": 0.030420318245887756,
      "learning_rate": 0.0013534243486922307,
      "loss": 1.0094,
      "step": 3026
    },
    {
      "epoch": 4.036,
      "grad_norm": 0.03574798256158829,
      "learning_rate": 0.0013530203509358027,
      "loss": 1.2102,
      "step": 3027
    },
    {
      "epoch": 4.037333333333334,
      "grad_norm": 0.025506658479571342,
      "learning_rate": 0.001352616287347915,
      "loss": 0.9918,
      "step": 3028
    },
    {
      "epoch": 4.038666666666667,
      "grad_norm": 0.024124128744006157,
      "learning_rate": 0.0013522121580039174,
      "loss": 1.0525,
      "step": 3029
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.05433843657374382,
      "learning_rate": 0.0013518079629791723,
      "loss": 1.0116,
      "step": 3030
    },
    {
      "epoch": 4.041333333333333,
      "grad_norm": 0.030711736530065536,
      "learning_rate": 0.001351403702349055,
      "loss": 0.9934,
      "step": 3031
    },
    {
      "epoch": 4.042666666666666,
      "grad_norm": 0.029699314385652542,
      "learning_rate": 0.001350999376188951,
      "loss": 1.1204,
      "step": 3032
    },
    {
      "epoch": 4.044,
      "grad_norm": 0.033604465425014496,
      "learning_rate": 0.0013505949845742597,
      "loss": 1.1158,
      "step": 3033
    },
    {
      "epoch": 4.045333333333334,
      "grad_norm": 0.02629753202199936,
      "learning_rate": 0.0013501905275803926,
      "loss": 1.2171,
      "step": 3034
    },
    {
      "epoch": 4.046666666666667,
      "grad_norm": 0.023042744025588036,
      "learning_rate": 0.0013497860052827729,
      "loss": 1.0332,
      "step": 3035
    },
    {
      "epoch": 4.048,
      "grad_norm": 0.03201066702604294,
      "learning_rate": 0.0013493814177568365,
      "loss": 1.0636,
      "step": 3036
    },
    {
      "epoch": 4.049333333333333,
      "grad_norm": 0.026504067704081535,
      "learning_rate": 0.0013489767650780307,
      "loss": 0.9553,
      "step": 3037
    },
    {
      "epoch": 4.050666666666666,
      "grad_norm": 0.026092588901519775,
      "learning_rate": 0.0013485720473218152,
      "loss": 1.0445,
      "step": 3038
    },
    {
      "epoch": 4.052,
      "grad_norm": 0.027867667376995087,
      "learning_rate": 0.0013481672645636626,
      "loss": 1.2869,
      "step": 3039
    },
    {
      "epoch": 4.053333333333334,
      "grad_norm": 0.04744614288210869,
      "learning_rate": 0.001347762416879057,
      "loss": 1.0322,
      "step": 3040
    },
    {
      "epoch": 4.054666666666667,
      "grad_norm": 0.03662195801734924,
      "learning_rate": 0.0013473575043434945,
      "loss": 1.302,
      "step": 3041
    },
    {
      "epoch": 4.056,
      "grad_norm": 0.045665375888347626,
      "learning_rate": 0.0013469525270324834,
      "loss": 1.0205,
      "step": 3042
    },
    {
      "epoch": 4.057333333333333,
      "grad_norm": 0.028027597814798355,
      "learning_rate": 0.001346547485021544,
      "loss": 0.9311,
      "step": 3043
    },
    {
      "epoch": 4.058666666666666,
      "grad_norm": 0.02741008624434471,
      "learning_rate": 0.001346142378386209,
      "loss": 0.9798,
      "step": 3044
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.02553350292146206,
      "learning_rate": 0.001345737207202023,
      "loss": 0.9239,
      "step": 3045
    },
    {
      "epoch": 4.061333333333334,
      "grad_norm": 0.02762431651353836,
      "learning_rate": 0.001345331971544542,
      "loss": 0.9859,
      "step": 3046
    },
    {
      "epoch": 4.062666666666667,
      "grad_norm": 0.029993468895554543,
      "learning_rate": 0.0013449266714893357,
      "loss": 1.0466,
      "step": 3047
    },
    {
      "epoch": 4.064,
      "grad_norm": 0.05011774227023125,
      "learning_rate": 0.0013445213071119838,
      "loss": 1.0149,
      "step": 3048
    },
    {
      "epoch": 4.065333333333333,
      "grad_norm": 0.029523160308599472,
      "learning_rate": 0.0013441158784880793,
      "loss": 1.0169,
      "step": 3049
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 0.02794499881565571,
      "learning_rate": 0.0013437103856932264,
      "loss": 0.952,
      "step": 3050
    },
    {
      "epoch": 4.068,
      "grad_norm": 0.038146231323480606,
      "learning_rate": 0.0013433048288030423,
      "loss": 1.105,
      "step": 3051
    },
    {
      "epoch": 4.069333333333334,
      "grad_norm": 0.032797496765851974,
      "learning_rate": 0.001342899207893155,
      "loss": 0.9017,
      "step": 3052
    },
    {
      "epoch": 4.070666666666667,
      "grad_norm": 0.029878202825784683,
      "learning_rate": 0.0013424935230392052,
      "loss": 1.2356,
      "step": 3053
    },
    {
      "epoch": 4.072,
      "grad_norm": 0.032488271594047546,
      "learning_rate": 0.0013420877743168449,
      "loss": 0.8233,
      "step": 3054
    },
    {
      "epoch": 4.073333333333333,
      "grad_norm": 0.027467230334877968,
      "learning_rate": 0.001341681961801739,
      "loss": 0.8487,
      "step": 3055
    },
    {
      "epoch": 4.074666666666666,
      "grad_norm": 0.034949202090501785,
      "learning_rate": 0.0013412760855695627,
      "loss": 0.9555,
      "step": 3056
    },
    {
      "epoch": 4.076,
      "grad_norm": 0.030505362898111343,
      "learning_rate": 0.001340870145696005,
      "loss": 1.0184,
      "step": 3057
    },
    {
      "epoch": 4.077333333333334,
      "grad_norm": 0.028073908761143684,
      "learning_rate": 0.0013404641422567658,
      "loss": 0.9446,
      "step": 3058
    },
    {
      "epoch": 4.078666666666667,
      "grad_norm": 0.027457676827907562,
      "learning_rate": 0.0013400580753275563,
      "loss": 0.9915,
      "step": 3059
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.030276348814368248,
      "learning_rate": 0.0013396519449841004,
      "loss": 0.9596,
      "step": 3060
    },
    {
      "epoch": 4.081333333333333,
      "grad_norm": 0.025098193436861038,
      "learning_rate": 0.0013392457513021338,
      "loss": 0.9198,
      "step": 3061
    },
    {
      "epoch": 4.082666666666666,
      "grad_norm": 0.030972344800829887,
      "learning_rate": 0.001338839494357403,
      "loss": 0.8207,
      "step": 3062
    },
    {
      "epoch": 4.084,
      "grad_norm": 0.03140546381473541,
      "learning_rate": 0.001338433174225668,
      "loss": 1.1212,
      "step": 3063
    },
    {
      "epoch": 4.085333333333334,
      "grad_norm": 0.02458946779370308,
      "learning_rate": 0.001338026790982699,
      "loss": 1.0816,
      "step": 3064
    },
    {
      "epoch": 4.086666666666667,
      "grad_norm": 0.027892131358385086,
      "learning_rate": 0.0013376203447042787,
      "loss": 1.1646,
      "step": 3065
    },
    {
      "epoch": 4.088,
      "grad_norm": 0.02705538272857666,
      "learning_rate": 0.0013372138354662017,
      "loss": 1.0408,
      "step": 3066
    },
    {
      "epoch": 4.089333333333333,
      "grad_norm": 0.02339162863790989,
      "learning_rate": 0.0013368072633442739,
      "loss": 1.0991,
      "step": 3067
    },
    {
      "epoch": 4.0906666666666665,
      "grad_norm": 0.030295521020889282,
      "learning_rate": 0.0013364006284143134,
      "loss": 1.0099,
      "step": 3068
    },
    {
      "epoch": 4.092,
      "grad_norm": 0.028955940157175064,
      "learning_rate": 0.0013359939307521493,
      "loss": 1.1211,
      "step": 3069
    },
    {
      "epoch": 4.093333333333334,
      "grad_norm": 0.02433384209871292,
      "learning_rate": 0.001335587170433623,
      "loss": 1.2335,
      "step": 3070
    },
    {
      "epoch": 4.094666666666667,
      "grad_norm": 0.032996248453855515,
      "learning_rate": 0.0013351803475345875,
      "loss": 1.3661,
      "step": 3071
    },
    {
      "epoch": 4.096,
      "grad_norm": 0.023346835747361183,
      "learning_rate": 0.0013347734621309076,
      "loss": 1.1922,
      "step": 3072
    },
    {
      "epoch": 4.097333333333333,
      "grad_norm": 0.02292601764202118,
      "learning_rate": 0.0013343665142984593,
      "loss": 0.8134,
      "step": 3073
    },
    {
      "epoch": 4.0986666666666665,
      "grad_norm": 0.027667148038744926,
      "learning_rate": 0.0013339595041131306,
      "loss": 1.0966,
      "step": 3074
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.026556547731161118,
      "learning_rate": 0.0013335524316508208,
      "loss": 1.0785,
      "step": 3075
    },
    {
      "epoch": 4.101333333333334,
      "grad_norm": 0.029427628964185715,
      "learning_rate": 0.001333145296987441,
      "loss": 1.0683,
      "step": 3076
    },
    {
      "epoch": 4.102666666666667,
      "grad_norm": 0.026586070656776428,
      "learning_rate": 0.0013327381001989142,
      "loss": 1.3282,
      "step": 3077
    },
    {
      "epoch": 4.104,
      "grad_norm": 0.02246629074215889,
      "learning_rate": 0.0013323308413611748,
      "loss": 0.9665,
      "step": 3078
    },
    {
      "epoch": 4.105333333333333,
      "grad_norm": 0.02908014878630638,
      "learning_rate": 0.0013319235205501683,
      "loss": 0.8266,
      "step": 3079
    },
    {
      "epoch": 4.1066666666666665,
      "grad_norm": 0.023172158747911453,
      "learning_rate": 0.0013315161378418526,
      "loss": 1.0482,
      "step": 3080
    },
    {
      "epoch": 4.108,
      "grad_norm": 0.02419627271592617,
      "learning_rate": 0.0013311086933121962,
      "loss": 1.1956,
      "step": 3081
    },
    {
      "epoch": 4.109333333333334,
      "grad_norm": 0.01865687593817711,
      "learning_rate": 0.00133070118703718,
      "loss": 1.1234,
      "step": 3082
    },
    {
      "epoch": 4.110666666666667,
      "grad_norm": 0.026324765756726265,
      "learning_rate": 0.0013302936190927957,
      "loss": 0.9788,
      "step": 3083
    },
    {
      "epoch": 4.112,
      "grad_norm": 0.021462026983499527,
      "learning_rate": 0.0013298859895550473,
      "loss": 0.8219,
      "step": 3084
    },
    {
      "epoch": 4.113333333333333,
      "grad_norm": 0.03147919848561287,
      "learning_rate": 0.0013294782984999492,
      "loss": 0.982,
      "step": 3085
    },
    {
      "epoch": 4.1146666666666665,
      "grad_norm": 0.02729114517569542,
      "learning_rate": 0.0013290705460035283,
      "loss": 0.8388,
      "step": 3086
    },
    {
      "epoch": 4.116,
      "grad_norm": 0.024032678455114365,
      "learning_rate": 0.0013286627321418228,
      "loss": 1.0057,
      "step": 3087
    },
    {
      "epoch": 4.117333333333334,
      "grad_norm": 0.023832567036151886,
      "learning_rate": 0.0013282548569908813,
      "loss": 0.9287,
      "step": 3088
    },
    {
      "epoch": 4.118666666666667,
      "grad_norm": 0.02866845764219761,
      "learning_rate": 0.0013278469206267653,
      "loss": 1.1737,
      "step": 3089
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.03249690681695938,
      "learning_rate": 0.0013274389231255465,
      "loss": 1.1814,
      "step": 3090
    },
    {
      "epoch": 4.121333333333333,
      "grad_norm": 0.026065433397889137,
      "learning_rate": 0.0013270308645633095,
      "loss": 0.994,
      "step": 3091
    },
    {
      "epoch": 4.1226666666666665,
      "grad_norm": 0.023297736421227455,
      "learning_rate": 0.0013266227450161481,
      "loss": 1.1743,
      "step": 3092
    },
    {
      "epoch": 4.124,
      "grad_norm": 0.03259853273630142,
      "learning_rate": 0.0013262145645601693,
      "loss": 0.8365,
      "step": 3093
    },
    {
      "epoch": 4.125333333333334,
      "grad_norm": 0.026139777153730392,
      "learning_rate": 0.001325806323271491,
      "loss": 1.318,
      "step": 3094
    },
    {
      "epoch": 4.126666666666667,
      "grad_norm": 0.024114742875099182,
      "learning_rate": 0.0013253980212262419,
      "loss": 0.8521,
      "step": 3095
    },
    {
      "epoch": 4.128,
      "grad_norm": 0.030129658058285713,
      "learning_rate": 0.0013249896585005628,
      "loss": 1.183,
      "step": 3096
    },
    {
      "epoch": 4.129333333333333,
      "grad_norm": 0.02917587198317051,
      "learning_rate": 0.0013245812351706052,
      "loss": 1.1924,
      "step": 3097
    },
    {
      "epoch": 4.1306666666666665,
      "grad_norm": 0.021876931190490723,
      "learning_rate": 0.0013241727513125321,
      "loss": 0.9273,
      "step": 3098
    },
    {
      "epoch": 4.132,
      "grad_norm": 0.03476261347532272,
      "learning_rate": 0.0013237642070025183,
      "loss": 1.0938,
      "step": 3099
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 0.025691581889986992,
      "learning_rate": 0.0013233556023167486,
      "loss": 0.9727,
      "step": 3100
    },
    {
      "epoch": 4.134666666666667,
      "grad_norm": 0.02688954398036003,
      "learning_rate": 0.0013229469373314206,
      "loss": 1.2174,
      "step": 3101
    },
    {
      "epoch": 4.136,
      "grad_norm": 0.02918986789882183,
      "learning_rate": 0.0013225382121227417,
      "loss": 0.9537,
      "step": 3102
    },
    {
      "epoch": 4.137333333333333,
      "grad_norm": 0.029373979195952415,
      "learning_rate": 0.001322129426766932,
      "loss": 0.9154,
      "step": 3103
    },
    {
      "epoch": 4.1386666666666665,
      "grad_norm": 0.037478383630514145,
      "learning_rate": 0.0013217205813402216,
      "loss": 1.2469,
      "step": 3104
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.02836635522544384,
      "learning_rate": 0.0013213116759188523,
      "loss": 1.0169,
      "step": 3105
    },
    {
      "epoch": 4.141333333333334,
      "grad_norm": 0.02708713337779045,
      "learning_rate": 0.0013209027105790772,
      "loss": 0.8663,
      "step": 3106
    },
    {
      "epoch": 4.142666666666667,
      "grad_norm": 0.02837340533733368,
      "learning_rate": 0.0013204936853971603,
      "loss": 1.0573,
      "step": 3107
    },
    {
      "epoch": 4.144,
      "grad_norm": 0.01846560277044773,
      "learning_rate": 0.0013200846004493769,
      "loss": 1.0872,
      "step": 3108
    },
    {
      "epoch": 4.145333333333333,
      "grad_norm": 0.1857139766216278,
      "learning_rate": 0.001319675455812013,
      "loss": 0.8064,
      "step": 3109
    },
    {
      "epoch": 4.1466666666666665,
      "grad_norm": 0.023009246215224266,
      "learning_rate": 0.0013192662515613674,
      "loss": 0.9856,
      "step": 3110
    },
    {
      "epoch": 4.148,
      "grad_norm": 0.020474841818213463,
      "learning_rate": 0.0013188569877737473,
      "loss": 1.1756,
      "step": 3111
    },
    {
      "epoch": 4.149333333333334,
      "grad_norm": 0.025370633229613304,
      "learning_rate": 0.0013184476645254732,
      "loss": 1.223,
      "step": 3112
    },
    {
      "epoch": 4.150666666666667,
      "grad_norm": 0.07220635563135147,
      "learning_rate": 0.001318038281892876,
      "loss": 1.048,
      "step": 3113
    },
    {
      "epoch": 4.152,
      "grad_norm": 0.025383763015270233,
      "learning_rate": 0.0013176288399522973,
      "loss": 0.9021,
      "step": 3114
    },
    {
      "epoch": 4.153333333333333,
      "grad_norm": 0.025290165096521378,
      "learning_rate": 0.0013172193387800907,
      "loss": 1.0558,
      "step": 3115
    },
    {
      "epoch": 4.1546666666666665,
      "grad_norm": 0.035929590463638306,
      "learning_rate": 0.0013168097784526194,
      "loss": 1.4219,
      "step": 3116
    },
    {
      "epoch": 4.156,
      "grad_norm": 0.03714795783162117,
      "learning_rate": 0.0013164001590462591,
      "loss": 1.1224,
      "step": 3117
    },
    {
      "epoch": 4.157333333333334,
      "grad_norm": 0.06770399212837219,
      "learning_rate": 0.0013159904806373957,
      "loss": 1.0332,
      "step": 3118
    },
    {
      "epoch": 4.158666666666667,
      "grad_norm": 0.02260722778737545,
      "learning_rate": 0.0013155807433024261,
      "loss": 0.9486,
      "step": 3119
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.03375241532921791,
      "learning_rate": 0.0013151709471177588,
      "loss": 0.9563,
      "step": 3120
    },
    {
      "epoch": 4.161333333333333,
      "grad_norm": 0.026092510670423508,
      "learning_rate": 0.0013147610921598123,
      "loss": 1.1555,
      "step": 3121
    },
    {
      "epoch": 4.1626666666666665,
      "grad_norm": 0.025493260473012924,
      "learning_rate": 0.001314351178505017,
      "loss": 0.9055,
      "step": 3122
    },
    {
      "epoch": 4.164,
      "grad_norm": 0.03016791306436062,
      "learning_rate": 0.001313941206229814,
      "loss": 1.1334,
      "step": 3123
    },
    {
      "epoch": 4.165333333333333,
      "grad_norm": 0.023559151217341423,
      "learning_rate": 0.001313531175410655,
      "loss": 0.9551,
      "step": 3124
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 0.04213999584317207,
      "learning_rate": 0.0013131210861240027,
      "loss": 1.3588,
      "step": 3125
    },
    {
      "epoch": 4.168,
      "grad_norm": 0.02988249994814396,
      "learning_rate": 0.001312710938446331,
      "loss": 1.0859,
      "step": 3126
    },
    {
      "epoch": 4.169333333333333,
      "grad_norm": 0.033092036843299866,
      "learning_rate": 0.001312300732454124,
      "loss": 1.0618,
      "step": 3127
    },
    {
      "epoch": 4.1706666666666665,
      "grad_norm": 0.027929456904530525,
      "learning_rate": 0.0013118904682238777,
      "loss": 0.8324,
      "step": 3128
    },
    {
      "epoch": 4.172,
      "grad_norm": 0.03071749582886696,
      "learning_rate": 0.0013114801458320986,
      "loss": 0.9853,
      "step": 3129
    },
    {
      "epoch": 4.173333333333334,
      "grad_norm": 0.03639694303274155,
      "learning_rate": 0.0013110697653553034,
      "loss": 1.1275,
      "step": 3130
    },
    {
      "epoch": 4.174666666666667,
      "grad_norm": 0.02392428182065487,
      "learning_rate": 0.0013106593268700205,
      "loss": 0.9976,
      "step": 3131
    },
    {
      "epoch": 4.176,
      "grad_norm": 0.028941528871655464,
      "learning_rate": 0.001310248830452788,
      "loss": 1.2097,
      "step": 3132
    },
    {
      "epoch": 4.177333333333333,
      "grad_norm": 0.027597609907388687,
      "learning_rate": 0.0013098382761801567,
      "loss": 1.0179,
      "step": 3133
    },
    {
      "epoch": 4.1786666666666665,
      "grad_norm": 0.04071345180273056,
      "learning_rate": 0.0013094276641286858,
      "loss": 0.8154,
      "step": 3134
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.024801161140203476,
      "learning_rate": 0.0013090169943749475,
      "loss": 0.9747,
      "step": 3135
    },
    {
      "epoch": 4.181333333333333,
      "grad_norm": 0.026831571012735367,
      "learning_rate": 0.0013086062669955233,
      "loss": 1.0301,
      "step": 3136
    },
    {
      "epoch": 4.182666666666667,
      "grad_norm": 0.027766095474362373,
      "learning_rate": 0.001308195482067006,
      "loss": 0.8533,
      "step": 3137
    },
    {
      "epoch": 4.184,
      "grad_norm": 0.0253462977707386,
      "learning_rate": 0.0013077846396659985,
      "loss": 0.9895,
      "step": 3138
    },
    {
      "epoch": 4.185333333333333,
      "grad_norm": 0.029741067439317703,
      "learning_rate": 0.0013073737398691158,
      "loss": 1.1366,
      "step": 3139
    },
    {
      "epoch": 4.1866666666666665,
      "grad_norm": 0.02884361520409584,
      "learning_rate": 0.0013069627827529825,
      "loss": 1.3534,
      "step": 3140
    },
    {
      "epoch": 4.188,
      "grad_norm": 0.02359742671251297,
      "learning_rate": 0.0013065517683942339,
      "loss": 1.0761,
      "step": 3141
    },
    {
      "epoch": 4.189333333333333,
      "grad_norm": 0.033894140273332596,
      "learning_rate": 0.0013061406968695162,
      "loss": 0.9691,
      "step": 3142
    },
    {
      "epoch": 4.190666666666667,
      "grad_norm": 0.027518875896930695,
      "learning_rate": 0.0013057295682554866,
      "loss": 0.7193,
      "step": 3143
    },
    {
      "epoch": 4.192,
      "grad_norm": 0.025422273203730583,
      "learning_rate": 0.0013053183826288123,
      "loss": 0.9979,
      "step": 3144
    },
    {
      "epoch": 4.193333333333333,
      "grad_norm": 0.03454527631402016,
      "learning_rate": 0.0013049071400661717,
      "loss": 1.0063,
      "step": 3145
    },
    {
      "epoch": 4.1946666666666665,
      "grad_norm": 0.027121394872665405,
      "learning_rate": 0.0013044958406442538,
      "loss": 0.9775,
      "step": 3146
    },
    {
      "epoch": 4.196,
      "grad_norm": 0.030890678986907005,
      "learning_rate": 0.0013040844844397572,
      "loss": 1.0416,
      "step": 3147
    },
    {
      "epoch": 4.197333333333333,
      "grad_norm": 0.022251063957810402,
      "learning_rate": 0.0013036730715293922,
      "loss": 1.2009,
      "step": 3148
    },
    {
      "epoch": 4.198666666666667,
      "grad_norm": 0.03292243555188179,
      "learning_rate": 0.0013032616019898798,
      "loss": 0.9239,
      "step": 3149
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.02789990045130253,
      "learning_rate": 0.0013028500758979506,
      "loss": 0.9747,
      "step": 3150
    },
    {
      "epoch": 4.201333333333333,
      "grad_norm": 0.027971651405096054,
      "learning_rate": 0.0013024384933303468,
      "loss": 1.191,
      "step": 3151
    },
    {
      "epoch": 4.2026666666666666,
      "grad_norm": 0.02628827467560768,
      "learning_rate": 0.00130202685436382,
      "loss": 1.0879,
      "step": 3152
    },
    {
      "epoch": 4.204,
      "grad_norm": 0.03418506681919098,
      "learning_rate": 0.0013016151590751332,
      "loss": 1.0018,
      "step": 3153
    },
    {
      "epoch": 4.205333333333333,
      "grad_norm": 0.023144369944930077,
      "learning_rate": 0.0013012034075410592,
      "loss": 1.1125,
      "step": 3154
    },
    {
      "epoch": 4.206666666666667,
      "grad_norm": 0.02066289633512497,
      "learning_rate": 0.001300791599838382,
      "loss": 0.7699,
      "step": 3155
    },
    {
      "epoch": 4.208,
      "grad_norm": 0.03134094923734665,
      "learning_rate": 0.0013003797360438961,
      "loss": 0.9459,
      "step": 3156
    },
    {
      "epoch": 4.209333333333333,
      "grad_norm": 0.044285885989665985,
      "learning_rate": 0.0012999678162344055,
      "loss": 1.2422,
      "step": 3157
    },
    {
      "epoch": 4.210666666666667,
      "grad_norm": 0.0308662261813879,
      "learning_rate": 0.0012995558404867256,
      "loss": 0.7163,
      "step": 3158
    },
    {
      "epoch": 4.212,
      "grad_norm": 0.03565765544772148,
      "learning_rate": 0.0012991438088776816,
      "loss": 1.2524,
      "step": 3159
    },
    {
      "epoch": 4.213333333333333,
      "grad_norm": 0.03276195004582405,
      "learning_rate": 0.0012987317214841099,
      "loss": 1.2426,
      "step": 3160
    },
    {
      "epoch": 4.214666666666667,
      "grad_norm": 0.03789020702242851,
      "learning_rate": 0.0012983195783828566,
      "loss": 0.9745,
      "step": 3161
    },
    {
      "epoch": 4.216,
      "grad_norm": 0.03305739536881447,
      "learning_rate": 0.0012979073796507785,
      "loss": 0.9875,
      "step": 3162
    },
    {
      "epoch": 4.217333333333333,
      "grad_norm": 0.02562101185321808,
      "learning_rate": 0.0012974951253647427,
      "loss": 1.0618,
      "step": 3163
    },
    {
      "epoch": 4.218666666666667,
      "grad_norm": 0.031474754214286804,
      "learning_rate": 0.001297082815601626,
      "loss": 0.9434,
      "step": 3164
    },
    {
      "epoch": 4.22,
      "grad_norm": 0.025797631591558456,
      "learning_rate": 0.0012966704504383168,
      "loss": 0.9478,
      "step": 3165
    },
    {
      "epoch": 4.221333333333333,
      "grad_norm": 0.025104394182562828,
      "learning_rate": 0.0012962580299517132,
      "loss": 0.9204,
      "step": 3166
    },
    {
      "epoch": 4.222666666666667,
      "grad_norm": 0.024327168241143227,
      "learning_rate": 0.0012958455542187238,
      "loss": 0.8049,
      "step": 3167
    },
    {
      "epoch": 4.224,
      "grad_norm": 0.034706611186265945,
      "learning_rate": 0.0012954330233162668,
      "loss": 1.0777,
      "step": 3168
    },
    {
      "epoch": 4.225333333333333,
      "grad_norm": 0.02184941992163658,
      "learning_rate": 0.0012950204373212717,
      "loss": 1.1626,
      "step": 3169
    },
    {
      "epoch": 4.226666666666667,
      "grad_norm": 0.03382868692278862,
      "learning_rate": 0.0012946077963106773,
      "loss": 1.2611,
      "step": 3170
    },
    {
      "epoch": 4.228,
      "grad_norm": 0.031821366399526596,
      "learning_rate": 0.0012941951003614337,
      "loss": 1.0505,
      "step": 3171
    },
    {
      "epoch": 4.229333333333333,
      "grad_norm": 0.024008827283978462,
      "learning_rate": 0.0012937823495505004,
      "loss": 1.0252,
      "step": 3172
    },
    {
      "epoch": 4.230666666666667,
      "grad_norm": 0.025356242433190346,
      "learning_rate": 0.0012933695439548475,
      "loss": 0.9046,
      "step": 3173
    },
    {
      "epoch": 4.232,
      "grad_norm": 0.03452303260564804,
      "learning_rate": 0.0012929566836514554,
      "loss": 1.1226,
      "step": 3174
    },
    {
      "epoch": 4.233333333333333,
      "grad_norm": 0.026461556553840637,
      "learning_rate": 0.0012925437687173144,
      "loss": 0.9361,
      "step": 3175
    },
    {
      "epoch": 4.234666666666667,
      "grad_norm": 0.03212412819266319,
      "learning_rate": 0.0012921307992294247,
      "loss": 1.3474,
      "step": 3176
    },
    {
      "epoch": 4.236,
      "grad_norm": 0.041802793741226196,
      "learning_rate": 0.0012917177752647978,
      "loss": 1.0369,
      "step": 3177
    },
    {
      "epoch": 4.237333333333333,
      "grad_norm": 0.03434393182396889,
      "learning_rate": 0.0012913046969004547,
      "loss": 1.049,
      "step": 3178
    },
    {
      "epoch": 4.238666666666667,
      "grad_norm": 0.027412233874201775,
      "learning_rate": 0.0012908915642134262,
      "loss": 1.4181,
      "step": 3179
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.021626722067594528,
      "learning_rate": 0.0012904783772807534,
      "loss": 1.0674,
      "step": 3180
    },
    {
      "epoch": 4.241333333333333,
      "grad_norm": 0.025928139686584473,
      "learning_rate": 0.001290065136179488,
      "loss": 0.9058,
      "step": 3181
    },
    {
      "epoch": 4.242666666666667,
      "grad_norm": 0.03281896561384201,
      "learning_rate": 0.001289651840986691,
      "loss": 1.1104,
      "step": 3182
    },
    {
      "epoch": 4.244,
      "grad_norm": 0.03096352145075798,
      "learning_rate": 0.0012892384917794347,
      "loss": 1.01,
      "step": 3183
    },
    {
      "epoch": 4.245333333333333,
      "grad_norm": 0.0368361659348011,
      "learning_rate": 0.0012888250886348004,
      "loss": 1.2315,
      "step": 3184
    },
    {
      "epoch": 4.246666666666667,
      "grad_norm": 0.04260239377617836,
      "learning_rate": 0.001288411631629879,
      "loss": 0.9426,
      "step": 3185
    },
    {
      "epoch": 4.248,
      "grad_norm": 0.02418104000389576,
      "learning_rate": 0.0012879981208417735,
      "loss": 0.9801,
      "step": 3186
    },
    {
      "epoch": 4.249333333333333,
      "grad_norm": 0.03170933574438095,
      "learning_rate": 0.0012875845563475946,
      "loss": 1.1628,
      "step": 3187
    },
    {
      "epoch": 4.250666666666667,
      "grad_norm": 0.0338381789624691,
      "learning_rate": 0.001287170938224465,
      "loss": 1.2994,
      "step": 3188
    },
    {
      "epoch": 4.252,
      "grad_norm": 0.02603188343346119,
      "learning_rate": 0.0012867572665495155,
      "loss": 1.0223,
      "step": 3189
    },
    {
      "epoch": 4.253333333333333,
      "grad_norm": 0.030191734433174133,
      "learning_rate": 0.001286343541399889,
      "loss": 1.2362,
      "step": 3190
    },
    {
      "epoch": 4.254666666666667,
      "grad_norm": 0.034828152507543564,
      "learning_rate": 0.001285929762852736,
      "loss": 1.0111,
      "step": 3191
    },
    {
      "epoch": 4.256,
      "grad_norm": 0.02306194044649601,
      "learning_rate": 0.001285515930985219,
      "loss": 0.8086,
      "step": 3192
    },
    {
      "epoch": 4.257333333333333,
      "grad_norm": 0.025017106905579567,
      "learning_rate": 0.0012851020458745093,
      "loss": 1.1332,
      "step": 3193
    },
    {
      "epoch": 4.258666666666667,
      "grad_norm": 0.03071759082376957,
      "learning_rate": 0.0012846881075977887,
      "loss": 1.1931,
      "step": 3194
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.031377650797367096,
      "learning_rate": 0.0012842741162322486,
      "loss": 1.0047,
      "step": 3195
    },
    {
      "epoch": 4.261333333333333,
      "grad_norm": 0.025044595822691917,
      "learning_rate": 0.0012838600718550902,
      "loss": 1.088,
      "step": 3196
    },
    {
      "epoch": 4.262666666666667,
      "grad_norm": 0.02717876061797142,
      "learning_rate": 0.0012834459745435247,
      "loss": 1.0238,
      "step": 3197
    },
    {
      "epoch": 4.264,
      "grad_norm": 0.030407145619392395,
      "learning_rate": 0.0012830318243747735,
      "loss": 1.2546,
      "step": 3198
    },
    {
      "epoch": 4.265333333333333,
      "grad_norm": 0.021494455635547638,
      "learning_rate": 0.0012826176214260677,
      "loss": 1.0216,
      "step": 3199
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 0.029232565313577652,
      "learning_rate": 0.0012822033657746478,
      "loss": 0.9526,
      "step": 3200
    },
    {
      "epoch": 4.268,
      "grad_norm": 0.03332448750734329,
      "learning_rate": 0.0012817890574977646,
      "loss": 1.0997,
      "step": 3201
    },
    {
      "epoch": 4.269333333333333,
      "grad_norm": 0.03073766455054283,
      "learning_rate": 0.0012813746966726782,
      "loss": 0.8664,
      "step": 3202
    },
    {
      "epoch": 4.270666666666667,
      "grad_norm": 0.02138698659837246,
      "learning_rate": 0.0012809602833766596,
      "loss": 0.7048,
      "step": 3203
    },
    {
      "epoch": 4.272,
      "grad_norm": 0.025776326656341553,
      "learning_rate": 0.0012805458176869883,
      "loss": 1.0656,
      "step": 3204
    },
    {
      "epoch": 4.273333333333333,
      "grad_norm": 0.03629760816693306,
      "learning_rate": 0.0012801312996809544,
      "loss": 1.1063,
      "step": 3205
    },
    {
      "epoch": 4.274666666666667,
      "grad_norm": 0.027702849358320236,
      "learning_rate": 0.0012797167294358576,
      "loss": 1.0525,
      "step": 3206
    },
    {
      "epoch": 4.276,
      "grad_norm": 0.02866770699620247,
      "learning_rate": 0.0012793021070290065,
      "loss": 1.1605,
      "step": 3207
    },
    {
      "epoch": 4.277333333333333,
      "grad_norm": 0.027496041730046272,
      "learning_rate": 0.001278887432537721,
      "loss": 1.0088,
      "step": 3208
    },
    {
      "epoch": 4.278666666666667,
      "grad_norm": 0.028082439675927162,
      "learning_rate": 0.0012784727060393293,
      "loss": 0.9245,
      "step": 3209
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.0287465900182724,
      "learning_rate": 0.0012780579276111701,
      "loss": 1.0059,
      "step": 3210
    },
    {
      "epoch": 4.281333333333333,
      "grad_norm": 0.03708457574248314,
      "learning_rate": 0.0012776430973305915,
      "loss": 1.1296,
      "step": 3211
    },
    {
      "epoch": 4.282666666666667,
      "grad_norm": 0.028266392648220062,
      "learning_rate": 0.0012772282152749518,
      "loss": 1.2296,
      "step": 3212
    },
    {
      "epoch": 4.284,
      "grad_norm": 0.023666879162192345,
      "learning_rate": 0.0012768132815216173,
      "loss": 1.0776,
      "step": 3213
    },
    {
      "epoch": 4.285333333333333,
      "grad_norm": 0.03761229291558266,
      "learning_rate": 0.0012763982961479664,
      "loss": 0.8612,
      "step": 3214
    },
    {
      "epoch": 4.286666666666667,
      "grad_norm": 0.027878154069185257,
      "learning_rate": 0.001275983259231385,
      "loss": 1.1516,
      "step": 3215
    },
    {
      "epoch": 4.288,
      "grad_norm": 0.022090593352913857,
      "learning_rate": 0.0012755681708492696,
      "loss": 0.9281,
      "step": 3216
    },
    {
      "epoch": 4.289333333333333,
      "grad_norm": 0.03187411278486252,
      "learning_rate": 0.0012751530310790263,
      "loss": 1.1291,
      "step": 3217
    },
    {
      "epoch": 4.290666666666667,
      "grad_norm": 0.02906365506350994,
      "learning_rate": 0.0012747378399980704,
      "loss": 1.0589,
      "step": 3218
    },
    {
      "epoch": 4.292,
      "grad_norm": 0.035461001098155975,
      "learning_rate": 0.0012743225976838276,
      "loss": 1.0147,
      "step": 3219
    },
    {
      "epoch": 4.293333333333333,
      "grad_norm": 0.02728424407541752,
      "learning_rate": 0.001273907304213732,
      "loss": 0.9712,
      "step": 3220
    },
    {
      "epoch": 4.294666666666667,
      "grad_norm": 0.035162586718797684,
      "learning_rate": 0.0012734919596652278,
      "loss": 1.0361,
      "step": 3221
    },
    {
      "epoch": 4.296,
      "grad_norm": 0.02343386597931385,
      "learning_rate": 0.0012730765641157688,
      "loss": 1.0524,
      "step": 3222
    },
    {
      "epoch": 4.2973333333333334,
      "grad_norm": 0.020932631567120552,
      "learning_rate": 0.0012726611176428182,
      "loss": 0.9094,
      "step": 3223
    },
    {
      "epoch": 4.298666666666667,
      "grad_norm": 0.02456033043563366,
      "learning_rate": 0.001272245620323849,
      "loss": 0.75,
      "step": 3224
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.026217175647616386,
      "learning_rate": 0.001271830072236343,
      "loss": 0.978,
      "step": 3225
    },
    {
      "epoch": 4.301333333333333,
      "grad_norm": 0.027500445023179054,
      "learning_rate": 0.001271414473457792,
      "loss": 1.0211,
      "step": 3226
    },
    {
      "epoch": 4.302666666666667,
      "grad_norm": 0.02492661215364933,
      "learning_rate": 0.0012709988240656972,
      "loss": 1.1714,
      "step": 3227
    },
    {
      "epoch": 4.304,
      "grad_norm": 0.038713086396455765,
      "learning_rate": 0.0012705831241375694,
      "loss": 1.1554,
      "step": 3228
    },
    {
      "epoch": 4.3053333333333335,
      "grad_norm": 0.037393033504486084,
      "learning_rate": 0.001270167373750928,
      "loss": 1.2549,
      "step": 3229
    },
    {
      "epoch": 4.306666666666667,
      "grad_norm": 0.03146728500723839,
      "learning_rate": 0.0012697515729833031,
      "loss": 1.1676,
      "step": 3230
    },
    {
      "epoch": 4.308,
      "grad_norm": 0.03193884715437889,
      "learning_rate": 0.001269335721912233,
      "loss": 1.2169,
      "step": 3231
    },
    {
      "epoch": 4.309333333333333,
      "grad_norm": 0.028902899473905563,
      "learning_rate": 0.0012689198206152657,
      "loss": 0.8332,
      "step": 3232
    },
    {
      "epoch": 4.310666666666666,
      "grad_norm": 0.03490158170461655,
      "learning_rate": 0.0012685038691699596,
      "loss": 1.0121,
      "step": 3233
    },
    {
      "epoch": 4.312,
      "grad_norm": 0.026445206254720688,
      "learning_rate": 0.0012680878676538804,
      "loss": 1.0784,
      "step": 3234
    },
    {
      "epoch": 4.3133333333333335,
      "grad_norm": 0.023235704749822617,
      "learning_rate": 0.0012676718161446052,
      "loss": 1.0426,
      "step": 3235
    },
    {
      "epoch": 4.314666666666667,
      "grad_norm": 0.027786336839199066,
      "learning_rate": 0.0012672557147197195,
      "loss": 1.0226,
      "step": 3236
    },
    {
      "epoch": 4.316,
      "grad_norm": 0.03862303867936134,
      "learning_rate": 0.0012668395634568175,
      "loss": 0.9985,
      "step": 3237
    },
    {
      "epoch": 4.317333333333333,
      "grad_norm": 0.12453621625900269,
      "learning_rate": 0.001266423362433504,
      "loss": 1.2017,
      "step": 3238
    },
    {
      "epoch": 4.318666666666667,
      "grad_norm": 0.03180398792028427,
      "learning_rate": 0.0012660071117273925,
      "loss": 1.0487,
      "step": 3239
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.027212079614400864,
      "learning_rate": 0.0012655908114161051,
      "loss": 0.9833,
      "step": 3240
    },
    {
      "epoch": 4.3213333333333335,
      "grad_norm": 0.03280707076191902,
      "learning_rate": 0.0012651744615772744,
      "loss": 0.9474,
      "step": 3241
    },
    {
      "epoch": 4.322666666666667,
      "grad_norm": 0.03324853628873825,
      "learning_rate": 0.001264758062288541,
      "loss": 0.9935,
      "step": 3242
    },
    {
      "epoch": 4.324,
      "grad_norm": 0.022868262603878975,
      "learning_rate": 0.0012643416136275557,
      "loss": 1.1571,
      "step": 3243
    },
    {
      "epoch": 4.325333333333333,
      "grad_norm": 0.03202250599861145,
      "learning_rate": 0.0012639251156719782,
      "loss": 0.9349,
      "step": 3244
    },
    {
      "epoch": 4.326666666666666,
      "grad_norm": 0.030280983075499535,
      "learning_rate": 0.0012635085684994767,
      "loss": 1.0899,
      "step": 3245
    },
    {
      "epoch": 4.328,
      "grad_norm": 0.03425484150648117,
      "learning_rate": 0.0012630919721877299,
      "loss": 1.0981,
      "step": 3246
    },
    {
      "epoch": 4.3293333333333335,
      "grad_norm": 0.03184516727924347,
      "learning_rate": 0.0012626753268144243,
      "loss": 1.0982,
      "step": 3247
    },
    {
      "epoch": 4.330666666666667,
      "grad_norm": 0.028159456327557564,
      "learning_rate": 0.0012622586324572565,
      "loss": 1.2478,
      "step": 3248
    },
    {
      "epoch": 4.332,
      "grad_norm": 0.027980862185359,
      "learning_rate": 0.0012618418891939321,
      "loss": 1.1071,
      "step": 3249
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.02374376356601715,
      "learning_rate": 0.0012614250971021658,
      "loss": 0.9201,
      "step": 3250
    },
    {
      "epoch": 4.334666666666667,
      "grad_norm": 0.02141287736594677,
      "learning_rate": 0.0012610082562596805,
      "loss": 0.7963,
      "step": 3251
    },
    {
      "epoch": 4.336,
      "grad_norm": 0.041904155164957047,
      "learning_rate": 0.0012605913667442096,
      "loss": 1.105,
      "step": 3252
    },
    {
      "epoch": 4.3373333333333335,
      "grad_norm": 0.023593297228217125,
      "learning_rate": 0.0012601744286334943,
      "loss": 0.9436,
      "step": 3253
    },
    {
      "epoch": 4.338666666666667,
      "grad_norm": 0.028508387506008148,
      "learning_rate": 0.0012597574420052861,
      "loss": 0.8763,
      "step": 3254
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.0236639566719532,
      "learning_rate": 0.001259340406937345,
      "loss": 0.859,
      "step": 3255
    },
    {
      "epoch": 4.341333333333333,
      "grad_norm": 0.02600918710231781,
      "learning_rate": 0.0012589233235074397,
      "loss": 1.1246,
      "step": 3256
    },
    {
      "epoch": 4.342666666666666,
      "grad_norm": 0.02629609778523445,
      "learning_rate": 0.001258506191793348,
      "loss": 0.7298,
      "step": 3257
    },
    {
      "epoch": 4.344,
      "grad_norm": 0.03271913155913353,
      "learning_rate": 0.001258089011872857,
      "loss": 1.1387,
      "step": 3258
    },
    {
      "epoch": 4.3453333333333335,
      "grad_norm": 0.02803007699549198,
      "learning_rate": 0.0012576717838237628,
      "loss": 1.1284,
      "step": 3259
    },
    {
      "epoch": 4.346666666666667,
      "grad_norm": 0.02295846678316593,
      "learning_rate": 0.00125725450772387,
      "loss": 1.0829,
      "step": 3260
    },
    {
      "epoch": 4.348,
      "grad_norm": 0.025099294260144234,
      "learning_rate": 0.0012568371836509935,
      "loss": 1.0139,
      "step": 3261
    },
    {
      "epoch": 4.349333333333333,
      "grad_norm": 0.028014803305268288,
      "learning_rate": 0.0012564198116829554,
      "loss": 1.345,
      "step": 3262
    },
    {
      "epoch": 4.350666666666667,
      "grad_norm": 0.03566683828830719,
      "learning_rate": 0.0012560023918975869,
      "loss": 1.0411,
      "step": 3263
    },
    {
      "epoch": 4.352,
      "grad_norm": 0.025789540261030197,
      "learning_rate": 0.0012555849243727297,
      "loss": 0.9498,
      "step": 3264
    },
    {
      "epoch": 4.3533333333333335,
      "grad_norm": 0.02750864066183567,
      "learning_rate": 0.001255167409186233,
      "loss": 1.1064,
      "step": 3265
    },
    {
      "epoch": 4.354666666666667,
      "grad_norm": 0.025751324370503426,
      "learning_rate": 0.0012547498464159557,
      "loss": 1.0843,
      "step": 3266
    },
    {
      "epoch": 4.356,
      "grad_norm": 0.03269180655479431,
      "learning_rate": 0.0012543322361397646,
      "loss": 0.8916,
      "step": 3267
    },
    {
      "epoch": 4.357333333333333,
      "grad_norm": 0.03625980019569397,
      "learning_rate": 0.001253914578435536,
      "loss": 0.9317,
      "step": 3268
    },
    {
      "epoch": 4.358666666666666,
      "grad_norm": 0.031151190400123596,
      "learning_rate": 0.0012534968733811554,
      "loss": 1.1335,
      "step": 3269
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.03453875705599785,
      "learning_rate": 0.0012530791210545163,
      "loss": 1.0049,
      "step": 3270
    },
    {
      "epoch": 4.3613333333333335,
      "grad_norm": 0.026621224358677864,
      "learning_rate": 0.0012526613215335216,
      "loss": 1.2248,
      "step": 3271
    },
    {
      "epoch": 4.362666666666667,
      "grad_norm": 0.026359299197793007,
      "learning_rate": 0.0012522434748960829,
      "loss": 1.2918,
      "step": 3272
    },
    {
      "epoch": 4.364,
      "grad_norm": 0.0398893766105175,
      "learning_rate": 0.0012518255812201202,
      "loss": 1.0635,
      "step": 3273
    },
    {
      "epoch": 4.365333333333333,
      "grad_norm": 0.0387939028441906,
      "learning_rate": 0.0012514076405835631,
      "loss": 1.1389,
      "step": 3274
    },
    {
      "epoch": 4.366666666666666,
      "grad_norm": 0.02936370112001896,
      "learning_rate": 0.0012509896530643488,
      "loss": 0.8609,
      "step": 3275
    },
    {
      "epoch": 4.368,
      "grad_norm": 0.030349906533956528,
      "learning_rate": 0.0012505716187404241,
      "loss": 1.0791,
      "step": 3276
    },
    {
      "epoch": 4.3693333333333335,
      "grad_norm": 0.048799220472574234,
      "learning_rate": 0.0012501535376897449,
      "loss": 1.0037,
      "step": 3277
    },
    {
      "epoch": 4.370666666666667,
      "grad_norm": 0.03137940168380737,
      "learning_rate": 0.0012497354099902745,
      "loss": 1.3562,
      "step": 3278
    },
    {
      "epoch": 4.372,
      "grad_norm": 0.029004432260990143,
      "learning_rate": 0.0012493172357199856,
      "loss": 1.0375,
      "step": 3279
    },
    {
      "epoch": 4.373333333333333,
      "grad_norm": 0.036725837737321854,
      "learning_rate": 0.00124889901495686,
      "loss": 1.3965,
      "step": 3280
    },
    {
      "epoch": 4.374666666666666,
      "grad_norm": 0.02843267098069191,
      "learning_rate": 0.0012484807477788876,
      "loss": 1.0764,
      "step": 3281
    },
    {
      "epoch": 4.376,
      "grad_norm": 0.030801404267549515,
      "learning_rate": 0.0012480624342640673,
      "loss": 1.0693,
      "step": 3282
    },
    {
      "epoch": 4.3773333333333335,
      "grad_norm": 0.03076142445206642,
      "learning_rate": 0.001247644074490406,
      "loss": 0.909,
      "step": 3283
    },
    {
      "epoch": 4.378666666666667,
      "grad_norm": 0.027165884152054787,
      "learning_rate": 0.0012472256685359202,
      "loss": 0.9028,
      "step": 3284
    },
    {
      "epoch": 4.38,
      "grad_norm": 0.02912355400621891,
      "learning_rate": 0.0012468072164786342,
      "loss": 1.0782,
      "step": 3285
    },
    {
      "epoch": 4.381333333333333,
      "grad_norm": 0.03283600136637688,
      "learning_rate": 0.0012463887183965813,
      "loss": 1.0498,
      "step": 3286
    },
    {
      "epoch": 4.382666666666666,
      "grad_norm": 0.03557934984564781,
      "learning_rate": 0.0012459701743678035,
      "loss": 0.9626,
      "step": 3287
    },
    {
      "epoch": 4.384,
      "grad_norm": 0.03368949517607689,
      "learning_rate": 0.0012455515844703511,
      "loss": 1.2265,
      "step": 3288
    },
    {
      "epoch": 4.3853333333333335,
      "grad_norm": 0.028427690267562866,
      "learning_rate": 0.001245132948782283,
      "loss": 1.1261,
      "step": 3289
    },
    {
      "epoch": 4.386666666666667,
      "grad_norm": 0.02748897485435009,
      "learning_rate": 0.0012447142673816661,
      "loss": 0.9583,
      "step": 3290
    },
    {
      "epoch": 4.388,
      "grad_norm": 0.02822616882622242,
      "learning_rate": 0.0012442955403465768,
      "loss": 1.2642,
      "step": 3291
    },
    {
      "epoch": 4.389333333333333,
      "grad_norm": 0.03472274914383888,
      "learning_rate": 0.0012438767677550998,
      "loss": 1.087,
      "step": 3292
    },
    {
      "epoch": 4.390666666666666,
      "grad_norm": 0.03276127576828003,
      "learning_rate": 0.0012434579496853277,
      "loss": 0.9985,
      "step": 3293
    },
    {
      "epoch": 4.392,
      "grad_norm": 0.02558831125497818,
      "learning_rate": 0.0012430390862153624,
      "loss": 0.8695,
      "step": 3294
    },
    {
      "epoch": 4.3933333333333335,
      "grad_norm": 0.032378505915403366,
      "learning_rate": 0.0012426201774233135,
      "loss": 0.9719,
      "step": 3295
    },
    {
      "epoch": 4.394666666666667,
      "grad_norm": 0.024448424577713013,
      "learning_rate": 0.0012422012233872992,
      "loss": 1.004,
      "step": 3296
    },
    {
      "epoch": 4.396,
      "grad_norm": 0.02152126654982567,
      "learning_rate": 0.0012417822241854466,
      "loss": 0.8976,
      "step": 3297
    },
    {
      "epoch": 4.397333333333333,
      "grad_norm": 0.02792074903845787,
      "learning_rate": 0.0012413631798958913,
      "loss": 0.8663,
      "step": 3298
    },
    {
      "epoch": 4.398666666666666,
      "grad_norm": 0.02581491693854332,
      "learning_rate": 0.0012409440905967763,
      "loss": 1.1344,
      "step": 3299
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.02895808406174183,
      "learning_rate": 0.0012405249563662538,
      "loss": 0.998,
      "step": 3300
    },
    {
      "epoch": 4.4013333333333335,
      "grad_norm": 0.026125263422727585,
      "learning_rate": 0.0012401057772824843,
      "loss": 0.9338,
      "step": 3301
    },
    {
      "epoch": 4.402666666666667,
      "grad_norm": 0.025808410719037056,
      "learning_rate": 0.0012396865534236364,
      "loss": 0.9725,
      "step": 3302
    },
    {
      "epoch": 4.404,
      "grad_norm": 0.02994222193956375,
      "learning_rate": 0.0012392672848678876,
      "loss": 0.8411,
      "step": 3303
    },
    {
      "epoch": 4.405333333333333,
      "grad_norm": 0.04165583848953247,
      "learning_rate": 0.0012388479716934234,
      "loss": 1.3317,
      "step": 3304
    },
    {
      "epoch": 4.406666666666666,
      "grad_norm": 0.03200298547744751,
      "learning_rate": 0.001238428613978437,
      "loss": 1.3723,
      "step": 3305
    },
    {
      "epoch": 4.408,
      "grad_norm": 0.05097039416432381,
      "learning_rate": 0.001238009211801131,
      "loss": 0.8788,
      "step": 3306
    },
    {
      "epoch": 4.4093333333333335,
      "grad_norm": 0.03459448367357254,
      "learning_rate": 0.0012375897652397157,
      "loss": 0.9646,
      "step": 3307
    },
    {
      "epoch": 4.410666666666667,
      "grad_norm": 0.051005903631448746,
      "learning_rate": 0.0012371702743724095,
      "loss": 0.9414,
      "step": 3308
    },
    {
      "epoch": 4.412,
      "grad_norm": 0.02386271394789219,
      "learning_rate": 0.0012367507392774398,
      "loss": 0.9914,
      "step": 3309
    },
    {
      "epoch": 4.413333333333333,
      "grad_norm": 0.027622796595096588,
      "learning_rate": 0.0012363311600330413,
      "loss": 0.9498,
      "step": 3310
    },
    {
      "epoch": 4.414666666666666,
      "grad_norm": 0.07596044987440109,
      "learning_rate": 0.0012359115367174577,
      "loss": 0.9696,
      "step": 3311
    },
    {
      "epoch": 4.416,
      "grad_norm": 0.03223287686705589,
      "learning_rate": 0.0012354918694089405,
      "loss": 1.0544,
      "step": 3312
    },
    {
      "epoch": 4.417333333333334,
      "grad_norm": 0.03153421729803085,
      "learning_rate": 0.0012350721581857497,
      "loss": 0.9536,
      "step": 3313
    },
    {
      "epoch": 4.418666666666667,
      "grad_norm": 0.029814645648002625,
      "learning_rate": 0.001234652403126153,
      "loss": 0.9989,
      "step": 3314
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.02611769363284111,
      "learning_rate": 0.0012342326043084267,
      "loss": 0.8902,
      "step": 3315
    },
    {
      "epoch": 4.421333333333333,
      "grad_norm": 0.028192568570375443,
      "learning_rate": 0.0012338127618108554,
      "loss": 1.1659,
      "step": 3316
    },
    {
      "epoch": 4.422666666666666,
      "grad_norm": 0.03533952310681343,
      "learning_rate": 0.0012333928757117312,
      "loss": 0.9625,
      "step": 3317
    },
    {
      "epoch": 4.424,
      "grad_norm": 0.040835749357938766,
      "learning_rate": 0.001232972946089355,
      "loss": 1.3424,
      "step": 3318
    },
    {
      "epoch": 4.425333333333334,
      "grad_norm": 0.054496221244335175,
      "learning_rate": 0.0012325529730220358,
      "loss": 0.7925,
      "step": 3319
    },
    {
      "epoch": 4.426666666666667,
      "grad_norm": 0.02903948724269867,
      "learning_rate": 0.0012321329565880897,
      "loss": 0.88,
      "step": 3320
    },
    {
      "epoch": 4.428,
      "grad_norm": 0.033603545278310776,
      "learning_rate": 0.0012317128968658425,
      "loss": 1.0322,
      "step": 3321
    },
    {
      "epoch": 4.429333333333333,
      "grad_norm": 0.10573505610227585,
      "learning_rate": 0.0012312927939336262,
      "loss": 1.193,
      "step": 3322
    },
    {
      "epoch": 4.430666666666666,
      "grad_norm": 0.030146922916173935,
      "learning_rate": 0.0012308726478697826,
      "loss": 0.9912,
      "step": 3323
    },
    {
      "epoch": 4.432,
      "grad_norm": 0.029167702421545982,
      "learning_rate": 0.0012304524587526608,
      "loss": 0.9022,
      "step": 3324
    },
    {
      "epoch": 4.433333333333334,
      "grad_norm": 0.02803642302751541,
      "learning_rate": 0.0012300322266606176,
      "loss": 0.9819,
      "step": 3325
    },
    {
      "epoch": 4.434666666666667,
      "grad_norm": 0.030091550201177597,
      "learning_rate": 0.0012296119516720186,
      "loss": 0.8462,
      "step": 3326
    },
    {
      "epoch": 4.436,
      "grad_norm": 0.03194649890065193,
      "learning_rate": 0.0012291916338652365,
      "loss": 0.9101,
      "step": 3327
    },
    {
      "epoch": 4.437333333333333,
      "grad_norm": 0.027332238852977753,
      "learning_rate": 0.0012287712733186525,
      "loss": 1.008,
      "step": 3328
    },
    {
      "epoch": 4.438666666666666,
      "grad_norm": 0.0274798683822155,
      "learning_rate": 0.0012283508701106558,
      "loss": 1.0309,
      "step": 3329
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.06588251143693924,
      "learning_rate": 0.0012279304243196436,
      "loss": 1.0867,
      "step": 3330
    },
    {
      "epoch": 4.441333333333334,
      "grad_norm": 0.02689223736524582,
      "learning_rate": 0.0012275099360240206,
      "loss": 1.1556,
      "step": 3331
    },
    {
      "epoch": 4.442666666666667,
      "grad_norm": 0.022636709734797478,
      "learning_rate": 0.0012270894053022,
      "loss": 0.975,
      "step": 3332
    },
    {
      "epoch": 4.444,
      "grad_norm": 0.03135747089982033,
      "learning_rate": 0.0012266688322326023,
      "loss": 0.9843,
      "step": 3333
    },
    {
      "epoch": 4.445333333333333,
      "grad_norm": 0.023270701989531517,
      "learning_rate": 0.0012262482168936563,
      "loss": 1.0762,
      "step": 3334
    },
    {
      "epoch": 4.446666666666666,
      "grad_norm": 0.022167084738612175,
      "learning_rate": 0.0012258275593637993,
      "loss": 0.8085,
      "step": 3335
    },
    {
      "epoch": 4.448,
      "grad_norm": 0.03219659999012947,
      "learning_rate": 0.0012254068597214748,
      "loss": 0.9121,
      "step": 3336
    },
    {
      "epoch": 4.449333333333334,
      "grad_norm": 0.02547997608780861,
      "learning_rate": 0.0012249861180451361,
      "loss": 1.3156,
      "step": 3337
    },
    {
      "epoch": 4.450666666666667,
      "grad_norm": 0.03034556843340397,
      "learning_rate": 0.0012245653344132423,
      "loss": 0.8345,
      "step": 3338
    },
    {
      "epoch": 4.452,
      "grad_norm": 0.022542398422956467,
      "learning_rate": 0.0012241445089042622,
      "loss": 0.9901,
      "step": 3339
    },
    {
      "epoch": 4.453333333333333,
      "grad_norm": 0.03112620860338211,
      "learning_rate": 0.0012237236415966715,
      "loss": 0.9363,
      "step": 3340
    },
    {
      "epoch": 4.454666666666666,
      "grad_norm": 0.04058670252561569,
      "learning_rate": 0.0012233027325689533,
      "loss": 1.0442,
      "step": 3341
    },
    {
      "epoch": 4.456,
      "grad_norm": 0.024098174646496773,
      "learning_rate": 0.0012228817818995996,
      "loss": 0.9663,
      "step": 3342
    },
    {
      "epoch": 4.457333333333334,
      "grad_norm": 0.028402473777532578,
      "learning_rate": 0.0012224607896671093,
      "loss": 1.0608,
      "step": 3343
    },
    {
      "epoch": 4.458666666666667,
      "grad_norm": 0.024542002007365227,
      "learning_rate": 0.0012220397559499893,
      "loss": 1.1206,
      "step": 3344
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.02692682296037674,
      "learning_rate": 0.0012216186808267544,
      "loss": 0.8112,
      "step": 3345
    },
    {
      "epoch": 4.461333333333333,
      "grad_norm": 0.030193421989679337,
      "learning_rate": 0.0012211975643759265,
      "loss": 0.964,
      "step": 3346
    },
    {
      "epoch": 4.462666666666666,
      "grad_norm": 0.07800131291151047,
      "learning_rate": 0.001220776406676036,
      "loss": 1.6289,
      "step": 3347
    },
    {
      "epoch": 4.464,
      "grad_norm": 0.02847939170897007,
      "learning_rate": 0.0012203552078056208,
      "loss": 1.0554,
      "step": 3348
    },
    {
      "epoch": 4.465333333333334,
      "grad_norm": 0.02425128035247326,
      "learning_rate": 0.001219933967843226,
      "loss": 1.2731,
      "step": 3349
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 0.027269534766674042,
      "learning_rate": 0.001219512686867405,
      "loss": 0.896,
      "step": 3350
    },
    {
      "epoch": 4.468,
      "grad_norm": 0.03894870728254318,
      "learning_rate": 0.0012190913649567183,
      "loss": 0.7767,
      "step": 3351
    },
    {
      "epoch": 4.469333333333333,
      "grad_norm": 0.024738330394029617,
      "learning_rate": 0.0012186700021897341,
      "loss": 0.9223,
      "step": 3352
    },
    {
      "epoch": 4.470666666666666,
      "grad_norm": 0.03278455138206482,
      "learning_rate": 0.0012182485986450288,
      "loss": 1.1331,
      "step": 3353
    },
    {
      "epoch": 4.4719999999999995,
      "grad_norm": 0.02228493057191372,
      "learning_rate": 0.0012178271544011864,
      "loss": 0.8452,
      "step": 3354
    },
    {
      "epoch": 4.473333333333334,
      "grad_norm": 0.027597475796937943,
      "learning_rate": 0.0012174056695367968,
      "loss": 1.1156,
      "step": 3355
    },
    {
      "epoch": 4.474666666666667,
      "grad_norm": 0.026550645008683205,
      "learning_rate": 0.0012169841441304601,
      "loss": 0.974,
      "step": 3356
    },
    {
      "epoch": 4.476,
      "grad_norm": 0.0260333139449358,
      "learning_rate": 0.0012165625782607817,
      "loss": 1.1495,
      "step": 3357
    },
    {
      "epoch": 4.477333333333333,
      "grad_norm": 0.027454571798443794,
      "learning_rate": 0.0012161409720063759,
      "loss": 1.2436,
      "step": 3358
    },
    {
      "epoch": 4.478666666666666,
      "grad_norm": 0.02829020284116268,
      "learning_rate": 0.0012157193254458641,
      "loss": 1.0587,
      "step": 3359
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.025069372728466988,
      "learning_rate": 0.0012152976386578748,
      "loss": 0.9079,
      "step": 3360
    },
    {
      "epoch": 4.481333333333334,
      "grad_norm": 0.02386137843132019,
      "learning_rate": 0.0012148759117210453,
      "loss": 0.9523,
      "step": 3361
    },
    {
      "epoch": 4.482666666666667,
      "grad_norm": 0.027084706351161003,
      "learning_rate": 0.0012144541447140188,
      "loss": 1.0631,
      "step": 3362
    },
    {
      "epoch": 4.484,
      "grad_norm": 0.022583922371268272,
      "learning_rate": 0.0012140323377154466,
      "loss": 0.9016,
      "step": 3363
    },
    {
      "epoch": 4.485333333333333,
      "grad_norm": 0.017949435859918594,
      "learning_rate": 0.0012136104908039877,
      "loss": 1.1095,
      "step": 3364
    },
    {
      "epoch": 4.486666666666666,
      "grad_norm": 0.03991657495498657,
      "learning_rate": 0.0012131886040583088,
      "loss": 0.9316,
      "step": 3365
    },
    {
      "epoch": 4.4879999999999995,
      "grad_norm": 0.03174889087677002,
      "learning_rate": 0.0012127666775570837,
      "loss": 0.9858,
      "step": 3366
    },
    {
      "epoch": 4.489333333333334,
      "grad_norm": 0.029920553788542747,
      "learning_rate": 0.0012123447113789926,
      "loss": 0.9175,
      "step": 3367
    },
    {
      "epoch": 4.490666666666667,
      "grad_norm": 0.03383979573845863,
      "learning_rate": 0.0012119227056027243,
      "loss": 1.0401,
      "step": 3368
    },
    {
      "epoch": 4.492,
      "grad_norm": 0.035804230719804764,
      "learning_rate": 0.001211500660306975,
      "loss": 0.8347,
      "step": 3369
    },
    {
      "epoch": 4.493333333333333,
      "grad_norm": 0.031829968094825745,
      "learning_rate": 0.001211078575570448,
      "loss": 1.2012,
      "step": 3370
    },
    {
      "epoch": 4.494666666666666,
      "grad_norm": 0.02944403514266014,
      "learning_rate": 0.0012106564514718543,
      "loss": 1.0561,
      "step": 3371
    },
    {
      "epoch": 4.496,
      "grad_norm": 0.032104965299367905,
      "learning_rate": 0.0012102342880899108,
      "loss": 1.235,
      "step": 3372
    },
    {
      "epoch": 4.497333333333334,
      "grad_norm": 0.029721850529313087,
      "learning_rate": 0.0012098120855033433,
      "loss": 1.3136,
      "step": 3373
    },
    {
      "epoch": 4.498666666666667,
      "grad_norm": 0.03964383900165558,
      "learning_rate": 0.001209389843790885,
      "loss": 1.0545,
      "step": 3374
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.02726561389863491,
      "learning_rate": 0.0012089675630312753,
      "loss": 0.9189,
      "step": 3375
    },
    {
      "epoch": 4.501333333333333,
      "grad_norm": 0.03451792895793915,
      "learning_rate": 0.001208545243303261,
      "loss": 1.1335,
      "step": 3376
    },
    {
      "epoch": 4.502666666666666,
      "grad_norm": 0.0299364160746336,
      "learning_rate": 0.0012081228846855974,
      "loss": 0.9852,
      "step": 3377
    },
    {
      "epoch": 4.504,
      "grad_norm": 0.023762231692671776,
      "learning_rate": 0.0012077004872570454,
      "loss": 0.9537,
      "step": 3378
    },
    {
      "epoch": 4.505333333333334,
      "grad_norm": 0.023010287433862686,
      "learning_rate": 0.0012072780510963743,
      "loss": 0.8216,
      "step": 3379
    },
    {
      "epoch": 4.506666666666667,
      "grad_norm": 0.025149032473564148,
      "learning_rate": 0.0012068555762823603,
      "loss": 1.2124,
      "step": 3380
    },
    {
      "epoch": 4.508,
      "grad_norm": 0.028303327038884163,
      "learning_rate": 0.001206433062893787,
      "loss": 0.7577,
      "step": 3381
    },
    {
      "epoch": 4.509333333333333,
      "grad_norm": 0.02588757872581482,
      "learning_rate": 0.0012060105110094446,
      "loss": 0.9177,
      "step": 3382
    },
    {
      "epoch": 4.510666666666666,
      "grad_norm": 0.19916829466819763,
      "learning_rate": 0.0012055879207081305,
      "loss": 1.0156,
      "step": 3383
    },
    {
      "epoch": 4.5120000000000005,
      "grad_norm": 0.026822609826922417,
      "learning_rate": 0.0012051652920686505,
      "loss": 1.0,
      "step": 3384
    },
    {
      "epoch": 4.513333333333334,
      "grad_norm": 0.026150241494178772,
      "learning_rate": 0.0012047426251698158,
      "loss": 1.1263,
      "step": 3385
    },
    {
      "epoch": 4.514666666666667,
      "grad_norm": 0.029824480414390564,
      "learning_rate": 0.0012043199200904467,
      "loss": 1.1648,
      "step": 3386
    },
    {
      "epoch": 4.516,
      "grad_norm": 0.024497905746102333,
      "learning_rate": 0.0012038971769093684,
      "loss": 1.2103,
      "step": 3387
    },
    {
      "epoch": 4.517333333333333,
      "grad_norm": 0.024398328736424446,
      "learning_rate": 0.0012034743957054146,
      "loss": 0.6485,
      "step": 3388
    },
    {
      "epoch": 4.518666666666666,
      "grad_norm": 0.032462891191244125,
      "learning_rate": 0.001203051576557426,
      "loss": 1.3121,
      "step": 3389
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.022356653586030006,
      "learning_rate": 0.0012026287195442502,
      "loss": 1.0977,
      "step": 3390
    },
    {
      "epoch": 4.521333333333334,
      "grad_norm": 0.02740558423101902,
      "learning_rate": 0.0012022058247447417,
      "loss": 1.0882,
      "step": 3391
    },
    {
      "epoch": 4.522666666666667,
      "grad_norm": 0.03635826334357262,
      "learning_rate": 0.0012017828922377624,
      "loss": 0.9813,
      "step": 3392
    },
    {
      "epoch": 4.524,
      "grad_norm": 0.02709958329796791,
      "learning_rate": 0.001201359922102181,
      "loss": 1.1845,
      "step": 3393
    },
    {
      "epoch": 4.525333333333333,
      "grad_norm": 0.022401170805096626,
      "learning_rate": 0.0012009369144168726,
      "loss": 0.9472,
      "step": 3394
    },
    {
      "epoch": 4.526666666666666,
      "grad_norm": 0.025319017469882965,
      "learning_rate": 0.001200513869260721,
      "loss": 1.1717,
      "step": 3395
    },
    {
      "epoch": 4.5280000000000005,
      "grad_norm": 0.046704791486263275,
      "learning_rate": 0.001200090786712615,
      "loss": 0.9447,
      "step": 3396
    },
    {
      "epoch": 4.529333333333334,
      "grad_norm": 0.023830745369195938,
      "learning_rate": 0.0011996676668514518,
      "loss": 1.3227,
      "step": 3397
    },
    {
      "epoch": 4.530666666666667,
      "grad_norm": 0.02132515050470829,
      "learning_rate": 0.0011992445097561352,
      "loss": 1.0178,
      "step": 3398
    },
    {
      "epoch": 4.532,
      "grad_norm": 0.021348172798752785,
      "learning_rate": 0.0011988213155055753,
      "loss": 0.9806,
      "step": 3399
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 0.02651067078113556,
      "learning_rate": 0.0011983980841786899,
      "loss": 0.851,
      "step": 3400
    },
    {
      "epoch": 4.534666666666666,
      "grad_norm": 0.03147081285715103,
      "learning_rate": 0.0011979748158544034,
      "loss": 1.0227,
      "step": 3401
    },
    {
      "epoch": 4.536,
      "grad_norm": 0.05606750398874283,
      "learning_rate": 0.0011975515106116472,
      "loss": 1.0765,
      "step": 3402
    },
    {
      "epoch": 4.537333333333334,
      "grad_norm": 0.024605179205536842,
      "learning_rate": 0.0011971281685293597,
      "loss": 1.0048,
      "step": 3403
    },
    {
      "epoch": 4.538666666666667,
      "grad_norm": 0.033438026905059814,
      "learning_rate": 0.001196704789686486,
      "loss": 0.7705,
      "step": 3404
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.022314466536045074,
      "learning_rate": 0.0011962813741619777,
      "loss": 0.928,
      "step": 3405
    },
    {
      "epoch": 4.541333333333333,
      "grad_norm": 0.03441733866930008,
      "learning_rate": 0.0011958579220347936,
      "loss": 1.3067,
      "step": 3406
    },
    {
      "epoch": 4.542666666666666,
      "grad_norm": 0.025694146752357483,
      "learning_rate": 0.0011954344333838993,
      "loss": 0.8921,
      "step": 3407
    },
    {
      "epoch": 4.5440000000000005,
      "grad_norm": 0.027398040518164635,
      "learning_rate": 0.001195010908288268,
      "loss": 1.0099,
      "step": 3408
    },
    {
      "epoch": 4.545333333333334,
      "grad_norm": 0.030747095122933388,
      "learning_rate": 0.0011945873468268782,
      "loss": 0.9818,
      "step": 3409
    },
    {
      "epoch": 4.546666666666667,
      "grad_norm": 0.030275581404566765,
      "learning_rate": 0.0011941637490787158,
      "loss": 1.2484,
      "step": 3410
    },
    {
      "epoch": 4.548,
      "grad_norm": 0.04703181982040405,
      "learning_rate": 0.001193740115122774,
      "loss": 1.1171,
      "step": 3411
    },
    {
      "epoch": 4.549333333333333,
      "grad_norm": 0.02838434837758541,
      "learning_rate": 0.001193316445038052,
      "loss": 1.1152,
      "step": 3412
    },
    {
      "epoch": 4.550666666666666,
      "grad_norm": 0.033250592648983,
      "learning_rate": 0.0011928927389035567,
      "loss": 1.1213,
      "step": 3413
    },
    {
      "epoch": 4.552,
      "grad_norm": 0.022625718265771866,
      "learning_rate": 0.0011924689967983005,
      "loss": 0.9446,
      "step": 3414
    },
    {
      "epoch": 4.553333333333334,
      "grad_norm": 0.039516109973192215,
      "learning_rate": 0.001192045218801303,
      "loss": 1.3532,
      "step": 3415
    },
    {
      "epoch": 4.554666666666667,
      "grad_norm": 0.023659054189920425,
      "learning_rate": 0.0011916214049915907,
      "loss": 1.029,
      "step": 3416
    },
    {
      "epoch": 4.556,
      "grad_norm": 0.034959085285663605,
      "learning_rate": 0.0011911975554481972,
      "loss": 1.1672,
      "step": 3417
    },
    {
      "epoch": 4.557333333333333,
      "grad_norm": 0.0259365476667881,
      "learning_rate": 0.0011907736702501618,
      "loss": 1.0833,
      "step": 3418
    },
    {
      "epoch": 4.558666666666666,
      "grad_norm": 0.025906464084982872,
      "learning_rate": 0.0011903497494765306,
      "loss": 1.0258,
      "step": 3419
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 0.06534271687269211,
      "learning_rate": 0.001189925793206357,
      "loss": 0.8787,
      "step": 3420
    },
    {
      "epoch": 4.561333333333334,
      "grad_norm": 0.02963532693684101,
      "learning_rate": 0.0011895018015187005,
      "loss": 1.1292,
      "step": 3421
    },
    {
      "epoch": 4.562666666666667,
      "grad_norm": 0.03447017818689346,
      "learning_rate": 0.0011890777744926276,
      "loss": 1.1688,
      "step": 3422
    },
    {
      "epoch": 4.564,
      "grad_norm": 0.03751631826162338,
      "learning_rate": 0.0011886537122072105,
      "loss": 1.1407,
      "step": 3423
    },
    {
      "epoch": 4.565333333333333,
      "grad_norm": 0.027064302936196327,
      "learning_rate": 0.0011882296147415297,
      "loss": 0.9737,
      "step": 3424
    },
    {
      "epoch": 4.566666666666666,
      "grad_norm": 0.03050038404762745,
      "learning_rate": 0.0011878054821746703,
      "loss": 0.998,
      "step": 3425
    },
    {
      "epoch": 4.568,
      "grad_norm": 0.026158222928643227,
      "learning_rate": 0.0011873813145857248,
      "loss": 0.9654,
      "step": 3426
    },
    {
      "epoch": 4.569333333333334,
      "grad_norm": 0.024835918098688126,
      "learning_rate": 0.0011869571120537922,
      "loss": 0.8436,
      "step": 3427
    },
    {
      "epoch": 4.570666666666667,
      "grad_norm": 0.024615449830889702,
      "learning_rate": 0.0011865328746579785,
      "loss": 1.0804,
      "step": 3428
    },
    {
      "epoch": 4.572,
      "grad_norm": 0.03259555250406265,
      "learning_rate": 0.001186108602477396,
      "loss": 1.2448,
      "step": 3429
    },
    {
      "epoch": 4.573333333333333,
      "grad_norm": 0.022113844752311707,
      "learning_rate": 0.0011856842955911624,
      "loss": 0.9552,
      "step": 3430
    },
    {
      "epoch": 4.574666666666666,
      "grad_norm": 0.02641960233449936,
      "learning_rate": 0.0011852599540784034,
      "loss": 0.8947,
      "step": 3431
    },
    {
      "epoch": 4.576,
      "grad_norm": 0.025040075182914734,
      "learning_rate": 0.00118483557801825,
      "loss": 0.9933,
      "step": 3432
    },
    {
      "epoch": 4.577333333333334,
      "grad_norm": 0.02666189894080162,
      "learning_rate": 0.0011844111674898404,
      "loss": 1.0716,
      "step": 3433
    },
    {
      "epoch": 4.578666666666667,
      "grad_norm": 0.032703425735235214,
      "learning_rate": 0.001183986722572319,
      "loss": 1.0794,
      "step": 3434
    },
    {
      "epoch": 4.58,
      "grad_norm": 0.02281036786735058,
      "learning_rate": 0.001183562243344836,
      "loss": 0.9662,
      "step": 3435
    },
    {
      "epoch": 4.581333333333333,
      "grad_norm": 0.01914812996983528,
      "learning_rate": 0.0011831377298865495,
      "loss": 1.1495,
      "step": 3436
    },
    {
      "epoch": 4.582666666666666,
      "grad_norm": 0.022439034655690193,
      "learning_rate": 0.0011827131822766223,
      "loss": 1.0497,
      "step": 3437
    },
    {
      "epoch": 4.584,
      "grad_norm": 0.02976105362176895,
      "learning_rate": 0.0011822886005942244,
      "loss": 0.892,
      "step": 3438
    },
    {
      "epoch": 4.585333333333334,
      "grad_norm": 0.028764646500349045,
      "learning_rate": 0.0011818639849185326,
      "loss": 1.0472,
      "step": 3439
    },
    {
      "epoch": 4.586666666666667,
      "grad_norm": 0.02863350883126259,
      "learning_rate": 0.0011814393353287288,
      "loss": 1.123,
      "step": 3440
    },
    {
      "epoch": 4.588,
      "grad_norm": 0.027182018384337425,
      "learning_rate": 0.0011810146519040021,
      "loss": 0.9045,
      "step": 3441
    },
    {
      "epoch": 4.589333333333333,
      "grad_norm": 0.03302939236164093,
      "learning_rate": 0.0011805899347235482,
      "loss": 1.0563,
      "step": 3442
    },
    {
      "epoch": 4.5906666666666665,
      "grad_norm": 0.027118315920233727,
      "learning_rate": 0.001180165183866568,
      "loss": 1.1229,
      "step": 3443
    },
    {
      "epoch": 4.592,
      "grad_norm": 0.023118535056710243,
      "learning_rate": 0.0011797403994122698,
      "loss": 0.818,
      "step": 3444
    },
    {
      "epoch": 4.593333333333334,
      "grad_norm": 0.033051278442144394,
      "learning_rate": 0.0011793155814398672,
      "loss": 0.9432,
      "step": 3445
    },
    {
      "epoch": 4.594666666666667,
      "grad_norm": 0.03590049222111702,
      "learning_rate": 0.0011788907300285811,
      "loss": 1.0964,
      "step": 3446
    },
    {
      "epoch": 4.596,
      "grad_norm": 0.02030702494084835,
      "learning_rate": 0.0011784658452576378,
      "loss": 0.9469,
      "step": 3447
    },
    {
      "epoch": 4.597333333333333,
      "grad_norm": 0.02928638830780983,
      "learning_rate": 0.0011780409272062699,
      "loss": 0.9002,
      "step": 3448
    },
    {
      "epoch": 4.5986666666666665,
      "grad_norm": 0.022028621286153793,
      "learning_rate": 0.0011776159759537164,
      "loss": 1.2412,
      "step": 3449
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.02708185836672783,
      "learning_rate": 0.0011771909915792229,
      "loss": 1.0998,
      "step": 3450
    },
    {
      "epoch": 4.601333333333334,
      "grad_norm": 0.027619436383247375,
      "learning_rate": 0.0011767659741620403,
      "loss": 1.1856,
      "step": 3451
    },
    {
      "epoch": 4.602666666666667,
      "grad_norm": 0.02645108476281166,
      "learning_rate": 0.0011763409237814265,
      "loss": 1.0179,
      "step": 3452
    },
    {
      "epoch": 4.604,
      "grad_norm": 0.021804608404636383,
      "learning_rate": 0.0011759158405166446,
      "loss": 0.6481,
      "step": 3453
    },
    {
      "epoch": 4.605333333333333,
      "grad_norm": 0.02467222511768341,
      "learning_rate": 0.0011754907244469648,
      "loss": 0.9529,
      "step": 3454
    },
    {
      "epoch": 4.6066666666666665,
      "grad_norm": 0.028130369260907173,
      "learning_rate": 0.0011750655756516634,
      "loss": 1.1898,
      "step": 3455
    },
    {
      "epoch": 4.608,
      "grad_norm": 0.025861069560050964,
      "learning_rate": 0.0011746403942100214,
      "loss": 0.8949,
      "step": 3456
    },
    {
      "epoch": 4.609333333333334,
      "grad_norm": 0.03784409165382385,
      "learning_rate": 0.001174215180201328,
      "loss": 1.1736,
      "step": 3457
    },
    {
      "epoch": 4.610666666666667,
      "grad_norm": 0.03079047240316868,
      "learning_rate": 0.0011737899337048766,
      "loss": 1.0222,
      "step": 3458
    },
    {
      "epoch": 4.612,
      "grad_norm": 0.031211359426379204,
      "learning_rate": 0.0011733646547999676,
      "loss": 0.9059,
      "step": 3459
    },
    {
      "epoch": 4.613333333333333,
      "grad_norm": 0.049424923956394196,
      "learning_rate": 0.0011729393435659077,
      "loss": 1.3257,
      "step": 3460
    },
    {
      "epoch": 4.6146666666666665,
      "grad_norm": 0.0404464453458786,
      "learning_rate": 0.0011725140000820088,
      "loss": 1.3524,
      "step": 3461
    },
    {
      "epoch": 4.616,
      "grad_norm": 0.028667425736784935,
      "learning_rate": 0.0011720886244275892,
      "loss": 1.062,
      "step": 3462
    },
    {
      "epoch": 4.617333333333333,
      "grad_norm": 0.028991620987653732,
      "learning_rate": 0.0011716632166819737,
      "loss": 1.0457,
      "step": 3463
    },
    {
      "epoch": 4.618666666666667,
      "grad_norm": 0.027455495670437813,
      "learning_rate": 0.0011712377769244916,
      "loss": 1.1345,
      "step": 3464
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.026697486639022827,
      "learning_rate": 0.0011708123052344803,
      "loss": 1.1042,
      "step": 3465
    },
    {
      "epoch": 4.621333333333333,
      "grad_norm": 0.03084820881485939,
      "learning_rate": 0.0011703868016912815,
      "loss": 0.9948,
      "step": 3466
    },
    {
      "epoch": 4.6226666666666665,
      "grad_norm": 0.031084708869457245,
      "learning_rate": 0.0011699612663742434,
      "loss": 1.0208,
      "step": 3467
    },
    {
      "epoch": 4.624,
      "grad_norm": 0.021523689851164818,
      "learning_rate": 0.0011695356993627203,
      "loss": 1.1334,
      "step": 3468
    },
    {
      "epoch": 4.625333333333334,
      "grad_norm": 0.027865760028362274,
      "learning_rate": 0.0011691101007360717,
      "loss": 1.2748,
      "step": 3469
    },
    {
      "epoch": 4.626666666666667,
      "grad_norm": 0.02734469063580036,
      "learning_rate": 0.0011686844705736641,
      "loss": 1.0882,
      "step": 3470
    },
    {
      "epoch": 4.628,
      "grad_norm": 0.033694613724946976,
      "learning_rate": 0.0011682588089548692,
      "loss": 1.0069,
      "step": 3471
    },
    {
      "epoch": 4.629333333333333,
      "grad_norm": 0.0255063995718956,
      "learning_rate": 0.001167833115959064,
      "loss": 0.9146,
      "step": 3472
    },
    {
      "epoch": 4.6306666666666665,
      "grad_norm": 0.021913910284638405,
      "learning_rate": 0.0011674073916656327,
      "loss": 1.0711,
      "step": 3473
    },
    {
      "epoch": 4.632,
      "grad_norm": 0.024669699370861053,
      "learning_rate": 0.0011669816361539647,
      "loss": 0.8473,
      "step": 3474
    },
    {
      "epoch": 4.633333333333333,
      "grad_norm": 0.02196997031569481,
      "learning_rate": 0.0011665558495034545,
      "loss": 1.0193,
      "step": 3475
    },
    {
      "epoch": 4.634666666666667,
      "grad_norm": 0.023546088486909866,
      "learning_rate": 0.0011661300317935038,
      "loss": 0.8591,
      "step": 3476
    },
    {
      "epoch": 4.636,
      "grad_norm": 0.027224570512771606,
      "learning_rate": 0.0011657041831035185,
      "loss": 1.1558,
      "step": 3477
    },
    {
      "epoch": 4.637333333333333,
      "grad_norm": 0.051136329770088196,
      "learning_rate": 0.0011652783035129117,
      "loss": 1.1013,
      "step": 3478
    },
    {
      "epoch": 4.6386666666666665,
      "grad_norm": 0.04581064730882645,
      "learning_rate": 0.0011648523931011019,
      "loss": 1.1215,
      "step": 3479
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.02893128991127014,
      "learning_rate": 0.001164426451947513,
      "loss": 1.0765,
      "step": 3480
    },
    {
      "epoch": 4.641333333333334,
      "grad_norm": 0.024960529059171677,
      "learning_rate": 0.0011640004801315744,
      "loss": 1.1417,
      "step": 3481
    },
    {
      "epoch": 4.642666666666667,
      "grad_norm": 0.025254247710108757,
      "learning_rate": 0.0011635744777327218,
      "loss": 1.0346,
      "step": 3482
    },
    {
      "epoch": 4.644,
      "grad_norm": 0.025892386212944984,
      "learning_rate": 0.0011631484448303964,
      "loss": 1.0211,
      "step": 3483
    },
    {
      "epoch": 4.645333333333333,
      "grad_norm": 0.020821090787649155,
      "learning_rate": 0.0011627223815040453,
      "loss": 1.0877,
      "step": 3484
    },
    {
      "epoch": 4.6466666666666665,
      "grad_norm": 0.03267933800816536,
      "learning_rate": 0.001162296287833121,
      "loss": 1.2895,
      "step": 3485
    },
    {
      "epoch": 4.648,
      "grad_norm": 0.030271567404270172,
      "learning_rate": 0.0011618701638970814,
      "loss": 1.2956,
      "step": 3486
    },
    {
      "epoch": 4.649333333333333,
      "grad_norm": 0.02748829312622547,
      "learning_rate": 0.0011614440097753908,
      "loss": 0.7741,
      "step": 3487
    },
    {
      "epoch": 4.650666666666667,
      "grad_norm": 0.028149619698524475,
      "learning_rate": 0.001161017825547518,
      "loss": 1.034,
      "step": 3488
    },
    {
      "epoch": 4.652,
      "grad_norm": 0.02634924091398716,
      "learning_rate": 0.0011605916112929387,
      "loss": 1.2969,
      "step": 3489
    },
    {
      "epoch": 4.653333333333333,
      "grad_norm": 0.023334326222538948,
      "learning_rate": 0.0011601653670911339,
      "loss": 0.8951,
      "step": 3490
    },
    {
      "epoch": 4.6546666666666665,
      "grad_norm": 0.05019676312804222,
      "learning_rate": 0.0011597390930215893,
      "loss": 1.2615,
      "step": 3491
    },
    {
      "epoch": 4.656,
      "grad_norm": 0.030841026455163956,
      "learning_rate": 0.0011593127891637966,
      "loss": 0.7964,
      "step": 3492
    },
    {
      "epoch": 4.657333333333334,
      "grad_norm": 0.027901334688067436,
      "learning_rate": 0.0011588864555972537,
      "loss": 1.0451,
      "step": 3493
    },
    {
      "epoch": 4.658666666666667,
      "grad_norm": 0.035644546151161194,
      "learning_rate": 0.0011584600924014635,
      "loss": 1.088,
      "step": 3494
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.02689523436129093,
      "learning_rate": 0.0011580336996559343,
      "loss": 0.8924,
      "step": 3495
    },
    {
      "epoch": 4.661333333333333,
      "grad_norm": 0.030054152011871338,
      "learning_rate": 0.0011576072774401802,
      "loss": 1.0024,
      "step": 3496
    },
    {
      "epoch": 4.6626666666666665,
      "grad_norm": 0.03229057788848877,
      "learning_rate": 0.001157180825833721,
      "loss": 0.9283,
      "step": 3497
    },
    {
      "epoch": 4.664,
      "grad_norm": 0.030826320871710777,
      "learning_rate": 0.0011567543449160808,
      "loss": 1.1799,
      "step": 3498
    },
    {
      "epoch": 4.665333333333333,
      "grad_norm": 0.03345745429396629,
      "learning_rate": 0.0011563278347667908,
      "loss": 1.193,
      "step": 3499
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.023456938564777374,
      "learning_rate": 0.0011559012954653865,
      "loss": 0.7249,
      "step": 3500
    },
    {
      "epoch": 4.668,
      "grad_norm": 0.025700584053993225,
      "learning_rate": 0.0011554747270914097,
      "loss": 1.1504,
      "step": 3501
    },
    {
      "epoch": 4.669333333333333,
      "grad_norm": 0.023138420656323433,
      "learning_rate": 0.0011550481297244067,
      "loss": 0.9212,
      "step": 3502
    },
    {
      "epoch": 4.6706666666666665,
      "grad_norm": 0.02837749384343624,
      "learning_rate": 0.0011546215034439293,
      "loss": 0.7563,
      "step": 3503
    },
    {
      "epoch": 4.672,
      "grad_norm": 0.029347119852900505,
      "learning_rate": 0.0011541948483295356,
      "loss": 1.0307,
      "step": 3504
    },
    {
      "epoch": 4.673333333333334,
      "grad_norm": 0.03910135105252266,
      "learning_rate": 0.0011537681644607886,
      "loss": 1.02,
      "step": 3505
    },
    {
      "epoch": 4.674666666666667,
      "grad_norm": 0.027078209444880486,
      "learning_rate": 0.0011533414519172562,
      "loss": 1.0782,
      "step": 3506
    },
    {
      "epoch": 4.676,
      "grad_norm": 0.027658188715577126,
      "learning_rate": 0.0011529147107785128,
      "loss": 1.0494,
      "step": 3507
    },
    {
      "epoch": 4.677333333333333,
      "grad_norm": 0.03283042088150978,
      "learning_rate": 0.0011524879411241365,
      "loss": 0.8884,
      "step": 3508
    },
    {
      "epoch": 4.6786666666666665,
      "grad_norm": 0.028240181505680084,
      "learning_rate": 0.0011520611430337116,
      "loss": 1.1314,
      "step": 3509
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.025994442403316498,
      "learning_rate": 0.0011516343165868278,
      "loss": 1.0457,
      "step": 3510
    },
    {
      "epoch": 4.681333333333333,
      "grad_norm": 0.028132671490311623,
      "learning_rate": 0.0011512074618630803,
      "loss": 1.0163,
      "step": 3511
    },
    {
      "epoch": 4.682666666666667,
      "grad_norm": 0.021269945427775383,
      "learning_rate": 0.0011507805789420693,
      "loss": 0.971,
      "step": 3512
    },
    {
      "epoch": 4.684,
      "grad_norm": 0.027720635756850243,
      "learning_rate": 0.0011503536679034,
      "loss": 0.9267,
      "step": 3513
    },
    {
      "epoch": 4.685333333333333,
      "grad_norm": 0.02862190082669258,
      "learning_rate": 0.0011499267288266825,
      "loss": 1.0601,
      "step": 3514
    },
    {
      "epoch": 4.6866666666666665,
      "grad_norm": 0.025160454213619232,
      "learning_rate": 0.0011494997617915333,
      "loss": 0.9232,
      "step": 3515
    },
    {
      "epoch": 4.688,
      "grad_norm": 0.03658692166209221,
      "learning_rate": 0.0011490727668775733,
      "loss": 1.04,
      "step": 3516
    },
    {
      "epoch": 4.689333333333334,
      "grad_norm": 0.03417380526661873,
      "learning_rate": 0.0011486457441644289,
      "loss": 0.9918,
      "step": 3517
    },
    {
      "epoch": 4.690666666666667,
      "grad_norm": 0.026010453701019287,
      "learning_rate": 0.0011482186937317317,
      "loss": 0.9308,
      "step": 3518
    },
    {
      "epoch": 4.692,
      "grad_norm": 0.0314469188451767,
      "learning_rate": 0.0011477916156591179,
      "loss": 0.9071,
      "step": 3519
    },
    {
      "epoch": 4.693333333333333,
      "grad_norm": 0.02981278859078884,
      "learning_rate": 0.0011473645100262293,
      "loss": 1.0299,
      "step": 3520
    },
    {
      "epoch": 4.6946666666666665,
      "grad_norm": 0.02650788240134716,
      "learning_rate": 0.0011469373769127133,
      "loss": 1.0464,
      "step": 3521
    },
    {
      "epoch": 4.696,
      "grad_norm": 0.024220721796154976,
      "learning_rate": 0.0011465102163982217,
      "loss": 1.1561,
      "step": 3522
    },
    {
      "epoch": 4.697333333333333,
      "grad_norm": 0.02505611814558506,
      "learning_rate": 0.0011460830285624118,
      "loss": 1.184,
      "step": 3523
    },
    {
      "epoch": 4.698666666666667,
      "grad_norm": 0.024572385475039482,
      "learning_rate": 0.0011456558134849456,
      "loss": 0.8674,
      "step": 3524
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.02844063937664032,
      "learning_rate": 0.0011452285712454905,
      "loss": 1.1168,
      "step": 3525
    },
    {
      "epoch": 4.701333333333333,
      "grad_norm": 0.021990163251757622,
      "learning_rate": 0.0011448013019237189,
      "loss": 1.0579,
      "step": 3526
    },
    {
      "epoch": 4.7026666666666666,
      "grad_norm": 0.027351928874850273,
      "learning_rate": 0.0011443740055993082,
      "loss": 0.8607,
      "step": 3527
    },
    {
      "epoch": 4.704,
      "grad_norm": 0.02859179489314556,
      "learning_rate": 0.0011439466823519413,
      "loss": 0.9698,
      "step": 3528
    },
    {
      "epoch": 4.705333333333334,
      "grad_norm": 0.031593866646289825,
      "learning_rate": 0.0011435193322613055,
      "loss": 1.0388,
      "step": 3529
    },
    {
      "epoch": 4.706666666666667,
      "grad_norm": 0.030783796682953835,
      "learning_rate": 0.001143091955407093,
      "loss": 1.058,
      "step": 3530
    },
    {
      "epoch": 4.708,
      "grad_norm": 0.025470133870840073,
      "learning_rate": 0.0011426645518690015,
      "loss": 0.8812,
      "step": 3531
    },
    {
      "epoch": 4.709333333333333,
      "grad_norm": 0.036641210317611694,
      "learning_rate": 0.0011422371217267337,
      "loss": 0.9606,
      "step": 3532
    },
    {
      "epoch": 4.710666666666667,
      "grad_norm": 0.023524362593889236,
      "learning_rate": 0.0011418096650599968,
      "loss": 1.2281,
      "step": 3533
    },
    {
      "epoch": 4.712,
      "grad_norm": 0.029067110270261765,
      "learning_rate": 0.0011413821819485036,
      "loss": 0.9948,
      "step": 3534
    },
    {
      "epoch": 4.713333333333333,
      "grad_norm": 0.028403617441654205,
      "learning_rate": 0.001140954672471971,
      "loss": 0.9299,
      "step": 3535
    },
    {
      "epoch": 4.714666666666667,
      "grad_norm": 0.033420830965042114,
      "learning_rate": 0.0011405271367101208,
      "loss": 1.0367,
      "step": 3536
    },
    {
      "epoch": 4.716,
      "grad_norm": 0.027735870331525803,
      "learning_rate": 0.001140099574742681,
      "loss": 1.0858,
      "step": 3537
    },
    {
      "epoch": 4.717333333333333,
      "grad_norm": 0.025592459365725517,
      "learning_rate": 0.0011396719866493832,
      "loss": 0.9559,
      "step": 3538
    },
    {
      "epoch": 4.718666666666667,
      "grad_norm": 0.03364960849285126,
      "learning_rate": 0.0011392443725099647,
      "loss": 1.2131,
      "step": 3539
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.02396312914788723,
      "learning_rate": 0.001138816732404167,
      "loss": 0.9491,
      "step": 3540
    },
    {
      "epoch": 4.721333333333334,
      "grad_norm": 0.03381191939115524,
      "learning_rate": 0.0011383890664117362,
      "loss": 0.8296,
      "step": 3541
    },
    {
      "epoch": 4.722666666666667,
      "grad_norm": 0.03568281978368759,
      "learning_rate": 0.0011379613746124245,
      "loss": 1.0121,
      "step": 3542
    },
    {
      "epoch": 4.724,
      "grad_norm": 0.026977241039276123,
      "learning_rate": 0.0011375336570859877,
      "loss": 0.9325,
      "step": 3543
    },
    {
      "epoch": 4.725333333333333,
      "grad_norm": 0.028655733913183212,
      "learning_rate": 0.0011371059139121868,
      "loss": 1.2306,
      "step": 3544
    },
    {
      "epoch": 4.726666666666667,
      "grad_norm": 0.02197093330323696,
      "learning_rate": 0.0011366781451707878,
      "loss": 1.2871,
      "step": 3545
    },
    {
      "epoch": 4.728,
      "grad_norm": 0.027347009629011154,
      "learning_rate": 0.0011362503509415617,
      "loss": 1.2638,
      "step": 3546
    },
    {
      "epoch": 4.729333333333333,
      "grad_norm": 0.02603890374302864,
      "learning_rate": 0.0011358225313042832,
      "loss": 0.9014,
      "step": 3547
    },
    {
      "epoch": 4.730666666666667,
      "grad_norm": 0.03743990138173103,
      "learning_rate": 0.0011353946863387322,
      "loss": 1.261,
      "step": 3548
    },
    {
      "epoch": 4.732,
      "grad_norm": 0.02685377188026905,
      "learning_rate": 0.0011349668161246944,
      "loss": 1.0903,
      "step": 3549
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 0.024785703048110008,
      "learning_rate": 0.0011345389207419588,
      "loss": 1.111,
      "step": 3550
    },
    {
      "epoch": 4.734666666666667,
      "grad_norm": 0.02597949281334877,
      "learning_rate": 0.0011341110002703195,
      "loss": 1.3977,
      "step": 3551
    },
    {
      "epoch": 4.736,
      "grad_norm": 0.021821293979883194,
      "learning_rate": 0.0011336830547895751,
      "loss": 1.0089,
      "step": 3552
    },
    {
      "epoch": 4.737333333333333,
      "grad_norm": 0.01947169192135334,
      "learning_rate": 0.0011332550843795297,
      "loss": 0.8857,
      "step": 3553
    },
    {
      "epoch": 4.738666666666667,
      "grad_norm": 0.03988155350089073,
      "learning_rate": 0.0011328270891199916,
      "loss": 0.8779,
      "step": 3554
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.02274065464735031,
      "learning_rate": 0.0011323990690907733,
      "loss": 1.1019,
      "step": 3555
    },
    {
      "epoch": 4.741333333333333,
      "grad_norm": 0.02943669818341732,
      "learning_rate": 0.001131971024371692,
      "loss": 0.8947,
      "step": 3556
    },
    {
      "epoch": 4.742666666666667,
      "grad_norm": 0.025042269378900528,
      "learning_rate": 0.0011315429550425703,
      "loss": 1.0975,
      "step": 3557
    },
    {
      "epoch": 4.744,
      "grad_norm": 0.024503342807292938,
      "learning_rate": 0.0011311148611832344,
      "loss": 1.1409,
      "step": 3558
    },
    {
      "epoch": 4.745333333333333,
      "grad_norm": 0.028830260038375854,
      "learning_rate": 0.0011306867428735158,
      "loss": 1.3739,
      "step": 3559
    },
    {
      "epoch": 4.746666666666667,
      "grad_norm": 0.026954911649227142,
      "learning_rate": 0.0011302586001932504,
      "loss": 0.8829,
      "step": 3560
    },
    {
      "epoch": 4.748,
      "grad_norm": 0.04613330960273743,
      "learning_rate": 0.001129830433222278,
      "loss": 1.1988,
      "step": 3561
    },
    {
      "epoch": 4.749333333333333,
      "grad_norm": 0.028265925124287605,
      "learning_rate": 0.001129402242040444,
      "loss": 1.1175,
      "step": 3562
    },
    {
      "epoch": 4.750666666666667,
      "grad_norm": 0.023373793810606003,
      "learning_rate": 0.0011289740267275974,
      "loss": 0.9968,
      "step": 3563
    },
    {
      "epoch": 4.752,
      "grad_norm": 0.034397561103105545,
      "learning_rate": 0.001128545787363592,
      "loss": 1.0344,
      "step": 3564
    },
    {
      "epoch": 4.753333333333333,
      "grad_norm": 0.0215272456407547,
      "learning_rate": 0.0011281175240282866,
      "loss": 0.9926,
      "step": 3565
    },
    {
      "epoch": 4.754666666666667,
      "grad_norm": 0.026778414845466614,
      "learning_rate": 0.0011276892368015435,
      "loss": 0.8983,
      "step": 3566
    },
    {
      "epoch": 4.756,
      "grad_norm": 0.021208129823207855,
      "learning_rate": 0.0011272609257632306,
      "loss": 1.2699,
      "step": 3567
    },
    {
      "epoch": 4.757333333333333,
      "grad_norm": 0.02338528074324131,
      "learning_rate": 0.001126832590993219,
      "loss": 0.961,
      "step": 3568
    },
    {
      "epoch": 4.758666666666667,
      "grad_norm": 0.027350403368473053,
      "learning_rate": 0.0011264042325713848,
      "loss": 1.2705,
      "step": 3569
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.023952264338731766,
      "learning_rate": 0.0011259758505776092,
      "loss": 1.1601,
      "step": 3570
    },
    {
      "epoch": 4.761333333333333,
      "grad_norm": 0.02771674282848835,
      "learning_rate": 0.0011255474450917765,
      "loss": 0.9925,
      "step": 3571
    },
    {
      "epoch": 4.762666666666667,
      "grad_norm": 0.03847961127758026,
      "learning_rate": 0.0011251190161937765,
      "loss": 0.9368,
      "step": 3572
    },
    {
      "epoch": 4.764,
      "grad_norm": 0.029111791402101517,
      "learning_rate": 0.0011246905639635029,
      "loss": 1.2421,
      "step": 3573
    },
    {
      "epoch": 4.765333333333333,
      "grad_norm": 0.10851164907217026,
      "learning_rate": 0.0011242620884808533,
      "loss": 0.6954,
      "step": 3574
    },
    {
      "epoch": 4.766666666666667,
      "grad_norm": 0.022254779934883118,
      "learning_rate": 0.0011238335898257304,
      "loss": 1.1221,
      "step": 3575
    },
    {
      "epoch": 4.768,
      "grad_norm": 0.029435673728585243,
      "learning_rate": 0.0011234050680780404,
      "loss": 1.0423,
      "step": 3576
    },
    {
      "epoch": 4.769333333333333,
      "grad_norm": 0.02608579210937023,
      "learning_rate": 0.001122976523317695,
      "loss": 1.085,
      "step": 3577
    },
    {
      "epoch": 4.770666666666667,
      "grad_norm": 0.03172712400555611,
      "learning_rate": 0.0011225479556246096,
      "loss": 1.2028,
      "step": 3578
    },
    {
      "epoch": 4.772,
      "grad_norm": 0.023519041016697884,
      "learning_rate": 0.0011221193650787031,
      "loss": 1.1324,
      "step": 3579
    },
    {
      "epoch": 4.773333333333333,
      "grad_norm": 0.02087934873998165,
      "learning_rate": 0.0011216907517598996,
      "loss": 0.7978,
      "step": 3580
    },
    {
      "epoch": 4.774666666666667,
      "grad_norm": 0.026462016627192497,
      "learning_rate": 0.0011212621157481275,
      "loss": 0.8901,
      "step": 3581
    },
    {
      "epoch": 4.776,
      "grad_norm": 0.03337359055876732,
      "learning_rate": 0.0011208334571233185,
      "loss": 0.9044,
      "step": 3582
    },
    {
      "epoch": 4.777333333333333,
      "grad_norm": 0.03416340798139572,
      "learning_rate": 0.0011204047759654097,
      "loss": 1.1021,
      "step": 3583
    },
    {
      "epoch": 4.778666666666666,
      "grad_norm": 0.027516305446624756,
      "learning_rate": 0.0011199760723543414,
      "loss": 1.096,
      "step": 3584
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.030731694772839546,
      "learning_rate": 0.001119547346370059,
      "loss": 1.0549,
      "step": 3585
    },
    {
      "epoch": 4.781333333333333,
      "grad_norm": 0.02798902429640293,
      "learning_rate": 0.0011191185980925114,
      "loss": 1.016,
      "step": 3586
    },
    {
      "epoch": 4.782666666666667,
      "grad_norm": 0.04874855652451515,
      "learning_rate": 0.0011186898276016515,
      "loss": 1.0469,
      "step": 3587
    },
    {
      "epoch": 4.784,
      "grad_norm": 0.02105077914893627,
      "learning_rate": 0.001118261034977437,
      "loss": 0.8133,
      "step": 3588
    },
    {
      "epoch": 4.785333333333333,
      "grad_norm": 0.024669738486409187,
      "learning_rate": 0.0011178322202998296,
      "loss": 1.1365,
      "step": 3589
    },
    {
      "epoch": 4.786666666666667,
      "grad_norm": 0.022935334593057632,
      "learning_rate": 0.0011174033836487945,
      "loss": 0.8747,
      "step": 3590
    },
    {
      "epoch": 4.788,
      "grad_norm": 0.024806590750813484,
      "learning_rate": 0.001116974525104302,
      "loss": 0.9144,
      "step": 3591
    },
    {
      "epoch": 4.789333333333333,
      "grad_norm": 0.027119647711515427,
      "learning_rate": 0.0011165456447463254,
      "loss": 1.0369,
      "step": 3592
    },
    {
      "epoch": 4.790666666666667,
      "grad_norm": 0.028086809441447258,
      "learning_rate": 0.0011161167426548425,
      "loss": 0.95,
      "step": 3593
    },
    {
      "epoch": 4.792,
      "grad_norm": 0.03340223431587219,
      "learning_rate": 0.0011156878189098355,
      "loss": 1.3252,
      "step": 3594
    },
    {
      "epoch": 4.793333333333333,
      "grad_norm": 0.022682035341858864,
      "learning_rate": 0.001115258873591291,
      "loss": 1.166,
      "step": 3595
    },
    {
      "epoch": 4.794666666666666,
      "grad_norm": 0.023197835311293602,
      "learning_rate": 0.001114829906779198,
      "loss": 0.8943,
      "step": 3596
    },
    {
      "epoch": 4.796,
      "grad_norm": 0.02320021018385887,
      "learning_rate": 0.001114400918553551,
      "loss": 1.1299,
      "step": 3597
    },
    {
      "epoch": 4.7973333333333334,
      "grad_norm": 0.03103344328701496,
      "learning_rate": 0.0011139719089943475,
      "loss": 1.4154,
      "step": 3598
    },
    {
      "epoch": 4.798666666666667,
      "grad_norm": 0.025969035923480988,
      "learning_rate": 0.0011135428781815899,
      "loss": 0.9851,
      "step": 3599
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.03203311935067177,
      "learning_rate": 0.0011131138261952845,
      "loss": 0.7747,
      "step": 3600
    },
    {
      "epoch": 4.801333333333333,
      "grad_norm": 0.029692109674215317,
      "learning_rate": 0.0011126847531154405,
      "loss": 0.765,
      "step": 3601
    },
    {
      "epoch": 4.802666666666667,
      "grad_norm": 0.02425556629896164,
      "learning_rate": 0.001112255659022072,
      "loss": 1.0219,
      "step": 3602
    },
    {
      "epoch": 4.804,
      "grad_norm": 0.032501596957445145,
      "learning_rate": 0.0011118265439951968,
      "loss": 1.3084,
      "step": 3603
    },
    {
      "epoch": 4.8053333333333335,
      "grad_norm": 0.029549816623330116,
      "learning_rate": 0.001111397408114836,
      "loss": 1.2419,
      "step": 3604
    },
    {
      "epoch": 4.806666666666667,
      "grad_norm": 0.03348636254668236,
      "learning_rate": 0.0011109682514610162,
      "loss": 0.9764,
      "step": 3605
    },
    {
      "epoch": 4.808,
      "grad_norm": 0.028872359544038773,
      "learning_rate": 0.001110539074113766,
      "loss": 1.0133,
      "step": 3606
    },
    {
      "epoch": 4.809333333333333,
      "grad_norm": 0.029898690059781075,
      "learning_rate": 0.0011101098761531192,
      "loss": 0.8667,
      "step": 3607
    },
    {
      "epoch": 4.810666666666666,
      "grad_norm": 0.027500977739691734,
      "learning_rate": 0.001109680657659112,
      "loss": 0.8996,
      "step": 3608
    },
    {
      "epoch": 4.812,
      "grad_norm": 0.028999609872698784,
      "learning_rate": 0.0011092514187117863,
      "loss": 0.7523,
      "step": 3609
    },
    {
      "epoch": 4.8133333333333335,
      "grad_norm": 0.03484439477324486,
      "learning_rate": 0.0011088221593911862,
      "loss": 1.0807,
      "step": 3610
    },
    {
      "epoch": 4.814666666666667,
      "grad_norm": 0.03560173511505127,
      "learning_rate": 0.0011083928797773611,
      "loss": 1.1063,
      "step": 3611
    },
    {
      "epoch": 4.816,
      "grad_norm": 0.03349332883954048,
      "learning_rate": 0.0011079635799503625,
      "loss": 1.115,
      "step": 3612
    },
    {
      "epoch": 4.817333333333333,
      "grad_norm": 0.0266374871134758,
      "learning_rate": 0.0011075342599902464,
      "loss": 1.1395,
      "step": 3613
    },
    {
      "epoch": 4.818666666666667,
      "grad_norm": 0.025328699499368668,
      "learning_rate": 0.0011071049199770733,
      "loss": 0.9671,
      "step": 3614
    },
    {
      "epoch": 4.82,
      "grad_norm": 0.027165941894054413,
      "learning_rate": 0.0011066755599909064,
      "loss": 1.0805,
      "step": 3615
    },
    {
      "epoch": 4.8213333333333335,
      "grad_norm": 0.02443869598209858,
      "learning_rate": 0.0011062461801118134,
      "loss": 0.8344,
      "step": 3616
    },
    {
      "epoch": 4.822666666666667,
      "grad_norm": 0.03003256395459175,
      "learning_rate": 0.0011058167804198654,
      "loss": 1.0658,
      "step": 3617
    },
    {
      "epoch": 4.824,
      "grad_norm": 0.021386368200182915,
      "learning_rate": 0.0011053873609951361,
      "loss": 0.8192,
      "step": 3618
    },
    {
      "epoch": 4.825333333333333,
      "grad_norm": 0.026486238464713097,
      "learning_rate": 0.0011049579219177049,
      "loss": 0.979,
      "step": 3619
    },
    {
      "epoch": 4.826666666666666,
      "grad_norm": 0.0332215242087841,
      "learning_rate": 0.0011045284632676536,
      "loss": 0.8935,
      "step": 3620
    },
    {
      "epoch": 4.828,
      "grad_norm": 0.02404356375336647,
      "learning_rate": 0.0011040989851250679,
      "loss": 0.8757,
      "step": 3621
    },
    {
      "epoch": 4.8293333333333335,
      "grad_norm": 0.024881431832909584,
      "learning_rate": 0.0011036694875700371,
      "loss": 1.2292,
      "step": 3622
    },
    {
      "epoch": 4.830666666666667,
      "grad_norm": 0.04542987048625946,
      "learning_rate": 0.0011032399706826545,
      "loss": 1.0032,
      "step": 3623
    },
    {
      "epoch": 4.832,
      "grad_norm": 0.026247115805745125,
      "learning_rate": 0.001102810434543016,
      "loss": 0.9816,
      "step": 3624
    },
    {
      "epoch": 4.833333333333333,
      "grad_norm": 0.03320730850100517,
      "learning_rate": 0.0011023808792312226,
      "loss": 0.8922,
      "step": 3625
    },
    {
      "epoch": 4.834666666666667,
      "grad_norm": 0.025548484176397324,
      "learning_rate": 0.0011019513048273774,
      "loss": 1.1873,
      "step": 3626
    },
    {
      "epoch": 4.836,
      "grad_norm": 0.030043553560972214,
      "learning_rate": 0.0011015217114115883,
      "loss": 1.2441,
      "step": 3627
    },
    {
      "epoch": 4.8373333333333335,
      "grad_norm": 0.022231673821806908,
      "learning_rate": 0.0011010920990639657,
      "loss": 0.8467,
      "step": 3628
    },
    {
      "epoch": 4.838666666666667,
      "grad_norm": 0.027358975261449814,
      "learning_rate": 0.001100662467864624,
      "loss": 1.0616,
      "step": 3629
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.022567877545952797,
      "learning_rate": 0.0011002328178936812,
      "loss": 0.6739,
      "step": 3630
    },
    {
      "epoch": 4.841333333333333,
      "grad_norm": 0.024169446900486946,
      "learning_rate": 0.0010998031492312585,
      "loss": 0.8921,
      "step": 3631
    },
    {
      "epoch": 4.842666666666666,
      "grad_norm": 0.028860319405794144,
      "learning_rate": 0.001099373461957481,
      "loss": 1.1248,
      "step": 3632
    },
    {
      "epoch": 4.844,
      "grad_norm": 0.021203728392720222,
      "learning_rate": 0.0010989437561524774,
      "loss": 0.9692,
      "step": 3633
    },
    {
      "epoch": 4.8453333333333335,
      "grad_norm": 0.024021418765187263,
      "learning_rate": 0.0010985140318963793,
      "loss": 0.9497,
      "step": 3634
    },
    {
      "epoch": 4.846666666666667,
      "grad_norm": 0.028537726029753685,
      "learning_rate": 0.0010980842892693214,
      "loss": 0.894,
      "step": 3635
    },
    {
      "epoch": 4.848,
      "grad_norm": 0.021912449970841408,
      "learning_rate": 0.0010976545283514429,
      "loss": 0.9496,
      "step": 3636
    },
    {
      "epoch": 4.849333333333333,
      "grad_norm": 0.05258333310484886,
      "learning_rate": 0.0010972247492228857,
      "loss": 1.0047,
      "step": 3637
    },
    {
      "epoch": 4.850666666666667,
      "grad_norm": 0.03321637585759163,
      "learning_rate": 0.0010967949519637957,
      "loss": 1.0619,
      "step": 3638
    },
    {
      "epoch": 4.852,
      "grad_norm": 0.030808785930275917,
      "learning_rate": 0.0010963651366543213,
      "loss": 0.8249,
      "step": 3639
    },
    {
      "epoch": 4.8533333333333335,
      "grad_norm": 0.02563530206680298,
      "learning_rate": 0.0010959353033746146,
      "loss": 0.907,
      "step": 3640
    },
    {
      "epoch": 4.854666666666667,
      "grad_norm": 0.03025319054722786,
      "learning_rate": 0.0010955054522048316,
      "loss": 0.9586,
      "step": 3641
    },
    {
      "epoch": 4.856,
      "grad_norm": 0.03190018981695175,
      "learning_rate": 0.001095075583225131,
      "loss": 1.1044,
      "step": 3642
    },
    {
      "epoch": 4.857333333333333,
      "grad_norm": 0.03071129508316517,
      "learning_rate": 0.001094645696515675,
      "loss": 0.9634,
      "step": 3643
    },
    {
      "epoch": 4.858666666666666,
      "grad_norm": 0.0205313041806221,
      "learning_rate": 0.0010942157921566295,
      "loss": 0.9637,
      "step": 3644
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.030146606266498566,
      "learning_rate": 0.001093785870228163,
      "loss": 0.9564,
      "step": 3645
    },
    {
      "epoch": 4.8613333333333335,
      "grad_norm": 0.0271772313863039,
      "learning_rate": 0.0010933559308104477,
      "loss": 1.0175,
      "step": 3646
    },
    {
      "epoch": 4.862666666666667,
      "grad_norm": 0.02186681143939495,
      "learning_rate": 0.0010929259739836587,
      "loss": 1.277,
      "step": 3647
    },
    {
      "epoch": 4.864,
      "grad_norm": 0.019727129489183426,
      "learning_rate": 0.0010924959998279754,
      "loss": 1.0836,
      "step": 3648
    },
    {
      "epoch": 4.865333333333333,
      "grad_norm": 0.02614864706993103,
      "learning_rate": 0.001092066008423579,
      "loss": 0.7399,
      "step": 3649
    },
    {
      "epoch": 4.866666666666667,
      "grad_norm": 0.025389818474650383,
      "learning_rate": 0.0010916359998506548,
      "loss": 1.0803,
      "step": 3650
    },
    {
      "epoch": 4.868,
      "grad_norm": 0.034722950309515,
      "learning_rate": 0.0010912059741893907,
      "loss": 1.1785,
      "step": 3651
    },
    {
      "epoch": 4.8693333333333335,
      "grad_norm": 0.02558116801083088,
      "learning_rate": 0.0010907759315199785,
      "loss": 1.1166,
      "step": 3652
    },
    {
      "epoch": 4.870666666666667,
      "grad_norm": 0.025635316967964172,
      "learning_rate": 0.0010903458719226128,
      "loss": 1.0839,
      "step": 3653
    },
    {
      "epoch": 4.872,
      "grad_norm": 0.04162190854549408,
      "learning_rate": 0.001089915795477492,
      "loss": 1.0236,
      "step": 3654
    },
    {
      "epoch": 4.873333333333333,
      "grad_norm": 0.02383594773709774,
      "learning_rate": 0.0010894857022648158,
      "loss": 1.1186,
      "step": 3655
    },
    {
      "epoch": 4.874666666666666,
      "grad_norm": 0.02401014044880867,
      "learning_rate": 0.001089055592364789,
      "loss": 1.0467,
      "step": 3656
    },
    {
      "epoch": 4.876,
      "grad_norm": 0.06002281233668327,
      "learning_rate": 0.0010886254658576185,
      "loss": 1.0625,
      "step": 3657
    },
    {
      "epoch": 4.8773333333333335,
      "grad_norm": 0.032336387783288956,
      "learning_rate": 0.0010881953228235148,
      "loss": 0.9981,
      "step": 3658
    },
    {
      "epoch": 4.878666666666667,
      "grad_norm": 0.027673950418829918,
      "learning_rate": 0.0010877651633426912,
      "loss": 1.1771,
      "step": 3659
    },
    {
      "epoch": 4.88,
      "grad_norm": 0.04054701328277588,
      "learning_rate": 0.001087334987495364,
      "loss": 0.907,
      "step": 3660
    },
    {
      "epoch": 4.881333333333333,
      "grad_norm": 0.024325810372829437,
      "learning_rate": 0.001086904795361753,
      "loss": 1.0011,
      "step": 3661
    },
    {
      "epoch": 4.882666666666667,
      "grad_norm": 0.03712466359138489,
      "learning_rate": 0.00108647458702208,
      "loss": 1.0768,
      "step": 3662
    },
    {
      "epoch": 4.884,
      "grad_norm": 0.03365657106041908,
      "learning_rate": 0.0010860443625565711,
      "loss": 0.9522,
      "step": 3663
    },
    {
      "epoch": 4.8853333333333335,
      "grad_norm": 0.03309571370482445,
      "learning_rate": 0.001085614122045455,
      "loss": 1.3709,
      "step": 3664
    },
    {
      "epoch": 4.886666666666667,
      "grad_norm": 0.030178993940353394,
      "learning_rate": 0.0010851838655689626,
      "loss": 1.0902,
      "step": 3665
    },
    {
      "epoch": 4.888,
      "grad_norm": 0.03210052102804184,
      "learning_rate": 0.0010847535932073287,
      "loss": 1.0729,
      "step": 3666
    },
    {
      "epoch": 4.889333333333333,
      "grad_norm": 0.02636079117655754,
      "learning_rate": 0.001084323305040791,
      "loss": 1.1494,
      "step": 3667
    },
    {
      "epoch": 4.890666666666666,
      "grad_norm": 0.15836742520332336,
      "learning_rate": 0.0010838930011495892,
      "loss": 0.9863,
      "step": 3668
    },
    {
      "epoch": 4.892,
      "grad_norm": 0.03598735108971596,
      "learning_rate": 0.0010834626816139676,
      "loss": 1.0976,
      "step": 3669
    },
    {
      "epoch": 4.8933333333333335,
      "grad_norm": 0.031110111624002457,
      "learning_rate": 0.0010830323465141718,
      "loss": 0.9725,
      "step": 3670
    },
    {
      "epoch": 4.894666666666667,
      "grad_norm": 0.02708248607814312,
      "learning_rate": 0.001082601995930451,
      "loss": 0.9818,
      "step": 3671
    },
    {
      "epoch": 4.896,
      "grad_norm": 0.03186647966504097,
      "learning_rate": 0.0010821716299430578,
      "loss": 1.0277,
      "step": 3672
    },
    {
      "epoch": 4.897333333333333,
      "grad_norm": 0.025230810046195984,
      "learning_rate": 0.0010817412486322465,
      "loss": 1.0825,
      "step": 3673
    },
    {
      "epoch": 4.898666666666666,
      "grad_norm": 0.040620822459459305,
      "learning_rate": 0.001081310852078275,
      "loss": 0.9926,
      "step": 3674
    },
    {
      "epoch": 4.9,
      "grad_norm": 0.10453267395496368,
      "learning_rate": 0.0010808804403614042,
      "loss": 0.9363,
      "step": 3675
    },
    {
      "epoch": 4.9013333333333335,
      "grad_norm": 0.023410413414239883,
      "learning_rate": 0.0010804500135618972,
      "loss": 1.2333,
      "step": 3676
    },
    {
      "epoch": 4.902666666666667,
      "grad_norm": 0.026838108897209167,
      "learning_rate": 0.001080019571760021,
      "loss": 1.0378,
      "step": 3677
    },
    {
      "epoch": 4.904,
      "grad_norm": 0.027544913813471794,
      "learning_rate": 0.0010795891150360436,
      "loss": 1.0946,
      "step": 3678
    },
    {
      "epoch": 4.905333333333333,
      "grad_norm": 0.029229532927274704,
      "learning_rate": 0.0010791586434702371,
      "loss": 1.1046,
      "step": 3679
    },
    {
      "epoch": 4.906666666666666,
      "grad_norm": 0.0334133580327034,
      "learning_rate": 0.001078728157142877,
      "loss": 0.9414,
      "step": 3680
    },
    {
      "epoch": 4.908,
      "grad_norm": 0.020040294155478477,
      "learning_rate": 0.0010782976561342396,
      "loss": 1.0716,
      "step": 3681
    },
    {
      "epoch": 4.9093333333333335,
      "grad_norm": 0.02589886635541916,
      "learning_rate": 0.001077867140524606,
      "loss": 1.1702,
      "step": 3682
    },
    {
      "epoch": 4.910666666666667,
      "grad_norm": 0.025413326919078827,
      "learning_rate": 0.001077436610394258,
      "loss": 0.8621,
      "step": 3683
    },
    {
      "epoch": 4.912,
      "grad_norm": 0.026319265365600586,
      "learning_rate": 0.0010770060658234816,
      "loss": 1.0381,
      "step": 3684
    },
    {
      "epoch": 4.913333333333333,
      "grad_norm": 0.05007980763912201,
      "learning_rate": 0.0010765755068925648,
      "loss": 0.9933,
      "step": 3685
    },
    {
      "epoch": 4.914666666666666,
      "grad_norm": 0.03495160862803459,
      "learning_rate": 0.001076144933681799,
      "loss": 1.3248,
      "step": 3686
    },
    {
      "epoch": 4.916,
      "grad_norm": 0.03159079700708389,
      "learning_rate": 0.0010757143462714776,
      "loss": 1.3011,
      "step": 3687
    },
    {
      "epoch": 4.917333333333334,
      "grad_norm": 0.024220488965511322,
      "learning_rate": 0.0010752837447418966,
      "loss": 1.0895,
      "step": 3688
    },
    {
      "epoch": 4.918666666666667,
      "grad_norm": 0.03404868021607399,
      "learning_rate": 0.0010748531291733547,
      "loss": 0.9773,
      "step": 3689
    },
    {
      "epoch": 4.92,
      "grad_norm": 0.030594469979405403,
      "learning_rate": 0.001074422499646154,
      "loss": 1.0745,
      "step": 3690
    },
    {
      "epoch": 4.921333333333333,
      "grad_norm": 0.03279300406575203,
      "learning_rate": 0.001073991856240598,
      "loss": 0.8863,
      "step": 3691
    },
    {
      "epoch": 4.922666666666666,
      "grad_norm": 0.023992741480469704,
      "learning_rate": 0.0010735611990369933,
      "loss": 0.8121,
      "step": 3692
    },
    {
      "epoch": 4.924,
      "grad_norm": 0.025096677243709564,
      "learning_rate": 0.0010731305281156498,
      "loss": 1.1164,
      "step": 3693
    },
    {
      "epoch": 4.925333333333334,
      "grad_norm": 0.022908519953489304,
      "learning_rate": 0.0010726998435568787,
      "loss": 1.152,
      "step": 3694
    },
    {
      "epoch": 4.926666666666667,
      "grad_norm": 0.029827725142240524,
      "learning_rate": 0.0010722691454409944,
      "loss": 1.1041,
      "step": 3695
    },
    {
      "epoch": 4.928,
      "grad_norm": 0.021824004128575325,
      "learning_rate": 0.0010718384338483141,
      "loss": 1.0226,
      "step": 3696
    },
    {
      "epoch": 4.929333333333333,
      "grad_norm": 0.028549866750836372,
      "learning_rate": 0.0010714077088591564,
      "loss": 1.4747,
      "step": 3697
    },
    {
      "epoch": 4.930666666666666,
      "grad_norm": 0.026753060519695282,
      "learning_rate": 0.0010709769705538438,
      "loss": 1.1913,
      "step": 3698
    },
    {
      "epoch": 4.932,
      "grad_norm": 0.028368255123496056,
      "learning_rate": 0.0010705462190127011,
      "loss": 0.8769,
      "step": 3699
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 0.028173750266432762,
      "learning_rate": 0.0010701154543160541,
      "loss": 1.1403,
      "step": 3700
    },
    {
      "epoch": 4.934666666666667,
      "grad_norm": 0.0257908646017313,
      "learning_rate": 0.0010696846765442327,
      "loss": 0.9095,
      "step": 3701
    },
    {
      "epoch": 4.936,
      "grad_norm": 0.020834092050790787,
      "learning_rate": 0.0010692538857775685,
      "loss": 1.011,
      "step": 3702
    },
    {
      "epoch": 4.937333333333333,
      "grad_norm": 0.02518063224852085,
      "learning_rate": 0.0010688230820963954,
      "loss": 0.9921,
      "step": 3703
    },
    {
      "epoch": 4.938666666666666,
      "grad_norm": 0.029466846957802773,
      "learning_rate": 0.0010683922655810502,
      "loss": 1.0403,
      "step": 3704
    },
    {
      "epoch": 4.9399999999999995,
      "grad_norm": 0.025638751685619354,
      "learning_rate": 0.0010679614363118717,
      "loss": 1.1763,
      "step": 3705
    },
    {
      "epoch": 4.941333333333334,
      "grad_norm": 0.03080008551478386,
      "learning_rate": 0.0010675305943692016,
      "loss": 1.0399,
      "step": 3706
    },
    {
      "epoch": 4.942666666666667,
      "grad_norm": 0.022814994677901268,
      "learning_rate": 0.0010670997398333829,
      "loss": 1.333,
      "step": 3707
    },
    {
      "epoch": 4.944,
      "grad_norm": 0.022925635799765587,
      "learning_rate": 0.001066668872784762,
      "loss": 0.9902,
      "step": 3708
    },
    {
      "epoch": 4.945333333333333,
      "grad_norm": 0.02747363969683647,
      "learning_rate": 0.0010662379933036872,
      "loss": 1.0821,
      "step": 3709
    },
    {
      "epoch": 4.946666666666666,
      "grad_norm": 0.021857477724552155,
      "learning_rate": 0.0010658071014705095,
      "loss": 1.0473,
      "step": 3710
    },
    {
      "epoch": 4.948,
      "grad_norm": 0.02915460243821144,
      "learning_rate": 0.0010653761973655818,
      "loss": 0.7782,
      "step": 3711
    },
    {
      "epoch": 4.949333333333334,
      "grad_norm": 0.029352756217122078,
      "learning_rate": 0.0010649452810692589,
      "loss": 1.3132,
      "step": 3712
    },
    {
      "epoch": 4.950666666666667,
      "grad_norm": 0.03934584930539131,
      "learning_rate": 0.0010645143526618983,
      "loss": 1.0713,
      "step": 3713
    },
    {
      "epoch": 4.952,
      "grad_norm": 0.0308879092335701,
      "learning_rate": 0.0010640834122238605,
      "loss": 1.155,
      "step": 3714
    },
    {
      "epoch": 4.953333333333333,
      "grad_norm": 0.03403760865330696,
      "learning_rate": 0.0010636524598355073,
      "loss": 0.721,
      "step": 3715
    },
    {
      "epoch": 4.954666666666666,
      "grad_norm": 0.034056663513183594,
      "learning_rate": 0.0010632214955772028,
      "loss": 1.0382,
      "step": 3716
    },
    {
      "epoch": 4.9559999999999995,
      "grad_norm": 0.02489295043051243,
      "learning_rate": 0.0010627905195293135,
      "loss": 1.0541,
      "step": 3717
    },
    {
      "epoch": 4.957333333333334,
      "grad_norm": 0.022709602490067482,
      "learning_rate": 0.0010623595317722082,
      "loss": 1.0376,
      "step": 3718
    },
    {
      "epoch": 4.958666666666667,
      "grad_norm": 0.02102249674499035,
      "learning_rate": 0.0010619285323862574,
      "loss": 0.9775,
      "step": 3719
    },
    {
      "epoch": 4.96,
      "grad_norm": 0.030094042420387268,
      "learning_rate": 0.001061497521451835,
      "loss": 1.2529,
      "step": 3720
    },
    {
      "epoch": 4.961333333333333,
      "grad_norm": 0.03018299676477909,
      "learning_rate": 0.0010610664990493152,
      "loss": 1.1397,
      "step": 3721
    },
    {
      "epoch": 4.962666666666666,
      "grad_norm": 0.023641057312488556,
      "learning_rate": 0.001060635465259076,
      "loss": 0.9069,
      "step": 3722
    },
    {
      "epoch": 4.964,
      "grad_norm": 0.03395065665245056,
      "learning_rate": 0.0010602044201614964,
      "loss": 0.974,
      "step": 3723
    },
    {
      "epoch": 4.965333333333334,
      "grad_norm": 0.02190423011779785,
      "learning_rate": 0.0010597733638369581,
      "loss": 1.2486,
      "step": 3724
    },
    {
      "epoch": 4.966666666666667,
      "grad_norm": 0.02290826290845871,
      "learning_rate": 0.0010593422963658453,
      "loss": 1.003,
      "step": 3725
    },
    {
      "epoch": 4.968,
      "grad_norm": 0.02714799903333187,
      "learning_rate": 0.0010589112178285432,
      "loss": 1.1009,
      "step": 3726
    },
    {
      "epoch": 4.969333333333333,
      "grad_norm": 0.0328049473464489,
      "learning_rate": 0.0010584801283054397,
      "loss": 0.8402,
      "step": 3727
    },
    {
      "epoch": 4.970666666666666,
      "grad_norm": 0.02782314084470272,
      "learning_rate": 0.0010580490278769244,
      "loss": 1.1865,
      "step": 3728
    },
    {
      "epoch": 4.9719999999999995,
      "grad_norm": 0.028488123789429665,
      "learning_rate": 0.0010576179166233894,
      "loss": 1.0312,
      "step": 3729
    },
    {
      "epoch": 4.973333333333334,
      "grad_norm": 0.031597428023815155,
      "learning_rate": 0.0010571867946252288,
      "loss": 1.0729,
      "step": 3730
    },
    {
      "epoch": 4.974666666666667,
      "grad_norm": 0.025676079094409943,
      "learning_rate": 0.0010567556619628382,
      "loss": 0.9425,
      "step": 3731
    },
    {
      "epoch": 4.976,
      "grad_norm": 0.029009610414505005,
      "learning_rate": 0.0010563245187166159,
      "loss": 1.0969,
      "step": 3732
    },
    {
      "epoch": 4.977333333333333,
      "grad_norm": 0.026901420205831528,
      "learning_rate": 0.0010558933649669614,
      "loss": 0.7297,
      "step": 3733
    },
    {
      "epoch": 4.978666666666666,
      "grad_norm": 0.028896737843751907,
      "learning_rate": 0.0010554622007942762,
      "loss": 0.9805,
      "step": 3734
    },
    {
      "epoch": 4.98,
      "grad_norm": 0.023760855197906494,
      "learning_rate": 0.0010550310262789648,
      "loss": 1.0946,
      "step": 3735
    },
    {
      "epoch": 4.981333333333334,
      "grad_norm": 0.028153199702501297,
      "learning_rate": 0.0010545998415014326,
      "loss": 0.9332,
      "step": 3736
    },
    {
      "epoch": 4.982666666666667,
      "grad_norm": 0.029818303883075714,
      "learning_rate": 0.0010541686465420873,
      "loss": 0.9987,
      "step": 3737
    },
    {
      "epoch": 4.984,
      "grad_norm": 0.03426551818847656,
      "learning_rate": 0.0010537374414813384,
      "loss": 0.9986,
      "step": 3738
    },
    {
      "epoch": 4.985333333333333,
      "grad_norm": 0.03276040405035019,
      "learning_rate": 0.0010533062263995967,
      "loss": 1.0828,
      "step": 3739
    },
    {
      "epoch": 4.986666666666666,
      "grad_norm": 0.02931753173470497,
      "learning_rate": 0.001052875001377276,
      "loss": 0.9421,
      "step": 3740
    },
    {
      "epoch": 4.9879999999999995,
      "grad_norm": 0.031376972794532776,
      "learning_rate": 0.0010524437664947916,
      "loss": 1.0933,
      "step": 3741
    },
    {
      "epoch": 4.989333333333334,
      "grad_norm": 0.02586940862238407,
      "learning_rate": 0.0010520125218325604,
      "loss": 0.8034,
      "step": 3742
    },
    {
      "epoch": 4.990666666666667,
      "grad_norm": 0.03096798062324524,
      "learning_rate": 0.001051581267471001,
      "loss": 0.7597,
      "step": 3743
    },
    {
      "epoch": 4.992,
      "grad_norm": 0.026361988857388496,
      "learning_rate": 0.0010511500034905339,
      "loss": 1.0573,
      "step": 3744
    },
    {
      "epoch": 4.993333333333333,
      "grad_norm": 0.02350696735084057,
      "learning_rate": 0.0010507187299715814,
      "loss": 0.9445,
      "step": 3745
    },
    {
      "epoch": 4.994666666666666,
      "grad_norm": 0.04510299861431122,
      "learning_rate": 0.0010502874469945679,
      "loss": 1.2778,
      "step": 3746
    },
    {
      "epoch": 4.996,
      "grad_norm": 0.03309260308742523,
      "learning_rate": 0.0010498561546399192,
      "loss": 1.0462,
      "step": 3747
    },
    {
      "epoch": 4.997333333333334,
      "grad_norm": 0.029587585479021072,
      "learning_rate": 0.0010494248529880634,
      "loss": 0.7873,
      "step": 3748
    },
    {
      "epoch": 4.998666666666667,
      "grad_norm": 0.025594273582100868,
      "learning_rate": 0.0010489935421194294,
      "loss": 0.9519,
      "step": 3749
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.03270251303911209,
      "learning_rate": 0.0010485622221144484,
      "loss": 1.1254,
      "step": 3750
    },
    {
      "epoch": 5.001333333333333,
      "grad_norm": 0.025724489241838455,
      "learning_rate": 0.0010481308930535532,
      "loss": 0.9793,
      "step": 3751
    },
    {
      "epoch": 5.002666666666666,
      "grad_norm": 0.04170002043247223,
      "learning_rate": 0.0010476995550171783,
      "loss": 1.0753,
      "step": 3752
    },
    {
      "epoch": 5.004,
      "grad_norm": 0.030796129256486893,
      "learning_rate": 0.0010472682080857605,
      "loss": 1.2447,
      "step": 3753
    },
    {
      "epoch": 5.005333333333334,
      "grad_norm": 0.027599139139056206,
      "learning_rate": 0.001046836852339737,
      "loss": 0.8577,
      "step": 3754
    },
    {
      "epoch": 5.006666666666667,
      "grad_norm": 0.03849079832434654,
      "learning_rate": 0.0010464054878595473,
      "loss": 1.0634,
      "step": 3755
    },
    {
      "epoch": 5.008,
      "grad_norm": 0.03503568843007088,
      "learning_rate": 0.0010459741147256325,
      "loss": 1.0043,
      "step": 3756
    },
    {
      "epoch": 5.009333333333333,
      "grad_norm": 0.07173868268728256,
      "learning_rate": 0.0010455427330184357,
      "loss": 0.9828,
      "step": 3757
    },
    {
      "epoch": 5.010666666666666,
      "grad_norm": 0.0351080447435379,
      "learning_rate": 0.001045111342818401,
      "loss": 1.0175,
      "step": 3758
    },
    {
      "epoch": 5.012,
      "grad_norm": 0.031135592609643936,
      "learning_rate": 0.0010446799442059749,
      "loss": 1.2154,
      "step": 3759
    },
    {
      "epoch": 5.013333333333334,
      "grad_norm": 0.027101630344986916,
      "learning_rate": 0.0010442485372616037,
      "loss": 0.9024,
      "step": 3760
    },
    {
      "epoch": 5.014666666666667,
      "grad_norm": 0.029965082183480263,
      "learning_rate": 0.0010438171220657373,
      "loss": 1.0768,
      "step": 3761
    },
    {
      "epoch": 5.016,
      "grad_norm": 0.035597383975982666,
      "learning_rate": 0.001043385698698826,
      "loss": 1.0462,
      "step": 3762
    },
    {
      "epoch": 5.017333333333333,
      "grad_norm": 0.026570720598101616,
      "learning_rate": 0.001042954267241322,
      "loss": 0.926,
      "step": 3763
    },
    {
      "epoch": 5.018666666666666,
      "grad_norm": 0.0256184134632349,
      "learning_rate": 0.0010425228277736792,
      "loss": 1.1788,
      "step": 3764
    },
    {
      "epoch": 5.02,
      "grad_norm": 0.03155691176652908,
      "learning_rate": 0.0010420913803763521,
      "loss": 1.1009,
      "step": 3765
    },
    {
      "epoch": 5.021333333333334,
      "grad_norm": 0.02705618180334568,
      "learning_rate": 0.0010416599251297975,
      "loss": 0.9653,
      "step": 3766
    },
    {
      "epoch": 5.022666666666667,
      "grad_norm": 0.021736225113272667,
      "learning_rate": 0.0010412284621144738,
      "loss": 0.9065,
      "step": 3767
    },
    {
      "epoch": 5.024,
      "grad_norm": 0.022222673520445824,
      "learning_rate": 0.00104079699141084,
      "loss": 1.043,
      "step": 3768
    },
    {
      "epoch": 5.025333333333333,
      "grad_norm": 0.024386251345276833,
      "learning_rate": 0.0010403655130993576,
      "loss": 1.1007,
      "step": 3769
    },
    {
      "epoch": 5.026666666666666,
      "grad_norm": 0.030086442828178406,
      "learning_rate": 0.0010399340272604885,
      "loss": 0.8702,
      "step": 3770
    },
    {
      "epoch": 5.028,
      "grad_norm": 0.025262361392378807,
      "learning_rate": 0.0010395025339746964,
      "loss": 1.0087,
      "step": 3771
    },
    {
      "epoch": 5.029333333333334,
      "grad_norm": 0.04071235656738281,
      "learning_rate": 0.0010390710333224467,
      "loss": 0.8968,
      "step": 3772
    },
    {
      "epoch": 5.030666666666667,
      "grad_norm": 0.023516276851296425,
      "learning_rate": 0.0010386395253842056,
      "loss": 0.8971,
      "step": 3773
    },
    {
      "epoch": 5.032,
      "grad_norm": 0.024864783510565758,
      "learning_rate": 0.0010382080102404416,
      "loss": 1.0375,
      "step": 3774
    },
    {
      "epoch": 5.033333333333333,
      "grad_norm": 0.035313017666339874,
      "learning_rate": 0.0010377764879716234,
      "loss": 1.0054,
      "step": 3775
    },
    {
      "epoch": 5.034666666666666,
      "grad_norm": 0.027131397277116776,
      "learning_rate": 0.0010373449586582218,
      "loss": 0.9841,
      "step": 3776
    },
    {
      "epoch": 5.036,
      "grad_norm": 0.040766965597867966,
      "learning_rate": 0.001036913422380708,
      "loss": 1.107,
      "step": 3777
    },
    {
      "epoch": 5.037333333333334,
      "grad_norm": 0.02602771855890751,
      "learning_rate": 0.001036481879219556,
      "loss": 0.8705,
      "step": 3778
    },
    {
      "epoch": 5.038666666666667,
      "grad_norm": 0.040959157049655914,
      "learning_rate": 0.0010360503292552404,
      "loss": 1.299,
      "step": 3779
    },
    {
      "epoch": 5.04,
      "grad_norm": 0.027345910668373108,
      "learning_rate": 0.0010356187725682358,
      "loss": 0.9451,
      "step": 3780
    },
    {
      "epoch": 5.041333333333333,
      "grad_norm": 0.03416132554411888,
      "learning_rate": 0.0010351872092390201,
      "loss": 0.9317,
      "step": 3781
    },
    {
      "epoch": 5.042666666666666,
      "grad_norm": 0.02871743217110634,
      "learning_rate": 0.001034755639348071,
      "loss": 1.0361,
      "step": 3782
    },
    {
      "epoch": 5.044,
      "grad_norm": 0.044397734105587006,
      "learning_rate": 0.0010343240629758683,
      "loss": 0.8744,
      "step": 3783
    },
    {
      "epoch": 5.045333333333334,
      "grad_norm": 0.024977833032608032,
      "learning_rate": 0.0010338924802028924,
      "loss": 1.0181,
      "step": 3784
    },
    {
      "epoch": 5.046666666666667,
      "grad_norm": 0.03612036630511284,
      "learning_rate": 0.0010334608911096255,
      "loss": 1.1027,
      "step": 3785
    },
    {
      "epoch": 5.048,
      "grad_norm": 0.02020738087594509,
      "learning_rate": 0.0010330292957765501,
      "loss": 0.973,
      "step": 3786
    },
    {
      "epoch": 5.049333333333333,
      "grad_norm": 0.1051471158862114,
      "learning_rate": 0.0010325976942841507,
      "loss": 1.1707,
      "step": 3787
    },
    {
      "epoch": 5.050666666666666,
      "grad_norm": 0.0199449323117733,
      "learning_rate": 0.0010321660867129124,
      "loss": 0.8682,
      "step": 3788
    },
    {
      "epoch": 5.052,
      "grad_norm": 0.031891532242298126,
      "learning_rate": 0.0010317344731433215,
      "loss": 0.8192,
      "step": 3789
    },
    {
      "epoch": 5.053333333333334,
      "grad_norm": 0.03649503365159035,
      "learning_rate": 0.0010313028536558665,
      "loss": 1.0235,
      "step": 3790
    },
    {
      "epoch": 5.054666666666667,
      "grad_norm": 0.033639825880527496,
      "learning_rate": 0.001030871228331035,
      "loss": 1.0645,
      "step": 3791
    },
    {
      "epoch": 5.056,
      "grad_norm": 0.02586878463625908,
      "learning_rate": 0.0010304395972493172,
      "loss": 0.8871,
      "step": 3792
    },
    {
      "epoch": 5.057333333333333,
      "grad_norm": 0.022088084369897842,
      "learning_rate": 0.001030007960491204,
      "loss": 1.0124,
      "step": 3793
    },
    {
      "epoch": 5.058666666666666,
      "grad_norm": 0.023313185200095177,
      "learning_rate": 0.001029576318137187,
      "loss": 1.0345,
      "step": 3794
    },
    {
      "epoch": 5.06,
      "grad_norm": 0.019360223785042763,
      "learning_rate": 0.0010291446702677598,
      "loss": 0.995,
      "step": 3795
    },
    {
      "epoch": 5.061333333333334,
      "grad_norm": 0.02470659650862217,
      "learning_rate": 0.0010287130169634156,
      "loss": 0.9332,
      "step": 3796
    },
    {
      "epoch": 5.062666666666667,
      "grad_norm": 0.03329902142286301,
      "learning_rate": 0.0010282813583046499,
      "loss": 0.9119,
      "step": 3797
    },
    {
      "epoch": 5.064,
      "grad_norm": 0.030417952686548233,
      "learning_rate": 0.0010278496943719583,
      "loss": 1.0056,
      "step": 3798
    },
    {
      "epoch": 5.065333333333333,
      "grad_norm": 0.026119472458958626,
      "learning_rate": 0.001027418025245838,
      "loss": 1.0065,
      "step": 3799
    },
    {
      "epoch": 5.066666666666666,
      "grad_norm": 0.023605596274137497,
      "learning_rate": 0.0010269863510067871,
      "loss": 1.0423,
      "step": 3800
    },
    {
      "epoch": 5.068,
      "grad_norm": 0.027434665709733963,
      "learning_rate": 0.001026554671735304,
      "loss": 1.1852,
      "step": 3801
    },
    {
      "epoch": 5.069333333333334,
      "grad_norm": 0.023811249062418938,
      "learning_rate": 0.0010261229875118889,
      "loss": 0.8125,
      "step": 3802
    },
    {
      "epoch": 5.070666666666667,
      "grad_norm": 0.030511189252138138,
      "learning_rate": 0.0010256912984170428,
      "loss": 1.1533,
      "step": 3803
    },
    {
      "epoch": 5.072,
      "grad_norm": 0.022940512746572495,
      "learning_rate": 0.0010252596045312666,
      "loss": 1.1996,
      "step": 3804
    },
    {
      "epoch": 5.073333333333333,
      "grad_norm": 0.0508083775639534,
      "learning_rate": 0.0010248279059350634,
      "loss": 0.9619,
      "step": 3805
    },
    {
      "epoch": 5.074666666666666,
      "grad_norm": 0.028898581862449646,
      "learning_rate": 0.0010243962027089363,
      "loss": 1.0237,
      "step": 3806
    },
    {
      "epoch": 5.076,
      "grad_norm": 0.02428385615348816,
      "learning_rate": 0.0010239644949333899,
      "loss": 1.4695,
      "step": 3807
    },
    {
      "epoch": 5.077333333333334,
      "grad_norm": 0.036955203860998154,
      "learning_rate": 0.0010235327826889295,
      "loss": 1.2618,
      "step": 3808
    },
    {
      "epoch": 5.078666666666667,
      "grad_norm": 0.026946812868118286,
      "learning_rate": 0.0010231010660560604,
      "loss": 1.1543,
      "step": 3809
    },
    {
      "epoch": 5.08,
      "grad_norm": 0.03248948976397514,
      "learning_rate": 0.00102266934511529,
      "loss": 1.1534,
      "step": 3810
    },
    {
      "epoch": 5.081333333333333,
      "grad_norm": 0.025455614551901817,
      "learning_rate": 0.001022237619947126,
      "loss": 1.0143,
      "step": 3811
    },
    {
      "epoch": 5.082666666666666,
      "grad_norm": 0.028954526409506798,
      "learning_rate": 0.0010218058906320759,
      "loss": 1.073,
      "step": 3812
    },
    {
      "epoch": 5.084,
      "grad_norm": 0.03554246202111244,
      "learning_rate": 0.0010213741572506496,
      "loss": 1.0852,
      "step": 3813
    },
    {
      "epoch": 5.085333333333334,
      "grad_norm": 0.022939816117286682,
      "learning_rate": 0.001020942419883357,
      "loss": 0.915,
      "step": 3814
    },
    {
      "epoch": 5.086666666666667,
      "grad_norm": 0.022706378251314163,
      "learning_rate": 0.0010205106786107085,
      "loss": 1.0238,
      "step": 3815
    },
    {
      "epoch": 5.088,
      "grad_norm": 0.058202702552080154,
      "learning_rate": 0.0010200789335132157,
      "loss": 1.0303,
      "step": 3816
    },
    {
      "epoch": 5.089333333333333,
      "grad_norm": 0.021035725250840187,
      "learning_rate": 0.0010196471846713906,
      "loss": 0.9634,
      "step": 3817
    },
    {
      "epoch": 5.0906666666666665,
      "grad_norm": 0.02667752094566822,
      "learning_rate": 0.0010192154321657456,
      "loss": 1.0368,
      "step": 3818
    },
    {
      "epoch": 5.092,
      "grad_norm": 0.02704273723065853,
      "learning_rate": 0.0010187836760767951,
      "loss": 0.9642,
      "step": 3819
    },
    {
      "epoch": 5.093333333333334,
      "grad_norm": 0.022075612097978592,
      "learning_rate": 0.0010183519164850526,
      "loss": 1.0317,
      "step": 3820
    },
    {
      "epoch": 5.094666666666667,
      "grad_norm": 0.03779682144522667,
      "learning_rate": 0.001017920153471033,
      "loss": 0.8934,
      "step": 3821
    },
    {
      "epoch": 5.096,
      "grad_norm": 0.024503502994775772,
      "learning_rate": 0.0010174883871152515,
      "loss": 1.0565,
      "step": 3822
    },
    {
      "epoch": 5.097333333333333,
      "grad_norm": 0.025104530155658722,
      "learning_rate": 0.0010170566174982248,
      "loss": 0.9791,
      "step": 3823
    },
    {
      "epoch": 5.0986666666666665,
      "grad_norm": 0.02775881625711918,
      "learning_rate": 0.001016624844700469,
      "loss": 0.8342,
      "step": 3824
    },
    {
      "epoch": 5.1,
      "grad_norm": 0.028288239613175392,
      "learning_rate": 0.0010161930688025015,
      "loss": 1.0685,
      "step": 3825
    },
    {
      "epoch": 5.101333333333334,
      "grad_norm": 0.0302365031093359,
      "learning_rate": 0.0010157612898848405,
      "loss": 1.0212,
      "step": 3826
    },
    {
      "epoch": 5.102666666666667,
      "grad_norm": 0.028109995648264885,
      "learning_rate": 0.0010153295080280041,
      "loss": 1.0585,
      "step": 3827
    },
    {
      "epoch": 5.104,
      "grad_norm": 0.023643890395760536,
      "learning_rate": 0.0010148977233125109,
      "loss": 1.1051,
      "step": 3828
    },
    {
      "epoch": 5.105333333333333,
      "grad_norm": 0.023211248219013214,
      "learning_rate": 0.001014465935818881,
      "loss": 1.2022,
      "step": 3829
    },
    {
      "epoch": 5.1066666666666665,
      "grad_norm": 0.027311621233820915,
      "learning_rate": 0.0010140341456276343,
      "loss": 1.0298,
      "step": 3830
    },
    {
      "epoch": 5.108,
      "grad_norm": 0.02528229169547558,
      "learning_rate": 0.001013602352819291,
      "loss": 0.955,
      "step": 3831
    },
    {
      "epoch": 5.109333333333334,
      "grad_norm": 0.022662654519081116,
      "learning_rate": 0.0010131705574743725,
      "loss": 0.8311,
      "step": 3832
    },
    {
      "epoch": 5.110666666666667,
      "grad_norm": 0.04869873821735382,
      "learning_rate": 0.0010127387596734,
      "loss": 1.0469,
      "step": 3833
    },
    {
      "epoch": 5.112,
      "grad_norm": 0.030178749933838844,
      "learning_rate": 0.0010123069594968953,
      "loss": 1.1678,
      "step": 3834
    },
    {
      "epoch": 5.113333333333333,
      "grad_norm": 0.025597067549824715,
      "learning_rate": 0.0010118751570253812,
      "loss": 1.2613,
      "step": 3835
    },
    {
      "epoch": 5.1146666666666665,
      "grad_norm": 0.02392439916729927,
      "learning_rate": 0.00101144335233938,
      "loss": 1.0129,
      "step": 3836
    },
    {
      "epoch": 5.116,
      "grad_norm": 0.0283180121332407,
      "learning_rate": 0.0010110115455194155,
      "loss": 1.2781,
      "step": 3837
    },
    {
      "epoch": 5.117333333333334,
      "grad_norm": 0.023905165493488312,
      "learning_rate": 0.0010105797366460108,
      "loss": 1.0919,
      "step": 3838
    },
    {
      "epoch": 5.118666666666667,
      "grad_norm": 0.028970781713724136,
      "learning_rate": 0.00101014792579969,
      "loss": 1.3112,
      "step": 3839
    },
    {
      "epoch": 5.12,
      "grad_norm": 0.02224254608154297,
      "learning_rate": 0.0010097161130609774,
      "loss": 1.0002,
      "step": 3840
    },
    {
      "epoch": 5.121333333333333,
      "grad_norm": 0.02827044203877449,
      "learning_rate": 0.0010092842985103983,
      "loss": 0.8637,
      "step": 3841
    },
    {
      "epoch": 5.1226666666666665,
      "grad_norm": 0.024088222533464432,
      "learning_rate": 0.001008852482228477,
      "loss": 0.7791,
      "step": 3842
    },
    {
      "epoch": 5.124,
      "grad_norm": 0.031866565346717834,
      "learning_rate": 0.0010084206642957392,
      "loss": 0.9269,
      "step": 3843
    },
    {
      "epoch": 5.125333333333334,
      "grad_norm": 0.034837935119867325,
      "learning_rate": 0.0010079888447927106,
      "loss": 0.85,
      "step": 3844
    },
    {
      "epoch": 5.126666666666667,
      "grad_norm": 0.026087084785103798,
      "learning_rate": 0.001007557023799917,
      "loss": 0.8488,
      "step": 3845
    },
    {
      "epoch": 5.128,
      "grad_norm": 0.03293653577566147,
      "learning_rate": 0.0010071252013978851,
      "loss": 0.951,
      "step": 3846
    },
    {
      "epoch": 5.129333333333333,
      "grad_norm": 0.028371920809149742,
      "learning_rate": 0.001006693377667141,
      "loss": 0.9226,
      "step": 3847
    },
    {
      "epoch": 5.1306666666666665,
      "grad_norm": 0.02423178404569626,
      "learning_rate": 0.0010062615526882112,
      "loss": 1.0278,
      "step": 3848
    },
    {
      "epoch": 5.132,
      "grad_norm": 0.02867254987359047,
      "learning_rate": 0.0010058297265416235,
      "loss": 1.2557,
      "step": 3849
    },
    {
      "epoch": 5.133333333333334,
      "grad_norm": 0.02606942318379879,
      "learning_rate": 0.0010053978993079045,
      "loss": 0.9988,
      "step": 3850
    },
    {
      "epoch": 5.134666666666667,
      "grad_norm": 0.030314018949866295,
      "learning_rate": 0.0010049660710675817,
      "loss": 0.8428,
      "step": 3851
    },
    {
      "epoch": 5.136,
      "grad_norm": 0.032149288803339005,
      "learning_rate": 0.0010045342419011832,
      "loss": 0.8109,
      "step": 3852
    },
    {
      "epoch": 5.137333333333333,
      "grad_norm": 0.028823696076869965,
      "learning_rate": 0.001004102411889236,
      "loss": 0.7672,
      "step": 3853
    },
    {
      "epoch": 5.1386666666666665,
      "grad_norm": 0.02233334444463253,
      "learning_rate": 0.0010036705811122687,
      "loss": 1.2258,
      "step": 3854
    },
    {
      "epoch": 5.14,
      "grad_norm": 0.027325276285409927,
      "learning_rate": 0.001003238749650809,
      "loss": 0.8301,
      "step": 3855
    },
    {
      "epoch": 5.141333333333334,
      "grad_norm": 0.028361886739730835,
      "learning_rate": 0.0010028069175853848,
      "loss": 0.9754,
      "step": 3856
    },
    {
      "epoch": 5.142666666666667,
      "grad_norm": 0.024056032299995422,
      "learning_rate": 0.0010023750849965255,
      "loss": 1.0582,
      "step": 3857
    },
    {
      "epoch": 5.144,
      "grad_norm": 0.028129560872912407,
      "learning_rate": 0.0010019432519647585,
      "loss": 0.9496,
      "step": 3858
    },
    {
      "epoch": 5.145333333333333,
      "grad_norm": 0.03451457619667053,
      "learning_rate": 0.0010015114185706127,
      "loss": 0.9896,
      "step": 3859
    },
    {
      "epoch": 5.1466666666666665,
      "grad_norm": 0.020598389208316803,
      "learning_rate": 0.0010010795848946165,
      "loss": 0.842,
      "step": 3860
    },
    {
      "epoch": 5.148,
      "grad_norm": 0.029038840904831886,
      "learning_rate": 0.0010006477510172984,
      "loss": 0.9549,
      "step": 3861
    },
    {
      "epoch": 5.149333333333334,
      "grad_norm": 0.035169489681720734,
      "learning_rate": 0.0010002159170191876,
      "loss": 0.9754,
      "step": 3862
    },
    {
      "epoch": 5.150666666666667,
      "grad_norm": 0.02447918988764286,
      "learning_rate": 0.0009997840829808126,
      "loss": 0.9934,
      "step": 3863
    },
    {
      "epoch": 5.152,
      "grad_norm": 0.024949271231889725,
      "learning_rate": 0.0009993522489827014,
      "loss": 1.0962,
      "step": 3864
    },
    {
      "epoch": 5.153333333333333,
      "grad_norm": 0.03107052855193615,
      "learning_rate": 0.0009989204151053838,
      "loss": 1.0234,
      "step": 3865
    },
    {
      "epoch": 5.1546666666666665,
      "grad_norm": 0.029012296348810196,
      "learning_rate": 0.0009984885814293878,
      "loss": 1.3326,
      "step": 3866
    },
    {
      "epoch": 5.156,
      "grad_norm": 0.030102387070655823,
      "learning_rate": 0.0009980567480352416,
      "loss": 1.0663,
      "step": 3867
    },
    {
      "epoch": 5.157333333333334,
      "grad_norm": 0.028009083122015,
      "learning_rate": 0.0009976249150034748,
      "loss": 1.2738,
      "step": 3868
    },
    {
      "epoch": 5.158666666666667,
      "grad_norm": 0.02267240360379219,
      "learning_rate": 0.000997193082414615,
      "loss": 0.9333,
      "step": 3869
    },
    {
      "epoch": 5.16,
      "grad_norm": 0.03316677361726761,
      "learning_rate": 0.0009967612503491913,
      "loss": 0.7308,
      "step": 3870
    },
    {
      "epoch": 5.161333333333333,
      "grad_norm": 0.02312169224023819,
      "learning_rate": 0.0009963294188877318,
      "loss": 0.7876,
      "step": 3871
    },
    {
      "epoch": 5.1626666666666665,
      "grad_norm": 0.025451742112636566,
      "learning_rate": 0.000995897588110764,
      "loss": 1.0732,
      "step": 3872
    },
    {
      "epoch": 5.164,
      "grad_norm": 0.026406828314065933,
      "learning_rate": 0.000995465758098817,
      "loss": 0.8022,
      "step": 3873
    },
    {
      "epoch": 5.165333333333333,
      "grad_norm": 0.03452238067984581,
      "learning_rate": 0.0009950339289324184,
      "loss": 1.383,
      "step": 3874
    },
    {
      "epoch": 5.166666666666667,
      "grad_norm": 0.022540727630257607,
      "learning_rate": 0.0009946021006920957,
      "loss": 0.9182,
      "step": 3875
    },
    {
      "epoch": 5.168,
      "grad_norm": 0.025514129549264908,
      "learning_rate": 0.000994170273458377,
      "loss": 1.3175,
      "step": 3876
    },
    {
      "epoch": 5.169333333333333,
      "grad_norm": 0.03916890546679497,
      "learning_rate": 0.0009937384473117888,
      "loss": 0.928,
      "step": 3877
    },
    {
      "epoch": 5.1706666666666665,
      "grad_norm": 0.025393564254045486,
      "learning_rate": 0.0009933066223328593,
      "loss": 0.9068,
      "step": 3878
    },
    {
      "epoch": 5.172,
      "grad_norm": 0.03314638137817383,
      "learning_rate": 0.0009928747986021151,
      "loss": 1.3103,
      "step": 3879
    },
    {
      "epoch": 5.173333333333334,
      "grad_norm": 0.0297697726637125,
      "learning_rate": 0.000992442976200083,
      "loss": 0.8609,
      "step": 3880
    },
    {
      "epoch": 5.174666666666667,
      "grad_norm": 0.024629555642604828,
      "learning_rate": 0.0009920111552072898,
      "loss": 0.8773,
      "step": 3881
    },
    {
      "epoch": 5.176,
      "grad_norm": 0.027476824820041656,
      "learning_rate": 0.0009915793357042609,
      "loss": 1.0943,
      "step": 3882
    },
    {
      "epoch": 5.177333333333333,
      "grad_norm": 0.029960835352540016,
      "learning_rate": 0.0009911475177715233,
      "loss": 0.9854,
      "step": 3883
    },
    {
      "epoch": 5.1786666666666665,
      "grad_norm": 0.02964780665934086,
      "learning_rate": 0.000990715701489602,
      "loss": 1.0335,
      "step": 3884
    },
    {
      "epoch": 5.18,
      "grad_norm": 0.023395851254463196,
      "learning_rate": 0.0009902838869390229,
      "loss": 1.226,
      "step": 3885
    },
    {
      "epoch": 5.181333333333333,
      "grad_norm": 0.022926049306988716,
      "learning_rate": 0.00098985207420031,
      "loss": 1.211,
      "step": 3886
    },
    {
      "epoch": 5.182666666666667,
      "grad_norm": 0.02507944032549858,
      "learning_rate": 0.0009894202633539894,
      "loss": 1.1102,
      "step": 3887
    },
    {
      "epoch": 5.184,
      "grad_norm": 0.025106001645326614,
      "learning_rate": 0.000988988454480585,
      "loss": 1.0464,
      "step": 3888
    },
    {
      "epoch": 5.185333333333333,
      "grad_norm": 0.031133538112044334,
      "learning_rate": 0.0009885566476606198,
      "loss": 0.9806,
      "step": 3889
    },
    {
      "epoch": 5.1866666666666665,
      "grad_norm": 0.02511942759156227,
      "learning_rate": 0.000988124842974619,
      "loss": 1.2068,
      "step": 3890
    },
    {
      "epoch": 5.188,
      "grad_norm": 0.02981841377913952,
      "learning_rate": 0.0009876930405031048,
      "loss": 0.9508,
      "step": 3891
    },
    {
      "epoch": 5.189333333333333,
      "grad_norm": 0.021047228947281837,
      "learning_rate": 0.0009872612403266003,
      "loss": 0.8759,
      "step": 3892
    },
    {
      "epoch": 5.190666666666667,
      "grad_norm": 0.023956019431352615,
      "learning_rate": 0.0009868294425256277,
      "loss": 1.1335,
      "step": 3893
    },
    {
      "epoch": 5.192,
      "grad_norm": 0.02400106191635132,
      "learning_rate": 0.000986397647180709,
      "loss": 0.8964,
      "step": 3894
    },
    {
      "epoch": 5.193333333333333,
      "grad_norm": 0.03531316667795181,
      "learning_rate": 0.000985965854372366,
      "loss": 1.1606,
      "step": 3895
    },
    {
      "epoch": 5.1946666666666665,
      "grad_norm": 0.022358544170856476,
      "learning_rate": 0.000985534064181119,
      "loss": 1.088,
      "step": 3896
    },
    {
      "epoch": 5.196,
      "grad_norm": 0.0603758841753006,
      "learning_rate": 0.0009851022766874892,
      "loss": 0.8319,
      "step": 3897
    },
    {
      "epoch": 5.197333333333333,
      "grad_norm": 0.026577264070510864,
      "learning_rate": 0.0009846704919719963,
      "loss": 0.8968,
      "step": 3898
    },
    {
      "epoch": 5.198666666666667,
      "grad_norm": 0.02492927946150303,
      "learning_rate": 0.0009842387101151597,
      "loss": 1.0767,
      "step": 3899
    },
    {
      "epoch": 5.2,
      "grad_norm": 0.023970110341906548,
      "learning_rate": 0.0009838069311974985,
      "loss": 1.0142,
      "step": 3900
    },
    {
      "epoch": 5.201333333333333,
      "grad_norm": 0.030587930232286453,
      "learning_rate": 0.0009833751552995311,
      "loss": 0.9633,
      "step": 3901
    },
    {
      "epoch": 5.2026666666666666,
      "grad_norm": 0.02613728679716587,
      "learning_rate": 0.0009829433825017755,
      "loss": 1.0319,
      "step": 3902
    },
    {
      "epoch": 5.204,
      "grad_norm": 0.02886172942817211,
      "learning_rate": 0.0009825116128847487,
      "loss": 1.1474,
      "step": 3903
    },
    {
      "epoch": 5.205333333333333,
      "grad_norm": 0.02228691056370735,
      "learning_rate": 0.0009820798465289672,
      "loss": 1.2293,
      "step": 3904
    },
    {
      "epoch": 5.206666666666667,
      "grad_norm": 0.022496866062283516,
      "learning_rate": 0.0009816480835149477,
      "loss": 1.1286,
      "step": 3905
    },
    {
      "epoch": 5.208,
      "grad_norm": 0.025375237688422203,
      "learning_rate": 0.0009812163239232051,
      "loss": 1.0253,
      "step": 3906
    },
    {
      "epoch": 5.209333333333333,
      "grad_norm": 0.039058882743120193,
      "learning_rate": 0.0009807845678342544,
      "loss": 1.0847,
      "step": 3907
    },
    {
      "epoch": 5.210666666666667,
      "grad_norm": 0.024146253243088722,
      "learning_rate": 0.0009803528153286098,
      "loss": 0.8988,
      "step": 3908
    },
    {
      "epoch": 5.212,
      "grad_norm": 0.025405466556549072,
      "learning_rate": 0.0009799210664867843,
      "loss": 1.3043,
      "step": 3909
    },
    {
      "epoch": 5.213333333333333,
      "grad_norm": 0.04745238646864891,
      "learning_rate": 0.0009794893213892918,
      "loss": 0.7588,
      "step": 3910
    },
    {
      "epoch": 5.214666666666667,
      "grad_norm": 0.023278087377548218,
      "learning_rate": 0.000979057580116643,
      "loss": 1.0225,
      "step": 3911
    },
    {
      "epoch": 5.216,
      "grad_norm": 0.06169268861413002,
      "learning_rate": 0.0009786258427493505,
      "loss": 0.9288,
      "step": 3912
    },
    {
      "epoch": 5.217333333333333,
      "grad_norm": 0.025576308369636536,
      "learning_rate": 0.0009781941093679246,
      "loss": 0.7595,
      "step": 3913
    },
    {
      "epoch": 5.218666666666667,
      "grad_norm": 0.023449193686246872,
      "learning_rate": 0.0009777623800528742,
      "loss": 1.0519,
      "step": 3914
    },
    {
      "epoch": 5.22,
      "grad_norm": 0.030315451323986053,
      "learning_rate": 0.00097733065488471,
      "loss": 1.0268,
      "step": 3915
    },
    {
      "epoch": 5.221333333333333,
      "grad_norm": 0.0279835294932127,
      "learning_rate": 0.0009768989339439395,
      "loss": 1.2255,
      "step": 3916
    },
    {
      "epoch": 5.222666666666667,
      "grad_norm": 0.0244652908295393,
      "learning_rate": 0.0009764672173110706,
      "loss": 1.0537,
      "step": 3917
    },
    {
      "epoch": 5.224,
      "grad_norm": 0.03038833849132061,
      "learning_rate": 0.00097603550506661,
      "loss": 1.0361,
      "step": 3918
    },
    {
      "epoch": 5.225333333333333,
      "grad_norm": 0.028357576578855515,
      "learning_rate": 0.0009756037972910637,
      "loss": 0.9711,
      "step": 3919
    },
    {
      "epoch": 5.226666666666667,
      "grad_norm": 0.040230892598629,
      "learning_rate": 0.0009751720940649369,
      "loss": 0.8522,
      "step": 3920
    },
    {
      "epoch": 5.228,
      "grad_norm": 0.031294286251068115,
      "learning_rate": 0.0009747403954687334,
      "loss": 1.0786,
      "step": 3921
    },
    {
      "epoch": 5.229333333333333,
      "grad_norm": 0.03446796163916588,
      "learning_rate": 0.0009743087015829574,
      "loss": 0.7485,
      "step": 3922
    },
    {
      "epoch": 5.230666666666667,
      "grad_norm": 0.023910675197839737,
      "learning_rate": 0.0009738770124881112,
      "loss": 1.0526,
      "step": 3923
    },
    {
      "epoch": 5.232,
      "grad_norm": 0.039052143692970276,
      "learning_rate": 0.0009734453282646961,
      "loss": 0.8444,
      "step": 3924
    },
    {
      "epoch": 5.233333333333333,
      "grad_norm": 0.020898936316370964,
      "learning_rate": 0.0009730136489932132,
      "loss": 0.6826,
      "step": 3925
    },
    {
      "epoch": 5.234666666666667,
      "grad_norm": 0.02259800210595131,
      "learning_rate": 0.0009725819747541619,
      "loss": 1.0413,
      "step": 3926
    },
    {
      "epoch": 5.236,
      "grad_norm": 0.03263593092560768,
      "learning_rate": 0.0009721503056280417,
      "loss": 1.5003,
      "step": 3927
    },
    {
      "epoch": 5.237333333333333,
      "grad_norm": 0.03568127751350403,
      "learning_rate": 0.0009717186416953504,
      "loss": 0.9587,
      "step": 3928
    },
    {
      "epoch": 5.238666666666667,
      "grad_norm": 0.029605917632579803,
      "learning_rate": 0.0009712869830365846,
      "loss": 0.7951,
      "step": 3929
    },
    {
      "epoch": 5.24,
      "grad_norm": 0.037714552134275436,
      "learning_rate": 0.0009708553297322406,
      "loss": 0.7808,
      "step": 3930
    },
    {
      "epoch": 5.241333333333333,
      "grad_norm": 0.025831323117017746,
      "learning_rate": 0.0009704236818628128,
      "loss": 0.8412,
      "step": 3931
    },
    {
      "epoch": 5.242666666666667,
      "grad_norm": 0.030421333387494087,
      "learning_rate": 0.0009699920395087962,
      "loss": 1.202,
      "step": 3932
    },
    {
      "epoch": 5.244,
      "grad_norm": 0.031736090779304504,
      "learning_rate": 0.0009695604027506828,
      "loss": 0.8732,
      "step": 3933
    },
    {
      "epoch": 5.245333333333333,
      "grad_norm": 0.03249623253941536,
      "learning_rate": 0.0009691287716689653,
      "loss": 0.8961,
      "step": 3934
    },
    {
      "epoch": 5.246666666666667,
      "grad_norm": 0.03201889619231224,
      "learning_rate": 0.000968697146344134,
      "loss": 0.9938,
      "step": 3935
    },
    {
      "epoch": 5.248,
      "grad_norm": 0.031020021066069603,
      "learning_rate": 0.0009682655268566782,
      "loss": 0.998,
      "step": 3936
    },
    {
      "epoch": 5.249333333333333,
      "grad_norm": 0.017382865771651268,
      "learning_rate": 0.0009678339132870879,
      "loss": 0.8407,
      "step": 3937
    },
    {
      "epoch": 5.250666666666667,
      "grad_norm": 0.0258098803460598,
      "learning_rate": 0.0009674023057158492,
      "loss": 0.8409,
      "step": 3938
    },
    {
      "epoch": 5.252,
      "grad_norm": 0.027690213173627853,
      "learning_rate": 0.0009669707042234501,
      "loss": 1.2246,
      "step": 3939
    },
    {
      "epoch": 5.253333333333333,
      "grad_norm": 0.04847889021039009,
      "learning_rate": 0.0009665391088903749,
      "loss": 1.1084,
      "step": 3940
    },
    {
      "epoch": 5.254666666666667,
      "grad_norm": 0.025218572467565536,
      "learning_rate": 0.0009661075197971074,
      "loss": 1.0844,
      "step": 3941
    },
    {
      "epoch": 5.256,
      "grad_norm": 0.02485773153603077,
      "learning_rate": 0.0009656759370241318,
      "loss": 1.1676,
      "step": 3942
    },
    {
      "epoch": 5.257333333333333,
      "grad_norm": 0.030375657603144646,
      "learning_rate": 0.0009652443606519288,
      "loss": 1.0889,
      "step": 3943
    },
    {
      "epoch": 5.258666666666667,
      "grad_norm": 0.033239029347896576,
      "learning_rate": 0.00096481279076098,
      "loss": 1.2073,
      "step": 3944
    },
    {
      "epoch": 5.26,
      "grad_norm": 0.024974627420306206,
      "learning_rate": 0.0009643812274317643,
      "loss": 0.9977,
      "step": 3945
    },
    {
      "epoch": 5.261333333333333,
      "grad_norm": 0.030780259519815445,
      "learning_rate": 0.0009639496707447599,
      "loss": 0.8966,
      "step": 3946
    },
    {
      "epoch": 5.262666666666667,
      "grad_norm": 0.02884363755583763,
      "learning_rate": 0.0009635181207804441,
      "loss": 1.154,
      "step": 3947
    },
    {
      "epoch": 5.264,
      "grad_norm": 0.028163043782114983,
      "learning_rate": 0.0009630865776192917,
      "loss": 0.8283,
      "step": 3948
    },
    {
      "epoch": 5.265333333333333,
      "grad_norm": 0.03343106061220169,
      "learning_rate": 0.0009626550413417785,
      "loss": 1.1426,
      "step": 3949
    },
    {
      "epoch": 5.266666666666667,
      "grad_norm": 0.029915805906057358,
      "learning_rate": 0.0009622235120283768,
      "loss": 1.1487,
      "step": 3950
    },
    {
      "epoch": 5.268,
      "grad_norm": 0.033684831112623215,
      "learning_rate": 0.0009617919897595585,
      "loss": 0.9424,
      "step": 3951
    },
    {
      "epoch": 5.269333333333333,
      "grad_norm": 0.029256967827677727,
      "learning_rate": 0.0009613604746157944,
      "loss": 1.0847,
      "step": 3952
    },
    {
      "epoch": 5.270666666666667,
      "grad_norm": 0.03142697364091873,
      "learning_rate": 0.0009609289666775533,
      "loss": 1.2018,
      "step": 3953
    },
    {
      "epoch": 5.272,
      "grad_norm": 0.0259932242333889,
      "learning_rate": 0.0009604974660253038,
      "loss": 1.2677,
      "step": 3954
    },
    {
      "epoch": 5.273333333333333,
      "grad_norm": 0.025265401229262352,
      "learning_rate": 0.000960065972739512,
      "loss": 0.977,
      "step": 3955
    },
    {
      "epoch": 5.274666666666667,
      "grad_norm": 0.03627517446875572,
      "learning_rate": 0.0009596344869006426,
      "loss": 1.1217,
      "step": 3956
    },
    {
      "epoch": 5.276,
      "grad_norm": 0.03144454210996628,
      "learning_rate": 0.0009592030085891601,
      "loss": 0.9117,
      "step": 3957
    },
    {
      "epoch": 5.277333333333333,
      "grad_norm": 0.02505916729569435,
      "learning_rate": 0.0009587715378855261,
      "loss": 0.7881,
      "step": 3958
    },
    {
      "epoch": 5.278666666666667,
      "grad_norm": 0.024713609367609024,
      "learning_rate": 0.0009583400748702025,
      "loss": 0.9029,
      "step": 3959
    },
    {
      "epoch": 5.28,
      "grad_norm": 0.02836727723479271,
      "learning_rate": 0.0009579086196236481,
      "loss": 1.0789,
      "step": 3960
    },
    {
      "epoch": 5.281333333333333,
      "grad_norm": 0.02787606231868267,
      "learning_rate": 0.0009574771722263211,
      "loss": 1.0956,
      "step": 3961
    },
    {
      "epoch": 5.282666666666667,
      "grad_norm": 0.030639689415693283,
      "learning_rate": 0.0009570457327586781,
      "loss": 0.8524,
      "step": 3962
    },
    {
      "epoch": 5.284,
      "grad_norm": 0.021798696368932724,
      "learning_rate": 0.000956614301301174,
      "loss": 1.0805,
      "step": 3963
    },
    {
      "epoch": 5.285333333333333,
      "grad_norm": 0.023511983454227448,
      "learning_rate": 0.0009561828779342628,
      "loss": 0.9105,
      "step": 3964
    },
    {
      "epoch": 5.286666666666667,
      "grad_norm": 0.02574518695473671,
      "learning_rate": 0.0009557514627383967,
      "loss": 0.9599,
      "step": 3965
    },
    {
      "epoch": 5.288,
      "grad_norm": 0.020997291430830956,
      "learning_rate": 0.0009553200557940254,
      "loss": 1.0556,
      "step": 3966
    },
    {
      "epoch": 5.289333333333333,
      "grad_norm": 0.02792726270854473,
      "learning_rate": 0.0009548886571815989,
      "loss": 1.1117,
      "step": 3967
    },
    {
      "epoch": 5.290666666666667,
      "grad_norm": 0.027414966374635696,
      "learning_rate": 0.0009544572669815642,
      "loss": 0.8725,
      "step": 3968
    },
    {
      "epoch": 5.292,
      "grad_norm": 0.023780807852745056,
      "learning_rate": 0.0009540258852743676,
      "loss": 1.129,
      "step": 3969
    },
    {
      "epoch": 5.293333333333333,
      "grad_norm": 0.033163800835609436,
      "learning_rate": 0.0009535945121404531,
      "loss": 1.0794,
      "step": 3970
    },
    {
      "epoch": 5.294666666666667,
      "grad_norm": 0.03569358214735985,
      "learning_rate": 0.0009531631476602633,
      "loss": 1.2016,
      "step": 3971
    },
    {
      "epoch": 5.296,
      "grad_norm": 0.03194783627986908,
      "learning_rate": 0.0009527317919142398,
      "loss": 0.9823,
      "step": 3972
    },
    {
      "epoch": 5.2973333333333334,
      "grad_norm": 0.024674419313669205,
      "learning_rate": 0.0009523004449828215,
      "loss": 1.029,
      "step": 3973
    },
    {
      "epoch": 5.298666666666667,
      "grad_norm": 0.02530561573803425,
      "learning_rate": 0.000951869106946447,
      "loss": 0.7287,
      "step": 3974
    },
    {
      "epoch": 5.3,
      "grad_norm": 0.0252668559551239,
      "learning_rate": 0.000951437777885552,
      "loss": 0.9582,
      "step": 3975
    },
    {
      "epoch": 5.301333333333333,
      "grad_norm": 0.030314672738313675,
      "learning_rate": 0.0009510064578805707,
      "loss": 0.985,
      "step": 3976
    },
    {
      "epoch": 5.302666666666667,
      "grad_norm": 0.02391214109957218,
      "learning_rate": 0.0009505751470119369,
      "loss": 1.2644,
      "step": 3977
    },
    {
      "epoch": 5.304,
      "grad_norm": 0.02277866192162037,
      "learning_rate": 0.0009501438453600807,
      "loss": 0.9354,
      "step": 3978
    },
    {
      "epoch": 5.3053333333333335,
      "grad_norm": 0.023448534309864044,
      "learning_rate": 0.0009497125530054324,
      "loss": 1.0685,
      "step": 3979
    },
    {
      "epoch": 5.306666666666667,
      "grad_norm": 0.020521488040685654,
      "learning_rate": 0.0009492812700284185,
      "loss": 1.1562,
      "step": 3980
    },
    {
      "epoch": 5.308,
      "grad_norm": 0.03908001631498337,
      "learning_rate": 0.0009488499965094664,
      "loss": 1.0413,
      "step": 3981
    },
    {
      "epoch": 5.309333333333333,
      "grad_norm": 0.03706821799278259,
      "learning_rate": 0.0009484187325289994,
      "loss": 1.0419,
      "step": 3982
    },
    {
      "epoch": 5.310666666666666,
      "grad_norm": 0.02578110434114933,
      "learning_rate": 0.0009479874781674397,
      "loss": 1.0391,
      "step": 3983
    },
    {
      "epoch": 5.312,
      "grad_norm": 0.0336134135723114,
      "learning_rate": 0.0009475562335052084,
      "loss": 0.9708,
      "step": 3984
    },
    {
      "epoch": 5.3133333333333335,
      "grad_norm": 0.023972230032086372,
      "learning_rate": 0.0009471249986227238,
      "loss": 1.1668,
      "step": 3985
    },
    {
      "epoch": 5.314666666666667,
      "grad_norm": 0.02910592593252659,
      "learning_rate": 0.0009466937736004034,
      "loss": 0.939,
      "step": 3986
    },
    {
      "epoch": 5.316,
      "grad_norm": 0.033328041434288025,
      "learning_rate": 0.0009462625585186622,
      "loss": 1.3891,
      "step": 3987
    },
    {
      "epoch": 5.317333333333333,
      "grad_norm": 0.02406344749033451,
      "learning_rate": 0.000945831353457913,
      "loss": 0.9717,
      "step": 3988
    },
    {
      "epoch": 5.318666666666667,
      "grad_norm": 0.03315463662147522,
      "learning_rate": 0.0009454001584985677,
      "loss": 0.8145,
      "step": 3989
    },
    {
      "epoch": 5.32,
      "grad_norm": 0.02331734262406826,
      "learning_rate": 0.000944968973721035,
      "loss": 1.0951,
      "step": 3990
    },
    {
      "epoch": 5.3213333333333335,
      "grad_norm": 0.02948112227022648,
      "learning_rate": 0.0009445377992057238,
      "loss": 1.0681,
      "step": 3991
    },
    {
      "epoch": 5.322666666666667,
      "grad_norm": 0.029273128136992455,
      "learning_rate": 0.0009441066350330391,
      "loss": 0.9947,
      "step": 3992
    },
    {
      "epoch": 5.324,
      "grad_norm": 0.02824903465807438,
      "learning_rate": 0.0009436754812833842,
      "loss": 0.8081,
      "step": 3993
    },
    {
      "epoch": 5.325333333333333,
      "grad_norm": 0.07389011234045029,
      "learning_rate": 0.0009432443380371618,
      "loss": 1.1528,
      "step": 3994
    },
    {
      "epoch": 5.326666666666666,
      "grad_norm": 0.02901003323495388,
      "learning_rate": 0.0009428132053747713,
      "loss": 0.79,
      "step": 3995
    },
    {
      "epoch": 5.328,
      "grad_norm": 0.035174403339624405,
      "learning_rate": 0.0009423820833766107,
      "loss": 0.8539,
      "step": 3996
    },
    {
      "epoch": 5.3293333333333335,
      "grad_norm": 0.025144822895526886,
      "learning_rate": 0.000941950972123076,
      "loss": 1.1066,
      "step": 3997
    },
    {
      "epoch": 5.330666666666667,
      "grad_norm": 0.03193482384085655,
      "learning_rate": 0.0009415198716945605,
      "loss": 1.0092,
      "step": 3998
    },
    {
      "epoch": 5.332,
      "grad_norm": 0.03202433884143829,
      "learning_rate": 0.000941088782171457,
      "loss": 0.9281,
      "step": 3999
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 0.0252823568880558,
      "learning_rate": 0.0009406577036341548,
      "loss": 0.8396,
      "step": 4000
    },
    {
      "epoch": 5.334666666666667,
      "grad_norm": 0.028866328299045563,
      "learning_rate": 0.0009402266361630418,
      "loss": 1.0815,
      "step": 4001
    },
    {
      "epoch": 5.336,
      "grad_norm": 0.03171573951840401,
      "learning_rate": 0.0009397955798385039,
      "loss": 1.2434,
      "step": 4002
    },
    {
      "epoch": 5.3373333333333335,
      "grad_norm": 0.03098366968333721,
      "learning_rate": 0.0009393645347409242,
      "loss": 1.1984,
      "step": 4003
    },
    {
      "epoch": 5.338666666666667,
      "grad_norm": 0.03260120749473572,
      "learning_rate": 0.0009389335009506851,
      "loss": 0.7917,
      "step": 4004
    },
    {
      "epoch": 5.34,
      "grad_norm": 0.033467262983322144,
      "learning_rate": 0.0009385024785481653,
      "loss": 0.9431,
      "step": 4005
    },
    {
      "epoch": 5.341333333333333,
      "grad_norm": 0.030813120305538177,
      "learning_rate": 0.0009380714676137427,
      "loss": 1.0901,
      "step": 4006
    },
    {
      "epoch": 5.342666666666666,
      "grad_norm": 0.024667544290423393,
      "learning_rate": 0.0009376404682277923,
      "loss": 1.1279,
      "step": 4007
    },
    {
      "epoch": 5.344,
      "grad_norm": 0.0262447576969862,
      "learning_rate": 0.0009372094804706866,
      "loss": 0.8875,
      "step": 4008
    },
    {
      "epoch": 5.3453333333333335,
      "grad_norm": 0.029718149453401566,
      "learning_rate": 0.0009367785044227975,
      "loss": 0.9566,
      "step": 4009
    },
    {
      "epoch": 5.346666666666667,
      "grad_norm": 0.02922782301902771,
      "learning_rate": 0.0009363475401644927,
      "loss": 1.0745,
      "step": 4010
    },
    {
      "epoch": 5.348,
      "grad_norm": 0.029452092945575714,
      "learning_rate": 0.0009359165877761397,
      "loss": 1.084,
      "step": 4011
    },
    {
      "epoch": 5.349333333333333,
      "grad_norm": 0.023833168670535088,
      "learning_rate": 0.000935485647338102,
      "loss": 0.8919,
      "step": 4012
    },
    {
      "epoch": 5.350666666666667,
      "grad_norm": 0.024854913353919983,
      "learning_rate": 0.0009350547189307414,
      "loss": 0.885,
      "step": 4013
    },
    {
      "epoch": 5.352,
      "grad_norm": 0.02623271755874157,
      "learning_rate": 0.0009346238026344185,
      "loss": 1.0974,
      "step": 4014
    },
    {
      "epoch": 5.3533333333333335,
      "grad_norm": 0.023215211927890778,
      "learning_rate": 0.0009341928985294906,
      "loss": 0.908,
      "step": 4015
    },
    {
      "epoch": 5.354666666666667,
      "grad_norm": 0.03300975263118744,
      "learning_rate": 0.000933762006696313,
      "loss": 1.1471,
      "step": 4016
    },
    {
      "epoch": 5.356,
      "grad_norm": 0.018864143639802933,
      "learning_rate": 0.0009333311272152385,
      "loss": 0.8877,
      "step": 4017
    },
    {
      "epoch": 5.357333333333333,
      "grad_norm": 0.023746490478515625,
      "learning_rate": 0.0009329002601666174,
      "loss": 0.9906,
      "step": 4018
    },
    {
      "epoch": 5.358666666666666,
      "grad_norm": 0.02857198379933834,
      "learning_rate": 0.0009324694056307989,
      "loss": 0.9573,
      "step": 4019
    },
    {
      "epoch": 5.36,
      "grad_norm": 0.03425490856170654,
      "learning_rate": 0.0009320385636881282,
      "loss": 1.0202,
      "step": 4020
    },
    {
      "epoch": 5.3613333333333335,
      "grad_norm": 0.03294959291815758,
      "learning_rate": 0.00093160773441895,
      "loss": 1.1461,
      "step": 4021
    },
    {
      "epoch": 5.362666666666667,
      "grad_norm": 0.02146019972860813,
      "learning_rate": 0.0009311769179036047,
      "loss": 0.98,
      "step": 4022
    },
    {
      "epoch": 5.364,
      "grad_norm": 0.04428362846374512,
      "learning_rate": 0.0009307461142224318,
      "loss": 1.3015,
      "step": 4023
    },
    {
      "epoch": 5.365333333333333,
      "grad_norm": 0.02700992487370968,
      "learning_rate": 0.0009303153234557676,
      "loss": 0.9717,
      "step": 4024
    },
    {
      "epoch": 5.366666666666666,
      "grad_norm": 0.0220956951379776,
      "learning_rate": 0.0009298845456839459,
      "loss": 1.0663,
      "step": 4025
    },
    {
      "epoch": 5.368,
      "grad_norm": 0.024602646008133888,
      "learning_rate": 0.000929453780987299,
      "loss": 1.0979,
      "step": 4026
    },
    {
      "epoch": 5.3693333333333335,
      "grad_norm": 0.019303789362311363,
      "learning_rate": 0.0009290230294461559,
      "loss": 0.8593,
      "step": 4027
    },
    {
      "epoch": 5.370666666666667,
      "grad_norm": 0.0200082715600729,
      "learning_rate": 0.0009285922911408436,
      "loss": 0.8674,
      "step": 4028
    },
    {
      "epoch": 5.372,
      "grad_norm": 0.019024042412638664,
      "learning_rate": 0.0009281615661516865,
      "loss": 1.1731,
      "step": 4029
    },
    {
      "epoch": 5.373333333333333,
      "grad_norm": 0.02633281797170639,
      "learning_rate": 0.0009277308545590057,
      "loss": 1.0024,
      "step": 4030
    },
    {
      "epoch": 5.374666666666666,
      "grad_norm": 0.027524126693606377,
      "learning_rate": 0.0009273001564431215,
      "loss": 0.9947,
      "step": 4031
    },
    {
      "epoch": 5.376,
      "grad_norm": 0.02179987169802189,
      "learning_rate": 0.0009268694718843503,
      "loss": 0.9814,
      "step": 4032
    },
    {
      "epoch": 5.3773333333333335,
      "grad_norm": 0.023383965715765953,
      "learning_rate": 0.0009264388009630067,
      "loss": 1.2638,
      "step": 4033
    },
    {
      "epoch": 5.378666666666667,
      "grad_norm": 0.0329553596675396,
      "learning_rate": 0.0009260081437594024,
      "loss": 0.8429,
      "step": 4034
    },
    {
      "epoch": 5.38,
      "grad_norm": 0.02461852692067623,
      "learning_rate": 0.0009255775003538462,
      "loss": 1.2817,
      "step": 4035
    },
    {
      "epoch": 5.381333333333333,
      "grad_norm": 0.022928273305296898,
      "learning_rate": 0.0009251468708266453,
      "loss": 0.9731,
      "step": 4036
    },
    {
      "epoch": 5.382666666666666,
      "grad_norm": 0.01899396814405918,
      "learning_rate": 0.0009247162552581035,
      "loss": 0.9768,
      "step": 4037
    },
    {
      "epoch": 5.384,
      "grad_norm": 0.03475766256451607,
      "learning_rate": 0.0009242856537285227,
      "loss": 1.3415,
      "step": 4038
    },
    {
      "epoch": 5.3853333333333335,
      "grad_norm": 0.034611642360687256,
      "learning_rate": 0.0009238550663182013,
      "loss": 0.9704,
      "step": 4039
    },
    {
      "epoch": 5.386666666666667,
      "grad_norm": 0.022700099274516106,
      "learning_rate": 0.0009234244931074351,
      "loss": 0.8727,
      "step": 4040
    },
    {
      "epoch": 5.388,
      "grad_norm": 0.026448678225278854,
      "learning_rate": 0.0009229939341765188,
      "loss": 1.0691,
      "step": 4041
    },
    {
      "epoch": 5.389333333333333,
      "grad_norm": 0.021507496014237404,
      "learning_rate": 0.0009225633896057421,
      "loss": 0.9579,
      "step": 4042
    },
    {
      "epoch": 5.390666666666666,
      "grad_norm": 0.02387600764632225,
      "learning_rate": 0.0009221328594753943,
      "loss": 0.8655,
      "step": 4043
    },
    {
      "epoch": 5.392,
      "grad_norm": 0.03316463902592659,
      "learning_rate": 0.0009217023438657605,
      "loss": 1.1679,
      "step": 4044
    },
    {
      "epoch": 5.3933333333333335,
      "grad_norm": 0.025986021384596825,
      "learning_rate": 0.000921271842857123,
      "loss": 1.0424,
      "step": 4045
    },
    {
      "epoch": 5.394666666666667,
      "grad_norm": 0.030636580660939217,
      "learning_rate": 0.0009208413565297629,
      "loss": 1.229,
      "step": 4046
    },
    {
      "epoch": 5.396,
      "grad_norm": 0.021867170929908752,
      "learning_rate": 0.0009204108849639564,
      "loss": 0.9681,
      "step": 4047
    },
    {
      "epoch": 5.397333333333333,
      "grad_norm": 0.02869797870516777,
      "learning_rate": 0.0009199804282399793,
      "loss": 1.1187,
      "step": 4048
    },
    {
      "epoch": 5.398666666666666,
      "grad_norm": 0.027560578659176826,
      "learning_rate": 0.0009195499864381028,
      "loss": 1.0051,
      "step": 4049
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.029882168397307396,
      "learning_rate": 0.0009191195596385959,
      "loss": 1.1608,
      "step": 4050
    },
    {
      "epoch": 5.4013333333333335,
      "grad_norm": 0.03496130183339119,
      "learning_rate": 0.0009186891479217252,
      "loss": 1.0794,
      "step": 4051
    },
    {
      "epoch": 5.402666666666667,
      "grad_norm": 0.02411552332341671,
      "learning_rate": 0.0009182587513677535,
      "loss": 0.7851,
      "step": 4052
    },
    {
      "epoch": 5.404,
      "grad_norm": 0.02845344878733158,
      "learning_rate": 0.0009178283700569424,
      "loss": 0.8517,
      "step": 4053
    },
    {
      "epoch": 5.405333333333333,
      "grad_norm": 0.024508055299520493,
      "learning_rate": 0.0009173980040695491,
      "loss": 0.9237,
      "step": 4054
    },
    {
      "epoch": 5.406666666666666,
      "grad_norm": 0.03114914521574974,
      "learning_rate": 0.0009169676534858285,
      "loss": 0.8969,
      "step": 4055
    },
    {
      "epoch": 5.408,
      "grad_norm": 0.021396389231085777,
      "learning_rate": 0.0009165373183860329,
      "loss": 1.1086,
      "step": 4056
    },
    {
      "epoch": 5.4093333333333335,
      "grad_norm": 0.033667128533124924,
      "learning_rate": 0.0009161069988504107,
      "loss": 1.1425,
      "step": 4057
    },
    {
      "epoch": 5.410666666666667,
      "grad_norm": 0.04324896261096001,
      "learning_rate": 0.0009156766949592095,
      "loss": 1.1361,
      "step": 4058
    },
    {
      "epoch": 5.412,
      "grad_norm": 0.027252882719039917,
      "learning_rate": 0.0009152464067926717,
      "loss": 0.8836,
      "step": 4059
    },
    {
      "epoch": 5.413333333333333,
      "grad_norm": 0.022003943100571632,
      "learning_rate": 0.0009148161344310377,
      "loss": 1.1557,
      "step": 4060
    },
    {
      "epoch": 5.414666666666666,
      "grad_norm": 0.02845446765422821,
      "learning_rate": 0.0009143858779545455,
      "loss": 0.8201,
      "step": 4061
    },
    {
      "epoch": 5.416,
      "grad_norm": 0.02687002532184124,
      "learning_rate": 0.0009139556374434288,
      "loss": 1.004,
      "step": 4062
    },
    {
      "epoch": 5.417333333333334,
      "grad_norm": 0.026329876855015755,
      "learning_rate": 0.0009135254129779201,
      "loss": 1.0577,
      "step": 4063
    },
    {
      "epoch": 5.418666666666667,
      "grad_norm": 0.025178615003824234,
      "learning_rate": 0.0009130952046382475,
      "loss": 0.9207,
      "step": 4064
    },
    {
      "epoch": 5.42,
      "grad_norm": 0.031142327934503555,
      "learning_rate": 0.0009126650125046361,
      "loss": 0.8458,
      "step": 4065
    },
    {
      "epoch": 5.421333333333333,
      "grad_norm": 0.028670398518443108,
      "learning_rate": 0.0009122348366573091,
      "loss": 0.8383,
      "step": 4066
    },
    {
      "epoch": 5.422666666666666,
      "grad_norm": 0.024596067145466805,
      "learning_rate": 0.0009118046771764851,
      "loss": 1.1998,
      "step": 4067
    },
    {
      "epoch": 5.424,
      "grad_norm": 0.023211989551782608,
      "learning_rate": 0.0009113745341423815,
      "loss": 1.0429,
      "step": 4068
    },
    {
      "epoch": 5.425333333333334,
      "grad_norm": 0.026912955567240715,
      "learning_rate": 0.0009109444076352109,
      "loss": 0.9196,
      "step": 4069
    },
    {
      "epoch": 5.426666666666667,
      "grad_norm": 0.023656731471419334,
      "learning_rate": 0.0009105142977351842,
      "loss": 0.8114,
      "step": 4070
    },
    {
      "epoch": 5.428,
      "grad_norm": 0.026039665564894676,
      "learning_rate": 0.0009100842045225084,
      "loss": 1.0369,
      "step": 4071
    },
    {
      "epoch": 5.429333333333333,
      "grad_norm": 0.030717911198735237,
      "learning_rate": 0.000909654128077387,
      "loss": 1.1203,
      "step": 4072
    },
    {
      "epoch": 5.430666666666666,
      "grad_norm": 0.03253953531384468,
      "learning_rate": 0.0009092240684800216,
      "loss": 1.1485,
      "step": 4073
    },
    {
      "epoch": 5.432,
      "grad_norm": 0.029255416244268417,
      "learning_rate": 0.0009087940258106092,
      "loss": 0.9908,
      "step": 4074
    },
    {
      "epoch": 5.433333333333334,
      "grad_norm": 0.024578377604484558,
      "learning_rate": 0.0009083640001493454,
      "loss": 0.7657,
      "step": 4075
    },
    {
      "epoch": 5.434666666666667,
      "grad_norm": 0.030571041628718376,
      "learning_rate": 0.0009079339915764212,
      "loss": 0.979,
      "step": 4076
    },
    {
      "epoch": 5.436,
      "grad_norm": 0.022181807085871696,
      "learning_rate": 0.0009075040001720247,
      "loss": 0.9678,
      "step": 4077
    },
    {
      "epoch": 5.437333333333333,
      "grad_norm": 0.02890903502702713,
      "learning_rate": 0.0009070740260163412,
      "loss": 0.9009,
      "step": 4078
    },
    {
      "epoch": 5.438666666666666,
      "grad_norm": 0.031782638281583786,
      "learning_rate": 0.0009066440691895524,
      "loss": 1.0858,
      "step": 4079
    },
    {
      "epoch": 5.44,
      "grad_norm": 0.026065999642014503,
      "learning_rate": 0.0009062141297718372,
      "loss": 1.0854,
      "step": 4080
    },
    {
      "epoch": 5.441333333333334,
      "grad_norm": 0.021144263446331024,
      "learning_rate": 0.0009057842078433707,
      "loss": 1.0267,
      "step": 4081
    },
    {
      "epoch": 5.442666666666667,
      "grad_norm": 0.027320822700858116,
      "learning_rate": 0.000905354303484325,
      "loss": 1.1375,
      "step": 4082
    },
    {
      "epoch": 5.444,
      "grad_norm": 0.03894096985459328,
      "learning_rate": 0.0009049244167748694,
      "loss": 1.2087,
      "step": 4083
    },
    {
      "epoch": 5.445333333333333,
      "grad_norm": 0.027745269238948822,
      "learning_rate": 0.0009044945477951685,
      "loss": 0.8895,
      "step": 4084
    },
    {
      "epoch": 5.446666666666666,
      "grad_norm": 0.025202076882123947,
      "learning_rate": 0.0009040646966253856,
      "loss": 0.9721,
      "step": 4085
    },
    {
      "epoch": 5.448,
      "grad_norm": 0.029513949528336525,
      "learning_rate": 0.0009036348633456791,
      "loss": 1.2386,
      "step": 4086
    },
    {
      "epoch": 5.449333333333334,
      "grad_norm": 0.027007758617401123,
      "learning_rate": 0.0009032050480362045,
      "loss": 1.0017,
      "step": 4087
    },
    {
      "epoch": 5.450666666666667,
      "grad_norm": 0.02407842129468918,
      "learning_rate": 0.0009027752507771144,
      "loss": 0.9531,
      "step": 4088
    },
    {
      "epoch": 5.452,
      "grad_norm": 0.025042718276381493,
      "learning_rate": 0.000902345471648557,
      "loss": 1.1135,
      "step": 4089
    },
    {
      "epoch": 5.453333333333333,
      "grad_norm": 0.032582636922597885,
      "learning_rate": 0.0009019157107306787,
      "loss": 1.0305,
      "step": 4090
    },
    {
      "epoch": 5.454666666666666,
      "grad_norm": 0.020495908334851265,
      "learning_rate": 0.0009014859681036211,
      "loss": 1.1071,
      "step": 4091
    },
    {
      "epoch": 5.456,
      "grad_norm": 0.034558072686195374,
      "learning_rate": 0.0009010562438475225,
      "loss": 1.1344,
      "step": 4092
    },
    {
      "epoch": 5.457333333333334,
      "grad_norm": 0.026115115731954575,
      "learning_rate": 0.0009006265380425189,
      "loss": 1.2252,
      "step": 4093
    },
    {
      "epoch": 5.458666666666667,
      "grad_norm": 0.031129635870456696,
      "learning_rate": 0.0009001968507687414,
      "loss": 0.8646,
      "step": 4094
    },
    {
      "epoch": 5.46,
      "grad_norm": 0.02093295194208622,
      "learning_rate": 0.0008997671821063191,
      "loss": 0.8793,
      "step": 4095
    },
    {
      "epoch": 5.461333333333333,
      "grad_norm": 0.02477293461561203,
      "learning_rate": 0.0008993375321353764,
      "loss": 0.8103,
      "step": 4096
    },
    {
      "epoch": 5.462666666666666,
      "grad_norm": 0.02470993809401989,
      "learning_rate": 0.0008989079009360343,
      "loss": 0.9815,
      "step": 4097
    },
    {
      "epoch": 5.464,
      "grad_norm": 0.026451468467712402,
      "learning_rate": 0.0008984782885884118,
      "loss": 1.025,
      "step": 4098
    },
    {
      "epoch": 5.465333333333334,
      "grad_norm": 0.027032148092985153,
      "learning_rate": 0.0008980486951726224,
      "loss": 1.1682,
      "step": 4099
    },
    {
      "epoch": 5.466666666666667,
      "grad_norm": 0.027475938200950623,
      "learning_rate": 0.0008976191207687775,
      "loss": 1.0496,
      "step": 4100
    },
    {
      "epoch": 5.468,
      "grad_norm": 0.03329376503825188,
      "learning_rate": 0.0008971895654569841,
      "loss": 1.0619,
      "step": 4101
    },
    {
      "epoch": 5.469333333333333,
      "grad_norm": 0.029651788994669914,
      "learning_rate": 0.0008967600293173457,
      "loss": 0.791,
      "step": 4102
    },
    {
      "epoch": 5.470666666666666,
      "grad_norm": 0.028676921501755714,
      "learning_rate": 0.000896330512429963,
      "loss": 1.1937,
      "step": 4103
    },
    {
      "epoch": 5.4719999999999995,
      "grad_norm": 0.022213080897927284,
      "learning_rate": 0.0008959010148749323,
      "loss": 0.9578,
      "step": 4104
    },
    {
      "epoch": 5.473333333333334,
      "grad_norm": 0.01843908801674843,
      "learning_rate": 0.0008954715367323467,
      "loss": 0.8803,
      "step": 4105
    },
    {
      "epoch": 5.474666666666667,
      "grad_norm": 0.026819420978426933,
      "learning_rate": 0.0008950420780822956,
      "loss": 1.0964,
      "step": 4106
    },
    {
      "epoch": 5.476,
      "grad_norm": 0.026214122772216797,
      "learning_rate": 0.000894612639004864,
      "loss": 1.0497,
      "step": 4107
    },
    {
      "epoch": 5.477333333333333,
      "grad_norm": 0.022337548434734344,
      "learning_rate": 0.0008941832195801351,
      "loss": 0.8519,
      "step": 4108
    },
    {
      "epoch": 5.478666666666666,
      "grad_norm": 0.030529815703630447,
      "learning_rate": 0.0008937538198881866,
      "loss": 0.9284,
      "step": 4109
    },
    {
      "epoch": 5.48,
      "grad_norm": 0.02986019104719162,
      "learning_rate": 0.0008933244400090936,
      "loss": 1.1773,
      "step": 4110
    },
    {
      "epoch": 5.481333333333334,
      "grad_norm": 0.022879039868712425,
      "learning_rate": 0.0008928950800229271,
      "loss": 1.0278,
      "step": 4111
    },
    {
      "epoch": 5.482666666666667,
      "grad_norm": 0.019939357414841652,
      "learning_rate": 0.0008924657400097536,
      "loss": 0.9384,
      "step": 4112
    },
    {
      "epoch": 5.484,
      "grad_norm": 0.02145346999168396,
      "learning_rate": 0.0008920364200496379,
      "loss": 1.0195,
      "step": 4113
    },
    {
      "epoch": 5.485333333333333,
      "grad_norm": 0.025371888652443886,
      "learning_rate": 0.0008916071202226391,
      "loss": 0.9638,
      "step": 4114
    },
    {
      "epoch": 5.486666666666666,
      "grad_norm": 0.02161581628024578,
      "learning_rate": 0.0008911778406088138,
      "loss": 1.0392,
      "step": 4115
    },
    {
      "epoch": 5.4879999999999995,
      "grad_norm": 0.02402343787252903,
      "learning_rate": 0.0008907485812882137,
      "loss": 1.0825,
      "step": 4116
    },
    {
      "epoch": 5.489333333333334,
      "grad_norm": 0.027194293215870857,
      "learning_rate": 0.0008903193423408879,
      "loss": 0.911,
      "step": 4117
    },
    {
      "epoch": 5.490666666666667,
      "grad_norm": 0.02633863501250744,
      "learning_rate": 0.0008898901238468813,
      "loss": 0.8532,
      "step": 4118
    },
    {
      "epoch": 5.492,
      "grad_norm": 0.0283507090061903,
      "learning_rate": 0.0008894609258862338,
      "loss": 1.0531,
      "step": 4119
    },
    {
      "epoch": 5.493333333333333,
      "grad_norm": 0.02447161264717579,
      "learning_rate": 0.0008890317485389838,
      "loss": 0.9478,
      "step": 4120
    },
    {
      "epoch": 5.494666666666666,
      "grad_norm": 0.045992109924554825,
      "learning_rate": 0.0008886025918851638,
      "loss": 0.9681,
      "step": 4121
    },
    {
      "epoch": 5.496,
      "grad_norm": 0.03890632092952728,
      "learning_rate": 0.0008881734560048036,
      "loss": 1.3041,
      "step": 4122
    },
    {
      "epoch": 5.497333333333334,
      "grad_norm": 0.02024746499955654,
      "learning_rate": 0.0008877443409779284,
      "loss": 1.0284,
      "step": 4123
    },
    {
      "epoch": 5.498666666666667,
      "grad_norm": 0.018765736371278763,
      "learning_rate": 0.0008873152468845596,
      "loss": 0.9271,
      "step": 4124
    },
    {
      "epoch": 5.5,
      "grad_norm": 0.033819667994976044,
      "learning_rate": 0.0008868861738047158,
      "loss": 1.0557,
      "step": 4125
    },
    {
      "epoch": 5.501333333333333,
      "grad_norm": 0.02859523892402649,
      "learning_rate": 0.0008864571218184101,
      "loss": 1.341,
      "step": 4126
    },
    {
      "epoch": 5.502666666666666,
      "grad_norm": 0.023759284988045692,
      "learning_rate": 0.0008860280910056527,
      "loss": 1.2054,
      "step": 4127
    },
    {
      "epoch": 5.504,
      "grad_norm": 0.02622274123132229,
      "learning_rate": 0.0008855990814464497,
      "loss": 1.0799,
      "step": 4128
    },
    {
      "epoch": 5.505333333333334,
      "grad_norm": 0.02874920330941677,
      "learning_rate": 0.0008851700932208021,
      "loss": 1.1695,
      "step": 4129
    },
    {
      "epoch": 5.506666666666667,
      "grad_norm": 0.032730259001255035,
      "learning_rate": 0.0008847411264087093,
      "loss": 1.0922,
      "step": 4130
    },
    {
      "epoch": 5.508,
      "grad_norm": 0.03078027069568634,
      "learning_rate": 0.0008843121810901642,
      "loss": 0.9675,
      "step": 4131
    },
    {
      "epoch": 5.509333333333333,
      "grad_norm": 0.023636357858777046,
      "learning_rate": 0.0008838832573451576,
      "loss": 1.1357,
      "step": 4132
    },
    {
      "epoch": 5.510666666666666,
      "grad_norm": 0.02129512093961239,
      "learning_rate": 0.0008834543552536751,
      "loss": 1.1987,
      "step": 4133
    },
    {
      "epoch": 5.5120000000000005,
      "grad_norm": 0.028388429433107376,
      "learning_rate": 0.0008830254748956981,
      "loss": 0.9736,
      "step": 4134
    },
    {
      "epoch": 5.513333333333334,
      "grad_norm": 0.02130122110247612,
      "learning_rate": 0.0008825966163512056,
      "loss": 1.0858,
      "step": 4135
    },
    {
      "epoch": 5.514666666666667,
      "grad_norm": 0.02532723732292652,
      "learning_rate": 0.0008821677797001706,
      "loss": 0.7098,
      "step": 4136
    },
    {
      "epoch": 5.516,
      "grad_norm": 0.027699926868081093,
      "learning_rate": 0.0008817389650225631,
      "loss": 1.0024,
      "step": 4137
    },
    {
      "epoch": 5.517333333333333,
      "grad_norm": 0.02231747657060623,
      "learning_rate": 0.0008813101723983489,
      "loss": 0.97,
      "step": 4138
    },
    {
      "epoch": 5.518666666666666,
      "grad_norm": 0.03354316204786301,
      "learning_rate": 0.0008808814019074888,
      "loss": 1.0831,
      "step": 4139
    },
    {
      "epoch": 5.52,
      "grad_norm": 0.026847822591662407,
      "learning_rate": 0.0008804526536299412,
      "loss": 1.1971,
      "step": 4140
    },
    {
      "epoch": 5.521333333333334,
      "grad_norm": 0.02168472297489643,
      "learning_rate": 0.0008800239276456585,
      "loss": 0.8634,
      "step": 4141
    },
    {
      "epoch": 5.522666666666667,
      "grad_norm": 0.02266773022711277,
      "learning_rate": 0.0008795952240345906,
      "loss": 1.1651,
      "step": 4142
    },
    {
      "epoch": 5.524,
      "grad_norm": 0.02344123087823391,
      "learning_rate": 0.0008791665428766819,
      "loss": 1.1698,
      "step": 4143
    },
    {
      "epoch": 5.525333333333333,
      "grad_norm": 0.03343998268246651,
      "learning_rate": 0.0008787378842518728,
      "loss": 1.0147,
      "step": 4144
    },
    {
      "epoch": 5.526666666666666,
      "grad_norm": 0.032457347959280014,
      "learning_rate": 0.0008783092482401005,
      "loss": 1.1389,
      "step": 4145
    },
    {
      "epoch": 5.5280000000000005,
      "grad_norm": 0.04096459224820137,
      "learning_rate": 0.0008778806349212968,
      "loss": 1.0013,
      "step": 4146
    },
    {
      "epoch": 5.529333333333334,
      "grad_norm": 0.02679493837058544,
      "learning_rate": 0.0008774520443753905,
      "loss": 1.0129,
      "step": 4147
    },
    {
      "epoch": 5.530666666666667,
      "grad_norm": 0.02142481505870819,
      "learning_rate": 0.000877023476682305,
      "loss": 1.0207,
      "step": 4148
    },
    {
      "epoch": 5.532,
      "grad_norm": 0.025153355672955513,
      "learning_rate": 0.0008765949319219595,
      "loss": 0.9716,
      "step": 4149
    },
    {
      "epoch": 5.533333333333333,
      "grad_norm": 0.03797759488224983,
      "learning_rate": 0.00087616641017427,
      "loss": 0.9922,
      "step": 4150
    },
    {
      "epoch": 5.534666666666666,
      "grad_norm": 0.026334604248404503,
      "learning_rate": 0.0008757379115191468,
      "loss": 0.9252,
      "step": 4151
    },
    {
      "epoch": 5.536,
      "grad_norm": 0.025383852422237396,
      "learning_rate": 0.0008753094360364973,
      "loss": 1.1952,
      "step": 4152
    },
    {
      "epoch": 5.537333333333334,
      "grad_norm": 0.032424841076135635,
      "learning_rate": 0.0008748809838062236,
      "loss": 1.3086,
      "step": 4153
    },
    {
      "epoch": 5.538666666666667,
      "grad_norm": 0.032955706119537354,
      "learning_rate": 0.0008744525549082235,
      "loss": 1.0044,
      "step": 4154
    },
    {
      "epoch": 5.54,
      "grad_norm": 0.021493127569556236,
      "learning_rate": 0.0008740241494223911,
      "loss": 1.0621,
      "step": 4155
    },
    {
      "epoch": 5.541333333333333,
      "grad_norm": 0.027989692986011505,
      "learning_rate": 0.0008735957674286151,
      "loss": 0.9716,
      "step": 4156
    },
    {
      "epoch": 5.542666666666666,
      "grad_norm": 0.032050009816884995,
      "learning_rate": 0.0008731674090067812,
      "loss": 1.313,
      "step": 4157
    },
    {
      "epoch": 5.5440000000000005,
      "grad_norm": 0.023838384076952934,
      "learning_rate": 0.0008727390742367698,
      "loss": 1.2319,
      "step": 4158
    },
    {
      "epoch": 5.545333333333334,
      "grad_norm": 0.026123106479644775,
      "learning_rate": 0.0008723107631984566,
      "loss": 1.0643,
      "step": 4159
    },
    {
      "epoch": 5.546666666666667,
      "grad_norm": 0.028822561725974083,
      "learning_rate": 0.0008718824759717138,
      "loss": 1.0483,
      "step": 4160
    },
    {
      "epoch": 5.548,
      "grad_norm": 0.024206606671214104,
      "learning_rate": 0.0008714542126364079,
      "loss": 1.095,
      "step": 4161
    },
    {
      "epoch": 5.549333333333333,
      "grad_norm": 0.025689393281936646,
      "learning_rate": 0.0008710259732724029,
      "loss": 1.117,
      "step": 4162
    },
    {
      "epoch": 5.550666666666666,
      "grad_norm": 0.02006298303604126,
      "learning_rate": 0.0008705977579595561,
      "loss": 0.9067,
      "step": 4163
    },
    {
      "epoch": 5.552,
      "grad_norm": 0.02327807806432247,
      "learning_rate": 0.000870169566777722,
      "loss": 1.0893,
      "step": 4164
    },
    {
      "epoch": 5.553333333333334,
      "grad_norm": 0.025298673659563065,
      "learning_rate": 0.0008697413998067499,
      "loss": 1.1516,
      "step": 4165
    },
    {
      "epoch": 5.554666666666667,
      "grad_norm": 0.026181655004620552,
      "learning_rate": 0.0008693132571264841,
      "loss": 1.1349,
      "step": 4166
    },
    {
      "epoch": 5.556,
      "grad_norm": 0.022412866353988647,
      "learning_rate": 0.0008688851388167657,
      "loss": 1.0998,
      "step": 4167
    },
    {
      "epoch": 5.557333333333333,
      "grad_norm": 0.024367354810237885,
      "learning_rate": 0.0008684570449574298,
      "loss": 1.3115,
      "step": 4168
    },
    {
      "epoch": 5.558666666666666,
      "grad_norm": 0.02379123494029045,
      "learning_rate": 0.0008680289756283081,
      "loss": 1.1203,
      "step": 4169
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 0.026822907850146294,
      "learning_rate": 0.0008676009309092273,
      "loss": 0.8307,
      "step": 4170
    },
    {
      "epoch": 5.561333333333334,
      "grad_norm": 0.021374372765421867,
      "learning_rate": 0.0008671729108800085,
      "loss": 1.062,
      "step": 4171
    },
    {
      "epoch": 5.562666666666667,
      "grad_norm": 0.03474929928779602,
      "learning_rate": 0.0008667449156204703,
      "loss": 0.9156,
      "step": 4172
    },
    {
      "epoch": 5.564,
      "grad_norm": 0.02892567589879036,
      "learning_rate": 0.0008663169452104247,
      "loss": 1.3026,
      "step": 4173
    },
    {
      "epoch": 5.565333333333333,
      "grad_norm": 0.02570471353828907,
      "learning_rate": 0.0008658889997296809,
      "loss": 1.1104,
      "step": 4174
    },
    {
      "epoch": 5.566666666666666,
      "grad_norm": 0.02957501821219921,
      "learning_rate": 0.0008654610792580415,
      "loss": 1.0928,
      "step": 4175
    },
    {
      "epoch": 5.568,
      "grad_norm": 0.020867079496383667,
      "learning_rate": 0.0008650331838753056,
      "loss": 1.2724,
      "step": 4176
    },
    {
      "epoch": 5.569333333333334,
      "grad_norm": 0.03813614323735237,
      "learning_rate": 0.0008646053136612678,
      "loss": 0.8869,
      "step": 4177
    },
    {
      "epoch": 5.570666666666667,
      "grad_norm": 0.030274396762251854,
      "learning_rate": 0.0008641774686957168,
      "loss": 1.0494,
      "step": 4178
    },
    {
      "epoch": 5.572,
      "grad_norm": 0.023844411596655846,
      "learning_rate": 0.0008637496490584384,
      "loss": 0.9858,
      "step": 4179
    },
    {
      "epoch": 5.573333333333333,
      "grad_norm": 0.024409715086221695,
      "learning_rate": 0.0008633218548292121,
      "loss": 1.1031,
      "step": 4180
    },
    {
      "epoch": 5.574666666666666,
      "grad_norm": 0.02100249193608761,
      "learning_rate": 0.0008628940860878133,
      "loss": 1.0762,
      "step": 4181
    },
    {
      "epoch": 5.576,
      "grad_norm": 0.02828643098473549,
      "learning_rate": 0.0008624663429140127,
      "loss": 0.8469,
      "step": 4182
    },
    {
      "epoch": 5.577333333333334,
      "grad_norm": 0.03688496723771095,
      "learning_rate": 0.0008620386253875756,
      "loss": 0.8873,
      "step": 4183
    },
    {
      "epoch": 5.578666666666667,
      "grad_norm": 0.024846874177455902,
      "learning_rate": 0.0008616109335882641,
      "loss": 1.0935,
      "step": 4184
    },
    {
      "epoch": 5.58,
      "grad_norm": 0.030274344608187675,
      "learning_rate": 0.0008611832675958336,
      "loss": 1.1518,
      "step": 4185
    },
    {
      "epoch": 5.581333333333333,
      "grad_norm": 0.026285450905561447,
      "learning_rate": 0.0008607556274900356,
      "loss": 1.0007,
      "step": 4186
    },
    {
      "epoch": 5.582666666666666,
      "grad_norm": 0.02177291363477707,
      "learning_rate": 0.000860328013350617,
      "loss": 1.2832,
      "step": 4187
    },
    {
      "epoch": 5.584,
      "grad_norm": 0.023226609453558922,
      "learning_rate": 0.0008599004252573191,
      "loss": 0.7743,
      "step": 4188
    },
    {
      "epoch": 5.585333333333334,
      "grad_norm": 0.02683565951883793,
      "learning_rate": 0.0008594728632898794,
      "loss": 1.0325,
      "step": 4189
    },
    {
      "epoch": 5.586666666666667,
      "grad_norm": 0.025448771193623543,
      "learning_rate": 0.0008590453275280297,
      "loss": 0.8145,
      "step": 4190
    },
    {
      "epoch": 5.588,
      "grad_norm": 0.028881335631012917,
      "learning_rate": 0.0008586178180514968,
      "loss": 1.0477,
      "step": 4191
    },
    {
      "epoch": 5.589333333333333,
      "grad_norm": 0.027225080877542496,
      "learning_rate": 0.0008581903349400035,
      "loss": 1.0127,
      "step": 4192
    },
    {
      "epoch": 5.5906666666666665,
      "grad_norm": 0.02199428156018257,
      "learning_rate": 0.0008577628782732663,
      "loss": 1.0215,
      "step": 4193
    },
    {
      "epoch": 5.592,
      "grad_norm": 0.025741146877408028,
      "learning_rate": 0.0008573354481309985,
      "loss": 1.0679,
      "step": 4194
    },
    {
      "epoch": 5.593333333333334,
      "grad_norm": 0.023173034191131592,
      "learning_rate": 0.0008569080445929073,
      "loss": 1.0182,
      "step": 4195
    },
    {
      "epoch": 5.594666666666667,
      "grad_norm": 0.02352113090455532,
      "learning_rate": 0.0008564806677386946,
      "loss": 1.2519,
      "step": 4196
    },
    {
      "epoch": 5.596,
      "grad_norm": 0.03440272808074951,
      "learning_rate": 0.0008560533176480587,
      "loss": 1.0342,
      "step": 4197
    },
    {
      "epoch": 5.597333333333333,
      "grad_norm": 0.025366615504026413,
      "learning_rate": 0.0008556259944006916,
      "loss": 0.9859,
      "step": 4198
    },
    {
      "epoch": 5.5986666666666665,
      "grad_norm": 0.032373689115047455,
      "learning_rate": 0.0008551986980762814,
      "loss": 0.9442,
      "step": 4199
    },
    {
      "epoch": 5.6,
      "grad_norm": 0.030664460733532906,
      "learning_rate": 0.00085477142875451,
      "loss": 1.0993,
      "step": 4200
    },
    {
      "epoch": 5.601333333333334,
      "grad_norm": 0.03192606940865517,
      "learning_rate": 0.0008543441865150545,
      "loss": 0.9214,
      "step": 4201
    },
    {
      "epoch": 5.602666666666667,
      "grad_norm": 0.029770540073513985,
      "learning_rate": 0.0008539169714375885,
      "loss": 0.8785,
      "step": 4202
    },
    {
      "epoch": 5.604,
      "grad_norm": 0.02560197189450264,
      "learning_rate": 0.0008534897836017783,
      "loss": 1.0323,
      "step": 4203
    },
    {
      "epoch": 5.605333333333333,
      "grad_norm": 0.03304627537727356,
      "learning_rate": 0.0008530626230872869,
      "loss": 0.923,
      "step": 4204
    },
    {
      "epoch": 5.6066666666666665,
      "grad_norm": 0.02473980002105236,
      "learning_rate": 0.0008526354899737705,
      "loss": 1.1824,
      "step": 4205
    },
    {
      "epoch": 5.608,
      "grad_norm": 0.03956839069724083,
      "learning_rate": 0.0008522083843408823,
      "loss": 0.8909,
      "step": 4206
    },
    {
      "epoch": 5.609333333333334,
      "grad_norm": 0.039681561291217804,
      "learning_rate": 0.0008517813062682687,
      "loss": 1.3374,
      "step": 4207
    },
    {
      "epoch": 5.610666666666667,
      "grad_norm": 0.024661438539624214,
      "learning_rate": 0.0008513542558355712,
      "loss": 1.0764,
      "step": 4208
    },
    {
      "epoch": 5.612,
      "grad_norm": 0.02424493432044983,
      "learning_rate": 0.0008509272331224269,
      "loss": 0.8608,
      "step": 4209
    },
    {
      "epoch": 5.613333333333333,
      "grad_norm": 0.029071174561977386,
      "learning_rate": 0.0008505002382084666,
      "loss": 0.7994,
      "step": 4210
    },
    {
      "epoch": 5.6146666666666665,
      "grad_norm": 0.023565998300909996,
      "learning_rate": 0.0008500732711733177,
      "loss": 0.9339,
      "step": 4211
    },
    {
      "epoch": 5.616,
      "grad_norm": 0.037511005997657776,
      "learning_rate": 0.0008496463320966005,
      "loss": 1.0408,
      "step": 4212
    },
    {
      "epoch": 5.617333333333333,
      "grad_norm": 0.03607992082834244,
      "learning_rate": 0.0008492194210579308,
      "loss": 1.1811,
      "step": 4213
    },
    {
      "epoch": 5.618666666666667,
      "grad_norm": 0.028353333473205566,
      "learning_rate": 0.0008487925381369197,
      "loss": 0.8122,
      "step": 4214
    },
    {
      "epoch": 5.62,
      "grad_norm": 0.03025040775537491,
      "learning_rate": 0.000848365683413172,
      "loss": 1.0756,
      "step": 4215
    },
    {
      "epoch": 5.621333333333333,
      "grad_norm": 0.027265729382634163,
      "learning_rate": 0.0008479388569662886,
      "loss": 0.941,
      "step": 4216
    },
    {
      "epoch": 5.6226666666666665,
      "grad_norm": 0.022205229848623276,
      "learning_rate": 0.0008475120588758641,
      "loss": 1.106,
      "step": 4217
    },
    {
      "epoch": 5.624,
      "grad_norm": 0.022774459794163704,
      "learning_rate": 0.0008470852892214874,
      "loss": 0.8757,
      "step": 4218
    },
    {
      "epoch": 5.625333333333334,
      "grad_norm": 0.031249143183231354,
      "learning_rate": 0.0008466585480827438,
      "loss": 1.0236,
      "step": 4219
    },
    {
      "epoch": 5.626666666666667,
      "grad_norm": 0.0300077423453331,
      "learning_rate": 0.0008462318355392112,
      "loss": 0.9779,
      "step": 4220
    },
    {
      "epoch": 5.628,
      "grad_norm": 0.03703206777572632,
      "learning_rate": 0.0008458051516704643,
      "loss": 1.4478,
      "step": 4221
    },
    {
      "epoch": 5.629333333333333,
      "grad_norm": 0.022783955559134483,
      "learning_rate": 0.000845378496556071,
      "loss": 0.7764,
      "step": 4222
    },
    {
      "epoch": 5.6306666666666665,
      "grad_norm": 0.023013265803456306,
      "learning_rate": 0.0008449518702755936,
      "loss": 1.2415,
      "step": 4223
    },
    {
      "epoch": 5.632,
      "grad_norm": 0.022157838568091393,
      "learning_rate": 0.0008445252729085906,
      "loss": 1.0436,
      "step": 4224
    },
    {
      "epoch": 5.633333333333333,
      "grad_norm": 0.02540009096264839,
      "learning_rate": 0.0008440987045346134,
      "loss": 1.019,
      "step": 4225
    },
    {
      "epoch": 5.634666666666667,
      "grad_norm": 0.023998193442821503,
      "learning_rate": 0.0008436721652332094,
      "loss": 1.1041,
      "step": 4226
    },
    {
      "epoch": 5.636,
      "grad_norm": 0.03060358576476574,
      "learning_rate": 0.0008432456550839195,
      "loss": 1.0759,
      "step": 4227
    },
    {
      "epoch": 5.637333333333333,
      "grad_norm": 0.03332558646798134,
      "learning_rate": 0.0008428191741662791,
      "loss": 1.1176,
      "step": 4228
    },
    {
      "epoch": 5.6386666666666665,
      "grad_norm": 0.02753896266222,
      "learning_rate": 0.0008423927225598198,
      "loss": 0.864,
      "step": 4229
    },
    {
      "epoch": 5.64,
      "grad_norm": 0.027061166241765022,
      "learning_rate": 0.0008419663003440657,
      "loss": 0.9417,
      "step": 4230
    },
    {
      "epoch": 5.641333333333334,
      "grad_norm": 0.025666849687695503,
      "learning_rate": 0.0008415399075985366,
      "loss": 0.8514,
      "step": 4231
    },
    {
      "epoch": 5.642666666666667,
      "grad_norm": 0.021027734503149986,
      "learning_rate": 0.0008411135444027466,
      "loss": 0.989,
      "step": 4232
    },
    {
      "epoch": 5.644,
      "grad_norm": 0.022033967077732086,
      "learning_rate": 0.0008406872108362033,
      "loss": 0.9619,
      "step": 4233
    },
    {
      "epoch": 5.645333333333333,
      "grad_norm": 0.02109375223517418,
      "learning_rate": 0.0008402609069784111,
      "loss": 1.1755,
      "step": 4234
    },
    {
      "epoch": 5.6466666666666665,
      "grad_norm": 0.025809776037931442,
      "learning_rate": 0.0008398346329088663,
      "loss": 0.9322,
      "step": 4235
    },
    {
      "epoch": 5.648,
      "grad_norm": 0.021790646016597748,
      "learning_rate": 0.0008394083887070614,
      "loss": 1.0357,
      "step": 4236
    },
    {
      "epoch": 5.649333333333333,
      "grad_norm": 0.0447598434984684,
      "learning_rate": 0.0008389821744524824,
      "loss": 0.939,
      "step": 4237
    },
    {
      "epoch": 5.650666666666667,
      "grad_norm": 0.025733130052685738,
      "learning_rate": 0.0008385559902246096,
      "loss": 1.001,
      "step": 4238
    },
    {
      "epoch": 5.652,
      "grad_norm": 0.020689180120825768,
      "learning_rate": 0.000838129836102919,
      "loss": 0.9845,
      "step": 4239
    },
    {
      "epoch": 5.653333333333333,
      "grad_norm": 0.028012290596961975,
      "learning_rate": 0.0008377037121668794,
      "loss": 0.9595,
      "step": 4240
    },
    {
      "epoch": 5.6546666666666665,
      "grad_norm": 0.037160709500312805,
      "learning_rate": 0.000837277618495955,
      "loss": 0.9785,
      "step": 4241
    },
    {
      "epoch": 5.656,
      "grad_norm": 0.02300296537578106,
      "learning_rate": 0.000836851555169604,
      "loss": 0.9817,
      "step": 4242
    },
    {
      "epoch": 5.657333333333334,
      "grad_norm": 0.026386480778455734,
      "learning_rate": 0.0008364255222672784,
      "loss": 1.1955,
      "step": 4243
    },
    {
      "epoch": 5.658666666666667,
      "grad_norm": 0.027397125959396362,
      "learning_rate": 0.000835999519868426,
      "loss": 0.7865,
      "step": 4244
    },
    {
      "epoch": 5.66,
      "grad_norm": 0.021366199478507042,
      "learning_rate": 0.0008355735480524873,
      "loss": 0.9583,
      "step": 4245
    },
    {
      "epoch": 5.661333333333333,
      "grad_norm": 0.04049151390790939,
      "learning_rate": 0.0008351476068988984,
      "loss": 1.1134,
      "step": 4246
    },
    {
      "epoch": 5.6626666666666665,
      "grad_norm": 0.04676588252186775,
      "learning_rate": 0.0008347216964870886,
      "loss": 1.0789,
      "step": 4247
    },
    {
      "epoch": 5.664,
      "grad_norm": 0.022044286131858826,
      "learning_rate": 0.0008342958168964816,
      "loss": 0.958,
      "step": 4248
    },
    {
      "epoch": 5.665333333333333,
      "grad_norm": 0.02495400421321392,
      "learning_rate": 0.0008338699682064967,
      "loss": 1.0749,
      "step": 4249
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 0.033446554094552994,
      "learning_rate": 0.0008334441504965456,
      "loss": 1.1768,
      "step": 4250
    },
    {
      "epoch": 5.668,
      "grad_norm": 0.02549024485051632,
      "learning_rate": 0.0008330183638460356,
      "loss": 0.8707,
      "step": 4251
    },
    {
      "epoch": 5.669333333333333,
      "grad_norm": 0.026598069816827774,
      "learning_rate": 0.0008325926083343671,
      "loss": 0.7485,
      "step": 4252
    },
    {
      "epoch": 5.6706666666666665,
      "grad_norm": 0.029323481023311615,
      "learning_rate": 0.000832166884040936,
      "loss": 0.8593,
      "step": 4253
    },
    {
      "epoch": 5.672,
      "grad_norm": 0.025514956563711166,
      "learning_rate": 0.0008317411910451312,
      "loss": 1.0527,
      "step": 4254
    },
    {
      "epoch": 5.673333333333334,
      "grad_norm": 0.03734181076288223,
      "learning_rate": 0.0008313155294263357,
      "loss": 0.947,
      "step": 4255
    },
    {
      "epoch": 5.674666666666667,
      "grad_norm": 0.021501637995243073,
      "learning_rate": 0.0008308898992639282,
      "loss": 0.9658,
      "step": 4256
    },
    {
      "epoch": 5.676,
      "grad_norm": 0.02353006601333618,
      "learning_rate": 0.0008304643006372798,
      "loss": 0.8549,
      "step": 4257
    },
    {
      "epoch": 5.677333333333333,
      "grad_norm": 0.021354496479034424,
      "learning_rate": 0.0008300387336257567,
      "loss": 1.0026,
      "step": 4258
    },
    {
      "epoch": 5.6786666666666665,
      "grad_norm": 0.03567911684513092,
      "learning_rate": 0.0008296131983087187,
      "loss": 1.0293,
      "step": 4259
    },
    {
      "epoch": 5.68,
      "grad_norm": 0.03768347576260567,
      "learning_rate": 0.0008291876947655195,
      "loss": 1.1547,
      "step": 4260
    },
    {
      "epoch": 5.681333333333333,
      "grad_norm": 0.025095397606492043,
      "learning_rate": 0.0008287622230755084,
      "loss": 0.7454,
      "step": 4261
    },
    {
      "epoch": 5.682666666666667,
      "grad_norm": 0.024460650980472565,
      "learning_rate": 0.0008283367833180266,
      "loss": 0.9661,
      "step": 4262
    },
    {
      "epoch": 5.684,
      "grad_norm": 0.026729796081781387,
      "learning_rate": 0.000827911375572411,
      "loss": 1.0031,
      "step": 4263
    },
    {
      "epoch": 5.685333333333333,
      "grad_norm": 0.04006004333496094,
      "learning_rate": 0.0008274859999179916,
      "loss": 0.9698,
      "step": 4264
    },
    {
      "epoch": 5.6866666666666665,
      "grad_norm": 0.02963697351515293,
      "learning_rate": 0.0008270606564340923,
      "loss": 1.1874,
      "step": 4265
    },
    {
      "epoch": 5.688,
      "grad_norm": 0.03314507007598877,
      "learning_rate": 0.0008266353452000324,
      "loss": 1.3534,
      "step": 4266
    },
    {
      "epoch": 5.689333333333334,
      "grad_norm": 0.026222793385386467,
      "learning_rate": 0.0008262100662951235,
      "loss": 1.0306,
      "step": 4267
    },
    {
      "epoch": 5.690666666666667,
      "grad_norm": 0.023706572130322456,
      "learning_rate": 0.0008257848197986724,
      "loss": 0.716,
      "step": 4268
    },
    {
      "epoch": 5.692,
      "grad_norm": 0.028064770624041557,
      "learning_rate": 0.0008253596057899788,
      "loss": 1.2966,
      "step": 4269
    },
    {
      "epoch": 5.693333333333333,
      "grad_norm": 0.027347348630428314,
      "learning_rate": 0.0008249344243483369,
      "loss": 0.7674,
      "step": 4270
    },
    {
      "epoch": 5.6946666666666665,
      "grad_norm": 0.028816351667046547,
      "learning_rate": 0.0008245092755530353,
      "loss": 0.9868,
      "step": 4271
    },
    {
      "epoch": 5.696,
      "grad_norm": 0.025021834298968315,
      "learning_rate": 0.0008240841594833553,
      "loss": 1.0231,
      "step": 4272
    },
    {
      "epoch": 5.697333333333333,
      "grad_norm": 0.029911378398537636,
      "learning_rate": 0.0008236590762185738,
      "loss": 1.1038,
      "step": 4273
    },
    {
      "epoch": 5.698666666666667,
      "grad_norm": 0.0241263248026371,
      "learning_rate": 0.00082323402583796,
      "loss": 0.8022,
      "step": 4274
    },
    {
      "epoch": 5.7,
      "grad_norm": 0.029783913865685463,
      "learning_rate": 0.0008228090084207773,
      "loss": 1.2657,
      "step": 4275
    },
    {
      "epoch": 5.701333333333333,
      "grad_norm": 0.02477998472750187,
      "learning_rate": 0.0008223840240462837,
      "loss": 0.7974,
      "step": 4276
    },
    {
      "epoch": 5.7026666666666666,
      "grad_norm": 0.026476003229618073,
      "learning_rate": 0.0008219590727937302,
      "loss": 0.9643,
      "step": 4277
    },
    {
      "epoch": 5.704,
      "grad_norm": 0.029501618817448616,
      "learning_rate": 0.0008215341547423623,
      "loss": 1.2582,
      "step": 4278
    },
    {
      "epoch": 5.705333333333334,
      "grad_norm": 0.03141770511865616,
      "learning_rate": 0.0008211092699714191,
      "loss": 0.8962,
      "step": 4279
    },
    {
      "epoch": 5.706666666666667,
      "grad_norm": 0.030445147305727005,
      "learning_rate": 0.0008206844185601328,
      "loss": 1.2209,
      "step": 4280
    },
    {
      "epoch": 5.708,
      "grad_norm": 0.024923942983150482,
      "learning_rate": 0.0008202596005877305,
      "loss": 0.9794,
      "step": 4281
    },
    {
      "epoch": 5.709333333333333,
      "grad_norm": 0.0325869657099247,
      "learning_rate": 0.000819834816133432,
      "loss": 1.4183,
      "step": 4282
    },
    {
      "epoch": 5.710666666666667,
      "grad_norm": 0.02234235219657421,
      "learning_rate": 0.000819410065276452,
      "loss": 1.0408,
      "step": 4283
    },
    {
      "epoch": 5.712,
      "grad_norm": 0.026282817125320435,
      "learning_rate": 0.0008189853480959981,
      "loss": 1.1662,
      "step": 4284
    },
    {
      "epoch": 5.713333333333333,
      "grad_norm": 0.02797464095056057,
      "learning_rate": 0.0008185606646712714,
      "loss": 0.9412,
      "step": 4285
    },
    {
      "epoch": 5.714666666666667,
      "grad_norm": 0.02205115184187889,
      "learning_rate": 0.0008181360150814679,
      "loss": 0.9751,
      "step": 4286
    },
    {
      "epoch": 5.716,
      "grad_norm": 0.030795127153396606,
      "learning_rate": 0.0008177113994057754,
      "loss": 1.0479,
      "step": 4287
    },
    {
      "epoch": 5.717333333333333,
      "grad_norm": 0.04069405421614647,
      "learning_rate": 0.0008172868177233779,
      "loss": 1.1637,
      "step": 4288
    },
    {
      "epoch": 5.718666666666667,
      "grad_norm": 0.02475302666425705,
      "learning_rate": 0.0008168622701134507,
      "loss": 1.0816,
      "step": 4289
    },
    {
      "epoch": 5.72,
      "grad_norm": 0.030430836603045464,
      "learning_rate": 0.0008164377566551639,
      "loss": 0.994,
      "step": 4290
    },
    {
      "epoch": 5.721333333333334,
      "grad_norm": 0.03037066012620926,
      "learning_rate": 0.0008160132774276814,
      "loss": 1.0007,
      "step": 4291
    },
    {
      "epoch": 5.722666666666667,
      "grad_norm": 0.03011944144964218,
      "learning_rate": 0.0008155888325101595,
      "loss": 0.9358,
      "step": 4292
    },
    {
      "epoch": 5.724,
      "grad_norm": 0.03611999750137329,
      "learning_rate": 0.0008151644219817501,
      "loss": 1.16,
      "step": 4293
    },
    {
      "epoch": 5.725333333333333,
      "grad_norm": 0.026852771639823914,
      "learning_rate": 0.0008147400459215969,
      "loss": 1.1318,
      "step": 4294
    },
    {
      "epoch": 5.726666666666667,
      "grad_norm": 0.029502712190151215,
      "learning_rate": 0.0008143157044088376,
      "loss": 1.2569,
      "step": 4295
    },
    {
      "epoch": 5.728,
      "grad_norm": 0.024822577834129333,
      "learning_rate": 0.0008138913975226043,
      "loss": 1.1662,
      "step": 4296
    },
    {
      "epoch": 5.729333333333333,
      "grad_norm": 0.027099810540676117,
      "learning_rate": 0.0008134671253420212,
      "loss": 0.9376,
      "step": 4297
    },
    {
      "epoch": 5.730666666666667,
      "grad_norm": 0.027428988367319107,
      "learning_rate": 0.0008130428879462077,
      "loss": 1.0047,
      "step": 4298
    },
    {
      "epoch": 5.732,
      "grad_norm": 0.0422443188726902,
      "learning_rate": 0.0008126186854142752,
      "loss": 0.9687,
      "step": 4299
    },
    {
      "epoch": 5.733333333333333,
      "grad_norm": 0.03367840126156807,
      "learning_rate": 0.00081219451782533,
      "loss": 0.9682,
      "step": 4300
    },
    {
      "epoch": 5.734666666666667,
      "grad_norm": 0.03321906924247742,
      "learning_rate": 0.0008117703852584706,
      "loss": 1.5419,
      "step": 4301
    },
    {
      "epoch": 5.736,
      "grad_norm": 0.029111137613654137,
      "learning_rate": 0.0008113462877927893,
      "loss": 1.2069,
      "step": 4302
    },
    {
      "epoch": 5.737333333333333,
      "grad_norm": 0.027119306847453117,
      "learning_rate": 0.0008109222255073727,
      "loss": 0.67,
      "step": 4303
    },
    {
      "epoch": 5.738666666666667,
      "grad_norm": 0.02764863707125187,
      "learning_rate": 0.0008104981984812993,
      "loss": 1.2064,
      "step": 4304
    },
    {
      "epoch": 5.74,
      "grad_norm": 0.021110422909259796,
      "learning_rate": 0.0008100742067936431,
      "loss": 1.2237,
      "step": 4305
    },
    {
      "epoch": 5.741333333333333,
      "grad_norm": 0.023125959560275078,
      "learning_rate": 0.0008096502505234698,
      "loss": 0.9181,
      "step": 4306
    },
    {
      "epoch": 5.742666666666667,
      "grad_norm": 0.024652980268001556,
      "learning_rate": 0.0008092263297498386,
      "loss": 1.0287,
      "step": 4307
    },
    {
      "epoch": 5.744,
      "grad_norm": 0.022547952830791473,
      "learning_rate": 0.0008088024445518032,
      "loss": 1.1513,
      "step": 4308
    },
    {
      "epoch": 5.745333333333333,
      "grad_norm": 0.024965574964880943,
      "learning_rate": 0.0008083785950084092,
      "loss": 1.0393,
      "step": 4309
    },
    {
      "epoch": 5.746666666666667,
      "grad_norm": 0.026722142472863197,
      "learning_rate": 0.0008079547811986971,
      "loss": 1.0415,
      "step": 4310
    },
    {
      "epoch": 5.748,
      "grad_norm": 0.027521610260009766,
      "learning_rate": 0.0008075310032016999,
      "loss": 1.0779,
      "step": 4311
    },
    {
      "epoch": 5.749333333333333,
      "grad_norm": 0.03091568686068058,
      "learning_rate": 0.0008071072610964435,
      "loss": 1.1653,
      "step": 4312
    },
    {
      "epoch": 5.750666666666667,
      "grad_norm": 0.028694406151771545,
      "learning_rate": 0.000806683554961948,
      "loss": 0.9149,
      "step": 4313
    },
    {
      "epoch": 5.752,
      "grad_norm": 0.02558780275285244,
      "learning_rate": 0.000806259884877226,
      "loss": 1.0858,
      "step": 4314
    },
    {
      "epoch": 5.753333333333333,
      "grad_norm": 0.03019043430685997,
      "learning_rate": 0.0008058362509212843,
      "loss": 0.9663,
      "step": 4315
    },
    {
      "epoch": 5.754666666666667,
      "grad_norm": 0.0320977121591568,
      "learning_rate": 0.0008054126531731222,
      "loss": 1.059,
      "step": 4316
    },
    {
      "epoch": 5.756,
      "grad_norm": 0.025843551382422447,
      "learning_rate": 0.0008049890917117322,
      "loss": 0.944,
      "step": 4317
    },
    {
      "epoch": 5.757333333333333,
      "grad_norm": 0.026597170159220695,
      "learning_rate": 0.0008045655666161006,
      "loss": 0.8887,
      "step": 4318
    },
    {
      "epoch": 5.758666666666667,
      "grad_norm": 0.024271950125694275,
      "learning_rate": 0.0008041420779652064,
      "loss": 0.918,
      "step": 4319
    },
    {
      "epoch": 5.76,
      "grad_norm": 0.025316614657640457,
      "learning_rate": 0.0008037186258380226,
      "loss": 1.1142,
      "step": 4320
    },
    {
      "epoch": 5.761333333333333,
      "grad_norm": 0.021694716066122055,
      "learning_rate": 0.0008032952103135143,
      "loss": 0.9091,
      "step": 4321
    },
    {
      "epoch": 5.762666666666667,
      "grad_norm": 0.02955729141831398,
      "learning_rate": 0.0008028718314706402,
      "loss": 0.8847,
      "step": 4322
    },
    {
      "epoch": 5.764,
      "grad_norm": 0.039441630244255066,
      "learning_rate": 0.0008024484893883527,
      "loss": 1.1657,
      "step": 4323
    },
    {
      "epoch": 5.765333333333333,
      "grad_norm": 0.025536268949508667,
      "learning_rate": 0.0008020251841455965,
      "loss": 1.1946,
      "step": 4324
    },
    {
      "epoch": 5.766666666666667,
      "grad_norm": 0.022619634866714478,
      "learning_rate": 0.0008016019158213102,
      "loss": 1.0707,
      "step": 4325
    },
    {
      "epoch": 5.768,
      "grad_norm": 0.03678230568766594,
      "learning_rate": 0.0008011786844944249,
      "loss": 1.0869,
      "step": 4326
    },
    {
      "epoch": 5.769333333333333,
      "grad_norm": 0.02537512592971325,
      "learning_rate": 0.0008007554902438649,
      "loss": 1.0964,
      "step": 4327
    },
    {
      "epoch": 5.770666666666667,
      "grad_norm": 0.030365601181983948,
      "learning_rate": 0.0008003323331485483,
      "loss": 0.8372,
      "step": 4328
    },
    {
      "epoch": 5.772,
      "grad_norm": 0.02735043875873089,
      "learning_rate": 0.0007999092132873851,
      "loss": 0.9561,
      "step": 4329
    },
    {
      "epoch": 5.773333333333333,
      "grad_norm": 0.02207699604332447,
      "learning_rate": 0.0007994861307392793,
      "loss": 0.8992,
      "step": 4330
    },
    {
      "epoch": 5.774666666666667,
      "grad_norm": 0.023473255336284637,
      "learning_rate": 0.0007990630855831275,
      "loss": 0.9571,
      "step": 4331
    },
    {
      "epoch": 5.776,
      "grad_norm": 0.027111131697893143,
      "learning_rate": 0.0007986400778978193,
      "loss": 1.0645,
      "step": 4332
    },
    {
      "epoch": 5.777333333333333,
      "grad_norm": 0.023768380284309387,
      "learning_rate": 0.0007982171077622378,
      "loss": 1.0828,
      "step": 4333
    },
    {
      "epoch": 5.778666666666666,
      "grad_norm": 0.024669349193572998,
      "learning_rate": 0.0007977941752552583,
      "loss": 0.995,
      "step": 4334
    },
    {
      "epoch": 5.78,
      "grad_norm": 0.028937477618455887,
      "learning_rate": 0.00079737128045575,
      "loss": 1.1685,
      "step": 4335
    },
    {
      "epoch": 5.781333333333333,
      "grad_norm": 0.028461016714572906,
      "learning_rate": 0.0007969484234425743,
      "loss": 1.0938,
      "step": 4336
    },
    {
      "epoch": 5.782666666666667,
      "grad_norm": 0.02795576862990856,
      "learning_rate": 0.0007965256042945855,
      "loss": 0.944,
      "step": 4337
    },
    {
      "epoch": 5.784,
      "grad_norm": 0.026903998106718063,
      "learning_rate": 0.000796102823090632,
      "loss": 0.8329,
      "step": 4338
    },
    {
      "epoch": 5.785333333333333,
      "grad_norm": 0.026295846328139305,
      "learning_rate": 0.0007956800799095536,
      "loss": 0.9012,
      "step": 4339
    },
    {
      "epoch": 5.786666666666667,
      "grad_norm": 0.0339311920106411,
      "learning_rate": 0.0007952573748301841,
      "loss": 1.0917,
      "step": 4340
    },
    {
      "epoch": 5.788,
      "grad_norm": 0.02734684944152832,
      "learning_rate": 0.0007948347079313494,
      "loss": 1.042,
      "step": 4341
    },
    {
      "epoch": 5.789333333333333,
      "grad_norm": 0.029191480949521065,
      "learning_rate": 0.0007944120792918694,
      "loss": 1.1121,
      "step": 4342
    },
    {
      "epoch": 5.790666666666667,
      "grad_norm": 0.031086133792996407,
      "learning_rate": 0.0007939894889905558,
      "loss": 0.9757,
      "step": 4343
    },
    {
      "epoch": 5.792,
      "grad_norm": 0.02123209275305271,
      "learning_rate": 0.0007935669371062132,
      "loss": 0.9405,
      "step": 4344
    },
    {
      "epoch": 5.793333333333333,
      "grad_norm": 0.031492169946432114,
      "learning_rate": 0.0007931444237176398,
      "loss": 1.1252,
      "step": 4345
    },
    {
      "epoch": 5.794666666666666,
      "grad_norm": 0.02455291897058487,
      "learning_rate": 0.0007927219489036257,
      "loss": 1.0602,
      "step": 4346
    },
    {
      "epoch": 5.796,
      "grad_norm": 0.03259454667568207,
      "learning_rate": 0.0007922995127429548,
      "loss": 0.8307,
      "step": 4347
    },
    {
      "epoch": 5.7973333333333334,
      "grad_norm": 0.027064545080065727,
      "learning_rate": 0.000791877115314403,
      "loss": 1.0947,
      "step": 4348
    },
    {
      "epoch": 5.798666666666667,
      "grad_norm": 0.024987718090415,
      "learning_rate": 0.0007914547566967389,
      "loss": 1.0434,
      "step": 4349
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.030333563685417175,
      "learning_rate": 0.000791032436968725,
      "loss": 0.9984,
      "step": 4350
    },
    {
      "epoch": 5.801333333333333,
      "grad_norm": 0.02468368597328663,
      "learning_rate": 0.0007906101562091151,
      "loss": 1.0247,
      "step": 4351
    },
    {
      "epoch": 5.802666666666667,
      "grad_norm": 0.023240085691213608,
      "learning_rate": 0.0007901879144966566,
      "loss": 0.7966,
      "step": 4352
    },
    {
      "epoch": 5.804,
      "grad_norm": 0.025310344994068146,
      "learning_rate": 0.0007897657119100896,
      "loss": 1.167,
      "step": 4353
    },
    {
      "epoch": 5.8053333333333335,
      "grad_norm": 0.03091660514473915,
      "learning_rate": 0.0007893435485281459,
      "loss": 1.174,
      "step": 4354
    },
    {
      "epoch": 5.806666666666667,
      "grad_norm": 0.029980365186929703,
      "learning_rate": 0.000788921424429552,
      "loss": 1.0795,
      "step": 4355
    },
    {
      "epoch": 5.808,
      "grad_norm": 0.024043740704655647,
      "learning_rate": 0.0007884993396930249,
      "loss": 1.0464,
      "step": 4356
    },
    {
      "epoch": 5.809333333333333,
      "grad_norm": 0.024926599115133286,
      "learning_rate": 0.000788077294397276,
      "loss": 1.1161,
      "step": 4357
    },
    {
      "epoch": 5.810666666666666,
      "grad_norm": 0.030231505632400513,
      "learning_rate": 0.000787655288621008,
      "loss": 0.9528,
      "step": 4358
    },
    {
      "epoch": 5.812,
      "grad_norm": 0.026787642389535904,
      "learning_rate": 0.0007872333224429167,
      "loss": 1.3953,
      "step": 4359
    },
    {
      "epoch": 5.8133333333333335,
      "grad_norm": 0.028893884271383286,
      "learning_rate": 0.0007868113959416911,
      "loss": 1.077,
      "step": 4360
    },
    {
      "epoch": 5.814666666666667,
      "grad_norm": 0.024229643866419792,
      "learning_rate": 0.000786389509196012,
      "loss": 1.272,
      "step": 4361
    },
    {
      "epoch": 5.816,
      "grad_norm": 0.030149998143315315,
      "learning_rate": 0.0007859676622845535,
      "loss": 1.0732,
      "step": 4362
    },
    {
      "epoch": 5.817333333333333,
      "grad_norm": 0.028931699693202972,
      "learning_rate": 0.0007855458552859817,
      "loss": 0.9057,
      "step": 4363
    },
    {
      "epoch": 5.818666666666667,
      "grad_norm": 0.027398210018873215,
      "learning_rate": 0.0007851240882789548,
      "loss": 1.2019,
      "step": 4364
    },
    {
      "epoch": 5.82,
      "grad_norm": 0.02305050753057003,
      "learning_rate": 0.0007847023613421251,
      "loss": 1.0893,
      "step": 4365
    },
    {
      "epoch": 5.8213333333333335,
      "grad_norm": 0.0243011973798275,
      "learning_rate": 0.0007842806745541361,
      "loss": 1.0918,
      "step": 4366
    },
    {
      "epoch": 5.822666666666667,
      "grad_norm": 0.025587866082787514,
      "learning_rate": 0.0007838590279936244,
      "loss": 1.2186,
      "step": 4367
    },
    {
      "epoch": 5.824,
      "grad_norm": 0.04768159240484238,
      "learning_rate": 0.0007834374217392187,
      "loss": 1.1118,
      "step": 4368
    },
    {
      "epoch": 5.825333333333333,
      "grad_norm": 0.03501260653138161,
      "learning_rate": 0.0007830158558695401,
      "loss": 1.1082,
      "step": 4369
    },
    {
      "epoch": 5.826666666666666,
      "grad_norm": 0.027819082140922546,
      "learning_rate": 0.0007825943304632033,
      "loss": 1.3495,
      "step": 4370
    },
    {
      "epoch": 5.828,
      "grad_norm": 0.0324162133038044,
      "learning_rate": 0.000782172845598814,
      "loss": 1.1111,
      "step": 4371
    },
    {
      "epoch": 5.8293333333333335,
      "grad_norm": 0.025911038741469383,
      "learning_rate": 0.0007817514013549713,
      "loss": 0.86,
      "step": 4372
    },
    {
      "epoch": 5.830666666666667,
      "grad_norm": 0.024661283940076828,
      "learning_rate": 0.0007813299978102661,
      "loss": 1.0443,
      "step": 4373
    },
    {
      "epoch": 5.832,
      "grad_norm": 0.027573425322771072,
      "learning_rate": 0.0007809086350432819,
      "loss": 1.1903,
      "step": 4374
    },
    {
      "epoch": 5.833333333333333,
      "grad_norm": 0.025302527472376823,
      "learning_rate": 0.0007804873131325953,
      "loss": 0.8277,
      "step": 4375
    },
    {
      "epoch": 5.834666666666667,
      "grad_norm": 0.029776938259601593,
      "learning_rate": 0.0007800660321567738,
      "loss": 1.1736,
      "step": 4376
    },
    {
      "epoch": 5.836,
      "grad_norm": 0.027580443769693375,
      "learning_rate": 0.0007796447921943792,
      "loss": 1.1175,
      "step": 4377
    },
    {
      "epoch": 5.8373333333333335,
      "grad_norm": 0.01981016807258129,
      "learning_rate": 0.0007792235933239641,
      "loss": 0.9634,
      "step": 4378
    },
    {
      "epoch": 5.838666666666667,
      "grad_norm": 0.022099729627370834,
      "learning_rate": 0.0007788024356240736,
      "loss": 0.8919,
      "step": 4379
    },
    {
      "epoch": 5.84,
      "grad_norm": 0.024650074541568756,
      "learning_rate": 0.000778381319173246,
      "loss": 1.0186,
      "step": 4380
    },
    {
      "epoch": 5.841333333333333,
      "grad_norm": 0.022710159420967102,
      "learning_rate": 0.0007779602440500105,
      "loss": 1.0377,
      "step": 4381
    },
    {
      "epoch": 5.842666666666666,
      "grad_norm": 0.0404680073261261,
      "learning_rate": 0.0007775392103328907,
      "loss": 1.1195,
      "step": 4382
    },
    {
      "epoch": 5.844,
      "grad_norm": 0.05758238956332207,
      "learning_rate": 0.0007771182181004005,
      "loss": 1.0658,
      "step": 4383
    },
    {
      "epoch": 5.8453333333333335,
      "grad_norm": 0.02430439554154873,
      "learning_rate": 0.0007766972674310466,
      "loss": 0.9879,
      "step": 4384
    },
    {
      "epoch": 5.846666666666667,
      "grad_norm": 0.02863069623708725,
      "learning_rate": 0.0007762763584033289,
      "loss": 1.0904,
      "step": 4385
    },
    {
      "epoch": 5.848,
      "grad_norm": 0.019783781841397285,
      "learning_rate": 0.0007758554910957377,
      "loss": 0.9237,
      "step": 4386
    },
    {
      "epoch": 5.849333333333333,
      "grad_norm": 0.029700571671128273,
      "learning_rate": 0.0007754346655867577,
      "loss": 0.9427,
      "step": 4387
    },
    {
      "epoch": 5.850666666666667,
      "grad_norm": 0.024165133014321327,
      "learning_rate": 0.0007750138819548641,
      "loss": 0.9448,
      "step": 4388
    },
    {
      "epoch": 5.852,
      "grad_norm": 0.05987410992383957,
      "learning_rate": 0.0007745931402785251,
      "loss": 1.2514,
      "step": 4389
    },
    {
      "epoch": 5.8533333333333335,
      "grad_norm": 0.021814819425344467,
      "learning_rate": 0.000774172440636201,
      "loss": 1.0615,
      "step": 4390
    },
    {
      "epoch": 5.854666666666667,
      "grad_norm": 0.027548188343644142,
      "learning_rate": 0.0007737517831063434,
      "loss": 1.1342,
      "step": 4391
    },
    {
      "epoch": 5.856,
      "grad_norm": 0.022499611601233482,
      "learning_rate": 0.0007733311677673979,
      "loss": 1.0942,
      "step": 4392
    },
    {
      "epoch": 5.857333333333333,
      "grad_norm": 0.026358067989349365,
      "learning_rate": 0.0007729105946978003,
      "loss": 1.1276,
      "step": 4393
    },
    {
      "epoch": 5.858666666666666,
      "grad_norm": 0.02278551645576954,
      "learning_rate": 0.0007724900639759797,
      "loss": 1.0006,
      "step": 4394
    },
    {
      "epoch": 5.86,
      "grad_norm": 0.025219455361366272,
      "learning_rate": 0.0007720695756803568,
      "loss": 1.1351,
      "step": 4395
    },
    {
      "epoch": 5.8613333333333335,
      "grad_norm": 0.024489153176546097,
      "learning_rate": 0.0007716491298893442,
      "loss": 0.8675,
      "step": 4396
    },
    {
      "epoch": 5.862666666666667,
      "grad_norm": 0.024361100047826767,
      "learning_rate": 0.0007712287266813477,
      "loss": 1.0705,
      "step": 4397
    },
    {
      "epoch": 5.864,
      "grad_norm": 0.026308998465538025,
      "learning_rate": 0.0007708083661347637,
      "loss": 0.9838,
      "step": 4398
    },
    {
      "epoch": 5.865333333333333,
      "grad_norm": 0.02449115179479122,
      "learning_rate": 0.0007703880483279817,
      "loss": 1.0626,
      "step": 4399
    },
    {
      "epoch": 5.866666666666667,
      "grad_norm": 0.024519089609384537,
      "learning_rate": 0.0007699677733393826,
      "loss": 1.1058,
      "step": 4400
    },
    {
      "epoch": 5.868,
      "grad_norm": 0.02772970125079155,
      "learning_rate": 0.0007695475412473391,
      "loss": 1.086,
      "step": 4401
    },
    {
      "epoch": 5.8693333333333335,
      "grad_norm": 0.024158937856554985,
      "learning_rate": 0.0007691273521302175,
      "loss": 1.0008,
      "step": 4402
    },
    {
      "epoch": 5.870666666666667,
      "grad_norm": 0.02010204643011093,
      "learning_rate": 0.0007687072060663737,
      "loss": 0.8509,
      "step": 4403
    },
    {
      "epoch": 5.872,
      "grad_norm": 0.023990008980035782,
      "learning_rate": 0.0007682871031341579,
      "loss": 1.1839,
      "step": 4404
    },
    {
      "epoch": 5.873333333333333,
      "grad_norm": 0.03170172497630119,
      "learning_rate": 0.0007678670434119104,
      "loss": 1.0118,
      "step": 4405
    },
    {
      "epoch": 5.874666666666666,
      "grad_norm": 0.029962606728076935,
      "learning_rate": 0.0007674470269779644,
      "loss": 0.7254,
      "step": 4406
    },
    {
      "epoch": 5.876,
      "grad_norm": 0.04910833761096001,
      "learning_rate": 0.0007670270539106451,
      "loss": 0.9007,
      "step": 4407
    },
    {
      "epoch": 5.8773333333333335,
      "grad_norm": 0.02737927809357643,
      "learning_rate": 0.0007666071242882686,
      "loss": 1.4379,
      "step": 4408
    },
    {
      "epoch": 5.878666666666667,
      "grad_norm": 0.05078556016087532,
      "learning_rate": 0.0007661872381891447,
      "loss": 0.8107,
      "step": 4409
    },
    {
      "epoch": 5.88,
      "grad_norm": 0.0313384011387825,
      "learning_rate": 0.0007657673956915735,
      "loss": 1.2028,
      "step": 4410
    },
    {
      "epoch": 5.881333333333333,
      "grad_norm": 0.022966083139181137,
      "learning_rate": 0.0007653475968738472,
      "loss": 0.9933,
      "step": 4411
    },
    {
      "epoch": 5.882666666666667,
      "grad_norm": 0.019778799265623093,
      "learning_rate": 0.0007649278418142507,
      "loss": 0.8967,
      "step": 4412
    },
    {
      "epoch": 5.884,
      "grad_norm": 0.026447733864188194,
      "learning_rate": 0.0007645081305910595,
      "loss": 1.2139,
      "step": 4413
    },
    {
      "epoch": 5.8853333333333335,
      "grad_norm": 0.018164092674851418,
      "learning_rate": 0.0007640884632825424,
      "loss": 1.1577,
      "step": 4414
    },
    {
      "epoch": 5.886666666666667,
      "grad_norm": 0.021539663895964622,
      "learning_rate": 0.000763668839966959,
      "loss": 0.9242,
      "step": 4415
    },
    {
      "epoch": 5.888,
      "grad_norm": 0.021476592868566513,
      "learning_rate": 0.0007632492607225604,
      "loss": 1.1198,
      "step": 4416
    },
    {
      "epoch": 5.889333333333333,
      "grad_norm": 0.028832418844103813,
      "learning_rate": 0.0007628297256275907,
      "loss": 1.0889,
      "step": 4417
    },
    {
      "epoch": 5.890666666666666,
      "grad_norm": 0.02547696977853775,
      "learning_rate": 0.0007624102347602844,
      "loss": 1.2581,
      "step": 4418
    },
    {
      "epoch": 5.892,
      "grad_norm": 0.02333195134997368,
      "learning_rate": 0.0007619907881988692,
      "loss": 0.8276,
      "step": 4419
    },
    {
      "epoch": 5.8933333333333335,
      "grad_norm": 0.027612142264842987,
      "learning_rate": 0.0007615713860215633,
      "loss": 0.9062,
      "step": 4420
    },
    {
      "epoch": 5.894666666666667,
      "grad_norm": 0.02375662885606289,
      "learning_rate": 0.0007611520283065769,
      "loss": 0.9775,
      "step": 4421
    },
    {
      "epoch": 5.896,
      "grad_norm": 0.025885870680212975,
      "learning_rate": 0.0007607327151321126,
      "loss": 0.8695,
      "step": 4422
    },
    {
      "epoch": 5.897333333333333,
      "grad_norm": 0.02255682647228241,
      "learning_rate": 0.0007603134465763635,
      "loss": 0.8647,
      "step": 4423
    },
    {
      "epoch": 5.898666666666666,
      "grad_norm": 0.026979360729455948,
      "learning_rate": 0.000759894222717516,
      "loss": 1.1293,
      "step": 4424
    },
    {
      "epoch": 5.9,
      "grad_norm": 0.02476257085800171,
      "learning_rate": 0.0007594750436337467,
      "loss": 0.9943,
      "step": 4425
    },
    {
      "epoch": 5.9013333333333335,
      "grad_norm": 0.022196155041456223,
      "learning_rate": 0.0007590559094032239,
      "loss": 0.8967,
      "step": 4426
    },
    {
      "epoch": 5.902666666666667,
      "grad_norm": 0.025082865729928017,
      "learning_rate": 0.0007586368201041092,
      "loss": 1.2747,
      "step": 4427
    },
    {
      "epoch": 5.904,
      "grad_norm": 0.022822901606559753,
      "learning_rate": 0.0007582177758145532,
      "loss": 1.0945,
      "step": 4428
    },
    {
      "epoch": 5.905333333333333,
      "grad_norm": 0.05936766788363457,
      "learning_rate": 0.0007577987766127009,
      "loss": 0.7391,
      "step": 4429
    },
    {
      "epoch": 5.906666666666666,
      "grad_norm": 0.020846672356128693,
      "learning_rate": 0.000757379822576687,
      "loss": 0.7405,
      "step": 4430
    },
    {
      "epoch": 5.908,
      "grad_norm": 0.04700877517461777,
      "learning_rate": 0.0007569609137846376,
      "loss": 0.9838,
      "step": 4431
    },
    {
      "epoch": 5.9093333333333335,
      "grad_norm": 0.028621289879083633,
      "learning_rate": 0.0007565420503146723,
      "loss": 0.8453,
      "step": 4432
    },
    {
      "epoch": 5.910666666666667,
      "grad_norm": 0.0202325489372015,
      "learning_rate": 0.0007561232322449003,
      "loss": 0.9148,
      "step": 4433
    },
    {
      "epoch": 5.912,
      "grad_norm": 0.031936291605234146,
      "learning_rate": 0.0007557044596534233,
      "loss": 1.0922,
      "step": 4434
    },
    {
      "epoch": 5.913333333333333,
      "grad_norm": 0.026359403505921364,
      "learning_rate": 0.000755285732618334,
      "loss": 0.9302,
      "step": 4435
    },
    {
      "epoch": 5.914666666666666,
      "grad_norm": 0.030814873054623604,
      "learning_rate": 0.0007548670512177173,
      "loss": 1.1848,
      "step": 4436
    },
    {
      "epoch": 5.916,
      "grad_norm": 0.031147494912147522,
      "learning_rate": 0.0007544484155296491,
      "loss": 0.8267,
      "step": 4437
    },
    {
      "epoch": 5.917333333333334,
      "grad_norm": 0.027047129347920418,
      "learning_rate": 0.0007540298256321964,
      "loss": 1.0562,
      "step": 4438
    },
    {
      "epoch": 5.918666666666667,
      "grad_norm": 0.023509696125984192,
      "learning_rate": 0.0007536112816034187,
      "loss": 0.986,
      "step": 4439
    },
    {
      "epoch": 5.92,
      "grad_norm": 0.04065953195095062,
      "learning_rate": 0.0007531927835213657,
      "loss": 0.9244,
      "step": 4440
    },
    {
      "epoch": 5.921333333333333,
      "grad_norm": 0.029302624985575676,
      "learning_rate": 0.0007527743314640799,
      "loss": 1.2112,
      "step": 4441
    },
    {
      "epoch": 5.922666666666666,
      "grad_norm": 0.030723081901669502,
      "learning_rate": 0.0007523559255095942,
      "loss": 1.0207,
      "step": 4442
    },
    {
      "epoch": 5.924,
      "grad_norm": 0.02724098414182663,
      "learning_rate": 0.0007519375657359331,
      "loss": 0.9661,
      "step": 4443
    },
    {
      "epoch": 5.925333333333334,
      "grad_norm": 0.03469426557421684,
      "learning_rate": 0.0007515192522211127,
      "loss": 1.0321,
      "step": 4444
    },
    {
      "epoch": 5.926666666666667,
      "grad_norm": 0.03215692192316055,
      "learning_rate": 0.00075110098504314,
      "loss": 1.1875,
      "step": 4445
    },
    {
      "epoch": 5.928,
      "grad_norm": 0.04801171272993088,
      "learning_rate": 0.0007506827642800146,
      "loss": 1.0743,
      "step": 4446
    },
    {
      "epoch": 5.929333333333333,
      "grad_norm": 0.022124765440821648,
      "learning_rate": 0.000750264590009726,
      "loss": 0.9699,
      "step": 4447
    },
    {
      "epoch": 5.930666666666666,
      "grad_norm": 0.023596763610839844,
      "learning_rate": 0.0007498464623102555,
      "loss": 1.0832,
      "step": 4448
    },
    {
      "epoch": 5.932,
      "grad_norm": 0.028363896533846855,
      "learning_rate": 0.000749428381259576,
      "loss": 1.1321,
      "step": 4449
    },
    {
      "epoch": 5.933333333333334,
      "grad_norm": 0.034896448254585266,
      "learning_rate": 0.0007490103469356513,
      "loss": 0.7939,
      "step": 4450
    },
    {
      "epoch": 5.934666666666667,
      "grad_norm": 0.02929341048002243,
      "learning_rate": 0.0007485923594164372,
      "loss": 1.0678,
      "step": 4451
    },
    {
      "epoch": 5.936,
      "grad_norm": 0.026450077071785927,
      "learning_rate": 0.00074817441877988,
      "loss": 0.8256,
      "step": 4452
    },
    {
      "epoch": 5.937333333333333,
      "grad_norm": 0.02746187336742878,
      "learning_rate": 0.0007477565251039171,
      "loss": 0.9784,
      "step": 4453
    },
    {
      "epoch": 5.938666666666666,
      "grad_norm": 0.023770807310938835,
      "learning_rate": 0.0007473386784664784,
      "loss": 0.7507,
      "step": 4454
    },
    {
      "epoch": 5.9399999999999995,
      "grad_norm": 0.028344426304101944,
      "learning_rate": 0.0007469208789454838,
      "loss": 1.0586,
      "step": 4455
    },
    {
      "epoch": 5.941333333333334,
      "grad_norm": 0.020066674798727036,
      "learning_rate": 0.0007465031266188449,
      "loss": 1.0186,
      "step": 4456
    },
    {
      "epoch": 5.942666666666667,
      "grad_norm": 0.025637520477175713,
      "learning_rate": 0.0007460854215644643,
      "loss": 1.1466,
      "step": 4457
    },
    {
      "epoch": 5.944,
      "grad_norm": 0.02911030687391758,
      "learning_rate": 0.0007456677638602355,
      "loss": 1.2383,
      "step": 4458
    },
    {
      "epoch": 5.945333333333333,
      "grad_norm": 0.02178063429892063,
      "learning_rate": 0.0007452501535840446,
      "loss": 1.138,
      "step": 4459
    },
    {
      "epoch": 5.946666666666666,
      "grad_norm": 0.02109156921505928,
      "learning_rate": 0.000744832590813767,
      "loss": 1.0899,
      "step": 4460
    },
    {
      "epoch": 5.948,
      "grad_norm": 0.022441549226641655,
      "learning_rate": 0.0007444150756272704,
      "loss": 0.9488,
      "step": 4461
    },
    {
      "epoch": 5.949333333333334,
      "grad_norm": 0.029552103951573372,
      "learning_rate": 0.0007439976081024133,
      "loss": 0.9443,
      "step": 4462
    },
    {
      "epoch": 5.950666666666667,
      "grad_norm": 0.03166496753692627,
      "learning_rate": 0.0007435801883170449,
      "loss": 0.9921,
      "step": 4463
    },
    {
      "epoch": 5.952,
      "grad_norm": 0.048100508749485016,
      "learning_rate": 0.0007431628163490066,
      "loss": 1.4734,
      "step": 4464
    },
    {
      "epoch": 5.953333333333333,
      "grad_norm": 0.025409162044525146,
      "learning_rate": 0.0007427454922761296,
      "loss": 1.0966,
      "step": 4465
    },
    {
      "epoch": 5.954666666666666,
      "grad_norm": 0.02570762299001217,
      "learning_rate": 0.0007423282161762373,
      "loss": 0.855,
      "step": 4466
    },
    {
      "epoch": 5.9559999999999995,
      "grad_norm": 0.02675386518239975,
      "learning_rate": 0.0007419109881271433,
      "loss": 1.0987,
      "step": 4467
    },
    {
      "epoch": 5.957333333333334,
      "grad_norm": 0.03311963751912117,
      "learning_rate": 0.0007414938082066521,
      "loss": 1.0074,
      "step": 4468
    },
    {
      "epoch": 5.958666666666667,
      "grad_norm": 0.045046012848615646,
      "learning_rate": 0.0007410766764925604,
      "loss": 1.2763,
      "step": 4469
    },
    {
      "epoch": 5.96,
      "grad_norm": 0.026945294812321663,
      "learning_rate": 0.0007406595930626549,
      "loss": 1.1583,
      "step": 4470
    },
    {
      "epoch": 5.961333333333333,
      "grad_norm": 0.040900759398937225,
      "learning_rate": 0.0007402425579947138,
      "loss": 0.9428,
      "step": 4471
    },
    {
      "epoch": 5.962666666666666,
      "grad_norm": 0.023618411272764206,
      "learning_rate": 0.0007398255713665058,
      "loss": 0.9681,
      "step": 4472
    },
    {
      "epoch": 5.964,
      "grad_norm": 0.032536063343286514,
      "learning_rate": 0.0007394086332557907,
      "loss": 1.0406,
      "step": 4473
    },
    {
      "epoch": 5.965333333333334,
      "grad_norm": 0.02916005626320839,
      "learning_rate": 0.0007389917437403197,
      "loss": 1.4428,
      "step": 4474
    },
    {
      "epoch": 5.966666666666667,
      "grad_norm": 0.030451282858848572,
      "learning_rate": 0.0007385749028978346,
      "loss": 1.1452,
      "step": 4475
    },
    {
      "epoch": 5.968,
      "grad_norm": 0.023116011172533035,
      "learning_rate": 0.000738158110806068,
      "loss": 0.9309,
      "step": 4476
    },
    {
      "epoch": 5.969333333333333,
      "grad_norm": 0.025166956707835197,
      "learning_rate": 0.0007377413675427433,
      "loss": 0.8699,
      "step": 4477
    },
    {
      "epoch": 5.970666666666666,
      "grad_norm": 0.02597063221037388,
      "learning_rate": 0.0007373246731855758,
      "loss": 1.0469,
      "step": 4478
    },
    {
      "epoch": 5.9719999999999995,
      "grad_norm": 0.021763242781162262,
      "learning_rate": 0.0007369080278122705,
      "loss": 1.0866,
      "step": 4479
    },
    {
      "epoch": 5.973333333333334,
      "grad_norm": 0.03027222864329815,
      "learning_rate": 0.0007364914315005234,
      "loss": 1.0539,
      "step": 4480
    },
    {
      "epoch": 5.974666666666667,
      "grad_norm": 0.020747238770127296,
      "learning_rate": 0.000736074884328022,
      "loss": 0.9363,
      "step": 4481
    },
    {
      "epoch": 5.976,
      "grad_norm": 0.032501086592674255,
      "learning_rate": 0.0007356583863724442,
      "loss": 1.1588,
      "step": 4482
    },
    {
      "epoch": 5.977333333333333,
      "grad_norm": 0.021568505093455315,
      "learning_rate": 0.0007352419377114591,
      "loss": 1.06,
      "step": 4483
    },
    {
      "epoch": 5.978666666666666,
      "grad_norm": 0.02786852791905403,
      "learning_rate": 0.0007348255384227259,
      "loss": 0.8793,
      "step": 4484
    },
    {
      "epoch": 5.98,
      "grad_norm": 0.022378554567694664,
      "learning_rate": 0.0007344091885838948,
      "loss": 1.1707,
      "step": 4485
    },
    {
      "epoch": 5.981333333333334,
      "grad_norm": 0.025166651234030724,
      "learning_rate": 0.0007339928882726076,
      "loss": 0.8778,
      "step": 4486
    },
    {
      "epoch": 5.982666666666667,
      "grad_norm": 0.0701851025223732,
      "learning_rate": 0.0007335766375664958,
      "loss": 1.237,
      "step": 4487
    },
    {
      "epoch": 5.984,
      "grad_norm": 0.027825988829135895,
      "learning_rate": 0.0007331604365431825,
      "loss": 1.4143,
      "step": 4488
    },
    {
      "epoch": 5.985333333333333,
      "grad_norm": 0.029087217524647713,
      "learning_rate": 0.0007327442852802811,
      "loss": 1.0345,
      "step": 4489
    },
    {
      "epoch": 5.986666666666666,
      "grad_norm": 0.03246453404426575,
      "learning_rate": 0.0007323281838553947,
      "loss": 0.9331,
      "step": 4490
    },
    {
      "epoch": 5.9879999999999995,
      "grad_norm": 0.02525106817483902,
      "learning_rate": 0.0007319121323461198,
      "loss": 0.9813,
      "step": 4491
    },
    {
      "epoch": 5.989333333333334,
      "grad_norm": 0.024457713589072227,
      "learning_rate": 0.0007314961308300407,
      "loss": 1.1543,
      "step": 4492
    },
    {
      "epoch": 5.990666666666667,
      "grad_norm": 0.030744165182113647,
      "learning_rate": 0.0007310801793847343,
      "loss": 0.9179,
      "step": 4493
    },
    {
      "epoch": 5.992,
      "grad_norm": 0.02691667154431343,
      "learning_rate": 0.0007306642780877674,
      "loss": 1.0459,
      "step": 4494
    },
    {
      "epoch": 5.993333333333333,
      "grad_norm": 0.02015767991542816,
      "learning_rate": 0.000730248427016697,
      "loss": 0.8794,
      "step": 4495
    },
    {
      "epoch": 5.994666666666666,
      "grad_norm": 0.02703222446143627,
      "learning_rate": 0.0007298326262490719,
      "loss": 1.0347,
      "step": 4496
    },
    {
      "epoch": 5.996,
      "grad_norm": 0.052052803337574005,
      "learning_rate": 0.0007294168758624306,
      "loss": 0.815,
      "step": 4497
    },
    {
      "epoch": 5.997333333333334,
      "grad_norm": 0.02680424228310585,
      "learning_rate": 0.0007290011759343029,
      "loss": 1.2332,
      "step": 4498
    },
    {
      "epoch": 5.998666666666667,
      "grad_norm": 0.023781264200806618,
      "learning_rate": 0.0007285855265422083,
      "loss": 1.1129,
      "step": 4499
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.02628486044704914,
      "learning_rate": 0.0007281699277636571,
      "loss": 0.97,
      "step": 4500
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.0384738445281982,
      "eval_runtime": 23.7617,
      "eval_samples_per_second": 21.042,
      "eval_steps_per_second": 2.651,
      "step": 4500
    },
    {
      "epoch": 6.001333333333333,
      "grad_norm": 0.03684287145733833,
      "learning_rate": 0.0007277543796761515,
      "loss": 1.0876,
      "step": 4501
    },
    {
      "epoch": 6.002666666666666,
      "grad_norm": 0.026452792808413506,
      "learning_rate": 0.0007273388823571818,
      "loss": 1.1881,
      "step": 4502
    },
    {
      "epoch": 6.004,
      "grad_norm": 0.027080468833446503,
      "learning_rate": 0.0007269234358842314,
      "loss": 1.1532,
      "step": 4503
    },
    {
      "epoch": 6.005333333333334,
      "grad_norm": 0.025275880470871925,
      "learning_rate": 0.0007265080403347728,
      "loss": 0.9389,
      "step": 4504
    },
    {
      "epoch": 6.006666666666667,
      "grad_norm": 0.032856520265340805,
      "learning_rate": 0.0007260926957862683,
      "loss": 0.7933,
      "step": 4505
    },
    {
      "epoch": 6.008,
      "grad_norm": 0.021242249757051468,
      "learning_rate": 0.0007256774023161728,
      "loss": 1.039,
      "step": 4506
    },
    {
      "epoch": 6.009333333333333,
      "grad_norm": 0.02039691060781479,
      "learning_rate": 0.0007252621600019293,
      "loss": 1.0988,
      "step": 4507
    },
    {
      "epoch": 6.010666666666666,
      "grad_norm": 0.030112210661172867,
      "learning_rate": 0.0007248469689209737,
      "loss": 1.083,
      "step": 4508
    },
    {
      "epoch": 6.012,
      "grad_norm": 0.05470067635178566,
      "learning_rate": 0.0007244318291507308,
      "loss": 1.272,
      "step": 4509
    },
    {
      "epoch": 6.013333333333334,
      "grad_norm": 0.025294752791523933,
      "learning_rate": 0.0007240167407686154,
      "loss": 0.7538,
      "step": 4510
    },
    {
      "epoch": 6.014666666666667,
      "grad_norm": 0.03859943524003029,
      "learning_rate": 0.0007236017038520341,
      "loss": 1.0294,
      "step": 4511
    },
    {
      "epoch": 6.016,
      "grad_norm": 0.02388458512723446,
      "learning_rate": 0.0007231867184783826,
      "loss": 0.9451,
      "step": 4512
    },
    {
      "epoch": 6.017333333333333,
      "grad_norm": 0.03573086857795715,
      "learning_rate": 0.0007227717847250486,
      "loss": 0.9074,
      "step": 4513
    },
    {
      "epoch": 6.018666666666666,
      "grad_norm": 0.02293715812265873,
      "learning_rate": 0.0007223569026694086,
      "loss": 1.0829,
      "step": 4514
    },
    {
      "epoch": 6.02,
      "grad_norm": 0.03255227953195572,
      "learning_rate": 0.00072194207238883,
      "loss": 1.3302,
      "step": 4515
    },
    {
      "epoch": 6.021333333333334,
      "grad_norm": 0.03117639757692814,
      "learning_rate": 0.0007215272939606709,
      "loss": 0.816,
      "step": 4516
    },
    {
      "epoch": 6.022666666666667,
      "grad_norm": 0.02691628783941269,
      "learning_rate": 0.0007211125674622792,
      "loss": 1.1323,
      "step": 4517
    },
    {
      "epoch": 6.024,
      "grad_norm": 0.029899409040808678,
      "learning_rate": 0.0007206978929709936,
      "loss": 0.9697,
      "step": 4518
    },
    {
      "epoch": 6.025333333333333,
      "grad_norm": 0.02965611033141613,
      "learning_rate": 0.0007202832705641429,
      "loss": 1.2802,
      "step": 4519
    },
    {
      "epoch": 6.026666666666666,
      "grad_norm": 0.024392494931817055,
      "learning_rate": 0.0007198687003190457,
      "loss": 1.1336,
      "step": 4520
    },
    {
      "epoch": 6.028,
      "grad_norm": 0.03795114904642105,
      "learning_rate": 0.0007194541823130118,
      "loss": 0.9179,
      "step": 4521
    },
    {
      "epoch": 6.029333333333334,
      "grad_norm": 0.029794204980134964,
      "learning_rate": 0.0007190397166233403,
      "loss": 1.0455,
      "step": 4522
    },
    {
      "epoch": 6.030666666666667,
      "grad_norm": 0.02636127918958664,
      "learning_rate": 0.0007186253033273218,
      "loss": 1.0395,
      "step": 4523
    },
    {
      "epoch": 6.032,
      "grad_norm": 0.029700573533773422,
      "learning_rate": 0.0007182109425022356,
      "loss": 1.0011,
      "step": 4524
    },
    {
      "epoch": 6.033333333333333,
      "grad_norm": 0.03513491898775101,
      "learning_rate": 0.0007177966342253524,
      "loss": 1.1822,
      "step": 4525
    },
    {
      "epoch": 6.034666666666666,
      "grad_norm": 0.02547152154147625,
      "learning_rate": 0.0007173823785739326,
      "loss": 0.8007,
      "step": 4526
    },
    {
      "epoch": 6.036,
      "grad_norm": 0.026281042024493217,
      "learning_rate": 0.0007169681756252264,
      "loss": 1.0103,
      "step": 4527
    },
    {
      "epoch": 6.037333333333334,
      "grad_norm": 0.02692996710538864,
      "learning_rate": 0.0007165540254564752,
      "loss": 1.2207,
      "step": 4528
    },
    {
      "epoch": 6.038666666666667,
      "grad_norm": 0.028790665790438652,
      "learning_rate": 0.0007161399281449096,
      "loss": 0.8591,
      "step": 4529
    },
    {
      "epoch": 6.04,
      "grad_norm": 0.023986725136637688,
      "learning_rate": 0.0007157258837677514,
      "loss": 1.1295,
      "step": 4530
    },
    {
      "epoch": 6.041333333333333,
      "grad_norm": 0.0278155654668808,
      "learning_rate": 0.0007153118924022114,
      "loss": 0.9064,
      "step": 4531
    },
    {
      "epoch": 6.042666666666666,
      "grad_norm": 0.03199814260005951,
      "learning_rate": 0.0007148979541254907,
      "loss": 0.8632,
      "step": 4532
    },
    {
      "epoch": 6.044,
      "grad_norm": 0.022933583706617355,
      "learning_rate": 0.0007144840690147811,
      "loss": 1.2493,
      "step": 4533
    },
    {
      "epoch": 6.045333333333334,
      "grad_norm": 0.02168019488453865,
      "learning_rate": 0.0007140702371472639,
      "loss": 1.1,
      "step": 4534
    },
    {
      "epoch": 6.046666666666667,
      "grad_norm": 0.029484618455171585,
      "learning_rate": 0.0007136564586001113,
      "loss": 1.1764,
      "step": 4535
    },
    {
      "epoch": 6.048,
      "grad_norm": 0.03310372680425644,
      "learning_rate": 0.0007132427334504845,
      "loss": 1.0832,
      "step": 4536
    },
    {
      "epoch": 6.049333333333333,
      "grad_norm": 0.027395181357860565,
      "learning_rate": 0.0007128290617755352,
      "loss": 0.9461,
      "step": 4537
    },
    {
      "epoch": 6.050666666666666,
      "grad_norm": 0.029785694554448128,
      "learning_rate": 0.0007124154436524055,
      "loss": 1.1604,
      "step": 4538
    },
    {
      "epoch": 6.052,
      "grad_norm": 0.02640230767428875,
      "learning_rate": 0.0007120018791582266,
      "loss": 1.0516,
      "step": 4539
    },
    {
      "epoch": 6.053333333333334,
      "grad_norm": 0.027452947571873665,
      "learning_rate": 0.000711588368370121,
      "loss": 1.0641,
      "step": 4540
    },
    {
      "epoch": 6.054666666666667,
      "grad_norm": 0.03031753934919834,
      "learning_rate": 0.0007111749113652002,
      "loss": 0.7772,
      "step": 4541
    },
    {
      "epoch": 6.056,
      "grad_norm": 0.03972227871417999,
      "learning_rate": 0.0007107615082205654,
      "loss": 1.1345,
      "step": 4542
    },
    {
      "epoch": 6.057333333333333,
      "grad_norm": 0.029027385637164116,
      "learning_rate": 0.0007103481590133092,
      "loss": 1.3354,
      "step": 4543
    },
    {
      "epoch": 6.058666666666666,
      "grad_norm": 0.029255451634526253,
      "learning_rate": 0.000709934863820512,
      "loss": 1.1689,
      "step": 4544
    },
    {
      "epoch": 6.06,
      "grad_norm": 0.024601304903626442,
      "learning_rate": 0.0007095216227192467,
      "loss": 1.0092,
      "step": 4545
    },
    {
      "epoch": 6.061333333333334,
      "grad_norm": 0.02272988110780716,
      "learning_rate": 0.0007091084357865741,
      "loss": 1.0534,
      "step": 4546
    },
    {
      "epoch": 6.062666666666667,
      "grad_norm": 0.026821855455636978,
      "learning_rate": 0.0007086953030995454,
      "loss": 1.0402,
      "step": 4547
    },
    {
      "epoch": 6.064,
      "grad_norm": 0.02489006146788597,
      "learning_rate": 0.0007082822247352023,
      "loss": 0.9699,
      "step": 4548
    },
    {
      "epoch": 6.065333333333333,
      "grad_norm": 0.031121229752898216,
      "learning_rate": 0.0007078692007705752,
      "loss": 1.1498,
      "step": 4549
    },
    {
      "epoch": 6.066666666666666,
      "grad_norm": 0.028731530532240868,
      "learning_rate": 0.000707456231282686,
      "loss": 1.0789,
      "step": 4550
    },
    {
      "epoch": 6.068,
      "grad_norm": 0.03229635953903198,
      "learning_rate": 0.000707043316348545,
      "loss": 0.9448,
      "step": 4551
    },
    {
      "epoch": 6.069333333333334,
      "grad_norm": 0.039677929133176804,
      "learning_rate": 0.0007066304560451527,
      "loss": 1.0627,
      "step": 4552
    },
    {
      "epoch": 6.070666666666667,
      "grad_norm": 0.02918156050145626,
      "learning_rate": 0.0007062176504494999,
      "loss": 1.0448,
      "step": 4553
    },
    {
      "epoch": 6.072,
      "grad_norm": 0.025877071544528008,
      "learning_rate": 0.0007058048996385664,
      "loss": 1.1858,
      "step": 4554
    },
    {
      "epoch": 6.073333333333333,
      "grad_norm": 0.02036152221262455,
      "learning_rate": 0.0007053922036893229,
      "loss": 1.1089,
      "step": 4555
    },
    {
      "epoch": 6.074666666666666,
      "grad_norm": 0.030057251453399658,
      "learning_rate": 0.0007049795626787288,
      "loss": 1.0986,
      "step": 4556
    },
    {
      "epoch": 6.076,
      "grad_norm": 0.03461991623044014,
      "learning_rate": 0.0007045669766837332,
      "loss": 0.8221,
      "step": 4557
    },
    {
      "epoch": 6.077333333333334,
      "grad_norm": 0.03784902021288872,
      "learning_rate": 0.0007041544457812765,
      "loss": 0.851,
      "step": 4558
    },
    {
      "epoch": 6.078666666666667,
      "grad_norm": 0.030315110459923744,
      "learning_rate": 0.0007037419700482867,
      "loss": 0.8774,
      "step": 4559
    },
    {
      "epoch": 6.08,
      "grad_norm": 0.02584042027592659,
      "learning_rate": 0.0007033295495616834,
      "loss": 0.9331,
      "step": 4560
    },
    {
      "epoch": 6.081333333333333,
      "grad_norm": 0.030378365889191628,
      "learning_rate": 0.0007029171843983743,
      "loss": 1.0003,
      "step": 4561
    },
    {
      "epoch": 6.082666666666666,
      "grad_norm": 0.024293186143040657,
      "learning_rate": 0.0007025048746352576,
      "loss": 1.129,
      "step": 4562
    },
    {
      "epoch": 6.084,
      "grad_norm": 0.01997709460556507,
      "learning_rate": 0.0007020926203492217,
      "loss": 1.0733,
      "step": 4563
    },
    {
      "epoch": 6.085333333333334,
      "grad_norm": 0.02460682950913906,
      "learning_rate": 0.0007016804216171433,
      "loss": 1.1808,
      "step": 4564
    },
    {
      "epoch": 6.086666666666667,
      "grad_norm": 0.02998049557209015,
      "learning_rate": 0.0007012682785158902,
      "loss": 1.0659,
      "step": 4565
    },
    {
      "epoch": 6.088,
      "grad_norm": 0.023699626326560974,
      "learning_rate": 0.0007008561911223186,
      "loss": 1.0096,
      "step": 4566
    },
    {
      "epoch": 6.089333333333333,
      "grad_norm": 0.020656364038586617,
      "learning_rate": 0.0007004441595132745,
      "loss": 1.0443,
      "step": 4567
    },
    {
      "epoch": 6.0906666666666665,
      "grad_norm": 0.028156695887446404,
      "learning_rate": 0.0007000321837655947,
      "loss": 0.8282,
      "step": 4568
    },
    {
      "epoch": 6.092,
      "grad_norm": 0.03087194822728634,
      "learning_rate": 0.0006996202639561041,
      "loss": 0.9985,
      "step": 4569
    },
    {
      "epoch": 6.093333333333334,
      "grad_norm": 0.021310720592737198,
      "learning_rate": 0.0006992084001616181,
      "loss": 0.9237,
      "step": 4570
    },
    {
      "epoch": 6.094666666666667,
      "grad_norm": 0.027213795110583305,
      "learning_rate": 0.0006987965924589407,
      "loss": 0.8371,
      "step": 4571
    },
    {
      "epoch": 6.096,
      "grad_norm": 0.030295494943857193,
      "learning_rate": 0.0006983848409248671,
      "loss": 1.0568,
      "step": 4572
    },
    {
      "epoch": 6.097333333333333,
      "grad_norm": 0.027603115886449814,
      "learning_rate": 0.0006979731456361802,
      "loss": 0.9458,
      "step": 4573
    },
    {
      "epoch": 6.0986666666666665,
      "grad_norm": 0.028586234897375107,
      "learning_rate": 0.0006975615066696534,
      "loss": 0.8894,
      "step": 4574
    },
    {
      "epoch": 6.1,
      "grad_norm": 0.02598612569272518,
      "learning_rate": 0.0006971499241020494,
      "loss": 1.2314,
      "step": 4575
    },
    {
      "epoch": 6.101333333333334,
      "grad_norm": 0.025239061564207077,
      "learning_rate": 0.0006967383980101201,
      "loss": 1.1261,
      "step": 4576
    },
    {
      "epoch": 6.102666666666667,
      "grad_norm": 0.027129024267196655,
      "learning_rate": 0.0006963269284706077,
      "loss": 0.8702,
      "step": 4577
    },
    {
      "epoch": 6.104,
      "grad_norm": 0.02739086002111435,
      "learning_rate": 0.0006959155155602432,
      "loss": 1.12,
      "step": 4578
    },
    {
      "epoch": 6.105333333333333,
      "grad_norm": 0.03116764687001705,
      "learning_rate": 0.0006955041593557465,
      "loss": 0.9307,
      "step": 4579
    },
    {
      "epoch": 6.1066666666666665,
      "grad_norm": 0.037883780896663666,
      "learning_rate": 0.0006950928599338287,
      "loss": 1.0504,
      "step": 4580
    },
    {
      "epoch": 6.108,
      "grad_norm": 0.04096971079707146,
      "learning_rate": 0.0006946816173711878,
      "loss": 1.1153,
      "step": 4581
    },
    {
      "epoch": 6.109333333333334,
      "grad_norm": 0.022895285859704018,
      "learning_rate": 0.0006942704317445136,
      "loss": 1.1181,
      "step": 4582
    },
    {
      "epoch": 6.110666666666667,
      "grad_norm": 0.024081364274024963,
      "learning_rate": 0.000693859303130484,
      "loss": 0.7723,
      "step": 4583
    },
    {
      "epoch": 6.112,
      "grad_norm": 0.02541685290634632,
      "learning_rate": 0.0006934482316057662,
      "loss": 0.9208,
      "step": 4584
    },
    {
      "epoch": 6.113333333333333,
      "grad_norm": 0.024540171027183533,
      "learning_rate": 0.0006930372172470179,
      "loss": 0.9841,
      "step": 4585
    },
    {
      "epoch": 6.1146666666666665,
      "grad_norm": 0.024006785824894905,
      "learning_rate": 0.0006926262601308842,
      "loss": 1.0181,
      "step": 4586
    },
    {
      "epoch": 6.116,
      "grad_norm": 0.028361588716506958,
      "learning_rate": 0.0006922153603340016,
      "loss": 1.0888,
      "step": 4587
    },
    {
      "epoch": 6.117333333333334,
      "grad_norm": 0.022586874663829803,
      "learning_rate": 0.0006918045179329946,
      "loss": 0.9254,
      "step": 4588
    },
    {
      "epoch": 6.118666666666667,
      "grad_norm": 0.026023035869002342,
      "learning_rate": 0.0006913937330044769,
      "loss": 0.8845,
      "step": 4589
    },
    {
      "epoch": 6.12,
      "grad_norm": 0.02509591355919838,
      "learning_rate": 0.0006909830056250527,
      "loss": 0.9424,
      "step": 4590
    },
    {
      "epoch": 6.121333333333333,
      "grad_norm": 0.02783496491611004,
      "learning_rate": 0.000690572335871314,
      "loss": 0.9817,
      "step": 4591
    },
    {
      "epoch": 6.1226666666666665,
      "grad_norm": 0.03934907168149948,
      "learning_rate": 0.0006901617238198437,
      "loss": 1.005,
      "step": 4592
    },
    {
      "epoch": 6.124,
      "grad_norm": 0.022483963519334793,
      "learning_rate": 0.0006897511695472123,
      "loss": 0.82,
      "step": 4593
    },
    {
      "epoch": 6.125333333333334,
      "grad_norm": 0.024875368922948837,
      "learning_rate": 0.0006893406731299798,
      "loss": 0.8158,
      "step": 4594
    },
    {
      "epoch": 6.126666666666667,
      "grad_norm": 0.024306615814566612,
      "learning_rate": 0.0006889302346446968,
      "loss": 0.983,
      "step": 4595
    },
    {
      "epoch": 6.128,
      "grad_norm": 0.025772875174880028,
      "learning_rate": 0.0006885198541679015,
      "loss": 0.8259,
      "step": 4596
    },
    {
      "epoch": 6.129333333333333,
      "grad_norm": 0.02981325052678585,
      "learning_rate": 0.0006881095317761224,
      "loss": 0.8642,
      "step": 4597
    },
    {
      "epoch": 6.1306666666666665,
      "grad_norm": 0.033184390515089035,
      "learning_rate": 0.0006876992675458765,
      "loss": 0.975,
      "step": 4598
    },
    {
      "epoch": 6.132,
      "grad_norm": 0.02961868606507778,
      "learning_rate": 0.0006872890615536693,
      "loss": 0.9533,
      "step": 4599
    },
    {
      "epoch": 6.133333333333334,
      "grad_norm": 0.027865154668688774,
      "learning_rate": 0.0006868789138759977,
      "loss": 0.9699,
      "step": 4600
    },
    {
      "epoch": 6.134666666666667,
      "grad_norm": 0.03007224202156067,
      "learning_rate": 0.0006864688245893451,
      "loss": 1.0368,
      "step": 4601
    },
    {
      "epoch": 6.136,
      "grad_norm": 0.026161441579461098,
      "learning_rate": 0.0006860587937701862,
      "loss": 1.2647,
      "step": 4602
    },
    {
      "epoch": 6.137333333333333,
      "grad_norm": 0.022747138515114784,
      "learning_rate": 0.0006856488214949832,
      "loss": 1.0136,
      "step": 4603
    },
    {
      "epoch": 6.1386666666666665,
      "grad_norm": 0.02615075558423996,
      "learning_rate": 0.0006852389078401877,
      "loss": 1.3088,
      "step": 4604
    },
    {
      "epoch": 6.14,
      "grad_norm": 0.022373812273144722,
      "learning_rate": 0.0006848290528822416,
      "loss": 0.85,
      "step": 4605
    },
    {
      "epoch": 6.141333333333334,
      "grad_norm": 0.020240718498826027,
      "learning_rate": 0.0006844192566975739,
      "loss": 0.9826,
      "step": 4606
    },
    {
      "epoch": 6.142666666666667,
      "grad_norm": 0.0251484178006649,
      "learning_rate": 0.0006840095193626045,
      "loss": 0.846,
      "step": 4607
    },
    {
      "epoch": 6.144,
      "grad_norm": 0.02882256731390953,
      "learning_rate": 0.0006835998409537411,
      "loss": 1.0288,
      "step": 4608
    },
    {
      "epoch": 6.145333333333333,
      "grad_norm": 0.02947123348712921,
      "learning_rate": 0.0006831902215473808,
      "loss": 1.0321,
      "step": 4609
    },
    {
      "epoch": 6.1466666666666665,
      "grad_norm": 0.02594611421227455,
      "learning_rate": 0.0006827806612199097,
      "loss": 1.0711,
      "step": 4610
    },
    {
      "epoch": 6.148,
      "grad_norm": 0.027513578534126282,
      "learning_rate": 0.0006823711600477025,
      "loss": 1.0056,
      "step": 4611
    },
    {
      "epoch": 6.149333333333334,
      "grad_norm": 0.038722917437553406,
      "learning_rate": 0.000681961718107124,
      "loss": 1.1982,
      "step": 4612
    },
    {
      "epoch": 6.150666666666667,
      "grad_norm": 0.035038694739341736,
      "learning_rate": 0.0006815523354745267,
      "loss": 0.9771,
      "step": 4613
    },
    {
      "epoch": 6.152,
      "grad_norm": 0.02413834258913994,
      "learning_rate": 0.0006811430122262528,
      "loss": 0.9279,
      "step": 4614
    },
    {
      "epoch": 6.153333333333333,
      "grad_norm": 0.028847530484199524,
      "learning_rate": 0.0006807337484386331,
      "loss": 1.3633,
      "step": 4615
    },
    {
      "epoch": 6.1546666666666665,
      "grad_norm": 0.024738652631640434,
      "learning_rate": 0.0006803245441879866,
      "loss": 0.7854,
      "step": 4616
    },
    {
      "epoch": 6.156,
      "grad_norm": 0.026228388771414757,
      "learning_rate": 0.0006799153995506233,
      "loss": 1.1719,
      "step": 4617
    },
    {
      "epoch": 6.157333333333334,
      "grad_norm": 0.02885754592716694,
      "learning_rate": 0.0006795063146028397,
      "loss": 1.0636,
      "step": 4618
    },
    {
      "epoch": 6.158666666666667,
      "grad_norm": 0.026177354156970978,
      "learning_rate": 0.0006790972894209229,
      "loss": 1.1449,
      "step": 4619
    },
    {
      "epoch": 6.16,
      "grad_norm": 0.02718256041407585,
      "learning_rate": 0.0006786883240811479,
      "loss": 0.8176,
      "step": 4620
    },
    {
      "epoch": 6.161333333333333,
      "grad_norm": 0.022467726841568947,
      "learning_rate": 0.0006782794186597785,
      "loss": 0.8949,
      "step": 4621
    },
    {
      "epoch": 6.1626666666666665,
      "grad_norm": 0.026255685836076736,
      "learning_rate": 0.0006778705732330681,
      "loss": 1.167,
      "step": 4622
    },
    {
      "epoch": 6.164,
      "grad_norm": 0.027204474434256554,
      "learning_rate": 0.0006774617878772582,
      "loss": 0.9507,
      "step": 4623
    },
    {
      "epoch": 6.165333333333333,
      "grad_norm": 0.024410473182797432,
      "learning_rate": 0.0006770530626685798,
      "loss": 0.9864,
      "step": 4624
    },
    {
      "epoch": 6.166666666666667,
      "grad_norm": 0.03119012713432312,
      "learning_rate": 0.0006766443976832517,
      "loss": 0.961,
      "step": 4625
    },
    {
      "epoch": 6.168,
      "grad_norm": 0.020132917910814285,
      "learning_rate": 0.000676235792997482,
      "loss": 1.2601,
      "step": 4626
    },
    {
      "epoch": 6.169333333333333,
      "grad_norm": 0.025055386126041412,
      "learning_rate": 0.0006758272486874679,
      "loss": 1.0169,
      "step": 4627
    },
    {
      "epoch": 6.1706666666666665,
      "grad_norm": 0.026371318846940994,
      "learning_rate": 0.0006754187648293949,
      "loss": 0.9227,
      "step": 4628
    },
    {
      "epoch": 6.172,
      "grad_norm": 0.025944208726286888,
      "learning_rate": 0.0006750103414994374,
      "loss": 1.2196,
      "step": 4629
    },
    {
      "epoch": 6.173333333333334,
      "grad_norm": 0.02580827660858631,
      "learning_rate": 0.0006746019787737583,
      "loss": 0.9424,
      "step": 4630
    },
    {
      "epoch": 6.174666666666667,
      "grad_norm": 0.02039346657693386,
      "learning_rate": 0.0006741936767285091,
      "loss": 0.9447,
      "step": 4631
    },
    {
      "epoch": 6.176,
      "grad_norm": 0.024174442514777184,
      "learning_rate": 0.0006737854354398308,
      "loss": 0.8828,
      "step": 4632
    },
    {
      "epoch": 6.177333333333333,
      "grad_norm": 0.03077436424791813,
      "learning_rate": 0.0006733772549838518,
      "loss": 0.9682,
      "step": 4633
    },
    {
      "epoch": 6.1786666666666665,
      "grad_norm": 0.027555478736758232,
      "learning_rate": 0.0006729691354366907,
      "loss": 1.0886,
      "step": 4634
    },
    {
      "epoch": 6.18,
      "grad_norm": 0.02869759127497673,
      "learning_rate": 0.0006725610768744534,
      "loss": 0.8118,
      "step": 4635
    },
    {
      "epoch": 6.181333333333333,
      "grad_norm": 0.021675067022442818,
      "learning_rate": 0.0006721530793732349,
      "loss": 0.8295,
      "step": 4636
    },
    {
      "epoch": 6.182666666666667,
      "grad_norm": 0.02314707823097706,
      "learning_rate": 0.0006717451430091189,
      "loss": 0.7648,
      "step": 4637
    },
    {
      "epoch": 6.184,
      "grad_norm": 0.02779506705701351,
      "learning_rate": 0.0006713372678581773,
      "loss": 1.0557,
      "step": 4638
    },
    {
      "epoch": 6.185333333333333,
      "grad_norm": 0.01959654688835144,
      "learning_rate": 0.0006709294539964716,
      "loss": 0.7268,
      "step": 4639
    },
    {
      "epoch": 6.1866666666666665,
      "grad_norm": 0.03856324404478073,
      "learning_rate": 0.000670521701500051,
      "loss": 0.9983,
      "step": 4640
    },
    {
      "epoch": 6.188,
      "grad_norm": 0.025100283324718475,
      "learning_rate": 0.000670114010444953,
      "loss": 1.026,
      "step": 4641
    },
    {
      "epoch": 6.189333333333333,
      "grad_norm": 0.02655240334570408,
      "learning_rate": 0.0006697063809072046,
      "loss": 1.2622,
      "step": 4642
    },
    {
      "epoch": 6.190666666666667,
      "grad_norm": 0.031238514930009842,
      "learning_rate": 0.0006692988129628201,
      "loss": 0.9658,
      "step": 4643
    },
    {
      "epoch": 6.192,
      "grad_norm": 0.02970975637435913,
      "learning_rate": 0.000668891306687804,
      "loss": 1.0047,
      "step": 4644
    },
    {
      "epoch": 6.193333333333333,
      "grad_norm": 0.03169682249426842,
      "learning_rate": 0.0006684838621581477,
      "loss": 1.0364,
      "step": 4645
    },
    {
      "epoch": 6.1946666666666665,
      "grad_norm": 0.03929292783141136,
      "learning_rate": 0.0006680764794498317,
      "loss": 0.8228,
      "step": 4646
    },
    {
      "epoch": 6.196,
      "grad_norm": 0.026295969262719154,
      "learning_rate": 0.0006676691586388255,
      "loss": 0.8215,
      "step": 4647
    },
    {
      "epoch": 6.197333333333333,
      "grad_norm": 0.022371385246515274,
      "learning_rate": 0.0006672618998010858,
      "loss": 0.9922,
      "step": 4648
    },
    {
      "epoch": 6.198666666666667,
      "grad_norm": 0.028540687635540962,
      "learning_rate": 0.0006668547030125591,
      "loss": 0.9061,
      "step": 4649
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.02122650109231472,
      "learning_rate": 0.0006664475683491796,
      "loss": 0.8207,
      "step": 4650
    },
    {
      "epoch": 6.201333333333333,
      "grad_norm": 0.03603840246796608,
      "learning_rate": 0.0006660404958868698,
      "loss": 0.9792,
      "step": 4651
    },
    {
      "epoch": 6.2026666666666666,
      "grad_norm": 0.030547644942998886,
      "learning_rate": 0.0006656334857015409,
      "loss": 0.9301,
      "step": 4652
    },
    {
      "epoch": 6.204,
      "grad_norm": 0.02786499261856079,
      "learning_rate": 0.0006652265378690922,
      "loss": 1.0155,
      "step": 4653
    },
    {
      "epoch": 6.205333333333333,
      "grad_norm": 0.02551327273249626,
      "learning_rate": 0.0006648196524654124,
      "loss": 0.8307,
      "step": 4654
    },
    {
      "epoch": 6.206666666666667,
      "grad_norm": 0.024184858426451683,
      "learning_rate": 0.0006644128295663771,
      "loss": 0.9851,
      "step": 4655
    },
    {
      "epoch": 6.208,
      "grad_norm": 0.023349694907665253,
      "learning_rate": 0.0006640060692478509,
      "loss": 1.2853,
      "step": 4656
    },
    {
      "epoch": 6.209333333333333,
      "grad_norm": 0.02249247394502163,
      "learning_rate": 0.000663599371585687,
      "loss": 0.8718,
      "step": 4657
    },
    {
      "epoch": 6.210666666666667,
      "grad_norm": 0.03147151693701744,
      "learning_rate": 0.000663192736655726,
      "loss": 0.958,
      "step": 4658
    },
    {
      "epoch": 6.212,
      "grad_norm": 0.031686536967754364,
      "learning_rate": 0.0006627861645337983,
      "loss": 1.0604,
      "step": 4659
    },
    {
      "epoch": 6.213333333333333,
      "grad_norm": 0.02390431985259056,
      "learning_rate": 0.0006623796552957211,
      "loss": 0.8842,
      "step": 4660
    },
    {
      "epoch": 6.214666666666667,
      "grad_norm": 0.028781931847333908,
      "learning_rate": 0.0006619732090173009,
      "loss": 0.9284,
      "step": 4661
    },
    {
      "epoch": 6.216,
      "grad_norm": 0.03541569039225578,
      "learning_rate": 0.0006615668257743321,
      "loss": 1.5628,
      "step": 4662
    },
    {
      "epoch": 6.217333333333333,
      "grad_norm": 0.029054608196020126,
      "learning_rate": 0.0006611605056425968,
      "loss": 1.061,
      "step": 4663
    },
    {
      "epoch": 6.218666666666667,
      "grad_norm": 0.022220270708203316,
      "learning_rate": 0.0006607542486978666,
      "loss": 1.0773,
      "step": 4664
    },
    {
      "epoch": 6.22,
      "grad_norm": 0.03059842437505722,
      "learning_rate": 0.0006603480550158994,
      "loss": 1.2967,
      "step": 4665
    },
    {
      "epoch": 6.221333333333333,
      "grad_norm": 0.027384528890252113,
      "learning_rate": 0.0006599419246724437,
      "loss": 1.2146,
      "step": 4666
    },
    {
      "epoch": 6.222666666666667,
      "grad_norm": 0.02509709633886814,
      "learning_rate": 0.0006595358577432345,
      "loss": 1.0266,
      "step": 4667
    },
    {
      "epoch": 6.224,
      "grad_norm": 0.024959715083241463,
      "learning_rate": 0.0006591298543039949,
      "loss": 0.9169,
      "step": 4668
    },
    {
      "epoch": 6.225333333333333,
      "grad_norm": 0.026954468339681625,
      "learning_rate": 0.0006587239144304374,
      "loss": 1.128,
      "step": 4669
    },
    {
      "epoch": 6.226666666666667,
      "grad_norm": 0.034793075174093246,
      "learning_rate": 0.0006583180381982611,
      "loss": 1.0368,
      "step": 4670
    },
    {
      "epoch": 6.228,
      "grad_norm": 0.02276436612010002,
      "learning_rate": 0.0006579122256831551,
      "loss": 1.1586,
      "step": 4671
    },
    {
      "epoch": 6.229333333333333,
      "grad_norm": 0.03222095966339111,
      "learning_rate": 0.0006575064769607952,
      "loss": 1.1761,
      "step": 4672
    },
    {
      "epoch": 6.230666666666667,
      "grad_norm": 0.028720391914248466,
      "learning_rate": 0.0006571007921068452,
      "loss": 1.1502,
      "step": 4673
    },
    {
      "epoch": 6.232,
      "grad_norm": 0.022101372480392456,
      "learning_rate": 0.000656695171196958,
      "loss": 0.9894,
      "step": 4674
    },
    {
      "epoch": 6.233333333333333,
      "grad_norm": 0.02786470204591751,
      "learning_rate": 0.0006562896143067733,
      "loss": 1.049,
      "step": 4675
    },
    {
      "epoch": 6.234666666666667,
      "grad_norm": 0.031328119337558746,
      "learning_rate": 0.0006558841215119209,
      "loss": 1.0227,
      "step": 4676
    },
    {
      "epoch": 6.236,
      "grad_norm": 0.02893858775496483,
      "learning_rate": 0.0006554786928880164,
      "loss": 0.8467,
      "step": 4677
    },
    {
      "epoch": 6.237333333333333,
      "grad_norm": 0.0256658885627985,
      "learning_rate": 0.0006550733285106645,
      "loss": 1.0098,
      "step": 4678
    },
    {
      "epoch": 6.238666666666667,
      "grad_norm": 0.03075157105922699,
      "learning_rate": 0.000654668028455458,
      "loss": 0.8194,
      "step": 4679
    },
    {
      "epoch": 6.24,
      "grad_norm": 0.023423336446285248,
      "learning_rate": 0.0006542627927979772,
      "loss": 1.234,
      "step": 4680
    },
    {
      "epoch": 6.241333333333333,
      "grad_norm": 0.020967600867152214,
      "learning_rate": 0.0006538576216137912,
      "loss": 0.9987,
      "step": 4681
    },
    {
      "epoch": 6.242666666666667,
      "grad_norm": 0.026260143145918846,
      "learning_rate": 0.0006534525149784563,
      "loss": 1.0639,
      "step": 4682
    },
    {
      "epoch": 6.244,
      "grad_norm": 0.09782671183347702,
      "learning_rate": 0.0006530474729675166,
      "loss": 0.9135,
      "step": 4683
    },
    {
      "epoch": 6.245333333333333,
      "grad_norm": 0.02478168159723282,
      "learning_rate": 0.0006526424956565057,
      "loss": 1.176,
      "step": 4684
    },
    {
      "epoch": 6.246666666666667,
      "grad_norm": 0.025725461542606354,
      "learning_rate": 0.000652237583120943,
      "loss": 0.9812,
      "step": 4685
    },
    {
      "epoch": 6.248,
      "grad_norm": 0.022997014224529266,
      "learning_rate": 0.0006518327354363373,
      "loss": 1.1109,
      "step": 4686
    },
    {
      "epoch": 6.249333333333333,
      "grad_norm": 0.02273198589682579,
      "learning_rate": 0.000651427952678185,
      "loss": 0.9003,
      "step": 4687
    },
    {
      "epoch": 6.250666666666667,
      "grad_norm": 0.07442786544561386,
      "learning_rate": 0.0006510232349219696,
      "loss": 1.5219,
      "step": 4688
    },
    {
      "epoch": 6.252,
      "grad_norm": 0.022817976772785187,
      "learning_rate": 0.0006506185822431639,
      "loss": 0.997,
      "step": 4689
    },
    {
      "epoch": 6.253333333333333,
      "grad_norm": 0.03723238408565521,
      "learning_rate": 0.0006502139947172272,
      "loss": 0.8008,
      "step": 4690
    },
    {
      "epoch": 6.254666666666667,
      "grad_norm": 0.021944433450698853,
      "learning_rate": 0.0006498094724196076,
      "loss": 0.9508,
      "step": 4691
    },
    {
      "epoch": 6.256,
      "grad_norm": 0.02501281537115574,
      "learning_rate": 0.0006494050154257407,
      "loss": 1.0523,
      "step": 4692
    },
    {
      "epoch": 6.257333333333333,
      "grad_norm": 0.02673529088497162,
      "learning_rate": 0.0006490006238110492,
      "loss": 1.074,
      "step": 4693
    },
    {
      "epoch": 6.258666666666667,
      "grad_norm": 0.03045414574444294,
      "learning_rate": 0.0006485962976509454,
      "loss": 1.1615,
      "step": 4694
    },
    {
      "epoch": 6.26,
      "grad_norm": 0.03390955552458763,
      "learning_rate": 0.0006481920370208274,
      "loss": 1.0287,
      "step": 4695
    },
    {
      "epoch": 6.261333333333333,
      "grad_norm": 0.029670558869838715,
      "learning_rate": 0.0006477878419960827,
      "loss": 1.0248,
      "step": 4696
    },
    {
      "epoch": 6.262666666666667,
      "grad_norm": 0.025469865649938583,
      "learning_rate": 0.0006473837126520854,
      "loss": 0.8255,
      "step": 4697
    },
    {
      "epoch": 6.264,
      "grad_norm": 0.031139671802520752,
      "learning_rate": 0.0006469796490641974,
      "loss": 0.9668,
      "step": 4698
    },
    {
      "epoch": 6.265333333333333,
      "grad_norm": 0.025301456451416016,
      "learning_rate": 0.0006465756513077696,
      "loss": 0.9768,
      "step": 4699
    },
    {
      "epoch": 6.266666666666667,
      "grad_norm": 0.032677967101335526,
      "learning_rate": 0.0006461717194581394,
      "loss": 0.9015,
      "step": 4700
    },
    {
      "epoch": 6.268,
      "grad_norm": 0.03582823649048805,
      "learning_rate": 0.0006457678535906322,
      "loss": 0.9724,
      "step": 4701
    },
    {
      "epoch": 6.269333333333333,
      "grad_norm": 0.177873894572258,
      "learning_rate": 0.0006453640537805613,
      "loss": 1.2449,
      "step": 4702
    },
    {
      "epoch": 6.270666666666667,
      "grad_norm": 0.03838871791958809,
      "learning_rate": 0.000644960320103227,
      "loss": 1.1971,
      "step": 4703
    },
    {
      "epoch": 6.272,
      "grad_norm": 0.026306044310331345,
      "learning_rate": 0.0006445566526339187,
      "loss": 1.0676,
      "step": 4704
    },
    {
      "epoch": 6.273333333333333,
      "grad_norm": 0.030559582635760307,
      "learning_rate": 0.000644153051447912,
      "loss": 0.8573,
      "step": 4705
    },
    {
      "epoch": 6.274666666666667,
      "grad_norm": 0.043838515877723694,
      "learning_rate": 0.0006437495166204709,
      "loss": 0.7301,
      "step": 4706
    },
    {
      "epoch": 6.276,
      "grad_norm": 0.02413257397711277,
      "learning_rate": 0.0006433460482268464,
      "loss": 1.1313,
      "step": 4707
    },
    {
      "epoch": 6.277333333333333,
      "grad_norm": 0.029748059809207916,
      "learning_rate": 0.0006429426463422783,
      "loss": 1.1096,
      "step": 4708
    },
    {
      "epoch": 6.278666666666667,
      "grad_norm": 0.022344151511788368,
      "learning_rate": 0.000642539311041993,
      "loss": 0.7122,
      "step": 4709
    },
    {
      "epoch": 6.28,
      "grad_norm": 0.027014922350645065,
      "learning_rate": 0.0006421360424012039,
      "loss": 1.1545,
      "step": 4710
    },
    {
      "epoch": 6.281333333333333,
      "grad_norm": 0.035474199801683426,
      "learning_rate": 0.000641732840495114,
      "loss": 0.8494,
      "step": 4711
    },
    {
      "epoch": 6.282666666666667,
      "grad_norm": 0.02078915759921074,
      "learning_rate": 0.000641329705398912,
      "loss": 0.971,
      "step": 4712
    },
    {
      "epoch": 6.284,
      "grad_norm": 0.02524752914905548,
      "learning_rate": 0.0006409266371877751,
      "loss": 1.005,
      "step": 4713
    },
    {
      "epoch": 6.285333333333333,
      "grad_norm": 0.03234944865107536,
      "learning_rate": 0.0006405236359368673,
      "loss": 1.0066,
      "step": 4714
    },
    {
      "epoch": 6.286666666666667,
      "grad_norm": 0.06190681830048561,
      "learning_rate": 0.0006401207017213406,
      "loss": 1.0479,
      "step": 4715
    },
    {
      "epoch": 6.288,
      "grad_norm": 0.022082852199673653,
      "learning_rate": 0.0006397178346163349,
      "loss": 1.2426,
      "step": 4716
    },
    {
      "epoch": 6.289333333333333,
      "grad_norm": 0.02295425906777382,
      "learning_rate": 0.0006393150346969765,
      "loss": 0.8213,
      "step": 4717
    },
    {
      "epoch": 6.290666666666667,
      "grad_norm": 0.031325776129961014,
      "learning_rate": 0.0006389123020383804,
      "loss": 1.3828,
      "step": 4718
    },
    {
      "epoch": 6.292,
      "grad_norm": 0.025049135088920593,
      "learning_rate": 0.000638509636715648,
      "loss": 0.9787,
      "step": 4719
    },
    {
      "epoch": 6.293333333333333,
      "grad_norm": 0.020947784185409546,
      "learning_rate": 0.0006381070388038684,
      "loss": 0.8786,
      "step": 4720
    },
    {
      "epoch": 6.294666666666667,
      "grad_norm": 0.033959269523620605,
      "learning_rate": 0.000637704508378119,
      "loss": 0.9412,
      "step": 4721
    },
    {
      "epoch": 6.296,
      "grad_norm": 0.029786396771669388,
      "learning_rate": 0.0006373020455134633,
      "loss": 1.1838,
      "step": 4722
    },
    {
      "epoch": 6.2973333333333334,
      "grad_norm": 0.030513158068060875,
      "learning_rate": 0.0006368996502849533,
      "loss": 0.9274,
      "step": 4723
    },
    {
      "epoch": 6.298666666666667,
      "grad_norm": 0.02401970699429512,
      "learning_rate": 0.0006364973227676278,
      "loss": 0.9685,
      "step": 4724
    },
    {
      "epoch": 6.3,
      "grad_norm": 0.028138190507888794,
      "learning_rate": 0.0006360950630365126,
      "loss": 1.0863,
      "step": 4725
    },
    {
      "epoch": 6.301333333333333,
      "grad_norm": 0.026195047423243523,
      "learning_rate": 0.0006356928711666219,
      "loss": 0.9991,
      "step": 4726
    },
    {
      "epoch": 6.302666666666667,
      "grad_norm": 0.02640477381646633,
      "learning_rate": 0.0006352907472329566,
      "loss": 1.0149,
      "step": 4727
    },
    {
      "epoch": 6.304,
      "grad_norm": 0.02404482290148735,
      "learning_rate": 0.0006348886913105049,
      "loss": 0.9841,
      "step": 4728
    },
    {
      "epoch": 6.3053333333333335,
      "grad_norm": 0.02247021347284317,
      "learning_rate": 0.0006344867034742428,
      "loss": 1.0783,
      "step": 4729
    },
    {
      "epoch": 6.306666666666667,
      "grad_norm": 0.022634273394942284,
      "learning_rate": 0.0006340847837991324,
      "loss": 0.8431,
      "step": 4730
    },
    {
      "epoch": 6.308,
      "grad_norm": 0.025362279266119003,
      "learning_rate": 0.000633682932360125,
      "loss": 1.376,
      "step": 4731
    },
    {
      "epoch": 6.309333333333333,
      "grad_norm": 0.031221190467476845,
      "learning_rate": 0.0006332811492321573,
      "loss": 0.9276,
      "step": 4732
    },
    {
      "epoch": 6.310666666666666,
      "grad_norm": 0.02979396842420101,
      "learning_rate": 0.0006328794344901546,
      "loss": 1.1981,
      "step": 4733
    },
    {
      "epoch": 6.312,
      "grad_norm": 0.024144306778907776,
      "learning_rate": 0.0006324777882090286,
      "loss": 1.1054,
      "step": 4734
    },
    {
      "epoch": 6.3133333333333335,
      "grad_norm": 0.024549998342990875,
      "learning_rate": 0.0006320762104636786,
      "loss": 1.1206,
      "step": 4735
    },
    {
      "epoch": 6.314666666666667,
      "grad_norm": 0.034769970923662186,
      "learning_rate": 0.0006316747013289912,
      "loss": 1.1752,
      "step": 4736
    },
    {
      "epoch": 6.316,
      "grad_norm": 0.028768207877874374,
      "learning_rate": 0.0006312732608798398,
      "loss": 0.7635,
      "step": 4737
    },
    {
      "epoch": 6.317333333333333,
      "grad_norm": 0.020397353917360306,
      "learning_rate": 0.0006308718891910858,
      "loss": 0.7985,
      "step": 4738
    },
    {
      "epoch": 6.318666666666667,
      "grad_norm": 0.048762477934360504,
      "learning_rate": 0.0006304705863375769,
      "loss": 0.9366,
      "step": 4739
    },
    {
      "epoch": 6.32,
      "grad_norm": 0.020148422569036484,
      "learning_rate": 0.0006300693523941481,
      "loss": 0.9764,
      "step": 4740
    },
    {
      "epoch": 6.3213333333333335,
      "grad_norm": 0.04535926878452301,
      "learning_rate": 0.0006296681874356223,
      "loss": 1.3307,
      "step": 4741
    },
    {
      "epoch": 6.322666666666667,
      "grad_norm": 0.03151058405637741,
      "learning_rate": 0.0006292670915368082,
      "loss": 1.0528,
      "step": 4742
    },
    {
      "epoch": 6.324,
      "grad_norm": 0.030131516978144646,
      "learning_rate": 0.0006288660647725034,
      "loss": 0.9786,
      "step": 4743
    },
    {
      "epoch": 6.325333333333333,
      "grad_norm": 0.029625514522194862,
      "learning_rate": 0.0006284651072174912,
      "loss": 1.1037,
      "step": 4744
    },
    {
      "epoch": 6.326666666666666,
      "grad_norm": 0.021014587953686714,
      "learning_rate": 0.000628064218946542,
      "loss": 0.907,
      "step": 4745
    },
    {
      "epoch": 6.328,
      "grad_norm": 0.02513820119202137,
      "learning_rate": 0.0006276634000344143,
      "loss": 1.1677,
      "step": 4746
    },
    {
      "epoch": 6.3293333333333335,
      "grad_norm": 0.024853359907865524,
      "learning_rate": 0.0006272626505558525,
      "loss": 0.8644,
      "step": 4747
    },
    {
      "epoch": 6.330666666666667,
      "grad_norm": 0.02011973410844803,
      "learning_rate": 0.0006268619705855894,
      "loss": 0.897,
      "step": 4748
    },
    {
      "epoch": 6.332,
      "grad_norm": 0.02157481759786606,
      "learning_rate": 0.0006264613601983434,
      "loss": 1.0648,
      "step": 4749
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 0.02143382653594017,
      "learning_rate": 0.0006260608194688206,
      "loss": 0.9665,
      "step": 4750
    },
    {
      "epoch": 6.334666666666667,
      "grad_norm": 0.026917843148112297,
      "learning_rate": 0.0006256603484717147,
      "loss": 1.0797,
      "step": 4751
    },
    {
      "epoch": 6.336,
      "grad_norm": 0.023001575842499733,
      "learning_rate": 0.0006252599472817049,
      "loss": 1.1567,
      "step": 4752
    },
    {
      "epoch": 6.3373333333333335,
      "grad_norm": 0.025615312159061432,
      "learning_rate": 0.0006248596159734589,
      "loss": 1.3611,
      "step": 4753
    },
    {
      "epoch": 6.338666666666667,
      "grad_norm": 0.025158992037177086,
      "learning_rate": 0.0006244593546216306,
      "loss": 1.023,
      "step": 4754
    },
    {
      "epoch": 6.34,
      "grad_norm": 0.04206708073616028,
      "learning_rate": 0.000624059163300861,
      "loss": 1.2341,
      "step": 4755
    },
    {
      "epoch": 6.341333333333333,
      "grad_norm": 0.02456280216574669,
      "learning_rate": 0.0006236590420857782,
      "loss": 0.8865,
      "step": 4756
    },
    {
      "epoch": 6.342666666666666,
      "grad_norm": 0.023041244596242905,
      "learning_rate": 0.0006232589910509963,
      "loss": 1.2445,
      "step": 4757
    },
    {
      "epoch": 6.344,
      "grad_norm": 0.02225615084171295,
      "learning_rate": 0.0006228590102711181,
      "loss": 1.0754,
      "step": 4758
    },
    {
      "epoch": 6.3453333333333335,
      "grad_norm": 0.031621746718883514,
      "learning_rate": 0.0006224590998207311,
      "loss": 0.9419,
      "step": 4759
    },
    {
      "epoch": 6.346666666666667,
      "grad_norm": 0.036070793867111206,
      "learning_rate": 0.0006220592597744123,
      "loss": 1.338,
      "step": 4760
    },
    {
      "epoch": 6.348,
      "grad_norm": 0.031788088381290436,
      "learning_rate": 0.0006216594902067232,
      "loss": 1.0701,
      "step": 4761
    },
    {
      "epoch": 6.349333333333333,
      "grad_norm": 0.08205348253250122,
      "learning_rate": 0.0006212597911922131,
      "loss": 0.9393,
      "step": 4762
    },
    {
      "epoch": 6.350666666666667,
      "grad_norm": 0.02428096905350685,
      "learning_rate": 0.0006208601628054184,
      "loss": 1.147,
      "step": 4763
    },
    {
      "epoch": 6.352,
      "grad_norm": 0.027700213715434074,
      "learning_rate": 0.0006204606051208617,
      "loss": 0.9747,
      "step": 4764
    },
    {
      "epoch": 6.3533333333333335,
      "grad_norm": 0.025097407400608063,
      "learning_rate": 0.0006200611182130534,
      "loss": 0.8834,
      "step": 4765
    },
    {
      "epoch": 6.354666666666667,
      "grad_norm": 0.04572015628218651,
      "learning_rate": 0.0006196617021564895,
      "loss": 0.9265,
      "step": 4766
    },
    {
      "epoch": 6.356,
      "grad_norm": 0.027423322200775146,
      "learning_rate": 0.0006192623570256535,
      "loss": 1.1321,
      "step": 4767
    },
    {
      "epoch": 6.357333333333333,
      "grad_norm": 0.01988518424332142,
      "learning_rate": 0.0006188630828950157,
      "loss": 1.1651,
      "step": 4768
    },
    {
      "epoch": 6.358666666666666,
      "grad_norm": 0.03180782496929169,
      "learning_rate": 0.0006184638798390326,
      "loss": 1.1108,
      "step": 4769
    },
    {
      "epoch": 6.36,
      "grad_norm": 0.026836063712835312,
      "learning_rate": 0.0006180647479321484,
      "loss": 1.1414,
      "step": 4770
    },
    {
      "epoch": 6.3613333333333335,
      "grad_norm": 0.027670452371239662,
      "learning_rate": 0.0006176656872487931,
      "loss": 0.9736,
      "step": 4771
    },
    {
      "epoch": 6.362666666666667,
      "grad_norm": 0.023496434092521667,
      "learning_rate": 0.0006172666978633838,
      "loss": 0.9838,
      "step": 4772
    },
    {
      "epoch": 6.364,
      "grad_norm": 0.02481374330818653,
      "learning_rate": 0.0006168677798503246,
      "loss": 1.2385,
      "step": 4773
    },
    {
      "epoch": 6.365333333333333,
      "grad_norm": 0.029002727940678596,
      "learning_rate": 0.0006164689332840052,
      "loss": 1.2089,
      "step": 4774
    },
    {
      "epoch": 6.366666666666666,
      "grad_norm": 0.031876422464847565,
      "learning_rate": 0.0006160701582388038,
      "loss": 0.9289,
      "step": 4775
    },
    {
      "epoch": 6.368,
      "grad_norm": 0.022237496450543404,
      "learning_rate": 0.0006156714547890838,
      "loss": 0.7089,
      "step": 4776
    },
    {
      "epoch": 6.3693333333333335,
      "grad_norm": 0.032016173005104065,
      "learning_rate": 0.0006152728230091953,
      "loss": 0.7908,
      "step": 4777
    },
    {
      "epoch": 6.370666666666667,
      "grad_norm": 0.027500975877046585,
      "learning_rate": 0.0006148742629734761,
      "loss": 1.2027,
      "step": 4778
    },
    {
      "epoch": 6.372,
      "grad_norm": 0.02667086012661457,
      "learning_rate": 0.0006144757747562489,
      "loss": 1.0005,
      "step": 4779
    },
    {
      "epoch": 6.373333333333333,
      "grad_norm": 0.025861939415335655,
      "learning_rate": 0.0006140773584318253,
      "loss": 1.0434,
      "step": 4780
    },
    {
      "epoch": 6.374666666666666,
      "grad_norm": 0.029900597408413887,
      "learning_rate": 0.0006136790140745014,
      "loss": 1.2477,
      "step": 4781
    },
    {
      "epoch": 6.376,
      "grad_norm": 0.020601889118552208,
      "learning_rate": 0.0006132807417585609,
      "loss": 1.1477,
      "step": 4782
    },
    {
      "epoch": 6.3773333333333335,
      "grad_norm": 0.0353422574698925,
      "learning_rate": 0.0006128825415582742,
      "loss": 0.9024,
      "step": 4783
    },
    {
      "epoch": 6.378666666666667,
      "grad_norm": 0.029940899461507797,
      "learning_rate": 0.000612484413547897,
      "loss": 0.7792,
      "step": 4784
    },
    {
      "epoch": 6.38,
      "grad_norm": 0.02390015870332718,
      "learning_rate": 0.0006120863578016736,
      "loss": 1.0826,
      "step": 4785
    },
    {
      "epoch": 6.381333333333333,
      "grad_norm": 0.02668646350502968,
      "learning_rate": 0.0006116883743938331,
      "loss": 1.1263,
      "step": 4786
    },
    {
      "epoch": 6.382666666666666,
      "grad_norm": 0.022878514602780342,
      "learning_rate": 0.0006112904633985914,
      "loss": 1.0449,
      "step": 4787
    },
    {
      "epoch": 6.384,
      "grad_norm": 0.031048206612467766,
      "learning_rate": 0.000610892624890152,
      "loss": 1.1122,
      "step": 4788
    },
    {
      "epoch": 6.3853333333333335,
      "grad_norm": 0.02526530623435974,
      "learning_rate": 0.0006104948589427035,
      "loss": 0.8717,
      "step": 4789
    },
    {
      "epoch": 6.386666666666667,
      "grad_norm": 0.028289668262004852,
      "learning_rate": 0.0006100971656304219,
      "loss": 1.2059,
      "step": 4790
    },
    {
      "epoch": 6.388,
      "grad_norm": 0.025606868788599968,
      "learning_rate": 0.0006096995450274691,
      "loss": 1.0361,
      "step": 4791
    },
    {
      "epoch": 6.389333333333333,
      "grad_norm": 0.022376537322998047,
      "learning_rate": 0.0006093019972079933,
      "loss": 0.9303,
      "step": 4792
    },
    {
      "epoch": 6.390666666666666,
      "grad_norm": 0.02401833049952984,
      "learning_rate": 0.0006089045222461301,
      "loss": 1.0065,
      "step": 4793
    },
    {
      "epoch": 6.392,
      "grad_norm": 0.030828651040792465,
      "learning_rate": 0.0006085071202160005,
      "loss": 1.2372,
      "step": 4794
    },
    {
      "epoch": 6.3933333333333335,
      "grad_norm": 0.027425047010183334,
      "learning_rate": 0.0006081097911917125,
      "loss": 1.0057,
      "step": 4795
    },
    {
      "epoch": 6.394666666666667,
      "grad_norm": 0.035571567714214325,
      "learning_rate": 0.0006077125352473598,
      "loss": 1.1276,
      "step": 4796
    },
    {
      "epoch": 6.396,
      "grad_norm": 0.033307842910289764,
      "learning_rate": 0.0006073153524570236,
      "loss": 1.0321,
      "step": 4797
    },
    {
      "epoch": 6.397333333333333,
      "grad_norm": 0.029318733140826225,
      "learning_rate": 0.0006069182428947706,
      "loss": 1.2343,
      "step": 4798
    },
    {
      "epoch": 6.398666666666666,
      "grad_norm": 0.026661096140742302,
      "learning_rate": 0.0006065212066346537,
      "loss": 1.0869,
      "step": 4799
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.0206022746860981,
      "learning_rate": 0.0006061242437507131,
      "loss": 1.0424,
      "step": 4800
    },
    {
      "epoch": 6.4013333333333335,
      "grad_norm": 0.024493468925356865,
      "learning_rate": 0.0006057273543169737,
      "loss": 1.1251,
      "step": 4801
    },
    {
      "epoch": 6.402666666666667,
      "grad_norm": 0.022999731823801994,
      "learning_rate": 0.0006053305384074487,
      "loss": 0.9496,
      "step": 4802
    },
    {
      "epoch": 6.404,
      "grad_norm": 0.02111937664449215,
      "learning_rate": 0.0006049337960961362,
      "loss": 1.2843,
      "step": 4803
    },
    {
      "epoch": 6.405333333333333,
      "grad_norm": 0.022210702300071716,
      "learning_rate": 0.0006045371274570208,
      "loss": 1.0605,
      "step": 4804
    },
    {
      "epoch": 6.406666666666666,
      "grad_norm": 0.031623151153326035,
      "learning_rate": 0.0006041405325640739,
      "loss": 0.8318,
      "step": 4805
    },
    {
      "epoch": 6.408,
      "grad_norm": 0.023633938282728195,
      "learning_rate": 0.000603744011491252,
      "loss": 0.9005,
      "step": 4806
    },
    {
      "epoch": 6.4093333333333335,
      "grad_norm": 0.029438592493534088,
      "learning_rate": 0.0006033475643124995,
      "loss": 0.9675,
      "step": 4807
    },
    {
      "epoch": 6.410666666666667,
      "grad_norm": 0.027204517275094986,
      "learning_rate": 0.0006029511911017459,
      "loss": 0.967,
      "step": 4808
    },
    {
      "epoch": 6.412,
      "grad_norm": 0.03329513594508171,
      "learning_rate": 0.0006025548919329067,
      "loss": 0.8666,
      "step": 4809
    },
    {
      "epoch": 6.413333333333333,
      "grad_norm": 0.029140423983335495,
      "learning_rate": 0.0006021586668798846,
      "loss": 0.9949,
      "step": 4810
    },
    {
      "epoch": 6.414666666666666,
      "grad_norm": 0.021815141662955284,
      "learning_rate": 0.0006017625160165671,
      "loss": 1.1462,
      "step": 4811
    },
    {
      "epoch": 6.416,
      "grad_norm": 0.025933289900422096,
      "learning_rate": 0.0006013664394168296,
      "loss": 1.2745,
      "step": 4812
    },
    {
      "epoch": 6.417333333333334,
      "grad_norm": 0.024098720401525497,
      "learning_rate": 0.0006009704371545323,
      "loss": 0.9979,
      "step": 4813
    },
    {
      "epoch": 6.418666666666667,
      "grad_norm": 0.023724475875496864,
      "learning_rate": 0.0006005745093035215,
      "loss": 0.9537,
      "step": 4814
    },
    {
      "epoch": 6.42,
      "grad_norm": 0.030747152864933014,
      "learning_rate": 0.000600178655937631,
      "loss": 0.9748,
      "step": 4815
    },
    {
      "epoch": 6.421333333333333,
      "grad_norm": 0.05147428438067436,
      "learning_rate": 0.0005997828771306791,
      "loss": 0.8227,
      "step": 4816
    },
    {
      "epoch": 6.422666666666666,
      "grad_norm": 0.028919072821736336,
      "learning_rate": 0.0005993871729564712,
      "loss": 1.0752,
      "step": 4817
    },
    {
      "epoch": 6.424,
      "grad_norm": 0.020619064569473267,
      "learning_rate": 0.0005989915434887985,
      "loss": 1.1449,
      "step": 4818
    },
    {
      "epoch": 6.425333333333334,
      "grad_norm": 0.030759766697883606,
      "learning_rate": 0.0005985959888014376,
      "loss": 1.0929,
      "step": 4819
    },
    {
      "epoch": 6.426666666666667,
      "grad_norm": 0.023926427587866783,
      "learning_rate": 0.0005982005089681526,
      "loss": 0.857,
      "step": 4820
    },
    {
      "epoch": 6.428,
      "grad_norm": 0.022913513705134392,
      "learning_rate": 0.0005978051040626924,
      "loss": 0.9835,
      "step": 4821
    },
    {
      "epoch": 6.429333333333333,
      "grad_norm": 0.028954999521374702,
      "learning_rate": 0.0005974097741587928,
      "loss": 1.047,
      "step": 4822
    },
    {
      "epoch": 6.430666666666666,
      "grad_norm": 0.024493448436260223,
      "learning_rate": 0.0005970145193301746,
      "loss": 0.9439,
      "step": 4823
    },
    {
      "epoch": 6.432,
      "grad_norm": 0.023125914856791496,
      "learning_rate": 0.0005966193396505452,
      "loss": 1.0466,
      "step": 4824
    },
    {
      "epoch": 6.433333333333334,
      "grad_norm": 0.018495848402380943,
      "learning_rate": 0.0005962242351935984,
      "loss": 1.0323,
      "step": 4825
    },
    {
      "epoch": 6.434666666666667,
      "grad_norm": 0.029224369674921036,
      "learning_rate": 0.0005958292060330132,
      "loss": 0.8232,
      "step": 4826
    },
    {
      "epoch": 6.436,
      "grad_norm": 0.029796622693538666,
      "learning_rate": 0.0005954342522424553,
      "loss": 0.7746,
      "step": 4827
    },
    {
      "epoch": 6.437333333333333,
      "grad_norm": 0.02697065845131874,
      "learning_rate": 0.0005950393738955755,
      "loss": 0.9634,
      "step": 4828
    },
    {
      "epoch": 6.438666666666666,
      "grad_norm": 0.024620089679956436,
      "learning_rate": 0.0005946445710660107,
      "loss": 1.1064,
      "step": 4829
    },
    {
      "epoch": 6.44,
      "grad_norm": 0.025298677384853363,
      "learning_rate": 0.0005942498438273848,
      "loss": 1.203,
      "step": 4830
    },
    {
      "epoch": 6.441333333333334,
      "grad_norm": 0.03333489969372749,
      "learning_rate": 0.0005938551922533062,
      "loss": 0.9778,
      "step": 4831
    },
    {
      "epoch": 6.442666666666667,
      "grad_norm": 0.021459950134158134,
      "learning_rate": 0.0005934606164173701,
      "loss": 0.7618,
      "step": 4832
    },
    {
      "epoch": 6.444,
      "grad_norm": 0.023650968447327614,
      "learning_rate": 0.0005930661163931572,
      "loss": 1.1454,
      "step": 4833
    },
    {
      "epoch": 6.445333333333333,
      "grad_norm": 0.028562474995851517,
      "learning_rate": 0.0005926716922542334,
      "loss": 0.8865,
      "step": 4834
    },
    {
      "epoch": 6.446666666666666,
      "grad_norm": 0.03713226690888405,
      "learning_rate": 0.0005922773440741524,
      "loss": 0.8725,
      "step": 4835
    },
    {
      "epoch": 6.448,
      "grad_norm": 0.023483136668801308,
      "learning_rate": 0.0005918830719264514,
      "loss": 0.9881,
      "step": 4836
    },
    {
      "epoch": 6.449333333333334,
      "grad_norm": 0.028762372210621834,
      "learning_rate": 0.0005914888758846553,
      "loss": 1.1071,
      "step": 4837
    },
    {
      "epoch": 6.450666666666667,
      "grad_norm": 0.03032245859503746,
      "learning_rate": 0.0005910947560222738,
      "loss": 0.9823,
      "step": 4838
    },
    {
      "epoch": 6.452,
      "grad_norm": 0.019401907920837402,
      "learning_rate": 0.0005907007124128024,
      "loss": 1.0964,
      "step": 4839
    },
    {
      "epoch": 6.453333333333333,
      "grad_norm": 0.025706252083182335,
      "learning_rate": 0.0005903067451297229,
      "loss": 0.805,
      "step": 4840
    },
    {
      "epoch": 6.454666666666666,
      "grad_norm": 0.023770427331328392,
      "learning_rate": 0.0005899128542465018,
      "loss": 0.9602,
      "step": 4841
    },
    {
      "epoch": 6.456,
      "grad_norm": 0.026766737923026085,
      "learning_rate": 0.0005895190398365935,
      "loss": 0.8445,
      "step": 4842
    },
    {
      "epoch": 6.457333333333334,
      "grad_norm": 0.019369041547179222,
      "learning_rate": 0.0005891253019734355,
      "loss": 0.6537,
      "step": 4843
    },
    {
      "epoch": 6.458666666666667,
      "grad_norm": 0.027768533676862717,
      "learning_rate": 0.0005887316407304533,
      "loss": 1.178,
      "step": 4844
    },
    {
      "epoch": 6.46,
      "grad_norm": 0.026912972331047058,
      "learning_rate": 0.0005883380561810563,
      "loss": 1.0557,
      "step": 4845
    },
    {
      "epoch": 6.461333333333333,
      "grad_norm": 0.023060163483023643,
      "learning_rate": 0.0005879445483986405,
      "loss": 0.916,
      "step": 4846
    },
    {
      "epoch": 6.462666666666666,
      "grad_norm": 0.025426223874092102,
      "learning_rate": 0.0005875511174565879,
      "loss": 0.6809,
      "step": 4847
    },
    {
      "epoch": 6.464,
      "grad_norm": 0.023595988750457764,
      "learning_rate": 0.0005871577634282654,
      "loss": 1.0849,
      "step": 4848
    },
    {
      "epoch": 6.465333333333334,
      "grad_norm": 0.022150903940200806,
      "learning_rate": 0.0005867644863870263,
      "loss": 1.1482,
      "step": 4849
    },
    {
      "epoch": 6.466666666666667,
      "grad_norm": 0.028310995548963547,
      "learning_rate": 0.0005863712864062089,
      "loss": 1.2928,
      "step": 4850
    },
    {
      "epoch": 6.468,
      "grad_norm": 0.02626175992190838,
      "learning_rate": 0.0005859781635591368,
      "loss": 0.8734,
      "step": 4851
    },
    {
      "epoch": 6.469333333333333,
      "grad_norm": 0.027902161702513695,
      "learning_rate": 0.000585585117919121,
      "loss": 1.215,
      "step": 4852
    },
    {
      "epoch": 6.470666666666666,
      "grad_norm": 0.023822106420993805,
      "learning_rate": 0.0005851921495594559,
      "loss": 1.0298,
      "step": 4853
    },
    {
      "epoch": 6.4719999999999995,
      "grad_norm": 0.028375858440995216,
      "learning_rate": 0.000584799258553423,
      "loss": 0.9859,
      "step": 4854
    },
    {
      "epoch": 6.473333333333334,
      "grad_norm": 0.02657637745141983,
      "learning_rate": 0.0005844064449742887,
      "loss": 1.1574,
      "step": 4855
    },
    {
      "epoch": 6.474666666666667,
      "grad_norm": 0.03432362526655197,
      "learning_rate": 0.0005840137088953051,
      "loss": 1.0078,
      "step": 4856
    },
    {
      "epoch": 6.476,
      "grad_norm": 0.026789318770170212,
      "learning_rate": 0.00058362105038971,
      "loss": 1.1162,
      "step": 4857
    },
    {
      "epoch": 6.477333333333333,
      "grad_norm": 0.023814355954527855,
      "learning_rate": 0.0005832284695307263,
      "loss": 0.9346,
      "step": 4858
    },
    {
      "epoch": 6.478666666666666,
      "grad_norm": 0.02317856065928936,
      "learning_rate": 0.0005828359663915632,
      "loss": 0.844,
      "step": 4859
    },
    {
      "epoch": 6.48,
      "grad_norm": 0.030441513285040855,
      "learning_rate": 0.0005824435410454149,
      "loss": 0.987,
      "step": 4860
    },
    {
      "epoch": 6.481333333333334,
      "grad_norm": 0.025082075968384743,
      "learning_rate": 0.0005820511935654602,
      "loss": 1.0084,
      "step": 4861
    },
    {
      "epoch": 6.482666666666667,
      "grad_norm": 0.021869173273444176,
      "learning_rate": 0.0005816589240248658,
      "loss": 1.1982,
      "step": 4862
    },
    {
      "epoch": 6.484,
      "grad_norm": 0.02628299407660961,
      "learning_rate": 0.0005812667324967813,
      "loss": 0.9197,
      "step": 4863
    },
    {
      "epoch": 6.485333333333333,
      "grad_norm": 0.03107680007815361,
      "learning_rate": 0.000580874619054343,
      "loss": 1.066,
      "step": 4864
    },
    {
      "epoch": 6.486666666666666,
      "grad_norm": 0.03221611678600311,
      "learning_rate": 0.0005804825837706731,
      "loss": 0.8044,
      "step": 4865
    },
    {
      "epoch": 6.4879999999999995,
      "grad_norm": 0.036812424659729004,
      "learning_rate": 0.0005800906267188772,
      "loss": 1.3265,
      "step": 4866
    },
    {
      "epoch": 6.489333333333334,
      "grad_norm": 0.022427894175052643,
      "learning_rate": 0.0005796987479720494,
      "loss": 0.9198,
      "step": 4867
    },
    {
      "epoch": 6.490666666666667,
      "grad_norm": 0.03054463118314743,
      "learning_rate": 0.0005793069476032664,
      "loss": 0.9855,
      "step": 4868
    },
    {
      "epoch": 6.492,
      "grad_norm": 0.019083470106124878,
      "learning_rate": 0.0005789152256855916,
      "loss": 1.0763,
      "step": 4869
    },
    {
      "epoch": 6.493333333333333,
      "grad_norm": 0.024549543857574463,
      "learning_rate": 0.000578523582292074,
      "loss": 0.9332,
      "step": 4870
    },
    {
      "epoch": 6.494666666666666,
      "grad_norm": 0.022630706429481506,
      "learning_rate": 0.0005781320174957463,
      "loss": 0.838,
      "step": 4871
    },
    {
      "epoch": 6.496,
      "grad_norm": 0.02962336130440235,
      "learning_rate": 0.0005777405313696294,
      "loss": 1.2944,
      "step": 4872
    },
    {
      "epoch": 6.497333333333334,
      "grad_norm": 0.024915063753724098,
      "learning_rate": 0.0005773491239867267,
      "loss": 0.8621,
      "step": 4873
    },
    {
      "epoch": 6.498666666666667,
      "grad_norm": 0.1547246128320694,
      "learning_rate": 0.0005769577954200285,
      "loss": 0.8375,
      "step": 4874
    },
    {
      "epoch": 6.5,
      "grad_norm": 0.029868870973587036,
      "learning_rate": 0.0005765665457425102,
      "loss": 1.1706,
      "step": 4875
    },
    {
      "epoch": 6.501333333333333,
      "grad_norm": 0.02286119945347309,
      "learning_rate": 0.0005761753750271313,
      "loss": 0.8206,
      "step": 4876
    },
    {
      "epoch": 6.502666666666666,
      "grad_norm": 0.018915018066763878,
      "learning_rate": 0.0005757842833468392,
      "loss": 0.8783,
      "step": 4877
    },
    {
      "epoch": 6.504,
      "grad_norm": 0.026765795424580574,
      "learning_rate": 0.0005753932707745635,
      "loss": 1.3272,
      "step": 4878
    },
    {
      "epoch": 6.505333333333334,
      "grad_norm": 0.025777705013751984,
      "learning_rate": 0.0005750023373832212,
      "loss": 0.8208,
      "step": 4879
    },
    {
      "epoch": 6.506666666666667,
      "grad_norm": 0.0421709306538105,
      "learning_rate": 0.0005746114832457139,
      "loss": 1.286,
      "step": 4880
    },
    {
      "epoch": 6.508,
      "grad_norm": 0.03237701952457428,
      "learning_rate": 0.0005742207084349273,
      "loss": 0.8571,
      "step": 4881
    },
    {
      "epoch": 6.509333333333333,
      "grad_norm": 0.02879755198955536,
      "learning_rate": 0.0005738300130237349,
      "loss": 1.3098,
      "step": 4882
    },
    {
      "epoch": 6.510666666666666,
      "grad_norm": 0.028440682217478752,
      "learning_rate": 0.0005734393970849925,
      "loss": 0.9144,
      "step": 4883
    },
    {
      "epoch": 6.5120000000000005,
      "grad_norm": 0.04944412037730217,
      "learning_rate": 0.0005730488606915429,
      "loss": 1.0853,
      "step": 4884
    },
    {
      "epoch": 6.513333333333334,
      "grad_norm": 0.030397603288292885,
      "learning_rate": 0.0005726584039162141,
      "loss": 0.8146,
      "step": 4885
    },
    {
      "epoch": 6.514666666666667,
      "grad_norm": 0.021459225565195084,
      "learning_rate": 0.0005722680268318175,
      "loss": 0.976,
      "step": 4886
    },
    {
      "epoch": 6.516,
      "grad_norm": 0.10148458927869797,
      "learning_rate": 0.0005718777295111524,
      "loss": 0.8085,
      "step": 4887
    },
    {
      "epoch": 6.517333333333333,
      "grad_norm": 0.025228966027498245,
      "learning_rate": 0.000571487512027,
      "loss": 0.9169,
      "step": 4888
    },
    {
      "epoch": 6.518666666666666,
      "grad_norm": 0.022605137899518013,
      "learning_rate": 0.0005710973744521299,
      "loss": 0.9721,
      "step": 4889
    },
    {
      "epoch": 6.52,
      "grad_norm": 0.024324683472514153,
      "learning_rate": 0.0005707073168592942,
      "loss": 0.8101,
      "step": 4890
    },
    {
      "epoch": 6.521333333333334,
      "grad_norm": 0.024824615567922592,
      "learning_rate": 0.0005703173393212314,
      "loss": 1.1518,
      "step": 4891
    },
    {
      "epoch": 6.522666666666667,
      "grad_norm": 0.026760844513773918,
      "learning_rate": 0.000569927441910665,
      "loss": 1.049,
      "step": 4892
    },
    {
      "epoch": 6.524,
      "grad_norm": 0.030112626031041145,
      "learning_rate": 0.0005695376247003025,
      "loss": 0.972,
      "step": 4893
    },
    {
      "epoch": 6.525333333333333,
      "grad_norm": 0.025605520233511925,
      "learning_rate": 0.0005691478877628384,
      "loss": 1.1669,
      "step": 4894
    },
    {
      "epoch": 6.526666666666666,
      "grad_norm": 0.04980460926890373,
      "learning_rate": 0.0005687582311709503,
      "loss": 1.209,
      "step": 4895
    },
    {
      "epoch": 6.5280000000000005,
      "grad_norm": 0.02049703523516655,
      "learning_rate": 0.0005683686549973017,
      "loss": 1.0129,
      "step": 4896
    },
    {
      "epoch": 6.529333333333334,
      "grad_norm": 0.03908916562795639,
      "learning_rate": 0.0005679791593145416,
      "loss": 0.9716,
      "step": 4897
    },
    {
      "epoch": 6.530666666666667,
      "grad_norm": 0.03524324297904968,
      "learning_rate": 0.0005675897441953019,
      "loss": 0.9517,
      "step": 4898
    },
    {
      "epoch": 6.532,
      "grad_norm": 0.03282441571354866,
      "learning_rate": 0.0005672004097122032,
      "loss": 1.0826,
      "step": 4899
    },
    {
      "epoch": 6.533333333333333,
      "grad_norm": 0.027545500546693802,
      "learning_rate": 0.000566811155937847,
      "loss": 0.8559,
      "step": 4900
    },
    {
      "epoch": 6.534666666666666,
      "grad_norm": 0.02225431054830551,
      "learning_rate": 0.0005664219829448223,
      "loss": 0.9566,
      "step": 4901
    },
    {
      "epoch": 6.536,
      "grad_norm": 0.03130722790956497,
      "learning_rate": 0.0005660328908057027,
      "loss": 1.0005,
      "step": 4902
    },
    {
      "epoch": 6.537333333333334,
      "grad_norm": 0.032036617398262024,
      "learning_rate": 0.0005656438795930452,
      "loss": 0.9865,
      "step": 4903
    },
    {
      "epoch": 6.538666666666667,
      "grad_norm": 0.027825089171528816,
      "learning_rate": 0.0005652549493793944,
      "loss": 1.1672,
      "step": 4904
    },
    {
      "epoch": 6.54,
      "grad_norm": 0.040527574717998505,
      "learning_rate": 0.0005648661002372768,
      "loss": 1.078,
      "step": 4905
    },
    {
      "epoch": 6.541333333333333,
      "grad_norm": 0.018870240077376366,
      "learning_rate": 0.0005644773322392061,
      "loss": 1.1446,
      "step": 4906
    },
    {
      "epoch": 6.542666666666666,
      "grad_norm": 0.022608600556850433,
      "learning_rate": 0.0005640886454576802,
      "loss": 0.8769,
      "step": 4907
    },
    {
      "epoch": 6.5440000000000005,
      "grad_norm": 0.03144850954413414,
      "learning_rate": 0.0005637000399651803,
      "loss": 1.0942,
      "step": 4908
    },
    {
      "epoch": 6.545333333333334,
      "grad_norm": 0.023179637268185616,
      "learning_rate": 0.0005633115158341757,
      "loss": 1.1003,
      "step": 4909
    },
    {
      "epoch": 6.546666666666667,
      "grad_norm": 0.02993086166679859,
      "learning_rate": 0.0005629230731371171,
      "loss": 0.9247,
      "step": 4910
    },
    {
      "epoch": 6.548,
      "grad_norm": 0.02965184859931469,
      "learning_rate": 0.0005625347119464421,
      "loss": 0.9691,
      "step": 4911
    },
    {
      "epoch": 6.549333333333333,
      "grad_norm": 0.026189805939793587,
      "learning_rate": 0.0005621464323345731,
      "loss": 1.2233,
      "step": 4912
    },
    {
      "epoch": 6.550666666666666,
      "grad_norm": 0.02492452785372734,
      "learning_rate": 0.0005617582343739152,
      "loss": 0.9206,
      "step": 4913
    },
    {
      "epoch": 6.552,
      "grad_norm": 0.028534002602100372,
      "learning_rate": 0.0005613701181368618,
      "loss": 0.8058,
      "step": 4914
    },
    {
      "epoch": 6.553333333333334,
      "grad_norm": 0.029141562059521675,
      "learning_rate": 0.0005609820836957871,
      "loss": 0.9666,
      "step": 4915
    },
    {
      "epoch": 6.554666666666667,
      "grad_norm": 0.022525450214743614,
      "learning_rate": 0.0005605941311230538,
      "loss": 1.2129,
      "step": 4916
    },
    {
      "epoch": 6.556,
      "grad_norm": 0.028500555083155632,
      "learning_rate": 0.0005602062604910063,
      "loss": 0.818,
      "step": 4917
    },
    {
      "epoch": 6.557333333333333,
      "grad_norm": 0.021728746592998505,
      "learning_rate": 0.0005598184718719753,
      "loss": 1.0505,
      "step": 4918
    },
    {
      "epoch": 6.558666666666666,
      "grad_norm": 0.025396713986992836,
      "learning_rate": 0.0005594307653382765,
      "loss": 1.3986,
      "step": 4919
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 0.026244454085826874,
      "learning_rate": 0.0005590431409622081,
      "loss": 0.773,
      "step": 4920
    },
    {
      "epoch": 6.561333333333334,
      "grad_norm": 0.023628240451216698,
      "learning_rate": 0.0005586555988160564,
      "loss": 0.9365,
      "step": 4921
    },
    {
      "epoch": 6.562666666666667,
      "grad_norm": 0.019990218803286552,
      "learning_rate": 0.0005582681389720892,
      "loss": 1.1881,
      "step": 4922
    },
    {
      "epoch": 6.564,
      "grad_norm": 0.036101218312978745,
      "learning_rate": 0.0005578807615025607,
      "loss": 0.9197,
      "step": 4923
    },
    {
      "epoch": 6.565333333333333,
      "grad_norm": 0.020345522090792656,
      "learning_rate": 0.0005574934664797097,
      "loss": 0.883,
      "step": 4924
    },
    {
      "epoch": 6.566666666666666,
      "grad_norm": 0.02693801373243332,
      "learning_rate": 0.0005571062539757581,
      "loss": 1.1505,
      "step": 4925
    },
    {
      "epoch": 6.568,
      "grad_norm": 0.022273877635598183,
      "learning_rate": 0.000556719124062915,
      "loss": 0.9473,
      "step": 4926
    },
    {
      "epoch": 6.569333333333334,
      "grad_norm": 0.023989055305719376,
      "learning_rate": 0.0005563320768133715,
      "loss": 1.0166,
      "step": 4927
    },
    {
      "epoch": 6.570666666666667,
      "grad_norm": 0.0364362932741642,
      "learning_rate": 0.0005559451122993047,
      "loss": 0.7169,
      "step": 4928
    },
    {
      "epoch": 6.572,
      "grad_norm": 0.023576781153678894,
      "learning_rate": 0.0005555582305928765,
      "loss": 1.0314,
      "step": 4929
    },
    {
      "epoch": 6.573333333333333,
      "grad_norm": 0.023483846336603165,
      "learning_rate": 0.0005551714317662317,
      "loss": 1.0668,
      "step": 4930
    },
    {
      "epoch": 6.574666666666666,
      "grad_norm": 0.03375362232327461,
      "learning_rate": 0.0005547847158915023,
      "loss": 1.2198,
      "step": 4931
    },
    {
      "epoch": 6.576,
      "grad_norm": 0.022493766620755196,
      "learning_rate": 0.0005543980830408022,
      "loss": 0.8512,
      "step": 4932
    },
    {
      "epoch": 6.577333333333334,
      "grad_norm": 0.030829500406980515,
      "learning_rate": 0.0005540115332862312,
      "loss": 1.243,
      "step": 4933
    },
    {
      "epoch": 6.578666666666667,
      "grad_norm": 0.021302755922079086,
      "learning_rate": 0.0005536250666998739,
      "loss": 1.2061,
      "step": 4934
    },
    {
      "epoch": 6.58,
      "grad_norm": 0.029288843274116516,
      "learning_rate": 0.0005532386833537977,
      "loss": 0.9127,
      "step": 4935
    },
    {
      "epoch": 6.581333333333333,
      "grad_norm": 0.027581749483942986,
      "learning_rate": 0.0005528523833200568,
      "loss": 1.4086,
      "step": 4936
    },
    {
      "epoch": 6.582666666666666,
      "grad_norm": 0.021708188578486443,
      "learning_rate": 0.0005524661666706876,
      "loss": 1.0902,
      "step": 4937
    },
    {
      "epoch": 6.584,
      "grad_norm": 0.021392084658145905,
      "learning_rate": 0.0005520800334777132,
      "loss": 0.9122,
      "step": 4938
    },
    {
      "epoch": 6.585333333333334,
      "grad_norm": 0.03301515430212021,
      "learning_rate": 0.0005516939838131394,
      "loss": 1.2017,
      "step": 4939
    },
    {
      "epoch": 6.586666666666667,
      "grad_norm": 0.02522498369216919,
      "learning_rate": 0.0005513080177489562,
      "loss": 1.0406,
      "step": 4940
    },
    {
      "epoch": 6.588,
      "grad_norm": 0.02134673297405243,
      "learning_rate": 0.0005509221353571403,
      "loss": 1.1264,
      "step": 4941
    },
    {
      "epoch": 6.589333333333333,
      "grad_norm": 0.016158239915966988,
      "learning_rate": 0.0005505363367096497,
      "loss": 0.791,
      "step": 4942
    },
    {
      "epoch": 6.5906666666666665,
      "grad_norm": 0.022636476904153824,
      "learning_rate": 0.0005501506218784301,
      "loss": 1.0671,
      "step": 4943
    },
    {
      "epoch": 6.592,
      "grad_norm": 0.026029108092188835,
      "learning_rate": 0.0005497649909354083,
      "loss": 0.9502,
      "step": 4944
    },
    {
      "epoch": 6.593333333333334,
      "grad_norm": 0.022291408851742744,
      "learning_rate": 0.0005493794439524979,
      "loss": 0.9872,
      "step": 4945
    },
    {
      "epoch": 6.594666666666667,
      "grad_norm": 0.029655834659934044,
      "learning_rate": 0.000548993981001596,
      "loss": 1.0186,
      "step": 4946
    },
    {
      "epoch": 6.596,
      "grad_norm": 0.026624420657753944,
      "learning_rate": 0.0005486086021545828,
      "loss": 0.9011,
      "step": 4947
    },
    {
      "epoch": 6.597333333333333,
      "grad_norm": 0.02897704392671585,
      "learning_rate": 0.0005482233074833258,
      "loss": 0.8106,
      "step": 4948
    },
    {
      "epoch": 6.5986666666666665,
      "grad_norm": 0.03065706044435501,
      "learning_rate": 0.0005478380970596737,
      "loss": 0.9789,
      "step": 4949
    },
    {
      "epoch": 6.6,
      "grad_norm": 0.028006086125969887,
      "learning_rate": 0.0005474529709554612,
      "loss": 1.107,
      "step": 4950
    },
    {
      "epoch": 6.601333333333334,
      "grad_norm": 0.02271374501287937,
      "learning_rate": 0.000547067929242507,
      "loss": 0.8931,
      "step": 4951
    },
    {
      "epoch": 6.602666666666667,
      "grad_norm": 0.027089351788163185,
      "learning_rate": 0.0005466829719926131,
      "loss": 1.0957,
      "step": 4952
    },
    {
      "epoch": 6.604,
      "grad_norm": 0.023889362812042236,
      "learning_rate": 0.0005462980992775679,
      "loss": 1.0552,
      "step": 4953
    },
    {
      "epoch": 6.605333333333333,
      "grad_norm": 0.024099284783005714,
      "learning_rate": 0.0005459133111691416,
      "loss": 0.938,
      "step": 4954
    },
    {
      "epoch": 6.6066666666666665,
      "grad_norm": 0.116904616355896,
      "learning_rate": 0.0005455286077390903,
      "loss": 0.827,
      "step": 4955
    },
    {
      "epoch": 6.608,
      "grad_norm": 0.031665388494729996,
      "learning_rate": 0.0005451439890591539,
      "loss": 1.3166,
      "step": 4956
    },
    {
      "epoch": 6.609333333333334,
      "grad_norm": 0.02681460790336132,
      "learning_rate": 0.0005447594552010553,
      "loss": 1.083,
      "step": 4957
    },
    {
      "epoch": 6.610666666666667,
      "grad_norm": 0.03381815925240517,
      "learning_rate": 0.0005443750062365041,
      "loss": 0.7869,
      "step": 4958
    },
    {
      "epoch": 6.612,
      "grad_norm": 0.030298005789518356,
      "learning_rate": 0.0005439906422371914,
      "loss": 1.1209,
      "step": 4959
    },
    {
      "epoch": 6.613333333333333,
      "grad_norm": 0.024995924904942513,
      "learning_rate": 0.0005436063632747941,
      "loss": 1.003,
      "step": 4960
    },
    {
      "epoch": 6.6146666666666665,
      "grad_norm": 0.02536061219871044,
      "learning_rate": 0.0005432221694209731,
      "loss": 0.691,
      "step": 4961
    },
    {
      "epoch": 6.616,
      "grad_norm": 0.022381218150258064,
      "learning_rate": 0.000542838060747372,
      "loss": 0.982,
      "step": 4962
    },
    {
      "epoch": 6.617333333333333,
      "grad_norm": 0.018838953226804733,
      "learning_rate": 0.0005424540373256213,
      "loss": 1.075,
      "step": 4963
    },
    {
      "epoch": 6.618666666666667,
      "grad_norm": 0.022988945245742798,
      "learning_rate": 0.0005420700992273325,
      "loss": 0.8661,
      "step": 4964
    },
    {
      "epoch": 6.62,
      "grad_norm": 0.023678116500377655,
      "learning_rate": 0.0005416862465241032,
      "loss": 0.8467,
      "step": 4965
    },
    {
      "epoch": 6.621333333333333,
      "grad_norm": 0.02332773245871067,
      "learning_rate": 0.0005413024792875145,
      "loss": 0.9335,
      "step": 4966
    },
    {
      "epoch": 6.6226666666666665,
      "grad_norm": 0.020415429025888443,
      "learning_rate": 0.0005409187975891316,
      "loss": 1.0048,
      "step": 4967
    },
    {
      "epoch": 6.624,
      "grad_norm": 0.034688763320446014,
      "learning_rate": 0.0005405352015005039,
      "loss": 0.8468,
      "step": 4968
    },
    {
      "epoch": 6.625333333333334,
      "grad_norm": 0.022049522027373314,
      "learning_rate": 0.0005401516910931641,
      "loss": 0.9192,
      "step": 4969
    },
    {
      "epoch": 6.626666666666667,
      "grad_norm": 0.030068302527070045,
      "learning_rate": 0.0005397682664386296,
      "loss": 1.2118,
      "step": 4970
    },
    {
      "epoch": 6.628,
      "grad_norm": 0.02883605659008026,
      "learning_rate": 0.0005393849276084017,
      "loss": 1.1505,
      "step": 4971
    },
    {
      "epoch": 6.629333333333333,
      "grad_norm": 0.028757527470588684,
      "learning_rate": 0.0005390016746739659,
      "loss": 1.0065,
      "step": 4972
    },
    {
      "epoch": 6.6306666666666665,
      "grad_norm": 0.02534293942153454,
      "learning_rate": 0.0005386185077067918,
      "loss": 1.0377,
      "step": 4973
    },
    {
      "epoch": 6.632,
      "grad_norm": 0.0322592593729496,
      "learning_rate": 0.0005382354267783316,
      "loss": 1.0016,
      "step": 4974
    },
    {
      "epoch": 6.633333333333333,
      "grad_norm": 0.03373745456337929,
      "learning_rate": 0.0005378524319600231,
      "loss": 0.7778,
      "step": 4975
    },
    {
      "epoch": 6.634666666666667,
      "grad_norm": 0.021638819947838783,
      "learning_rate": 0.0005374695233232873,
      "loss": 1.1006,
      "step": 4976
    },
    {
      "epoch": 6.636,
      "grad_norm": 0.02387736178934574,
      "learning_rate": 0.0005370867009395294,
      "loss": 0.898,
      "step": 4977
    },
    {
      "epoch": 6.637333333333333,
      "grad_norm": 0.04590064287185669,
      "learning_rate": 0.0005367039648801385,
      "loss": 0.9769,
      "step": 4978
    },
    {
      "epoch": 6.6386666666666665,
      "grad_norm": 0.026505006477236748,
      "learning_rate": 0.0005363213152164866,
      "loss": 1.2804,
      "step": 4979
    },
    {
      "epoch": 6.64,
      "grad_norm": 0.027375834062695503,
      "learning_rate": 0.0005359387520199317,
      "loss": 1.0869,
      "step": 4980
    },
    {
      "epoch": 6.641333333333334,
      "grad_norm": 0.02301361784338951,
      "learning_rate": 0.0005355562753618133,
      "loss": 1.125,
      "step": 4981
    },
    {
      "epoch": 6.642666666666667,
      "grad_norm": 0.02357860840857029,
      "learning_rate": 0.0005351738853134566,
      "loss": 1.0639,
      "step": 4982
    },
    {
      "epoch": 6.644,
      "grad_norm": 0.025690492242574692,
      "learning_rate": 0.0005347915819461699,
      "loss": 0.9676,
      "step": 4983
    },
    {
      "epoch": 6.645333333333333,
      "grad_norm": 0.029126746580004692,
      "learning_rate": 0.0005344093653312445,
      "loss": 1.114,
      "step": 4984
    },
    {
      "epoch": 6.6466666666666665,
      "grad_norm": 0.02515747956931591,
      "learning_rate": 0.000534027235539958,
      "loss": 1.1494,
      "step": 4985
    },
    {
      "epoch": 6.648,
      "grad_norm": 0.030675342306494713,
      "learning_rate": 0.0005336451926435688,
      "loss": 1.1539,
      "step": 4986
    },
    {
      "epoch": 6.649333333333333,
      "grad_norm": 0.027021562680602074,
      "learning_rate": 0.000533263236713321,
      "loss": 1.2752,
      "step": 4987
    },
    {
      "epoch": 6.650666666666667,
      "grad_norm": 0.025488225743174553,
      "learning_rate": 0.0005328813678204425,
      "loss": 0.8505,
      "step": 4988
    },
    {
      "epoch": 6.652,
      "grad_norm": 0.025330638512969017,
      "learning_rate": 0.000532499586036143,
      "loss": 0.9315,
      "step": 4989
    },
    {
      "epoch": 6.653333333333333,
      "grad_norm": 0.029809042811393738,
      "learning_rate": 0.0005321178914316192,
      "loss": 1.0492,
      "step": 4990
    },
    {
      "epoch": 6.6546666666666665,
      "grad_norm": 0.025824785232543945,
      "learning_rate": 0.0005317362840780484,
      "loss": 0.9471,
      "step": 4991
    },
    {
      "epoch": 6.656,
      "grad_norm": 0.02082270383834839,
      "learning_rate": 0.0005313547640465936,
      "loss": 1.0658,
      "step": 4992
    },
    {
      "epoch": 6.657333333333334,
      "grad_norm": 0.030396729707717896,
      "learning_rate": 0.0005309733314084008,
      "loss": 1.0139,
      "step": 4993
    },
    {
      "epoch": 6.658666666666667,
      "grad_norm": 0.023599088191986084,
      "learning_rate": 0.0005305919862345998,
      "loss": 0.9225,
      "step": 4994
    },
    {
      "epoch": 6.66,
      "grad_norm": 0.033013444393873215,
      "learning_rate": 0.0005302107285963045,
      "loss": 0.9844,
      "step": 4995
    },
    {
      "epoch": 6.661333333333333,
      "grad_norm": 0.029111484065651894,
      "learning_rate": 0.0005298295585646111,
      "loss": 0.9707,
      "step": 4996
    },
    {
      "epoch": 6.6626666666666665,
      "grad_norm": 0.022248651832342148,
      "learning_rate": 0.0005294484762106011,
      "loss": 1.0064,
      "step": 4997
    },
    {
      "epoch": 6.664,
      "grad_norm": 0.021128203719854355,
      "learning_rate": 0.0005290674816053389,
      "loss": 1.0873,
      "step": 4998
    },
    {
      "epoch": 6.665333333333333,
      "grad_norm": 0.021864671260118484,
      "learning_rate": 0.0005286865748198725,
      "loss": 1.0954,
      "step": 4999
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.022582536563277245,
      "learning_rate": 0.0005283057559252342,
      "loss": 1.2537,
      "step": 5000
    },
    {
      "epoch": 6.668,
      "grad_norm": 0.04877503216266632,
      "learning_rate": 0.0005279250249924383,
      "loss": 0.8364,
      "step": 5001
    },
    {
      "epoch": 6.669333333333333,
      "grad_norm": 0.028597893193364143,
      "learning_rate": 0.0005275443820924843,
      "loss": 1.0927,
      "step": 5002
    },
    {
      "epoch": 6.6706666666666665,
      "grad_norm": 0.022571222856640816,
      "learning_rate": 0.0005271638272963548,
      "loss": 1.1919,
      "step": 5003
    },
    {
      "epoch": 6.672,
      "grad_norm": 0.025306135416030884,
      "learning_rate": 0.000526783360675016,
      "loss": 0.7987,
      "step": 5004
    },
    {
      "epoch": 6.673333333333334,
      "grad_norm": 0.027578972280025482,
      "learning_rate": 0.0005264029822994176,
      "loss": 0.964,
      "step": 5005
    },
    {
      "epoch": 6.674666666666667,
      "grad_norm": 0.03393322601914406,
      "learning_rate": 0.0005260226922404923,
      "loss": 1.0777,
      "step": 5006
    },
    {
      "epoch": 6.676,
      "grad_norm": 0.028846940025687218,
      "learning_rate": 0.000525642490569157,
      "loss": 1.2439,
      "step": 5007
    },
    {
      "epoch": 6.677333333333333,
      "grad_norm": 0.027303645387291908,
      "learning_rate": 0.0005252623773563123,
      "loss": 1.0108,
      "step": 5008
    },
    {
      "epoch": 6.6786666666666665,
      "grad_norm": 0.02203601598739624,
      "learning_rate": 0.0005248823526728416,
      "loss": 1.1208,
      "step": 5009
    },
    {
      "epoch": 6.68,
      "grad_norm": 0.03064066171646118,
      "learning_rate": 0.0005245024165896126,
      "loss": 1.3769,
      "step": 5010
    },
    {
      "epoch": 6.681333333333333,
      "grad_norm": 0.023997874930500984,
      "learning_rate": 0.0005241225691774754,
      "loss": 1.1072,
      "step": 5011
    },
    {
      "epoch": 6.682666666666667,
      "grad_norm": 0.034172579646110535,
      "learning_rate": 0.0005237428105072646,
      "loss": 1.1546,
      "step": 5012
    },
    {
      "epoch": 6.684,
      "grad_norm": 0.02143675461411476,
      "learning_rate": 0.0005233631406497976,
      "loss": 1.0856,
      "step": 5013
    },
    {
      "epoch": 6.685333333333333,
      "grad_norm": 0.024304356426000595,
      "learning_rate": 0.0005229835596758756,
      "loss": 1.048,
      "step": 5014
    },
    {
      "epoch": 6.6866666666666665,
      "grad_norm": 0.022318584844470024,
      "learning_rate": 0.0005226040676562835,
      "loss": 1.0696,
      "step": 5015
    },
    {
      "epoch": 6.688,
      "grad_norm": 0.027687009423971176,
      "learning_rate": 0.0005222246646617886,
      "loss": 0.9895,
      "step": 5016
    },
    {
      "epoch": 6.689333333333334,
      "grad_norm": 0.019993236288428307,
      "learning_rate": 0.0005218453507631422,
      "loss": 1.0088,
      "step": 5017
    },
    {
      "epoch": 6.690666666666667,
      "grad_norm": 0.029886985197663307,
      "learning_rate": 0.0005214661260310793,
      "loss": 1.2972,
      "step": 5018
    },
    {
      "epoch": 6.692,
      "grad_norm": 0.021258249878883362,
      "learning_rate": 0.0005210869905363178,
      "loss": 0.8317,
      "step": 5019
    },
    {
      "epoch": 6.693333333333333,
      "grad_norm": 0.030744653195142746,
      "learning_rate": 0.0005207079443495595,
      "loss": 0.9021,
      "step": 5020
    },
    {
      "epoch": 6.6946666666666665,
      "grad_norm": 0.026846978813409805,
      "learning_rate": 0.000520328987541489,
      "loss": 1.1644,
      "step": 5021
    },
    {
      "epoch": 6.696,
      "grad_norm": 0.03018515184521675,
      "learning_rate": 0.0005199501201827741,
      "loss": 1.133,
      "step": 5022
    },
    {
      "epoch": 6.697333333333333,
      "grad_norm": 0.03337829187512398,
      "learning_rate": 0.0005195713423440664,
      "loss": 0.9798,
      "step": 5023
    },
    {
      "epoch": 6.698666666666667,
      "grad_norm": 0.02353760041296482,
      "learning_rate": 0.0005191926540960006,
      "loss": 0.7083,
      "step": 5024
    },
    {
      "epoch": 6.7,
      "grad_norm": 0.021444687619805336,
      "learning_rate": 0.0005188140555091949,
      "loss": 0.9756,
      "step": 5025
    },
    {
      "epoch": 6.701333333333333,
      "grad_norm": 0.024529913440346718,
      "learning_rate": 0.0005184355466542505,
      "loss": 0.9581,
      "step": 5026
    },
    {
      "epoch": 6.7026666666666666,
      "grad_norm": 0.024913376197218895,
      "learning_rate": 0.0005180571276017522,
      "loss": 1.0333,
      "step": 5027
    },
    {
      "epoch": 6.704,
      "grad_norm": 0.02687734179198742,
      "learning_rate": 0.0005176787984222674,
      "loss": 0.9782,
      "step": 5028
    },
    {
      "epoch": 6.705333333333334,
      "grad_norm": 0.032750073820352554,
      "learning_rate": 0.0005173005591863473,
      "loss": 1.2226,
      "step": 5029
    },
    {
      "epoch": 6.706666666666667,
      "grad_norm": 0.03189896419644356,
      "learning_rate": 0.0005169224099645263,
      "loss": 0.8579,
      "step": 5030
    },
    {
      "epoch": 6.708,
      "grad_norm": 0.028163986280560493,
      "learning_rate": 0.0005165443508273218,
      "loss": 1.0814,
      "step": 5031
    },
    {
      "epoch": 6.709333333333333,
      "grad_norm": 0.02340029738843441,
      "learning_rate": 0.0005161663818452349,
      "loss": 1.1716,
      "step": 5032
    },
    {
      "epoch": 6.710666666666667,
      "grad_norm": 0.027837522327899933,
      "learning_rate": 0.0005157885030887488,
      "loss": 1.2334,
      "step": 5033
    },
    {
      "epoch": 6.712,
      "grad_norm": 0.040962401777505875,
      "learning_rate": 0.0005154107146283311,
      "loss": 1.0598,
      "step": 5034
    },
    {
      "epoch": 6.713333333333333,
      "grad_norm": 0.028925282880663872,
      "learning_rate": 0.0005150330165344317,
      "loss": 0.9965,
      "step": 5035
    },
    {
      "epoch": 6.714666666666667,
      "grad_norm": 0.04056837409734726,
      "learning_rate": 0.0005146554088774843,
      "loss": 1.3239,
      "step": 5036
    },
    {
      "epoch": 6.716,
      "grad_norm": 0.024604860693216324,
      "learning_rate": 0.0005142778917279057,
      "loss": 1.0779,
      "step": 5037
    },
    {
      "epoch": 6.717333333333333,
      "grad_norm": 0.029950741678476334,
      "learning_rate": 0.0005139004651560945,
      "loss": 1.2622,
      "step": 5038
    },
    {
      "epoch": 6.718666666666667,
      "grad_norm": 0.02188500016927719,
      "learning_rate": 0.0005135231292324341,
      "loss": 1.1983,
      "step": 5039
    },
    {
      "epoch": 6.72,
      "grad_norm": 0.028096238151192665,
      "learning_rate": 0.0005131458840272905,
      "loss": 0.9307,
      "step": 5040
    },
    {
      "epoch": 6.721333333333334,
      "grad_norm": 0.025284478440880775,
      "learning_rate": 0.0005127687296110122,
      "loss": 1.0254,
      "step": 5041
    },
    {
      "epoch": 6.722666666666667,
      "grad_norm": 0.03223733976483345,
      "learning_rate": 0.0005123916660539315,
      "loss": 0.8507,
      "step": 5042
    },
    {
      "epoch": 6.724,
      "grad_norm": 0.022016186267137527,
      "learning_rate": 0.0005120146934263637,
      "loss": 0.8547,
      "step": 5043
    },
    {
      "epoch": 6.725333333333333,
      "grad_norm": 0.023931296542286873,
      "learning_rate": 0.0005116378117986062,
      "loss": 0.7965,
      "step": 5044
    },
    {
      "epoch": 6.726666666666667,
      "grad_norm": 0.03289395198225975,
      "learning_rate": 0.0005112610212409407,
      "loss": 0.8293,
      "step": 5045
    },
    {
      "epoch": 6.728,
      "grad_norm": 0.03549233078956604,
      "learning_rate": 0.000510884321823631,
      "loss": 1.1837,
      "step": 5046
    },
    {
      "epoch": 6.729333333333333,
      "grad_norm": 0.031816259026527405,
      "learning_rate": 0.0005105077136169242,
      "loss": 1.3181,
      "step": 5047
    },
    {
      "epoch": 6.730666666666667,
      "grad_norm": 0.025868218392133713,
      "learning_rate": 0.0005101311966910514,
      "loss": 0.8893,
      "step": 5048
    },
    {
      "epoch": 6.732,
      "grad_norm": 0.02136543206870556,
      "learning_rate": 0.0005097547711162242,
      "loss": 0.8782,
      "step": 5049
    },
    {
      "epoch": 6.733333333333333,
      "grad_norm": 0.01742776669561863,
      "learning_rate": 0.0005093784369626397,
      "loss": 0.8833,
      "step": 5050
    },
    {
      "epoch": 6.734666666666667,
      "grad_norm": 0.02683640643954277,
      "learning_rate": 0.0005090021943004765,
      "loss": 1.1949,
      "step": 5051
    },
    {
      "epoch": 6.736,
      "grad_norm": 0.025193247944116592,
      "learning_rate": 0.0005086260431998967,
      "loss": 0.9825,
      "step": 5052
    },
    {
      "epoch": 6.737333333333333,
      "grad_norm": 0.021889446303248405,
      "learning_rate": 0.0005082499837310457,
      "loss": 0.9527,
      "step": 5053
    },
    {
      "epoch": 6.738666666666667,
      "grad_norm": 0.022732971236109734,
      "learning_rate": 0.0005078740159640502,
      "loss": 0.9181,
      "step": 5054
    },
    {
      "epoch": 6.74,
      "grad_norm": 0.025769563391804695,
      "learning_rate": 0.0005074981399690218,
      "loss": 0.9826,
      "step": 5055
    },
    {
      "epoch": 6.741333333333333,
      "grad_norm": 0.03808482736349106,
      "learning_rate": 0.0005071223558160539,
      "loss": 0.9718,
      "step": 5056
    },
    {
      "epoch": 6.742666666666667,
      "grad_norm": 0.03169042989611626,
      "learning_rate": 0.0005067466635752227,
      "loss": 0.9144,
      "step": 5057
    },
    {
      "epoch": 6.744,
      "grad_norm": 0.02822526916861534,
      "learning_rate": 0.0005063710633165881,
      "loss": 1.0773,
      "step": 5058
    },
    {
      "epoch": 6.745333333333333,
      "grad_norm": 0.03489484265446663,
      "learning_rate": 0.0005059955551101917,
      "loss": 0.9649,
      "step": 5059
    },
    {
      "epoch": 6.746666666666667,
      "grad_norm": 0.026401765644550323,
      "learning_rate": 0.0005056201390260586,
      "loss": 0.9274,
      "step": 5060
    },
    {
      "epoch": 6.748,
      "grad_norm": 0.02285166271030903,
      "learning_rate": 0.0005052448151341968,
      "loss": 0.9849,
      "step": 5061
    },
    {
      "epoch": 6.749333333333333,
      "grad_norm": 0.02374003827571869,
      "learning_rate": 0.0005048695835045969,
      "loss": 0.9449,
      "step": 5062
    },
    {
      "epoch": 6.750666666666667,
      "grad_norm": 0.026635251939296722,
      "learning_rate": 0.0005044944442072326,
      "loss": 0.9518,
      "step": 5063
    },
    {
      "epoch": 6.752,
      "grad_norm": 0.0358525849878788,
      "learning_rate": 0.0005041193973120595,
      "loss": 0.8541,
      "step": 5064
    },
    {
      "epoch": 6.753333333333333,
      "grad_norm": 0.03625248372554779,
      "learning_rate": 0.0005037444428890169,
      "loss": 1.0121,
      "step": 5065
    },
    {
      "epoch": 6.754666666666667,
      "grad_norm": 0.025049831718206406,
      "learning_rate": 0.0005033695810080266,
      "loss": 0.9364,
      "step": 5066
    },
    {
      "epoch": 6.756,
      "grad_norm": 0.02279442921280861,
      "learning_rate": 0.000502994811738993,
      "loss": 0.9527,
      "step": 5067
    },
    {
      "epoch": 6.757333333333333,
      "grad_norm": 0.030393797904253006,
      "learning_rate": 0.0005026201351518033,
      "loss": 0.764,
      "step": 5068
    },
    {
      "epoch": 6.758666666666667,
      "grad_norm": 0.037077855318784714,
      "learning_rate": 0.0005022455513163276,
      "loss": 1.1294,
      "step": 5069
    },
    {
      "epoch": 6.76,
      "grad_norm": 0.02704480290412903,
      "learning_rate": 0.0005018710603024187,
      "loss": 0.9228,
      "step": 5070
    },
    {
      "epoch": 6.761333333333333,
      "grad_norm": 0.024912154302001,
      "learning_rate": 0.0005014966621799113,
      "loss": 1.1759,
      "step": 5071
    },
    {
      "epoch": 6.762666666666667,
      "grad_norm": 0.026373235508799553,
      "learning_rate": 0.0005011223570186237,
      "loss": 0.9782,
      "step": 5072
    },
    {
      "epoch": 6.764,
      "grad_norm": 0.024694588035345078,
      "learning_rate": 0.0005007481448883566,
      "loss": 0.8261,
      "step": 5073
    },
    {
      "epoch": 6.765333333333333,
      "grad_norm": 0.027529407292604446,
      "learning_rate": 0.0005003740258588933,
      "loss": 0.8898,
      "step": 5074
    },
    {
      "epoch": 6.766666666666667,
      "grad_norm": 0.025798005983233452,
      "learning_rate": 0.0005000000000000002,
      "loss": 0.8681,
      "step": 5075
    },
    {
      "epoch": 6.768,
      "grad_norm": 0.023815125226974487,
      "learning_rate": 0.0004996260673814251,
      "loss": 1.0338,
      "step": 5076
    },
    {
      "epoch": 6.769333333333333,
      "grad_norm": 0.023409321904182434,
      "learning_rate": 0.0004992522280728995,
      "loss": 1.1264,
      "step": 5077
    },
    {
      "epoch": 6.770666666666667,
      "grad_norm": 0.02259727567434311,
      "learning_rate": 0.0004988784821441372,
      "loss": 0.9961,
      "step": 5078
    },
    {
      "epoch": 6.772,
      "grad_norm": 0.021786067634820938,
      "learning_rate": 0.0004985048296648345,
      "loss": 1.2124,
      "step": 5079
    },
    {
      "epoch": 6.773333333333333,
      "grad_norm": 0.024816015735268593,
      "learning_rate": 0.0004981312707046711,
      "loss": 1.2901,
      "step": 5080
    },
    {
      "epoch": 6.774666666666667,
      "grad_norm": 0.02219233289361,
      "learning_rate": 0.0004977578053333073,
      "loss": 0.8727,
      "step": 5081
    },
    {
      "epoch": 6.776,
      "grad_norm": 0.03217346966266632,
      "learning_rate": 0.0004973844336203879,
      "loss": 1.0567,
      "step": 5082
    },
    {
      "epoch": 6.777333333333333,
      "grad_norm": 0.019074322655797005,
      "learning_rate": 0.0004970111556355392,
      "loss": 1.0719,
      "step": 5083
    },
    {
      "epoch": 6.778666666666666,
      "grad_norm": 0.02774745039641857,
      "learning_rate": 0.0004966379714483705,
      "loss": 1.0565,
      "step": 5084
    },
    {
      "epoch": 6.78,
      "grad_norm": 0.01996360719203949,
      "learning_rate": 0.0004962648811284737,
      "loss": 0.8054,
      "step": 5085
    },
    {
      "epoch": 6.781333333333333,
      "grad_norm": 0.02597975730895996,
      "learning_rate": 0.0004958918847454224,
      "loss": 1.1615,
      "step": 5086
    },
    {
      "epoch": 6.782666666666667,
      "grad_norm": 0.026374677196145058,
      "learning_rate": 0.0004955189823687733,
      "loss": 1.1398,
      "step": 5087
    },
    {
      "epoch": 6.784,
      "grad_norm": 0.02323174849152565,
      "learning_rate": 0.0004951461740680655,
      "loss": 1.0936,
      "step": 5088
    },
    {
      "epoch": 6.785333333333333,
      "grad_norm": 0.03725454583764076,
      "learning_rate": 0.0004947734599128206,
      "loss": 1.0762,
      "step": 5089
    },
    {
      "epoch": 6.786666666666667,
      "grad_norm": 0.032805200666189194,
      "learning_rate": 0.000494400839972543,
      "loss": 0.9962,
      "step": 5090
    },
    {
      "epoch": 6.788,
      "grad_norm": 0.01977808214724064,
      "learning_rate": 0.0004940283143167184,
      "loss": 0.7902,
      "step": 5091
    },
    {
      "epoch": 6.789333333333333,
      "grad_norm": 0.02435571700334549,
      "learning_rate": 0.0004936558830148157,
      "loss": 1.0854,
      "step": 5092
    },
    {
      "epoch": 6.790666666666667,
      "grad_norm": 0.022920673713088036,
      "learning_rate": 0.0004932835461362864,
      "loss": 0.9695,
      "step": 5093
    },
    {
      "epoch": 6.792,
      "grad_norm": 0.0354737751185894,
      "learning_rate": 0.0004929113037505642,
      "loss": 1.0653,
      "step": 5094
    },
    {
      "epoch": 6.793333333333333,
      "grad_norm": 0.02465805597603321,
      "learning_rate": 0.0004925391559270652,
      "loss": 1.4583,
      "step": 5095
    },
    {
      "epoch": 6.794666666666666,
      "grad_norm": 0.027629729360342026,
      "learning_rate": 0.0004921671027351866,
      "loss": 1.1569,
      "step": 5096
    },
    {
      "epoch": 6.796,
      "grad_norm": 0.02163921296596527,
      "learning_rate": 0.0004917951442443109,
      "loss": 0.8553,
      "step": 5097
    },
    {
      "epoch": 6.7973333333333334,
      "grad_norm": 0.02570558339357376,
      "learning_rate": 0.0004914232805237999,
      "loss": 1.1741,
      "step": 5098
    },
    {
      "epoch": 6.798666666666667,
      "grad_norm": 0.021747129037976265,
      "learning_rate": 0.0004910515116429993,
      "loss": 0.8607,
      "step": 5099
    },
    {
      "epoch": 6.8,
      "grad_norm": 0.020369822159409523,
      "learning_rate": 0.0004906798376712373,
      "loss": 0.839,
      "step": 5100
    },
    {
      "epoch": 6.801333333333333,
      "grad_norm": 0.02664550580084324,
      "learning_rate": 0.0004903082586778227,
      "loss": 0.9954,
      "step": 5101
    },
    {
      "epoch": 6.802666666666667,
      "grad_norm": 0.023700235411524773,
      "learning_rate": 0.0004899367747320493,
      "loss": 1.0081,
      "step": 5102
    },
    {
      "epoch": 6.804,
      "grad_norm": 0.024685174226760864,
      "learning_rate": 0.0004895653859031906,
      "loss": 1.16,
      "step": 5103
    },
    {
      "epoch": 6.8053333333333335,
      "grad_norm": 0.026887329295277596,
      "learning_rate": 0.0004891940922605036,
      "loss": 1.2565,
      "step": 5104
    },
    {
      "epoch": 6.806666666666667,
      "grad_norm": 0.02192576229572296,
      "learning_rate": 0.0004888228938732279,
      "loss": 1.22,
      "step": 5105
    },
    {
      "epoch": 6.808,
      "grad_norm": 0.030069034546613693,
      "learning_rate": 0.0004884517908105837,
      "loss": 0.9027,
      "step": 5106
    },
    {
      "epoch": 6.809333333333333,
      "grad_norm": 0.027560044080018997,
      "learning_rate": 0.00048808078314177593,
      "loss": 0.8799,
      "step": 5107
    },
    {
      "epoch": 6.810666666666666,
      "grad_norm": 0.05206388235092163,
      "learning_rate": 0.0004877098709359894,
      "loss": 1.1729,
      "step": 5108
    },
    {
      "epoch": 6.812,
      "grad_norm": 0.024485256522893906,
      "learning_rate": 0.0004873390542623922,
      "loss": 1.0971,
      "step": 5109
    },
    {
      "epoch": 6.8133333333333335,
      "grad_norm": 0.024007556959986687,
      "learning_rate": 0.000486968333190135,
      "loss": 0.8997,
      "step": 5110
    },
    {
      "epoch": 6.814666666666667,
      "grad_norm": 0.02374383620917797,
      "learning_rate": 0.000486597707788349,
      "loss": 0.8995,
      "step": 5111
    },
    {
      "epoch": 6.816,
      "grad_norm": 0.02558455988764763,
      "learning_rate": 0.00048622717812615005,
      "loss": 0.7265,
      "step": 5112
    },
    {
      "epoch": 6.817333333333333,
      "grad_norm": 0.031851138919591904,
      "learning_rate": 0.0004858567442726336,
      "loss": 0.981,
      "step": 5113
    },
    {
      "epoch": 6.818666666666667,
      "grad_norm": 0.032796092331409454,
      "learning_rate": 0.00048548640629687877,
      "loss": 0.8114,
      "step": 5114
    },
    {
      "epoch": 6.82,
      "grad_norm": 0.033424779772758484,
      "learning_rate": 0.0004851161642679466,
      "loss": 1.0646,
      "step": 5115
    },
    {
      "epoch": 6.8213333333333335,
      "grad_norm": 0.024613771587610245,
      "learning_rate": 0.00048474601825487996,
      "loss": 1.2288,
      "step": 5116
    },
    {
      "epoch": 6.822666666666667,
      "grad_norm": 0.01964566297829151,
      "learning_rate": 0.0004843759683267043,
      "loss": 0.7801,
      "step": 5117
    },
    {
      "epoch": 6.824,
      "grad_norm": 0.02407125011086464,
      "learning_rate": 0.00048400601455242544,
      "loss": 1.0466,
      "step": 5118
    },
    {
      "epoch": 6.825333333333333,
      "grad_norm": 0.01986776851117611,
      "learning_rate": 0.00048363615700103437,
      "loss": 1.0643,
      "step": 5119
    },
    {
      "epoch": 6.826666666666666,
      "grad_norm": 0.026877041906118393,
      "learning_rate": 0.00048326639574150125,
      "loss": 1.1678,
      "step": 5120
    },
    {
      "epoch": 6.828,
      "grad_norm": 0.024884650483727455,
      "learning_rate": 0.00048289673084277954,
      "loss": 0.8746,
      "step": 5121
    },
    {
      "epoch": 6.8293333333333335,
      "grad_norm": 0.02613532543182373,
      "learning_rate": 0.000482527162373805,
      "loss": 0.988,
      "step": 5122
    },
    {
      "epoch": 6.830666666666667,
      "grad_norm": 0.02381063997745514,
      "learning_rate": 0.000482157690403494,
      "loss": 0.9769,
      "step": 5123
    },
    {
      "epoch": 6.832,
      "grad_norm": 0.02257455326616764,
      "learning_rate": 0.00048178831500074736,
      "loss": 0.9323,
      "step": 5124
    },
    {
      "epoch": 6.833333333333333,
      "grad_norm": 0.023665649816393852,
      "learning_rate": 0.00048141903623444537,
      "loss": 0.7569,
      "step": 5125
    },
    {
      "epoch": 6.834666666666667,
      "grad_norm": 0.025888638570904732,
      "learning_rate": 0.0004810498541734517,
      "loss": 0.8721,
      "step": 5126
    },
    {
      "epoch": 6.836,
      "grad_norm": 0.02257746458053589,
      "learning_rate": 0.0004806807688866119,
      "loss": 0.8891,
      "step": 5127
    },
    {
      "epoch": 6.8373333333333335,
      "grad_norm": 0.02047012373805046,
      "learning_rate": 0.0004803117804427524,
      "loss": 0.9608,
      "step": 5128
    },
    {
      "epoch": 6.838666666666667,
      "grad_norm": 0.022362155839800835,
      "learning_rate": 0.00047994288891068347,
      "loss": 1.0963,
      "step": 5129
    },
    {
      "epoch": 6.84,
      "grad_norm": 0.02909843623638153,
      "learning_rate": 0.0004795740943591955,
      "loss": 0.995,
      "step": 5130
    },
    {
      "epoch": 6.841333333333333,
      "grad_norm": 0.03193124011158943,
      "learning_rate": 0.00047920539685706177,
      "loss": 1.0022,
      "step": 5131
    },
    {
      "epoch": 6.842666666666666,
      "grad_norm": 0.02882908284664154,
      "learning_rate": 0.0004788367964730376,
      "loss": 1.0468,
      "step": 5132
    },
    {
      "epoch": 6.844,
      "grad_norm": 0.024780625477433205,
      "learning_rate": 0.00047846829327585874,
      "loss": 1.0276,
      "step": 5133
    },
    {
      "epoch": 6.8453333333333335,
      "grad_norm": 0.02753814496099949,
      "learning_rate": 0.00047809988733424526,
      "loss": 0.7143,
      "step": 5134
    },
    {
      "epoch": 6.846666666666667,
      "grad_norm": 0.029758719727396965,
      "learning_rate": 0.0004777315787168968,
      "loss": 1.2416,
      "step": 5135
    },
    {
      "epoch": 6.848,
      "grad_norm": 0.033762529492378235,
      "learning_rate": 0.000477363367492496,
      "loss": 1.036,
      "step": 5136
    },
    {
      "epoch": 6.849333333333333,
      "grad_norm": 0.02049962803721428,
      "learning_rate": 0.0004769952537297075,
      "loss": 0.9361,
      "step": 5137
    },
    {
      "epoch": 6.850666666666667,
      "grad_norm": 0.026538804173469543,
      "learning_rate": 0.0004766272374971764,
      "loss": 0.8994,
      "step": 5138
    },
    {
      "epoch": 6.852,
      "grad_norm": 0.023213060572743416,
      "learning_rate": 0.00047625931886353213,
      "loss": 1.0143,
      "step": 5139
    },
    {
      "epoch": 6.8533333333333335,
      "grad_norm": 0.024364199489355087,
      "learning_rate": 0.0004758914978973831,
      "loss": 0.9551,
      "step": 5140
    },
    {
      "epoch": 6.854666666666667,
      "grad_norm": 0.02371937409043312,
      "learning_rate": 0.0004755237746673212,
      "loss": 1.0999,
      "step": 5141
    },
    {
      "epoch": 6.856,
      "grad_norm": 0.027299268171191216,
      "learning_rate": 0.00047515614924192016,
      "loss": 1.4193,
      "step": 5142
    },
    {
      "epoch": 6.857333333333333,
      "grad_norm": 0.03566505387425423,
      "learning_rate": 0.0004747886216897338,
      "loss": 1.0633,
      "step": 5143
    },
    {
      "epoch": 6.858666666666666,
      "grad_norm": 0.027740636840462685,
      "learning_rate": 0.0004744211920793005,
      "loss": 1.0995,
      "step": 5144
    },
    {
      "epoch": 6.86,
      "grad_norm": 0.022706713527441025,
      "learning_rate": 0.000474053860479137,
      "loss": 0.7247,
      "step": 5145
    },
    {
      "epoch": 6.8613333333333335,
      "grad_norm": 0.028211528435349464,
      "learning_rate": 0.00047368662695774534,
      "loss": 1.2539,
      "step": 5146
    },
    {
      "epoch": 6.862666666666667,
      "grad_norm": 0.024948375299572945,
      "learning_rate": 0.0004733194915836062,
      "loss": 0.6386,
      "step": 5147
    },
    {
      "epoch": 6.864,
      "grad_norm": 0.025524858385324478,
      "learning_rate": 0.00047295245442518363,
      "loss": 0.7872,
      "step": 5148
    },
    {
      "epoch": 6.865333333333333,
      "grad_norm": 0.027875764295458794,
      "learning_rate": 0.0004725855155509233,
      "loss": 1.3396,
      "step": 5149
    },
    {
      "epoch": 6.866666666666667,
      "grad_norm": 0.03653766214847565,
      "learning_rate": 0.0004722186750292511,
      "loss": 1.5282,
      "step": 5150
    },
    {
      "epoch": 6.868,
      "grad_norm": 0.035877570509910583,
      "learning_rate": 0.0004718519329285771,
      "loss": 1.2046,
      "step": 5151
    },
    {
      "epoch": 6.8693333333333335,
      "grad_norm": 0.025746913626790047,
      "learning_rate": 0.0004714852893172905,
      "loss": 1.2008,
      "step": 5152
    },
    {
      "epoch": 6.870666666666667,
      "grad_norm": 0.023135574534535408,
      "learning_rate": 0.00047111874426376376,
      "loss": 0.8286,
      "step": 5153
    },
    {
      "epoch": 6.872,
      "grad_norm": 0.02369859255850315,
      "learning_rate": 0.00047075229783635075,
      "loss": 0.9219,
      "step": 5154
    },
    {
      "epoch": 6.873333333333333,
      "grad_norm": 0.031435009092092514,
      "learning_rate": 0.0004703859501033854,
      "loss": 1.0405,
      "step": 5155
    },
    {
      "epoch": 6.874666666666666,
      "grad_norm": 0.031140191480517387,
      "learning_rate": 0.0004700197011331859,
      "loss": 1.2642,
      "step": 5156
    },
    {
      "epoch": 6.876,
      "grad_norm": 0.02052162028849125,
      "learning_rate": 0.0004696535509940498,
      "loss": 1.1775,
      "step": 5157
    },
    {
      "epoch": 6.8773333333333335,
      "grad_norm": 0.02848617359995842,
      "learning_rate": 0.0004692874997542571,
      "loss": 0.9653,
      "step": 5158
    },
    {
      "epoch": 6.878666666666667,
      "grad_norm": 0.02275417558848858,
      "learning_rate": 0.00046892154748206974,
      "loss": 0.6617,
      "step": 5159
    },
    {
      "epoch": 6.88,
      "grad_norm": 0.028425971046090126,
      "learning_rate": 0.00046855569424572954,
      "loss": 1.115,
      "step": 5160
    },
    {
      "epoch": 6.881333333333333,
      "grad_norm": 0.024080494418740273,
      "learning_rate": 0.0004681899401134626,
      "loss": 0.9796,
      "step": 5161
    },
    {
      "epoch": 6.882666666666667,
      "grad_norm": 0.024644557386636734,
      "learning_rate": 0.00046782428515347395,
      "loss": 1.1031,
      "step": 5162
    },
    {
      "epoch": 6.884,
      "grad_norm": 0.02594633959233761,
      "learning_rate": 0.0004674587294339513,
      "loss": 0.7837,
      "step": 5163
    },
    {
      "epoch": 6.8853333333333335,
      "grad_norm": 0.023778604343533516,
      "learning_rate": 0.00046709327302306436,
      "loss": 1.0775,
      "step": 5164
    },
    {
      "epoch": 6.886666666666667,
      "grad_norm": 0.03274765610694885,
      "learning_rate": 0.00046672791598896237,
      "loss": 1.2482,
      "step": 5165
    },
    {
      "epoch": 6.888,
      "grad_norm": 0.023593585938215256,
      "learning_rate": 0.0004663626583997789,
      "loss": 0.8381,
      "step": 5166
    },
    {
      "epoch": 6.889333333333333,
      "grad_norm": 0.029820013791322708,
      "learning_rate": 0.0004659975003236262,
      "loss": 1.1447,
      "step": 5167
    },
    {
      "epoch": 6.890666666666666,
      "grad_norm": 0.02540801651775837,
      "learning_rate": 0.00046563244182859975,
      "loss": 1.0623,
      "step": 5168
    },
    {
      "epoch": 6.892,
      "grad_norm": 0.0253730621188879,
      "learning_rate": 0.0004652674829827761,
      "loss": 1.028,
      "step": 5169
    },
    {
      "epoch": 6.8933333333333335,
      "grad_norm": 0.03236430510878563,
      "learning_rate": 0.000464902623854212,
      "loss": 0.9499,
      "step": 5170
    },
    {
      "epoch": 6.894666666666667,
      "grad_norm": 0.024229582399129868,
      "learning_rate": 0.0004645378645109479,
      "loss": 1.1582,
      "step": 5171
    },
    {
      "epoch": 6.896,
      "grad_norm": 0.02198765054345131,
      "learning_rate": 0.0004641732050210031,
      "loss": 1.149,
      "step": 5172
    },
    {
      "epoch": 6.897333333333333,
      "grad_norm": 0.024284489452838898,
      "learning_rate": 0.000463808645452381,
      "loss": 1.1452,
      "step": 5173
    },
    {
      "epoch": 6.898666666666666,
      "grad_norm": 0.032744280993938446,
      "learning_rate": 0.0004634441858730637,
      "loss": 1.1612,
      "step": 5174
    },
    {
      "epoch": 6.9,
      "grad_norm": 0.023106254637241364,
      "learning_rate": 0.0004630798263510162,
      "loss": 0.909,
      "step": 5175
    },
    {
      "epoch": 6.9013333333333335,
      "grad_norm": 0.031634364277124405,
      "learning_rate": 0.0004627155669541848,
      "loss": 1.2165,
      "step": 5176
    },
    {
      "epoch": 6.902666666666667,
      "grad_norm": 0.027972254902124405,
      "learning_rate": 0.000462351407750496,
      "loss": 0.8245,
      "step": 5177
    },
    {
      "epoch": 6.904,
      "grad_norm": 0.02937210537493229,
      "learning_rate": 0.0004619873488078596,
      "loss": 0.9213,
      "step": 5178
    },
    {
      "epoch": 6.905333333333333,
      "grad_norm": 0.025981754064559937,
      "learning_rate": 0.0004616233901941648,
      "loss": 1.2442,
      "step": 5179
    },
    {
      "epoch": 6.906666666666666,
      "grad_norm": 0.020180296152830124,
      "learning_rate": 0.00046125953197728275,
      "loss": 0.9865,
      "step": 5180
    },
    {
      "epoch": 6.908,
      "grad_norm": 0.04998844116926193,
      "learning_rate": 0.00046089577422506666,
      "loss": 1.2132,
      "step": 5181
    },
    {
      "epoch": 6.9093333333333335,
      "grad_norm": 0.04672222211956978,
      "learning_rate": 0.00046053211700534903,
      "loss": 1.1422,
      "step": 5182
    },
    {
      "epoch": 6.910666666666667,
      "grad_norm": 0.028880175203084946,
      "learning_rate": 0.0004601685603859466,
      "loss": 0.9637,
      "step": 5183
    },
    {
      "epoch": 6.912,
      "grad_norm": 0.026111409068107605,
      "learning_rate": 0.00045980510443465415,
      "loss": 0.9477,
      "step": 5184
    },
    {
      "epoch": 6.913333333333333,
      "grad_norm": 0.020983923226594925,
      "learning_rate": 0.00045944174921924995,
      "loss": 1.2734,
      "step": 5185
    },
    {
      "epoch": 6.914666666666666,
      "grad_norm": 0.026085015386343002,
      "learning_rate": 0.00045907849480749286,
      "loss": 1.0197,
      "step": 5186
    },
    {
      "epoch": 6.916,
      "grad_norm": 0.017569750547409058,
      "learning_rate": 0.00045871534126712166,
      "loss": 0.8796,
      "step": 5187
    },
    {
      "epoch": 6.917333333333334,
      "grad_norm": 0.02021324820816517,
      "learning_rate": 0.000458352288665859,
      "loss": 0.9578,
      "step": 5188
    },
    {
      "epoch": 6.918666666666667,
      "grad_norm": 0.019068555906414986,
      "learning_rate": 0.0004579893370714061,
      "loss": 0.8294,
      "step": 5189
    },
    {
      "epoch": 6.92,
      "grad_norm": 0.030155081301927567,
      "learning_rate": 0.0004576264865514467,
      "loss": 0.957,
      "step": 5190
    },
    {
      "epoch": 6.921333333333333,
      "grad_norm": 0.03816820681095123,
      "learning_rate": 0.00045726373717364576,
      "loss": 0.9329,
      "step": 5191
    },
    {
      "epoch": 6.922666666666666,
      "grad_norm": 0.029737472534179688,
      "learning_rate": 0.00045690108900564797,
      "loss": 1.3114,
      "step": 5192
    },
    {
      "epoch": 6.924,
      "grad_norm": 0.022516125813126564,
      "learning_rate": 0.0004565385421150816,
      "loss": 1.0588,
      "step": 5193
    },
    {
      "epoch": 6.925333333333334,
      "grad_norm": 0.026104867458343506,
      "learning_rate": 0.0004561760965695536,
      "loss": 0.9512,
      "step": 5194
    },
    {
      "epoch": 6.926666666666667,
      "grad_norm": 0.029914673417806625,
      "learning_rate": 0.00045581375243665336,
      "loss": 0.9955,
      "step": 5195
    },
    {
      "epoch": 6.928,
      "grad_norm": 0.034175705164670944,
      "learning_rate": 0.000455451509783951,
      "loss": 0.9875,
      "step": 5196
    },
    {
      "epoch": 6.929333333333333,
      "grad_norm": 0.04684366658329964,
      "learning_rate": 0.00045508936867899775,
      "loss": 0.8885,
      "step": 5197
    },
    {
      "epoch": 6.930666666666666,
      "grad_norm": 0.02501441165804863,
      "learning_rate": 0.00045472732918932644,
      "loss": 0.8423,
      "step": 5198
    },
    {
      "epoch": 6.932,
      "grad_norm": 0.028126485645771027,
      "learning_rate": 0.0004543653913824496,
      "loss": 1.4615,
      "step": 5199
    },
    {
      "epoch": 6.933333333333334,
      "grad_norm": 0.027778293937444687,
      "learning_rate": 0.0004540035553258619,
      "loss": 0.9044,
      "step": 5200
    },
    {
      "epoch": 6.934666666666667,
      "grad_norm": 0.029531806707382202,
      "learning_rate": 0.00045364182108703887,
      "loss": 1.084,
      "step": 5201
    },
    {
      "epoch": 6.936,
      "grad_norm": 0.02231726050376892,
      "learning_rate": 0.000453280188733437,
      "loss": 1.1311,
      "step": 5202
    },
    {
      "epoch": 6.937333333333333,
      "grad_norm": 0.02867794781923294,
      "learning_rate": 0.0004529186583324939,
      "loss": 1.0637,
      "step": 5203
    },
    {
      "epoch": 6.938666666666666,
      "grad_norm": 0.020561853423714638,
      "learning_rate": 0.00045255722995162694,
      "loss": 0.9428,
      "step": 5204
    },
    {
      "epoch": 6.9399999999999995,
      "grad_norm": 0.02531730942428112,
      "learning_rate": 0.0004521959036582372,
      "loss": 1.0691,
      "step": 5205
    },
    {
      "epoch": 6.941333333333334,
      "grad_norm": 0.023967014625668526,
      "learning_rate": 0.0004518346795197036,
      "loss": 0.8985,
      "step": 5206
    },
    {
      "epoch": 6.942666666666667,
      "grad_norm": 0.02060236595571041,
      "learning_rate": 0.00045147355760338803,
      "loss": 0.9432,
      "step": 5207
    },
    {
      "epoch": 6.944,
      "grad_norm": 0.02755875699222088,
      "learning_rate": 0.0004511125379766331,
      "loss": 1.1066,
      "step": 5208
    },
    {
      "epoch": 6.945333333333333,
      "grad_norm": 0.027391569688916206,
      "learning_rate": 0.0004507516207067608,
      "loss": 1.1547,
      "step": 5209
    },
    {
      "epoch": 6.946666666666666,
      "grad_norm": 0.0322924368083477,
      "learning_rate": 0.0004503908058610767,
      "loss": 1.0999,
      "step": 5210
    },
    {
      "epoch": 6.948,
      "grad_norm": 0.025081897154450417,
      "learning_rate": 0.00045003009350686477,
      "loss": 1.1035,
      "step": 5211
    },
    {
      "epoch": 6.949333333333334,
      "grad_norm": 0.02679472044110298,
      "learning_rate": 0.00044966948371139107,
      "loss": 1.093,
      "step": 5212
    },
    {
      "epoch": 6.950666666666667,
      "grad_norm": 0.027621641755104065,
      "learning_rate": 0.00044930897654190283,
      "loss": 1.1241,
      "step": 5213
    },
    {
      "epoch": 6.952,
      "grad_norm": 0.02729925885796547,
      "learning_rate": 0.00044894857206562654,
      "loss": 1.3301,
      "step": 5214
    },
    {
      "epoch": 6.953333333333333,
      "grad_norm": 0.026733551174402237,
      "learning_rate": 0.0004485882703497721,
      "loss": 0.8667,
      "step": 5215
    },
    {
      "epoch": 6.954666666666666,
      "grad_norm": 0.02837950736284256,
      "learning_rate": 0.0004482280714615278,
      "loss": 1.081,
      "step": 5216
    },
    {
      "epoch": 6.9559999999999995,
      "grad_norm": 0.024823948740959167,
      "learning_rate": 0.0004478679754680639,
      "loss": 0.8727,
      "step": 5217
    },
    {
      "epoch": 6.957333333333334,
      "grad_norm": 0.018784979358315468,
      "learning_rate": 0.000447507982436532,
      "loss": 1.0394,
      "step": 5218
    },
    {
      "epoch": 6.958666666666667,
      "grad_norm": 0.026689205318689346,
      "learning_rate": 0.0004471480924340625,
      "loss": 0.7441,
      "step": 5219
    },
    {
      "epoch": 6.96,
      "grad_norm": 0.036784663796424866,
      "learning_rate": 0.0004467883055277695,
      "loss": 0.8181,
      "step": 5220
    },
    {
      "epoch": 6.961333333333333,
      "grad_norm": 0.019522517919540405,
      "learning_rate": 0.00044642862178474507,
      "loss": 1.1604,
      "step": 5221
    },
    {
      "epoch": 6.962666666666666,
      "grad_norm": 0.021052276715636253,
      "learning_rate": 0.00044606904127206373,
      "loss": 1.1973,
      "step": 5222
    },
    {
      "epoch": 6.964,
      "grad_norm": 0.019896125420928,
      "learning_rate": 0.0004457095640567803,
      "loss": 0.7937,
      "step": 5223
    },
    {
      "epoch": 6.965333333333334,
      "grad_norm": 0.02318909764289856,
      "learning_rate": 0.0004453501902059303,
      "loss": 0.9218,
      "step": 5224
    },
    {
      "epoch": 6.966666666666667,
      "grad_norm": 0.022914357483386993,
      "learning_rate": 0.0004449909197865303,
      "loss": 1.1144,
      "step": 5225
    },
    {
      "epoch": 6.968,
      "grad_norm": 0.023264529183506966,
      "learning_rate": 0.00044463175286557654,
      "loss": 1.1882,
      "step": 5226
    },
    {
      "epoch": 6.969333333333333,
      "grad_norm": 0.02454380691051483,
      "learning_rate": 0.00044427268951004715,
      "loss": 0.8273,
      "step": 5227
    },
    {
      "epoch": 6.970666666666666,
      "grad_norm": 0.020868387073278427,
      "learning_rate": 0.0004439137297869005,
      "loss": 1.2291,
      "step": 5228
    },
    {
      "epoch": 6.9719999999999995,
      "grad_norm": 0.02687128819525242,
      "learning_rate": 0.0004435548737630756,
      "loss": 0.8779,
      "step": 5229
    },
    {
      "epoch": 6.973333333333334,
      "grad_norm": 0.02277384325861931,
      "learning_rate": 0.0004431961215054924,
      "loss": 1.1573,
      "step": 5230
    },
    {
      "epoch": 6.974666666666667,
      "grad_norm": 0.03228224441409111,
      "learning_rate": 0.0004428374730810506,
      "loss": 1.2049,
      "step": 5231
    },
    {
      "epoch": 6.976,
      "grad_norm": 0.027007710188627243,
      "learning_rate": 0.00044247892855663165,
      "loss": 0.9812,
      "step": 5232
    },
    {
      "epoch": 6.977333333333333,
      "grad_norm": 0.02929583564400673,
      "learning_rate": 0.00044212048799909697,
      "loss": 1.1618,
      "step": 5233
    },
    {
      "epoch": 6.978666666666666,
      "grad_norm": 0.026784123852849007,
      "learning_rate": 0.00044176215147528897,
      "loss": 0.9066,
      "step": 5234
    },
    {
      "epoch": 6.98,
      "grad_norm": 0.02701234072446823,
      "learning_rate": 0.0004414039190520308,
      "loss": 0.9686,
      "step": 5235
    },
    {
      "epoch": 6.981333333333334,
      "grad_norm": 0.02031688764691353,
      "learning_rate": 0.0004410457907961251,
      "loss": 0.8586,
      "step": 5236
    },
    {
      "epoch": 6.982666666666667,
      "grad_norm": 0.021108992397785187,
      "learning_rate": 0.0004406877667743561,
      "loss": 0.7268,
      "step": 5237
    },
    {
      "epoch": 6.984,
      "grad_norm": 0.02793939784169197,
      "learning_rate": 0.00044032984705348843,
      "loss": 1.1059,
      "step": 5238
    },
    {
      "epoch": 6.985333333333333,
      "grad_norm": 0.039460036903619766,
      "learning_rate": 0.00043997203170026737,
      "loss": 0.9952,
      "step": 5239
    },
    {
      "epoch": 6.986666666666666,
      "grad_norm": 0.03060462884604931,
      "learning_rate": 0.00043961432078141874,
      "loss": 1.3209,
      "step": 5240
    },
    {
      "epoch": 6.9879999999999995,
      "grad_norm": 0.026469646021723747,
      "learning_rate": 0.000439256714363648,
      "loss": 0.969,
      "step": 5241
    },
    {
      "epoch": 6.989333333333334,
      "grad_norm": 0.03842467442154884,
      "learning_rate": 0.0004388992125136422,
      "loss": 0.9993,
      "step": 5242
    },
    {
      "epoch": 6.990666666666667,
      "grad_norm": 0.03952547162771225,
      "learning_rate": 0.00043854181529806845,
      "loss": 1.2386,
      "step": 5243
    },
    {
      "epoch": 6.992,
      "grad_norm": 0.034478798508644104,
      "learning_rate": 0.00043818452278357445,
      "loss": 1.1171,
      "step": 5244
    },
    {
      "epoch": 6.993333333333333,
      "grad_norm": 0.026832303032279015,
      "learning_rate": 0.00043782733503678885,
      "loss": 0.9461,
      "step": 5245
    },
    {
      "epoch": 6.994666666666666,
      "grad_norm": 0.02393507957458496,
      "learning_rate": 0.0004374702521243192,
      "loss": 0.856,
      "step": 5246
    },
    {
      "epoch": 6.996,
      "grad_norm": 0.02178475446999073,
      "learning_rate": 0.00043711327411275527,
      "loss": 1.2991,
      "step": 5247
    },
    {
      "epoch": 6.997333333333334,
      "grad_norm": 0.029149161651730537,
      "learning_rate": 0.0004367564010686662,
      "loss": 0.8451,
      "step": 5248
    },
    {
      "epoch": 6.998666666666667,
      "grad_norm": 0.051189132034778595,
      "learning_rate": 0.0004363996330586022,
      "loss": 1.0163,
      "step": 5249
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.022531140595674515,
      "learning_rate": 0.0004360429701490934,
      "loss": 0.9787,
      "step": 5250
    },
    {
      "epoch": 7.001333333333333,
      "grad_norm": 0.024562394246459007,
      "learning_rate": 0.0004356864124066506,
      "loss": 0.731,
      "step": 5251
    },
    {
      "epoch": 7.002666666666666,
      "grad_norm": 0.023757725954055786,
      "learning_rate": 0.00043532995989776536,
      "loss": 1.02,
      "step": 5252
    },
    {
      "epoch": 7.004,
      "grad_norm": 0.026505840942263603,
      "learning_rate": 0.00043497361268890835,
      "loss": 0.891,
      "step": 5253
    },
    {
      "epoch": 7.005333333333334,
      "grad_norm": 0.030179811641573906,
      "learning_rate": 0.00043461737084653175,
      "loss": 1.0795,
      "step": 5254
    },
    {
      "epoch": 7.006666666666667,
      "grad_norm": 0.025015858933329582,
      "learning_rate": 0.00043426123443706776,
      "loss": 0.8793,
      "step": 5255
    },
    {
      "epoch": 7.008,
      "grad_norm": 0.023137612268328667,
      "learning_rate": 0.0004339052035269291,
      "loss": 0.7588,
      "step": 5256
    },
    {
      "epoch": 7.009333333333333,
      "grad_norm": 0.028576785698533058,
      "learning_rate": 0.0004335492781825088,
      "loss": 1.2957,
      "step": 5257
    },
    {
      "epoch": 7.010666666666666,
      "grad_norm": 0.027709169313311577,
      "learning_rate": 0.0004331934584701794,
      "loss": 1.1591,
      "step": 5258
    },
    {
      "epoch": 7.012,
      "grad_norm": 0.024983279407024384,
      "learning_rate": 0.0004328377444562948,
      "loss": 0.9878,
      "step": 5259
    },
    {
      "epoch": 7.013333333333334,
      "grad_norm": 0.02690194360911846,
      "learning_rate": 0.0004324821362071887,
      "loss": 1.1158,
      "step": 5260
    },
    {
      "epoch": 7.014666666666667,
      "grad_norm": 0.020785711705684662,
      "learning_rate": 0.0004321266337891752,
      "loss": 1.0752,
      "step": 5261
    },
    {
      "epoch": 7.016,
      "grad_norm": 0.023929806426167488,
      "learning_rate": 0.00043177123726854895,
      "loss": 0.8637,
      "step": 5262
    },
    {
      "epoch": 7.017333333333333,
      "grad_norm": 0.026225287467241287,
      "learning_rate": 0.0004314159467115837,
      "loss": 0.9089,
      "step": 5263
    },
    {
      "epoch": 7.018666666666666,
      "grad_norm": 0.022210387513041496,
      "learning_rate": 0.0004310607621845348,
      "loss": 0.9917,
      "step": 5264
    },
    {
      "epoch": 7.02,
      "grad_norm": 0.03145777806639671,
      "learning_rate": 0.0004307056837536373,
      "loss": 0.9949,
      "step": 5265
    },
    {
      "epoch": 7.021333333333334,
      "grad_norm": 0.02197377383708954,
      "learning_rate": 0.0004303507114851062,
      "loss": 1.0735,
      "step": 5266
    },
    {
      "epoch": 7.022666666666667,
      "grad_norm": 0.051175523549318314,
      "learning_rate": 0.0004299958454451376,
      "loss": 1.1936,
      "step": 5267
    },
    {
      "epoch": 7.024,
      "grad_norm": 0.022307178005576134,
      "learning_rate": 0.00042964108569990624,
      "loss": 1.0346,
      "step": 5268
    },
    {
      "epoch": 7.025333333333333,
      "grad_norm": 0.028089722618460655,
      "learning_rate": 0.0004292864323155684,
      "loss": 0.7877,
      "step": 5269
    },
    {
      "epoch": 7.026666666666666,
      "grad_norm": 0.029266370460391045,
      "learning_rate": 0.00042893188535826,
      "loss": 0.657,
      "step": 5270
    },
    {
      "epoch": 7.028,
      "grad_norm": 0.029547814279794693,
      "learning_rate": 0.00042857744489409723,
      "loss": 1.204,
      "step": 5271
    },
    {
      "epoch": 7.029333333333334,
      "grad_norm": 0.038544636219739914,
      "learning_rate": 0.00042822311098917644,
      "loss": 1.1587,
      "step": 5272
    },
    {
      "epoch": 7.030666666666667,
      "grad_norm": 0.02898797206580639,
      "learning_rate": 0.00042786888370957424,
      "loss": 0.7629,
      "step": 5273
    },
    {
      "epoch": 7.032,
      "grad_norm": 0.025546332821249962,
      "learning_rate": 0.00042751476312134654,
      "loss": 1.0429,
      "step": 5274
    },
    {
      "epoch": 7.033333333333333,
      "grad_norm": 0.021601051092147827,
      "learning_rate": 0.0004271607492905303,
      "loss": 0.9486,
      "step": 5275
    },
    {
      "epoch": 7.034666666666666,
      "grad_norm": 0.02377179078757763,
      "learning_rate": 0.0004268068422831423,
      "loss": 0.959,
      "step": 5276
    },
    {
      "epoch": 7.036,
      "grad_norm": 0.02515566721558571,
      "learning_rate": 0.0004264530421651792,
      "loss": 1.0999,
      "step": 5277
    },
    {
      "epoch": 7.037333333333334,
      "grad_norm": 0.021980581805109978,
      "learning_rate": 0.00042609934900261846,
      "loss": 0.9496,
      "step": 5278
    },
    {
      "epoch": 7.038666666666667,
      "grad_norm": 0.024053988978266716,
      "learning_rate": 0.00042574576286141607,
      "loss": 1.0037,
      "step": 5279
    },
    {
      "epoch": 7.04,
      "grad_norm": 0.028795333579182625,
      "learning_rate": 0.00042539228380750947,
      "loss": 0.9895,
      "step": 5280
    },
    {
      "epoch": 7.041333333333333,
      "grad_norm": 0.02969871647655964,
      "learning_rate": 0.00042503891190681566,
      "loss": 1.0731,
      "step": 5281
    },
    {
      "epoch": 7.042666666666666,
      "grad_norm": 0.025375856086611748,
      "learning_rate": 0.00042468564722523164,
      "loss": 1.1361,
      "step": 5282
    },
    {
      "epoch": 7.044,
      "grad_norm": 0.029803546145558357,
      "learning_rate": 0.0004243324898286348,
      "loss": 1.2256,
      "step": 5283
    },
    {
      "epoch": 7.045333333333334,
      "grad_norm": 0.023830721154808998,
      "learning_rate": 0.00042397943978288145,
      "loss": 0.8072,
      "step": 5284
    },
    {
      "epoch": 7.046666666666667,
      "grad_norm": 0.02676873281598091,
      "learning_rate": 0.00042362649715380887,
      "loss": 1.0437,
      "step": 5285
    },
    {
      "epoch": 7.048,
      "grad_norm": 0.02541293203830719,
      "learning_rate": 0.0004232736620072341,
      "loss": 1.1258,
      "step": 5286
    },
    {
      "epoch": 7.049333333333333,
      "grad_norm": 0.02660427987575531,
      "learning_rate": 0.00042292093440895394,
      "loss": 1.0067,
      "step": 5287
    },
    {
      "epoch": 7.050666666666666,
      "grad_norm": 0.023871904239058495,
      "learning_rate": 0.00042256831442474585,
      "loss": 1.0483,
      "step": 5288
    },
    {
      "epoch": 7.052,
      "grad_norm": 0.021619776263833046,
      "learning_rate": 0.0004222158021203657,
      "loss": 0.7656,
      "step": 5289
    },
    {
      "epoch": 7.053333333333334,
      "grad_norm": 0.03276786953210831,
      "learning_rate": 0.00042186339756155065,
      "loss": 1.0571,
      "step": 5290
    },
    {
      "epoch": 7.054666666666667,
      "grad_norm": 0.028402142226696014,
      "learning_rate": 0.00042151110081401735,
      "loss": 1.0448,
      "step": 5291
    },
    {
      "epoch": 7.056,
      "grad_norm": 0.025873806327581406,
      "learning_rate": 0.00042115891194346224,
      "loss": 0.8419,
      "step": 5292
    },
    {
      "epoch": 7.057333333333333,
      "grad_norm": 0.03710957244038582,
      "learning_rate": 0.00042080683101556216,
      "loss": 1.0723,
      "step": 5293
    },
    {
      "epoch": 7.058666666666666,
      "grad_norm": 0.02339852787554264,
      "learning_rate": 0.0004204548580959727,
      "loss": 0.9452,
      "step": 5294
    },
    {
      "epoch": 7.06,
      "grad_norm": 0.02576236054301262,
      "learning_rate": 0.0004201029932503303,
      "loss": 0.881,
      "step": 5295
    },
    {
      "epoch": 7.061333333333334,
      "grad_norm": 0.035880111157894135,
      "learning_rate": 0.0004197512365442508,
      "loss": 1.1932,
      "step": 5296
    },
    {
      "epoch": 7.062666666666667,
      "grad_norm": 0.020672377198934555,
      "learning_rate": 0.0004193995880433303,
      "loss": 0.906,
      "step": 5297
    },
    {
      "epoch": 7.064,
      "grad_norm": 0.02678694948554039,
      "learning_rate": 0.00041904804781314433,
      "loss": 0.8824,
      "step": 5298
    },
    {
      "epoch": 7.065333333333333,
      "grad_norm": 0.023336157202720642,
      "learning_rate": 0.00041869661591924814,
      "loss": 1.318,
      "step": 5299
    },
    {
      "epoch": 7.066666666666666,
      "grad_norm": 0.02253268100321293,
      "learning_rate": 0.00041834529242717756,
      "loss": 0.9228,
      "step": 5300
    },
    {
      "epoch": 7.068,
      "grad_norm": 0.025840679183602333,
      "learning_rate": 0.0004179940774024469,
      "loss": 0.8744,
      "step": 5301
    },
    {
      "epoch": 7.069333333333334,
      "grad_norm": 0.03803678974509239,
      "learning_rate": 0.00041764297091055115,
      "loss": 1.077,
      "step": 5302
    },
    {
      "epoch": 7.070666666666667,
      "grad_norm": 0.022934013977646828,
      "learning_rate": 0.000417291973016965,
      "loss": 1.0758,
      "step": 5303
    },
    {
      "epoch": 7.072,
      "grad_norm": 0.02551100216805935,
      "learning_rate": 0.0004169410837871427,
      "loss": 1.0178,
      "step": 5304
    },
    {
      "epoch": 7.073333333333333,
      "grad_norm": 0.026483261957764626,
      "learning_rate": 0.0004165903032865187,
      "loss": 1.0707,
      "step": 5305
    },
    {
      "epoch": 7.074666666666666,
      "grad_norm": 0.031206345185637474,
      "learning_rate": 0.00041623963158050615,
      "loss": 0.8931,
      "step": 5306
    },
    {
      "epoch": 7.076,
      "grad_norm": 0.02469104714691639,
      "learning_rate": 0.00041588906873449863,
      "loss": 1.0509,
      "step": 5307
    },
    {
      "epoch": 7.077333333333334,
      "grad_norm": 0.03135311231017113,
      "learning_rate": 0.00041553861481386954,
      "loss": 1.0192,
      "step": 5308
    },
    {
      "epoch": 7.078666666666667,
      "grad_norm": 0.03656554967164993,
      "learning_rate": 0.00041518826988397176,
      "loss": 1.0958,
      "step": 5309
    },
    {
      "epoch": 7.08,
      "grad_norm": 0.029061144217848778,
      "learning_rate": 0.00041483803401013797,
      "loss": 1.2549,
      "step": 5310
    },
    {
      "epoch": 7.081333333333333,
      "grad_norm": 0.027057278901338577,
      "learning_rate": 0.00041448790725767996,
      "loss": 1.2408,
      "step": 5311
    },
    {
      "epoch": 7.082666666666666,
      "grad_norm": 0.025951139628887177,
      "learning_rate": 0.0004141378896918897,
      "loss": 1.0107,
      "step": 5312
    },
    {
      "epoch": 7.084,
      "grad_norm": 0.026132553815841675,
      "learning_rate": 0.00041378798137803877,
      "loss": 0.9706,
      "step": 5313
    },
    {
      "epoch": 7.085333333333334,
      "grad_norm": 0.020862041041254997,
      "learning_rate": 0.0004134381823813783,
      "loss": 0.9101,
      "step": 5314
    },
    {
      "epoch": 7.086666666666667,
      "grad_norm": 0.04807950183749199,
      "learning_rate": 0.0004130884927671396,
      "loss": 1.104,
      "step": 5315
    },
    {
      "epoch": 7.088,
      "grad_norm": 0.025454478338360786,
      "learning_rate": 0.0004127389126005319,
      "loss": 0.8887,
      "step": 5316
    },
    {
      "epoch": 7.089333333333333,
      "grad_norm": 0.02674499712884426,
      "learning_rate": 0.0004123894419467457,
      "loss": 0.854,
      "step": 5317
    },
    {
      "epoch": 7.0906666666666665,
      "grad_norm": 0.02560802549123764,
      "learning_rate": 0.0004120400808709506,
      "loss": 1.1494,
      "step": 5318
    },
    {
      "epoch": 7.092,
      "grad_norm": 0.02468199096620083,
      "learning_rate": 0.0004116908294382955,
      "loss": 1.136,
      "step": 5319
    },
    {
      "epoch": 7.093333333333334,
      "grad_norm": 0.02759948931634426,
      "learning_rate": 0.00041134168771390944,
      "loss": 0.9982,
      "step": 5320
    },
    {
      "epoch": 7.094666666666667,
      "grad_norm": 0.024687061086297035,
      "learning_rate": 0.0004109926557628999,
      "loss": 1.07,
      "step": 5321
    },
    {
      "epoch": 7.096,
      "grad_norm": 0.02825774811208248,
      "learning_rate": 0.000410643733650355,
      "loss": 1.0407,
      "step": 5322
    },
    {
      "epoch": 7.097333333333333,
      "grad_norm": 0.029184795916080475,
      "learning_rate": 0.00041029492144134186,
      "loss": 0.8619,
      "step": 5323
    },
    {
      "epoch": 7.0986666666666665,
      "grad_norm": 0.03312604874372482,
      "learning_rate": 0.0004099462192009071,
      "loss": 0.9849,
      "step": 5324
    },
    {
      "epoch": 7.1,
      "grad_norm": 0.02237408049404621,
      "learning_rate": 0.00040959762699407763,
      "loss": 0.9423,
      "step": 5325
    },
    {
      "epoch": 7.101333333333334,
      "grad_norm": 0.02152634598314762,
      "learning_rate": 0.00040924914488585785,
      "loss": 0.8868,
      "step": 5326
    },
    {
      "epoch": 7.102666666666667,
      "grad_norm": 0.024800077080726624,
      "learning_rate": 0.0004089007729412343,
      "loss": 1.2116,
      "step": 5327
    },
    {
      "epoch": 7.104,
      "grad_norm": 0.024914827197790146,
      "learning_rate": 0.00040855251122517056,
      "loss": 1.0551,
      "step": 5328
    },
    {
      "epoch": 7.105333333333333,
      "grad_norm": 0.025529613718390465,
      "learning_rate": 0.00040820435980261127,
      "loss": 0.8357,
      "step": 5329
    },
    {
      "epoch": 7.1066666666666665,
      "grad_norm": 0.02256292849779129,
      "learning_rate": 0.00040785631873847995,
      "loss": 0.7049,
      "step": 5330
    },
    {
      "epoch": 7.108,
      "grad_norm": 0.030717745423316956,
      "learning_rate": 0.0004075083880976788,
      "loss": 1.1798,
      "step": 5331
    },
    {
      "epoch": 7.109333333333334,
      "grad_norm": 0.02708882838487625,
      "learning_rate": 0.00040716056794509113,
      "loss": 1.2076,
      "step": 5332
    },
    {
      "epoch": 7.110666666666667,
      "grad_norm": 0.024695372208952904,
      "learning_rate": 0.00040681285834557804,
      "loss": 0.9444,
      "step": 5333
    },
    {
      "epoch": 7.112,
      "grad_norm": 0.025243505835533142,
      "learning_rate": 0.0004064652593639808,
      "loss": 1.1283,
      "step": 5334
    },
    {
      "epoch": 7.113333333333333,
      "grad_norm": 0.027949223294854164,
      "learning_rate": 0.0004061177710651202,
      "loss": 0.85,
      "step": 5335
    },
    {
      "epoch": 7.1146666666666665,
      "grad_norm": 0.029451578855514526,
      "learning_rate": 0.0004057703935137951,
      "loss": 0.7696,
      "step": 5336
    },
    {
      "epoch": 7.116,
      "grad_norm": 0.028697535395622253,
      "learning_rate": 0.0004054231267747861,
      "loss": 0.8891,
      "step": 5337
    },
    {
      "epoch": 7.117333333333334,
      "grad_norm": 0.020691508427262306,
      "learning_rate": 0.0004050759709128505,
      "loss": 0.9211,
      "step": 5338
    },
    {
      "epoch": 7.118666666666667,
      "grad_norm": 0.03012799099087715,
      "learning_rate": 0.0004047289259927267,
      "loss": 0.9555,
      "step": 5339
    },
    {
      "epoch": 7.12,
      "grad_norm": 0.031969521194696426,
      "learning_rate": 0.00040438199207913216,
      "loss": 0.8942,
      "step": 5340
    },
    {
      "epoch": 7.121333333333333,
      "grad_norm": 0.022161513566970825,
      "learning_rate": 0.0004040351692367623,
      "loss": 0.935,
      "step": 5341
    },
    {
      "epoch": 7.1226666666666665,
      "grad_norm": 0.03649159520864487,
      "learning_rate": 0.0004036884575302943,
      "loss": 1.1551,
      "step": 5342
    },
    {
      "epoch": 7.124,
      "grad_norm": 0.03245539590716362,
      "learning_rate": 0.00040334185702438184,
      "loss": 1.076,
      "step": 5343
    },
    {
      "epoch": 7.125333333333334,
      "grad_norm": 0.037999995052814484,
      "learning_rate": 0.00040299536778366,
      "loss": 1.0735,
      "step": 5344
    },
    {
      "epoch": 7.126666666666667,
      "grad_norm": 0.02725907973945141,
      "learning_rate": 0.0004026489898727419,
      "loss": 0.9674,
      "step": 5345
    },
    {
      "epoch": 7.128,
      "grad_norm": 0.030588343739509583,
      "learning_rate": 0.0004023027233562206,
      "loss": 0.9699,
      "step": 5346
    },
    {
      "epoch": 7.129333333333333,
      "grad_norm": 0.025538431480526924,
      "learning_rate": 0.00040195656829866835,
      "loss": 1.2623,
      "step": 5347
    },
    {
      "epoch": 7.1306666666666665,
      "grad_norm": 0.02483237534761429,
      "learning_rate": 0.00040161052476463534,
      "loss": 0.9535,
      "step": 5348
    },
    {
      "epoch": 7.132,
      "grad_norm": 0.02271602302789688,
      "learning_rate": 0.00040126459281865324,
      "loss": 1.0035,
      "step": 5349
    },
    {
      "epoch": 7.133333333333334,
      "grad_norm": 0.027102438732981682,
      "learning_rate": 0.0004009187725252309,
      "loss": 0.8123,
      "step": 5350
    },
    {
      "epoch": 7.134666666666667,
      "grad_norm": 0.020563390105962753,
      "learning_rate": 0.00040057306394885716,
      "loss": 1.0424,
      "step": 5351
    },
    {
      "epoch": 7.136,
      "grad_norm": 0.025414103642106056,
      "learning_rate": 0.0004002274671540006,
      "loss": 0.9018,
      "step": 5352
    },
    {
      "epoch": 7.137333333333333,
      "grad_norm": 0.03416163846850395,
      "learning_rate": 0.00039988198220510695,
      "loss": 1.1888,
      "step": 5353
    },
    {
      "epoch": 7.1386666666666665,
      "grad_norm": 0.02755817025899887,
      "learning_rate": 0.0003995366091666042,
      "loss": 0.8799,
      "step": 5354
    },
    {
      "epoch": 7.14,
      "grad_norm": 0.03405481576919556,
      "learning_rate": 0.00039919134810289646,
      "loss": 1.1419,
      "step": 5355
    },
    {
      "epoch": 7.141333333333334,
      "grad_norm": 0.020818965509533882,
      "learning_rate": 0.0003988461990783687,
      "loss": 0.8202,
      "step": 5356
    },
    {
      "epoch": 7.142666666666667,
      "grad_norm": 0.019830280914902687,
      "learning_rate": 0.00039850116215738485,
      "loss": 1.3764,
      "step": 5357
    },
    {
      "epoch": 7.144,
      "grad_norm": 0.024791212752461433,
      "learning_rate": 0.00039815623740428665,
      "loss": 0.8218,
      "step": 5358
    },
    {
      "epoch": 7.145333333333333,
      "grad_norm": 0.02391483262181282,
      "learning_rate": 0.00039781142488339705,
      "loss": 0.9912,
      "step": 5359
    },
    {
      "epoch": 7.1466666666666665,
      "grad_norm": 0.02836090885102749,
      "learning_rate": 0.000397466724659016,
      "loss": 0.91,
      "step": 5360
    },
    {
      "epoch": 7.148,
      "grad_norm": 0.026267940178513527,
      "learning_rate": 0.00039712213679542386,
      "loss": 1.0274,
      "step": 5361
    },
    {
      "epoch": 7.149333333333334,
      "grad_norm": 0.03232622146606445,
      "learning_rate": 0.00039677766135687976,
      "loss": 0.9302,
      "step": 5362
    },
    {
      "epoch": 7.150666666666667,
      "grad_norm": 0.02792990393936634,
      "learning_rate": 0.0003964332984076208,
      "loss": 0.8863,
      "step": 5363
    },
    {
      "epoch": 7.152,
      "grad_norm": 0.03750341385602951,
      "learning_rate": 0.0003960890480118653,
      "loss": 0.7783,
      "step": 5364
    },
    {
      "epoch": 7.153333333333333,
      "grad_norm": 0.02559993416070938,
      "learning_rate": 0.0003957449102338083,
      "loss": 0.7836,
      "step": 5365
    },
    {
      "epoch": 7.1546666666666665,
      "grad_norm": 0.030353033915162086,
      "learning_rate": 0.00039540088513762516,
      "loss": 1.297,
      "step": 5366
    },
    {
      "epoch": 7.156,
      "grad_norm": 0.029976535588502884,
      "learning_rate": 0.00039505697278747033,
      "loss": 0.9138,
      "step": 5367
    },
    {
      "epoch": 7.157333333333334,
      "grad_norm": 0.022672368213534355,
      "learning_rate": 0.0003947131732474756,
      "loss": 1.1533,
      "step": 5368
    },
    {
      "epoch": 7.158666666666667,
      "grad_norm": 0.028206998482346535,
      "learning_rate": 0.0003943694865817544,
      "loss": 0.9166,
      "step": 5369
    },
    {
      "epoch": 7.16,
      "grad_norm": 0.029256662353873253,
      "learning_rate": 0.0003940259128543967,
      "loss": 1.0368,
      "step": 5370
    },
    {
      "epoch": 7.161333333333333,
      "grad_norm": 0.026098240166902542,
      "learning_rate": 0.0003936824521294725,
      "loss": 0.9218,
      "step": 5371
    },
    {
      "epoch": 7.1626666666666665,
      "grad_norm": 0.02094714157283306,
      "learning_rate": 0.00039333910447103104,
      "loss": 0.8151,
      "step": 5372
    },
    {
      "epoch": 7.164,
      "grad_norm": 0.024229908362030983,
      "learning_rate": 0.000392995869943099,
      "loss": 1.1429,
      "step": 5373
    },
    {
      "epoch": 7.165333333333333,
      "grad_norm": 0.029548417776823044,
      "learning_rate": 0.00039265274860968423,
      "loss": 0.957,
      "step": 5374
    },
    {
      "epoch": 7.166666666666667,
      "grad_norm": 0.028180347755551338,
      "learning_rate": 0.00039230974053477085,
      "loss": 1.0905,
      "step": 5375
    },
    {
      "epoch": 7.168,
      "grad_norm": 0.02204071544110775,
      "learning_rate": 0.00039196684578232476,
      "loss": 1.1745,
      "step": 5376
    },
    {
      "epoch": 7.169333333333333,
      "grad_norm": 0.026196038350462914,
      "learning_rate": 0.00039162406441628803,
      "loss": 1.2517,
      "step": 5377
    },
    {
      "epoch": 7.1706666666666665,
      "grad_norm": 0.02076374925673008,
      "learning_rate": 0.0003912813965005831,
      "loss": 0.9527,
      "step": 5378
    },
    {
      "epoch": 7.172,
      "grad_norm": 0.0349181666970253,
      "learning_rate": 0.0003909388420991113,
      "loss": 1.3358,
      "step": 5379
    },
    {
      "epoch": 7.173333333333334,
      "grad_norm": 0.03265202045440674,
      "learning_rate": 0.00039059640127575134,
      "loss": 1.2039,
      "step": 5380
    },
    {
      "epoch": 7.174666666666667,
      "grad_norm": 0.022119898349046707,
      "learning_rate": 0.0003902540740943632,
      "loss": 1.0008,
      "step": 5381
    },
    {
      "epoch": 7.176,
      "grad_norm": 0.02774222195148468,
      "learning_rate": 0.00038991186061878316,
      "loss": 1.1179,
      "step": 5382
    },
    {
      "epoch": 7.177333333333333,
      "grad_norm": 0.03346260264515877,
      "learning_rate": 0.0003895697609128278,
      "loss": 0.9896,
      "step": 5383
    },
    {
      "epoch": 7.1786666666666665,
      "grad_norm": 0.02691100910305977,
      "learning_rate": 0.0003892277750402924,
      "loss": 0.9401,
      "step": 5384
    },
    {
      "epoch": 7.18,
      "grad_norm": 0.0220345351845026,
      "learning_rate": 0.00038888590306494975,
      "loss": 0.9028,
      "step": 5385
    },
    {
      "epoch": 7.181333333333333,
      "grad_norm": 0.035746484994888306,
      "learning_rate": 0.0003885441450505536,
      "loss": 1.2605,
      "step": 5386
    },
    {
      "epoch": 7.182666666666667,
      "grad_norm": 0.030464285984635353,
      "learning_rate": 0.0003882025010608343,
      "loss": 0.9858,
      "step": 5387
    },
    {
      "epoch": 7.184,
      "grad_norm": 0.01793137937784195,
      "learning_rate": 0.0003878609711595021,
      "loss": 0.8509,
      "step": 5388
    },
    {
      "epoch": 7.185333333333333,
      "grad_norm": 0.027570392936468124,
      "learning_rate": 0.0003875195554102462,
      "loss": 1.3284,
      "step": 5389
    },
    {
      "epoch": 7.1866666666666665,
      "grad_norm": 0.02436426840722561,
      "learning_rate": 0.0003871782538767328,
      "loss": 0.8679,
      "step": 5390
    },
    {
      "epoch": 7.188,
      "grad_norm": 0.021786727011203766,
      "learning_rate": 0.0003868370666226094,
      "loss": 0.9033,
      "step": 5391
    },
    {
      "epoch": 7.189333333333333,
      "grad_norm": 0.02700364962220192,
      "learning_rate": 0.0003864959937115,
      "loss": 0.921,
      "step": 5392
    },
    {
      "epoch": 7.190666666666667,
      "grad_norm": 0.021445350721478462,
      "learning_rate": 0.0003861550352070083,
      "loss": 0.9955,
      "step": 5393
    },
    {
      "epoch": 7.192,
      "grad_norm": 0.024419201537966728,
      "learning_rate": 0.00038581419117271677,
      "loss": 0.9705,
      "step": 5394
    },
    {
      "epoch": 7.193333333333333,
      "grad_norm": 0.03198469430208206,
      "learning_rate": 0.00038547346167218524,
      "loss": 0.9955,
      "step": 5395
    },
    {
      "epoch": 7.1946666666666665,
      "grad_norm": 0.03096744418144226,
      "learning_rate": 0.00038513284676895454,
      "loss": 1.0709,
      "step": 5396
    },
    {
      "epoch": 7.196,
      "grad_norm": 0.02818160690367222,
      "learning_rate": 0.00038479234652654174,
      "loss": 0.9411,
      "step": 5397
    },
    {
      "epoch": 7.197333333333333,
      "grad_norm": 0.03175946697592735,
      "learning_rate": 0.00038445196100844384,
      "loss": 1.0996,
      "step": 5398
    },
    {
      "epoch": 7.198666666666667,
      "grad_norm": 0.025762898847460747,
      "learning_rate": 0.00038411169027813654,
      "loss": 1.2246,
      "step": 5399
    },
    {
      "epoch": 7.2,
      "grad_norm": 0.028067201375961304,
      "learning_rate": 0.00038377153439907266,
      "loss": 0.9188,
      "step": 5400
    },
    {
      "epoch": 7.201333333333333,
      "grad_norm": 0.022403666749596596,
      "learning_rate": 0.00038343149343468606,
      "loss": 1.0563,
      "step": 5401
    },
    {
      "epoch": 7.2026666666666666,
      "grad_norm": 0.029136616736650467,
      "learning_rate": 0.00038309156744838647,
      "loss": 1.1535,
      "step": 5402
    },
    {
      "epoch": 7.204,
      "grad_norm": 0.02381301112473011,
      "learning_rate": 0.00038275175650356484,
      "loss": 1.129,
      "step": 5403
    },
    {
      "epoch": 7.205333333333333,
      "grad_norm": 0.02308301068842411,
      "learning_rate": 0.00038241206066358835,
      "loss": 0.9625,
      "step": 5404
    },
    {
      "epoch": 7.206666666666667,
      "grad_norm": 0.026407968252897263,
      "learning_rate": 0.00038207247999180407,
      "loss": 1.178,
      "step": 5405
    },
    {
      "epoch": 7.208,
      "grad_norm": 0.029529191553592682,
      "learning_rate": 0.0003817330145515374,
      "loss": 1.124,
      "step": 5406
    },
    {
      "epoch": 7.209333333333333,
      "grad_norm": 0.02128642052412033,
      "learning_rate": 0.0003813936644060911,
      "loss": 1.162,
      "step": 5407
    },
    {
      "epoch": 7.210666666666667,
      "grad_norm": 0.021780965849757195,
      "learning_rate": 0.0003810544296187489,
      "loss": 1.2388,
      "step": 5408
    },
    {
      "epoch": 7.212,
      "grad_norm": 0.0258976761251688,
      "learning_rate": 0.0003807153102527704,
      "loss": 0.8175,
      "step": 5409
    },
    {
      "epoch": 7.213333333333333,
      "grad_norm": 0.0217361468821764,
      "learning_rate": 0.0003803763063713951,
      "loss": 0.8759,
      "step": 5410
    },
    {
      "epoch": 7.214666666666667,
      "grad_norm": 0.03719485178589821,
      "learning_rate": 0.0003800374180378411,
      "loss": 0.9932,
      "step": 5411
    },
    {
      "epoch": 7.216,
      "grad_norm": 0.0314808264374733,
      "learning_rate": 0.0003796986453153034,
      "loss": 1.0032,
      "step": 5412
    },
    {
      "epoch": 7.217333333333333,
      "grad_norm": 0.028775891289114952,
      "learning_rate": 0.00037935998826695797,
      "loss": 1.1265,
      "step": 5413
    },
    {
      "epoch": 7.218666666666667,
      "grad_norm": 0.02724457159638405,
      "learning_rate": 0.0003790214469559566,
      "loss": 0.9675,
      "step": 5414
    },
    {
      "epoch": 7.22,
      "grad_norm": 0.03231852874159813,
      "learning_rate": 0.0003786830214454314,
      "loss": 1.204,
      "step": 5415
    },
    {
      "epoch": 7.221333333333333,
      "grad_norm": 0.023235008120536804,
      "learning_rate": 0.0003783447117984922,
      "loss": 1.0856,
      "step": 5416
    },
    {
      "epoch": 7.222666666666667,
      "grad_norm": 0.029885832220315933,
      "learning_rate": 0.0003780065180782263,
      "loss": 0.9456,
      "step": 5417
    },
    {
      "epoch": 7.224,
      "grad_norm": 0.023695221170783043,
      "learning_rate": 0.00037766844034770156,
      "loss": 1.0305,
      "step": 5418
    },
    {
      "epoch": 7.225333333333333,
      "grad_norm": 0.023768609389662743,
      "learning_rate": 0.00037733047866996195,
      "loss": 1.0427,
      "step": 5419
    },
    {
      "epoch": 7.226666666666667,
      "grad_norm": 0.02875158004462719,
      "learning_rate": 0.0003769926331080311,
      "loss": 1.0578,
      "step": 5420
    },
    {
      "epoch": 7.228,
      "grad_norm": 0.023791080340743065,
      "learning_rate": 0.0003766549037249112,
      "loss": 0.8628,
      "step": 5421
    },
    {
      "epoch": 7.229333333333333,
      "grad_norm": 0.025471502915024757,
      "learning_rate": 0.0003763172905835809,
      "loss": 1.1067,
      "step": 5422
    },
    {
      "epoch": 7.230666666666667,
      "grad_norm": 0.02682245336472988,
      "learning_rate": 0.00037597979374700007,
      "loss": 0.9301,
      "step": 5423
    },
    {
      "epoch": 7.232,
      "grad_norm": 0.027049776166677475,
      "learning_rate": 0.0003756424132781043,
      "loss": 0.9993,
      "step": 5424
    },
    {
      "epoch": 7.233333333333333,
      "grad_norm": 0.022956928238272667,
      "learning_rate": 0.00037530514923980886,
      "loss": 1.0591,
      "step": 5425
    },
    {
      "epoch": 7.234666666666667,
      "grad_norm": 0.026448296383023262,
      "learning_rate": 0.00037496800169500724,
      "loss": 0.8563,
      "step": 5426
    },
    {
      "epoch": 7.236,
      "grad_norm": 0.029525822028517723,
      "learning_rate": 0.0003746309707065699,
      "loss": 1.0055,
      "step": 5427
    },
    {
      "epoch": 7.237333333333333,
      "grad_norm": 0.030521608889102936,
      "learning_rate": 0.00037429405633734804,
      "loss": 0.9129,
      "step": 5428
    },
    {
      "epoch": 7.238666666666667,
      "grad_norm": 0.02590918354690075,
      "learning_rate": 0.0003739572586501686,
      "loss": 1.0434,
      "step": 5429
    },
    {
      "epoch": 7.24,
      "grad_norm": 0.02657446078956127,
      "learning_rate": 0.0003736205777078381,
      "loss": 0.856,
      "step": 5430
    },
    {
      "epoch": 7.241333333333333,
      "grad_norm": 0.02427656017243862,
      "learning_rate": 0.00037328401357314113,
      "loss": 0.9535,
      "step": 5431
    },
    {
      "epoch": 7.242666666666667,
      "grad_norm": 0.01992049068212509,
      "learning_rate": 0.0003729475663088404,
      "loss": 1.0492,
      "step": 5432
    },
    {
      "epoch": 7.244,
      "grad_norm": 0.029260655865073204,
      "learning_rate": 0.000372611235977677,
      "loss": 0.8415,
      "step": 5433
    },
    {
      "epoch": 7.245333333333333,
      "grad_norm": 0.028825853019952774,
      "learning_rate": 0.00037227502264236914,
      "loss": 0.9964,
      "step": 5434
    },
    {
      "epoch": 7.246666666666667,
      "grad_norm": 0.024001160636544228,
      "learning_rate": 0.00037193892636561544,
      "loss": 1.0457,
      "step": 5435
    },
    {
      "epoch": 7.248,
      "grad_norm": 0.020010948181152344,
      "learning_rate": 0.00037160294721009027,
      "loss": 0.9486,
      "step": 5436
    },
    {
      "epoch": 7.249333333333333,
      "grad_norm": 0.02262267656624317,
      "learning_rate": 0.00037126708523844753,
      "loss": 0.9474,
      "step": 5437
    },
    {
      "epoch": 7.250666666666667,
      "grad_norm": 0.03414571285247803,
      "learning_rate": 0.00037093134051331944,
      "loss": 0.8356,
      "step": 5438
    },
    {
      "epoch": 7.252,
      "grad_norm": 0.02482433430850506,
      "learning_rate": 0.0003705957130973149,
      "loss": 1.1728,
      "step": 5439
    },
    {
      "epoch": 7.253333333333333,
      "grad_norm": 0.022159306332468987,
      "learning_rate": 0.0003702602030530231,
      "loss": 0.8508,
      "step": 5440
    },
    {
      "epoch": 7.254666666666667,
      "grad_norm": 0.026178253814578056,
      "learning_rate": 0.0003699248104430093,
      "loss": 1.0499,
      "step": 5441
    },
    {
      "epoch": 7.256,
      "grad_norm": 0.0286913700401783,
      "learning_rate": 0.000369589535329818,
      "loss": 0.978,
      "step": 5442
    },
    {
      "epoch": 7.257333333333333,
      "grad_norm": 0.025581782683730125,
      "learning_rate": 0.0003692543777759718,
      "loss": 0.9405,
      "step": 5443
    },
    {
      "epoch": 7.258666666666667,
      "grad_norm": 0.025284478440880775,
      "learning_rate": 0.00036891933784397024,
      "loss": 0.9795,
      "step": 5444
    },
    {
      "epoch": 7.26,
      "grad_norm": 0.02457968145608902,
      "learning_rate": 0.0003685844155962931,
      "loss": 0.8705,
      "step": 5445
    },
    {
      "epoch": 7.261333333333333,
      "grad_norm": 0.020659007132053375,
      "learning_rate": 0.00036824961109539566,
      "loss": 0.8706,
      "step": 5446
    },
    {
      "epoch": 7.262666666666667,
      "grad_norm": 0.029833147302269936,
      "learning_rate": 0.0003679149244037131,
      "loss": 0.9438,
      "step": 5447
    },
    {
      "epoch": 7.264,
      "grad_norm": 0.022219091653823853,
      "learning_rate": 0.0003675803555836582,
      "loss": 0.9131,
      "step": 5448
    },
    {
      "epoch": 7.265333333333333,
      "grad_norm": 0.024949586018919945,
      "learning_rate": 0.0003672459046976206,
      "loss": 0.8382,
      "step": 5449
    },
    {
      "epoch": 7.266666666666667,
      "grad_norm": 0.0330098420381546,
      "learning_rate": 0.0003669115718079702,
      "loss": 1.0565,
      "step": 5450
    },
    {
      "epoch": 7.268,
      "grad_norm": 0.02809782139956951,
      "learning_rate": 0.0003665773569770526,
      "loss": 0.8505,
      "step": 5451
    },
    {
      "epoch": 7.269333333333333,
      "grad_norm": 0.04280126467347145,
      "learning_rate": 0.0003662432602671929,
      "loss": 1.2393,
      "step": 5452
    },
    {
      "epoch": 7.270666666666667,
      "grad_norm": 0.0276416577398777,
      "learning_rate": 0.0003659092817406935,
      "loss": 1.1492,
      "step": 5453
    },
    {
      "epoch": 7.272,
      "grad_norm": 0.02789500541985035,
      "learning_rate": 0.0003655754214598349,
      "loss": 1.0016,
      "step": 5454
    },
    {
      "epoch": 7.273333333333333,
      "grad_norm": 0.028006667271256447,
      "learning_rate": 0.0003652416794868759,
      "loss": 0.9069,
      "step": 5455
    },
    {
      "epoch": 7.274666666666667,
      "grad_norm": 0.03234491124749184,
      "learning_rate": 0.00036490805588405254,
      "loss": 1.1169,
      "step": 5456
    },
    {
      "epoch": 7.276,
      "grad_norm": 0.03256223723292351,
      "learning_rate": 0.0003645745507135791,
      "loss": 0.9238,
      "step": 5457
    },
    {
      "epoch": 7.277333333333333,
      "grad_norm": 0.02538742497563362,
      "learning_rate": 0.00036424116403764806,
      "loss": 1.1159,
      "step": 5458
    },
    {
      "epoch": 7.278666666666667,
      "grad_norm": 0.02802368253469467,
      "learning_rate": 0.0003639078959184297,
      "loss": 0.8549,
      "step": 5459
    },
    {
      "epoch": 7.28,
      "grad_norm": 0.02636745385825634,
      "learning_rate": 0.00036357474641807196,
      "loss": 1.135,
      "step": 5460
    },
    {
      "epoch": 7.281333333333333,
      "grad_norm": 0.026395782828330994,
      "learning_rate": 0.00036324171559870056,
      "loss": 0.8632,
      "step": 5461
    },
    {
      "epoch": 7.282666666666667,
      "grad_norm": 0.023013541474938393,
      "learning_rate": 0.0003629088035224195,
      "loss": 1.2591,
      "step": 5462
    },
    {
      "epoch": 7.284,
      "grad_norm": 0.02593310922384262,
      "learning_rate": 0.00036257601025131025,
      "loss": 0.9202,
      "step": 5463
    },
    {
      "epoch": 7.285333333333333,
      "grad_norm": 0.03046635165810585,
      "learning_rate": 0.00036224333584743265,
      "loss": 1.1813,
      "step": 5464
    },
    {
      "epoch": 7.286666666666667,
      "grad_norm": 0.023569218814373016,
      "learning_rate": 0.000361910780372824,
      "loss": 0.9346,
      "step": 5465
    },
    {
      "epoch": 7.288,
      "grad_norm": 0.03305581957101822,
      "learning_rate": 0.00036157834388949906,
      "loss": 0.9564,
      "step": 5466
    },
    {
      "epoch": 7.289333333333333,
      "grad_norm": 0.028368093073368073,
      "learning_rate": 0.0003612460264594509,
      "loss": 0.8944,
      "step": 5467
    },
    {
      "epoch": 7.290666666666667,
      "grad_norm": 0.02665315940976143,
      "learning_rate": 0.00036091382814465054,
      "loss": 0.8441,
      "step": 5468
    },
    {
      "epoch": 7.292,
      "grad_norm": 0.02752712182700634,
      "learning_rate": 0.00036058174900704644,
      "loss": 1.09,
      "step": 5469
    },
    {
      "epoch": 7.293333333333333,
      "grad_norm": 0.026605669409036636,
      "learning_rate": 0.0003602497891085652,
      "loss": 0.8972,
      "step": 5470
    },
    {
      "epoch": 7.294666666666667,
      "grad_norm": 0.024527370929718018,
      "learning_rate": 0.0003599179485111103,
      "loss": 1.0904,
      "step": 5471
    },
    {
      "epoch": 7.296,
      "grad_norm": 0.02517842873930931,
      "learning_rate": 0.00035958622727656374,
      "loss": 0.9682,
      "step": 5472
    },
    {
      "epoch": 7.2973333333333334,
      "grad_norm": 0.02936837449669838,
      "learning_rate": 0.00035925462546678545,
      "loss": 1.1031,
      "step": 5473
    },
    {
      "epoch": 7.298666666666667,
      "grad_norm": 0.027358626946806908,
      "learning_rate": 0.0003589231431436125,
      "loss": 0.8721,
      "step": 5474
    },
    {
      "epoch": 7.3,
      "grad_norm": 0.02513517066836357,
      "learning_rate": 0.0003585917803688603,
      "loss": 0.9675,
      "step": 5475
    },
    {
      "epoch": 7.301333333333333,
      "grad_norm": 0.029181284829974174,
      "learning_rate": 0.00035826053720432107,
      "loss": 1.2706,
      "step": 5476
    },
    {
      "epoch": 7.302666666666667,
      "grad_norm": 0.042961716651916504,
      "learning_rate": 0.00035792941371176536,
      "loss": 0.9261,
      "step": 5477
    },
    {
      "epoch": 7.304,
      "grad_norm": 0.027317147701978683,
      "learning_rate": 0.00035759840995294137,
      "loss": 0.8387,
      "step": 5478
    },
    {
      "epoch": 7.3053333333333335,
      "grad_norm": 0.027214260771870613,
      "learning_rate": 0.0003572675259895749,
      "loss": 1.0651,
      "step": 5479
    },
    {
      "epoch": 7.306666666666667,
      "grad_norm": 0.022435881197452545,
      "learning_rate": 0.0003569367618833694,
      "loss": 1.2155,
      "step": 5480
    },
    {
      "epoch": 7.308,
      "grad_norm": 0.029603958129882812,
      "learning_rate": 0.000356606117696006,
      "loss": 0.9059,
      "step": 5481
    },
    {
      "epoch": 7.309333333333333,
      "grad_norm": 0.02670944295823574,
      "learning_rate": 0.00035627559348914387,
      "loss": 0.9729,
      "step": 5482
    },
    {
      "epoch": 7.310666666666666,
      "grad_norm": 0.025413531810045242,
      "learning_rate": 0.00035594518932441854,
      "loss": 0.9113,
      "step": 5483
    },
    {
      "epoch": 7.312,
      "grad_norm": 0.03495274484157562,
      "learning_rate": 0.0003556149052634443,
      "loss": 1.042,
      "step": 5484
    },
    {
      "epoch": 7.3133333333333335,
      "grad_norm": 0.038882993161678314,
      "learning_rate": 0.0003552847413678129,
      "loss": 1.1196,
      "step": 5485
    },
    {
      "epoch": 7.314666666666667,
      "grad_norm": 0.018270248547196388,
      "learning_rate": 0.00035495469769909326,
      "loss": 0.9406,
      "step": 5486
    },
    {
      "epoch": 7.316,
      "grad_norm": 0.026606939733028412,
      "learning_rate": 0.0003546247743188328,
      "loss": 1.1812,
      "step": 5487
    },
    {
      "epoch": 7.317333333333333,
      "grad_norm": 0.024950439110398293,
      "learning_rate": 0.0003542949712885549,
      "loss": 0.9322,
      "step": 5488
    },
    {
      "epoch": 7.318666666666667,
      "grad_norm": 0.04509606584906578,
      "learning_rate": 0.00035396528866976195,
      "loss": 1.5008,
      "step": 5489
    },
    {
      "epoch": 7.32,
      "grad_norm": 0.026679234579205513,
      "learning_rate": 0.0003536357265239333,
      "loss": 0.852,
      "step": 5490
    },
    {
      "epoch": 7.3213333333333335,
      "grad_norm": 0.026300551369786263,
      "learning_rate": 0.0003533062849125258,
      "loss": 0.8239,
      "step": 5491
    },
    {
      "epoch": 7.322666666666667,
      "grad_norm": 0.025393549352884293,
      "learning_rate": 0.0003529769638969744,
      "loss": 0.7869,
      "step": 5492
    },
    {
      "epoch": 7.324,
      "grad_norm": 0.026445379480719566,
      "learning_rate": 0.00035264776353869043,
      "loss": 0.7096,
      "step": 5493
    },
    {
      "epoch": 7.325333333333333,
      "grad_norm": 0.025976359844207764,
      "learning_rate": 0.00035231868389906375,
      "loss": 1.1246,
      "step": 5494
    },
    {
      "epoch": 7.326666666666666,
      "grad_norm": 0.02279823273420334,
      "learning_rate": 0.0003519897250394611,
      "loss": 1.0968,
      "step": 5495
    },
    {
      "epoch": 7.328,
      "grad_norm": 0.028717031702399254,
      "learning_rate": 0.00035166088702122736,
      "loss": 0.9798,
      "step": 5496
    },
    {
      "epoch": 7.3293333333333335,
      "grad_norm": 0.027852410450577736,
      "learning_rate": 0.0003513321699056844,
      "loss": 0.9995,
      "step": 5497
    },
    {
      "epoch": 7.330666666666667,
      "grad_norm": 0.023097822442650795,
      "learning_rate": 0.0003510035737541311,
      "loss": 1.0345,
      "step": 5498
    },
    {
      "epoch": 7.332,
      "grad_norm": 0.02650013566017151,
      "learning_rate": 0.0003506750986278445,
      "loss": 0.8009,
      "step": 5499
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 0.024402432143688202,
      "learning_rate": 0.00035034674458807893,
      "loss": 0.9059,
      "step": 5500
    },
    {
      "epoch": 7.334666666666667,
      "grad_norm": 0.020343618467450142,
      "learning_rate": 0.0003500185116960661,
      "loss": 0.9088,
      "step": 5501
    },
    {
      "epoch": 7.336,
      "grad_norm": 0.03036266192793846,
      "learning_rate": 0.0003496904000130151,
      "loss": 1.1953,
      "step": 5502
    },
    {
      "epoch": 7.3373333333333335,
      "grad_norm": 0.026608435437083244,
      "learning_rate": 0.00034936240960011265,
      "loss": 1.1594,
      "step": 5503
    },
    {
      "epoch": 7.338666666666667,
      "grad_norm": 0.026900285854935646,
      "learning_rate": 0.000349034540518522,
      "loss": 1.2856,
      "step": 5504
    },
    {
      "epoch": 7.34,
      "grad_norm": 0.020336247980594635,
      "learning_rate": 0.0003487067928293848,
      "loss": 0.981,
      "step": 5505
    },
    {
      "epoch": 7.341333333333333,
      "grad_norm": 0.02318953163921833,
      "learning_rate": 0.0003483791665938194,
      "loss": 1.0355,
      "step": 5506
    },
    {
      "epoch": 7.342666666666666,
      "grad_norm": 0.030743099749088287,
      "learning_rate": 0.0003480516618729219,
      "loss": 1.0027,
      "step": 5507
    },
    {
      "epoch": 7.344,
      "grad_norm": 0.018372762948274612,
      "learning_rate": 0.000347724278727766,
      "loss": 1.1727,
      "step": 5508
    },
    {
      "epoch": 7.3453333333333335,
      "grad_norm": 0.02487925998866558,
      "learning_rate": 0.0003473970172194015,
      "loss": 0.9801,
      "step": 5509
    },
    {
      "epoch": 7.346666666666667,
      "grad_norm": 0.026936816051602364,
      "learning_rate": 0.00034706987740885687,
      "loss": 1.269,
      "step": 5510
    },
    {
      "epoch": 7.348,
      "grad_norm": 0.02308541350066662,
      "learning_rate": 0.00034674285935713714,
      "loss": 1.0086,
      "step": 5511
    },
    {
      "epoch": 7.349333333333333,
      "grad_norm": 0.02318647876381874,
      "learning_rate": 0.00034641596312522495,
      "loss": 0.9836,
      "step": 5512
    },
    {
      "epoch": 7.350666666666667,
      "grad_norm": 0.0316690057516098,
      "learning_rate": 0.00034608918877408035,
      "loss": 1.0117,
      "step": 5513
    },
    {
      "epoch": 7.352,
      "grad_norm": 0.05041573569178581,
      "learning_rate": 0.00034576253636463993,
      "loss": 1.3602,
      "step": 5514
    },
    {
      "epoch": 7.3533333333333335,
      "grad_norm": 0.024752188473939896,
      "learning_rate": 0.0003454360059578182,
      "loss": 1.4456,
      "step": 5515
    },
    {
      "epoch": 7.354666666666667,
      "grad_norm": 0.036475636065006256,
      "learning_rate": 0.0003451095976145069,
      "loss": 0.7874,
      "step": 5516
    },
    {
      "epoch": 7.356,
      "grad_norm": 0.027125153690576553,
      "learning_rate": 0.0003447833113955747,
      "loss": 1.1901,
      "step": 5517
    },
    {
      "epoch": 7.357333333333333,
      "grad_norm": 0.026155486702919006,
      "learning_rate": 0.00034445714736186826,
      "loss": 0.7246,
      "step": 5518
    },
    {
      "epoch": 7.358666666666666,
      "grad_norm": 0.029332036152482033,
      "learning_rate": 0.0003441311055742099,
      "loss": 1.1017,
      "step": 5519
    },
    {
      "epoch": 7.36,
      "grad_norm": 0.027121402323246002,
      "learning_rate": 0.0003438051860934007,
      "loss": 0.9527,
      "step": 5520
    },
    {
      "epoch": 7.3613333333333335,
      "grad_norm": 0.025548817589879036,
      "learning_rate": 0.00034347938898021815,
      "loss": 0.9433,
      "step": 5521
    },
    {
      "epoch": 7.362666666666667,
      "grad_norm": 0.028166236355900764,
      "learning_rate": 0.0003431537142954171,
      "loss": 1.0152,
      "step": 5522
    },
    {
      "epoch": 7.364,
      "grad_norm": 0.03097541257739067,
      "learning_rate": 0.00034282816209972957,
      "loss": 0.9118,
      "step": 5523
    },
    {
      "epoch": 7.365333333333333,
      "grad_norm": 0.02613772824406624,
      "learning_rate": 0.0003425027324538652,
      "loss": 0.9579,
      "step": 5524
    },
    {
      "epoch": 7.366666666666666,
      "grad_norm": 0.022465946152806282,
      "learning_rate": 0.0003421774254185096,
      "loss": 1.1793,
      "step": 5525
    },
    {
      "epoch": 7.368,
      "grad_norm": 0.022105088457465172,
      "learning_rate": 0.00034185224105432653,
      "loss": 1.0311,
      "step": 5526
    },
    {
      "epoch": 7.3693333333333335,
      "grad_norm": 0.02457440458238125,
      "learning_rate": 0.00034152717942195657,
      "loss": 0.9901,
      "step": 5527
    },
    {
      "epoch": 7.370666666666667,
      "grad_norm": 0.02535863034427166,
      "learning_rate": 0.00034120224058201745,
      "loss": 0.9338,
      "step": 5528
    },
    {
      "epoch": 7.372,
      "grad_norm": 0.023308897390961647,
      "learning_rate": 0.00034087742459510393,
      "loss": 0.9388,
      "step": 5529
    },
    {
      "epoch": 7.373333333333333,
      "grad_norm": 0.027232030406594276,
      "learning_rate": 0.00034055273152178834,
      "loss": 1.0884,
      "step": 5530
    },
    {
      "epoch": 7.374666666666666,
      "grad_norm": 0.02381415106356144,
      "learning_rate": 0.0003402281614226188,
      "loss": 0.8403,
      "step": 5531
    },
    {
      "epoch": 7.376,
      "grad_norm": 0.024478798732161522,
      "learning_rate": 0.0003399037143581218,
      "loss": 1.1047,
      "step": 5532
    },
    {
      "epoch": 7.3773333333333335,
      "grad_norm": 0.027804292738437653,
      "learning_rate": 0.0003395793903888006,
      "loss": 1.0682,
      "step": 5533
    },
    {
      "epoch": 7.378666666666667,
      "grad_norm": 0.023795904591679573,
      "learning_rate": 0.000339255189575135,
      "loss": 0.8849,
      "step": 5534
    },
    {
      "epoch": 7.38,
      "grad_norm": 0.033436235040426254,
      "learning_rate": 0.00033893111197758275,
      "loss": 0.8672,
      "step": 5535
    },
    {
      "epoch": 7.381333333333333,
      "grad_norm": 0.028513597324490547,
      "learning_rate": 0.00033860715765657735,
      "loss": 1.1799,
      "step": 5536
    },
    {
      "epoch": 7.382666666666666,
      "grad_norm": 0.023202842101454735,
      "learning_rate": 0.00033828332667253027,
      "loss": 1.0708,
      "step": 5537
    },
    {
      "epoch": 7.384,
      "grad_norm": 0.027517827227711678,
      "learning_rate": 0.0003379596190858296,
      "loss": 0.8389,
      "step": 5538
    },
    {
      "epoch": 7.3853333333333335,
      "grad_norm": 0.02261100709438324,
      "learning_rate": 0.0003376360349568409,
      "loss": 1.1928,
      "step": 5539
    },
    {
      "epoch": 7.386666666666667,
      "grad_norm": 0.03330073133111,
      "learning_rate": 0.0003373125743459063,
      "loss": 0.7923,
      "step": 5540
    },
    {
      "epoch": 7.388,
      "grad_norm": 0.024098752066493034,
      "learning_rate": 0.00033698923731334453,
      "loss": 1.0635,
      "step": 5541
    },
    {
      "epoch": 7.389333333333333,
      "grad_norm": 0.024804839864373207,
      "learning_rate": 0.0003366660239194519,
      "loss": 1.2206,
      "step": 5542
    },
    {
      "epoch": 7.390666666666666,
      "grad_norm": 0.02454974874854088,
      "learning_rate": 0.00033634293422450147,
      "loss": 0.8933,
      "step": 5543
    },
    {
      "epoch": 7.392,
      "grad_norm": 0.022998040542006493,
      "learning_rate": 0.00033601996828874325,
      "loss": 0.9431,
      "step": 5544
    },
    {
      "epoch": 7.3933333333333335,
      "grad_norm": 0.0243363194167614,
      "learning_rate": 0.0003356971261724043,
      "loss": 0.7281,
      "step": 5545
    },
    {
      "epoch": 7.394666666666667,
      "grad_norm": 0.028117166832089424,
      "learning_rate": 0.0003353744079356881,
      "loss": 1.1552,
      "step": 5546
    },
    {
      "epoch": 7.396,
      "grad_norm": 0.025083327665925026,
      "learning_rate": 0.00033505181363877533,
      "loss": 1.1942,
      "step": 5547
    },
    {
      "epoch": 7.397333333333333,
      "grad_norm": 0.02515789493918419,
      "learning_rate": 0.0003347293433418237,
      "loss": 1.1301,
      "step": 5548
    },
    {
      "epoch": 7.398666666666666,
      "grad_norm": 0.03292825445532799,
      "learning_rate": 0.0003344069971049678,
      "loss": 0.9285,
      "step": 5549
    },
    {
      "epoch": 7.4,
      "grad_norm": 0.02333485148847103,
      "learning_rate": 0.00033408477498831913,
      "loss": 0.9883,
      "step": 5550
    },
    {
      "epoch": 7.4013333333333335,
      "grad_norm": 0.027130311354994774,
      "learning_rate": 0.00033376267705196525,
      "loss": 0.8685,
      "step": 5551
    },
    {
      "epoch": 7.402666666666667,
      "grad_norm": 0.021500224247574806,
      "learning_rate": 0.00033344070335597167,
      "loss": 1.074,
      "step": 5552
    },
    {
      "epoch": 7.404,
      "grad_norm": 0.025148924440145493,
      "learning_rate": 0.00033311885396038,
      "loss": 0.8329,
      "step": 5553
    },
    {
      "epoch": 7.405333333333333,
      "grad_norm": 0.020192673429846764,
      "learning_rate": 0.0003327971289252092,
      "loss": 0.8992,
      "step": 5554
    },
    {
      "epoch": 7.406666666666666,
      "grad_norm": 0.019027728587388992,
      "learning_rate": 0.0003324755283104548,
      "loss": 0.9112,
      "step": 5555
    },
    {
      "epoch": 7.408,
      "grad_norm": 0.022602690383791924,
      "learning_rate": 0.0003321540521760883,
      "loss": 0.8523,
      "step": 5556
    },
    {
      "epoch": 7.4093333333333335,
      "grad_norm": 0.02371085062623024,
      "learning_rate": 0.0003318327005820599,
      "loss": 1.201,
      "step": 5557
    },
    {
      "epoch": 7.410666666666667,
      "grad_norm": 0.047316212207078934,
      "learning_rate": 0.0003315114735882947,
      "loss": 1.1973,
      "step": 5558
    },
    {
      "epoch": 7.412,
      "grad_norm": 0.0231027789413929,
      "learning_rate": 0.00033119037125469553,
      "loss": 0.9824,
      "step": 5559
    },
    {
      "epoch": 7.413333333333333,
      "grad_norm": 0.024455346167087555,
      "learning_rate": 0.0003308693936411421,
      "loss": 0.9384,
      "step": 5560
    },
    {
      "epoch": 7.414666666666666,
      "grad_norm": 0.027382539585232735,
      "learning_rate": 0.0003305485408074894,
      "loss": 0.8649,
      "step": 5561
    },
    {
      "epoch": 7.416,
      "grad_norm": 0.025429870933294296,
      "learning_rate": 0.0003302278128135718,
      "loss": 0.9669,
      "step": 5562
    },
    {
      "epoch": 7.417333333333334,
      "grad_norm": 0.024609550833702087,
      "learning_rate": 0.00032990720971919784,
      "loss": 1.0149,
      "step": 5563
    },
    {
      "epoch": 7.418666666666667,
      "grad_norm": 0.03132007643580437,
      "learning_rate": 0.000329586731584154,
      "loss": 1.0902,
      "step": 5564
    },
    {
      "epoch": 7.42,
      "grad_norm": 0.023875301703810692,
      "learning_rate": 0.0003292663784682036,
      "loss": 0.944,
      "step": 5565
    },
    {
      "epoch": 7.421333333333333,
      "grad_norm": 0.035709574818611145,
      "learning_rate": 0.00032894615043108544,
      "loss": 1.0495,
      "step": 5566
    },
    {
      "epoch": 7.422666666666666,
      "grad_norm": 0.026417234912514687,
      "learning_rate": 0.00032862604753251703,
      "loss": 1.078,
      "step": 5567
    },
    {
      "epoch": 7.424,
      "grad_norm": 0.021386755630373955,
      "learning_rate": 0.0003283060698321904,
      "loss": 0.8994,
      "step": 5568
    },
    {
      "epoch": 7.425333333333334,
      "grad_norm": 0.028648370876908302,
      "learning_rate": 0.0003279862173897754,
      "loss": 0.6825,
      "step": 5569
    },
    {
      "epoch": 7.426666666666667,
      "grad_norm": 0.025966517627239227,
      "learning_rate": 0.00032766649026491856,
      "loss": 1.0966,
      "step": 5570
    },
    {
      "epoch": 7.428,
      "grad_norm": 0.031619295477867126,
      "learning_rate": 0.0003273468885172427,
      "loss": 1.1924,
      "step": 5571
    },
    {
      "epoch": 7.429333333333333,
      "grad_norm": 0.02136634662747383,
      "learning_rate": 0.0003270274122063477,
      "loss": 0.7962,
      "step": 5572
    },
    {
      "epoch": 7.430666666666666,
      "grad_norm": 0.03059161640703678,
      "learning_rate": 0.000326708061391809,
      "loss": 1.3388,
      "step": 5573
    },
    {
      "epoch": 7.432,
      "grad_norm": 0.03148415684700012,
      "learning_rate": 0.0003263888361331797,
      "loss": 1.0535,
      "step": 5574
    },
    {
      "epoch": 7.433333333333334,
      "grad_norm": 0.024142403155565262,
      "learning_rate": 0.0003260697364899892,
      "loss": 1.0273,
      "step": 5575
    },
    {
      "epoch": 7.434666666666667,
      "grad_norm": 0.025503994897007942,
      "learning_rate": 0.0003257507625217432,
      "loss": 1.0801,
      "step": 5576
    },
    {
      "epoch": 7.436,
      "grad_norm": 0.019099703058600426,
      "learning_rate": 0.0003254319142879246,
      "loss": 1.0348,
      "step": 5577
    },
    {
      "epoch": 7.437333333333333,
      "grad_norm": 0.035806458443403244,
      "learning_rate": 0.00032511319184799157,
      "loss": 1.1138,
      "step": 5578
    },
    {
      "epoch": 7.438666666666666,
      "grad_norm": 0.02804073877632618,
      "learning_rate": 0.00032479459526138087,
      "loss": 1.2386,
      "step": 5579
    },
    {
      "epoch": 7.44,
      "grad_norm": 0.018816016614437103,
      "learning_rate": 0.00032447612458750365,
      "loss": 0.795,
      "step": 5580
    },
    {
      "epoch": 7.441333333333334,
      "grad_norm": 0.024587435647845268,
      "learning_rate": 0.0003241577798857488,
      "loss": 0.8944,
      "step": 5581
    },
    {
      "epoch": 7.442666666666667,
      "grad_norm": 0.024263665080070496,
      "learning_rate": 0.0003238395612154818,
      "loss": 1.3143,
      "step": 5582
    },
    {
      "epoch": 7.444,
      "grad_norm": 0.02344338782131672,
      "learning_rate": 0.0003235214686360432,
      "loss": 0.9739,
      "step": 5583
    },
    {
      "epoch": 7.445333333333333,
      "grad_norm": 0.027302471920847893,
      "learning_rate": 0.0003232035022067525,
      "loss": 1.3717,
      "step": 5584
    },
    {
      "epoch": 7.446666666666666,
      "grad_norm": 0.02299359068274498,
      "learning_rate": 0.0003228856619869034,
      "loss": 0.9892,
      "step": 5585
    },
    {
      "epoch": 7.448,
      "grad_norm": 0.026060564443469048,
      "learning_rate": 0.0003225679480357671,
      "loss": 1.3584,
      "step": 5586
    },
    {
      "epoch": 7.449333333333334,
      "grad_norm": 0.02678617276251316,
      "learning_rate": 0.0003222503604125915,
      "loss": 1.2263,
      "step": 5587
    },
    {
      "epoch": 7.450666666666667,
      "grad_norm": 0.02199304662644863,
      "learning_rate": 0.00032193289917659953,
      "loss": 0.9353,
      "step": 5588
    },
    {
      "epoch": 7.452,
      "grad_norm": 0.021480215713381767,
      "learning_rate": 0.000321615564386993,
      "loss": 1.0611,
      "step": 5589
    },
    {
      "epoch": 7.453333333333333,
      "grad_norm": 0.026102636009454727,
      "learning_rate": 0.00032129835610294756,
      "loss": 0.817,
      "step": 5590
    },
    {
      "epoch": 7.454666666666666,
      "grad_norm": 0.024869555607438087,
      "learning_rate": 0.0003209812743836168,
      "loss": 1.0003,
      "step": 5591
    },
    {
      "epoch": 7.456,
      "grad_norm": 0.025574466213583946,
      "learning_rate": 0.0003206643192881307,
      "loss": 1.109,
      "step": 5592
    },
    {
      "epoch": 7.457333333333334,
      "grad_norm": 0.021421801298856735,
      "learning_rate": 0.0003203474908755941,
      "loss": 1.0107,
      "step": 5593
    },
    {
      "epoch": 7.458666666666667,
      "grad_norm": 0.026770848780870438,
      "learning_rate": 0.0003200307892050909,
      "loss": 0.8873,
      "step": 5594
    },
    {
      "epoch": 7.46,
      "grad_norm": 0.027329739183187485,
      "learning_rate": 0.00031971421433567873,
      "loss": 1.1298,
      "step": 5595
    },
    {
      "epoch": 7.461333333333333,
      "grad_norm": 0.0233989916741848,
      "learning_rate": 0.00031939776632639293,
      "loss": 0.9842,
      "step": 5596
    },
    {
      "epoch": 7.462666666666666,
      "grad_norm": 0.02522544004023075,
      "learning_rate": 0.00031908144523624536,
      "loss": 0.9547,
      "step": 5597
    },
    {
      "epoch": 7.464,
      "grad_norm": 0.022113008424639702,
      "learning_rate": 0.00031876525112422286,
      "loss": 0.9068,
      "step": 5598
    },
    {
      "epoch": 7.465333333333334,
      "grad_norm": 0.03337123245000839,
      "learning_rate": 0.00031844918404929056,
      "loss": 0.9971,
      "step": 5599
    },
    {
      "epoch": 7.466666666666667,
      "grad_norm": 0.024482402950525284,
      "learning_rate": 0.00031813324407038826,
      "loss": 0.9123,
      "step": 5600
    },
    {
      "epoch": 7.468,
      "grad_norm": 0.025586379691958427,
      "learning_rate": 0.0003178174312464326,
      "loss": 0.7137,
      "step": 5601
    },
    {
      "epoch": 7.469333333333333,
      "grad_norm": 0.021112630143761635,
      "learning_rate": 0.00031750174563631705,
      "loss": 0.9684,
      "step": 5602
    },
    {
      "epoch": 7.470666666666666,
      "grad_norm": 0.02461559884250164,
      "learning_rate": 0.0003171861872989098,
      "loss": 0.9703,
      "step": 5603
    },
    {
      "epoch": 7.4719999999999995,
      "grad_norm": 0.02185835689306259,
      "learning_rate": 0.0003168707562930578,
      "loss": 0.851,
      "step": 5604
    },
    {
      "epoch": 7.473333333333334,
      "grad_norm": 0.02418183721601963,
      "learning_rate": 0.00031655545267758156,
      "loss": 1.0085,
      "step": 5605
    },
    {
      "epoch": 7.474666666666667,
      "grad_norm": 0.023248033598065376,
      "learning_rate": 0.0003162402765112802,
      "loss": 1.1281,
      "step": 5606
    },
    {
      "epoch": 7.476,
      "grad_norm": 0.03407534211874008,
      "learning_rate": 0.0003159252278529271,
      "loss": 1.0575,
      "step": 5607
    },
    {
      "epoch": 7.477333333333333,
      "grad_norm": 0.027468210086226463,
      "learning_rate": 0.0003156103067612731,
      "loss": 0.9452,
      "step": 5608
    },
    {
      "epoch": 7.478666666666666,
      "grad_norm": 0.026427464559674263,
      "learning_rate": 0.00031529551329504514,
      "loss": 0.7751,
      "step": 5609
    },
    {
      "epoch": 7.48,
      "grad_norm": 0.024632422253489494,
      "learning_rate": 0.0003149808475129452,
      "loss": 0.9701,
      "step": 5610
    },
    {
      "epoch": 7.481333333333334,
      "grad_norm": 0.02855387143790722,
      "learning_rate": 0.0003146663094736536,
      "loss": 1.1192,
      "step": 5611
    },
    {
      "epoch": 7.482666666666667,
      "grad_norm": 0.027295833453536034,
      "learning_rate": 0.0003143518992358246,
      "loss": 1.0098,
      "step": 5612
    },
    {
      "epoch": 7.484,
      "grad_norm": 0.021281396970152855,
      "learning_rate": 0.0003140376168580901,
      "loss": 0.9744,
      "step": 5613
    },
    {
      "epoch": 7.485333333333333,
      "grad_norm": 0.0337800607085228,
      "learning_rate": 0.0003137234623990577,
      "loss": 1.061,
      "step": 5614
    },
    {
      "epoch": 7.486666666666666,
      "grad_norm": 0.031989291310310364,
      "learning_rate": 0.0003134094359173104,
      "loss": 1.1179,
      "step": 5615
    },
    {
      "epoch": 7.4879999999999995,
      "grad_norm": 0.03009404055774212,
      "learning_rate": 0.0003130955374714094,
      "loss": 0.9009,
      "step": 5616
    },
    {
      "epoch": 7.489333333333334,
      "grad_norm": 0.029538482427597046,
      "learning_rate": 0.00031278176711988946,
      "loss": 0.8437,
      "step": 5617
    },
    {
      "epoch": 7.490666666666667,
      "grad_norm": 0.030606213957071304,
      "learning_rate": 0.0003124681249212632,
      "loss": 1.0314,
      "step": 5618
    },
    {
      "epoch": 7.492,
      "grad_norm": 0.02915899269282818,
      "learning_rate": 0.00031215461093401907,
      "loss": 1.1818,
      "step": 5619
    },
    {
      "epoch": 7.493333333333333,
      "grad_norm": 0.028091665357351303,
      "learning_rate": 0.0003118412252166205,
      "loss": 1.3128,
      "step": 5620
    },
    {
      "epoch": 7.494666666666666,
      "grad_norm": 0.029442237690091133,
      "learning_rate": 0.0003115279678275089,
      "loss": 1.1649,
      "step": 5621
    },
    {
      "epoch": 7.496,
      "grad_norm": 0.026662025600671768,
      "learning_rate": 0.0003112148388250999,
      "loss": 1.1759,
      "step": 5622
    },
    {
      "epoch": 7.497333333333334,
      "grad_norm": 0.02290804497897625,
      "learning_rate": 0.0003109018382677864,
      "loss": 0.881,
      "step": 5623
    },
    {
      "epoch": 7.498666666666667,
      "grad_norm": 0.04346274212002754,
      "learning_rate": 0.0003105889662139371,
      "loss": 1.0605,
      "step": 5624
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.02466622367501259,
      "learning_rate": 0.00031027622272189573,
      "loss": 0.8552,
      "step": 5625
    },
    {
      "epoch": 7.501333333333333,
      "grad_norm": 0.03438206762075424,
      "learning_rate": 0.0003099636078499841,
      "loss": 1.0798,
      "step": 5626
    },
    {
      "epoch": 7.502666666666666,
      "grad_norm": 0.026088131591677666,
      "learning_rate": 0.0003096511216564979,
      "loss": 1.0638,
      "step": 5627
    },
    {
      "epoch": 7.504,
      "grad_norm": 0.025260956957936287,
      "learning_rate": 0.00030933876419971006,
      "loss": 0.9145,
      "step": 5628
    },
    {
      "epoch": 7.505333333333334,
      "grad_norm": 0.032067202031612396,
      "learning_rate": 0.00030902653553786964,
      "loss": 1.0972,
      "step": 5629
    },
    {
      "epoch": 7.506666666666667,
      "grad_norm": 0.02222876250743866,
      "learning_rate": 0.0003087144357292001,
      "loss": 1.0718,
      "step": 5630
    },
    {
      "epoch": 7.508,
      "grad_norm": 0.027294093742966652,
      "learning_rate": 0.0003084024648319034,
      "loss": 0.9706,
      "step": 5631
    },
    {
      "epoch": 7.509333333333333,
      "grad_norm": 0.026758329942822456,
      "learning_rate": 0.0003080906229041547,
      "loss": 1.0128,
      "step": 5632
    },
    {
      "epoch": 7.510666666666666,
      "grad_norm": 0.03318726643919945,
      "learning_rate": 0.00030777891000410775,
      "loss": 0.9981,
      "step": 5633
    },
    {
      "epoch": 7.5120000000000005,
      "grad_norm": 0.026643013581633568,
      "learning_rate": 0.00030746732618989027,
      "loss": 0.8406,
      "step": 5634
    },
    {
      "epoch": 7.513333333333334,
      "grad_norm": 0.02861301228404045,
      "learning_rate": 0.0003071558715196066,
      "loss": 1.0453,
      "step": 5635
    },
    {
      "epoch": 7.514666666666667,
      "grad_norm": 0.03374072536826134,
      "learning_rate": 0.00030684454605133747,
      "loss": 1.2329,
      "step": 5636
    },
    {
      "epoch": 7.516,
      "grad_norm": 0.023865515366196632,
      "learning_rate": 0.0003065333498431381,
      "loss": 1.1405,
      "step": 5637
    },
    {
      "epoch": 7.517333333333333,
      "grad_norm": 0.049804192036390305,
      "learning_rate": 0.0003062222829530419,
      "loss": 0.9508,
      "step": 5638
    },
    {
      "epoch": 7.518666666666666,
      "grad_norm": 0.021976767107844353,
      "learning_rate": 0.00030591134543905563,
      "loss": 1.0056,
      "step": 5639
    },
    {
      "epoch": 7.52,
      "grad_norm": 0.022662917152047157,
      "learning_rate": 0.00030560053735916373,
      "loss": 0.9408,
      "step": 5640
    },
    {
      "epoch": 7.521333333333334,
      "grad_norm": 0.035993900150060654,
      "learning_rate": 0.00030528985877132595,
      "loss": 1.0642,
      "step": 5641
    },
    {
      "epoch": 7.522666666666667,
      "grad_norm": 0.023268267512321472,
      "learning_rate": 0.0003049793097334771,
      "loss": 0.8338,
      "step": 5642
    },
    {
      "epoch": 7.524,
      "grad_norm": 0.027296841144561768,
      "learning_rate": 0.00030466889030352976,
      "loss": 1.156,
      "step": 5643
    },
    {
      "epoch": 7.525333333333333,
      "grad_norm": 0.0295378640294075,
      "learning_rate": 0.00030435860053937004,
      "loss": 0.8334,
      "step": 5644
    },
    {
      "epoch": 7.526666666666666,
      "grad_norm": 0.027407843619585037,
      "learning_rate": 0.00030404844049886136,
      "loss": 1.0415,
      "step": 5645
    },
    {
      "epoch": 7.5280000000000005,
      "grad_norm": 0.025011014193296432,
      "learning_rate": 0.00030373841023984303,
      "loss": 0.8852,
      "step": 5646
    },
    {
      "epoch": 7.529333333333334,
      "grad_norm": 0.04204900935292244,
      "learning_rate": 0.0003034285098201286,
      "loss": 1.1741,
      "step": 5647
    },
    {
      "epoch": 7.530666666666667,
      "grad_norm": 0.026393359526991844,
      "learning_rate": 0.0003031187392975099,
      "loss": 1.289,
      "step": 5648
    },
    {
      "epoch": 7.532,
      "grad_norm": 0.03570866212248802,
      "learning_rate": 0.00030280909872975193,
      "loss": 1.1082,
      "step": 5649
    },
    {
      "epoch": 7.533333333333333,
      "grad_norm": 0.02008746936917305,
      "learning_rate": 0.0003024995881745972,
      "loss": 1.0807,
      "step": 5650
    },
    {
      "epoch": 7.534666666666666,
      "grad_norm": 0.025826191529631615,
      "learning_rate": 0.0003021902076897637,
      "loss": 0.9607,
      "step": 5651
    },
    {
      "epoch": 7.536,
      "grad_norm": 0.03121078759431839,
      "learning_rate": 0.00030188095733294385,
      "loss": 0.9207,
      "step": 5652
    },
    {
      "epoch": 7.537333333333334,
      "grad_norm": 0.028028743341565132,
      "learning_rate": 0.00030157183716180834,
      "loss": 1.0848,
      "step": 5653
    },
    {
      "epoch": 7.538666666666667,
      "grad_norm": 0.029075078666210175,
      "learning_rate": 0.0003012628472340009,
      "loss": 1.0964,
      "step": 5654
    },
    {
      "epoch": 7.54,
      "grad_norm": 0.024786515161395073,
      "learning_rate": 0.0003009539876071427,
      "loss": 1.0826,
      "step": 5655
    },
    {
      "epoch": 7.541333333333333,
      "grad_norm": 0.028602151200175285,
      "learning_rate": 0.00030064525833883014,
      "loss": 0.9474,
      "step": 5656
    },
    {
      "epoch": 7.542666666666666,
      "grad_norm": 0.026046395301818848,
      "learning_rate": 0.00030033665948663447,
      "loss": 0.7801,
      "step": 5657
    },
    {
      "epoch": 7.5440000000000005,
      "grad_norm": 0.030982784926891327,
      "learning_rate": 0.00030002819110810475,
      "loss": 0.8521,
      "step": 5658
    },
    {
      "epoch": 7.545333333333334,
      "grad_norm": 0.02378961071372032,
      "learning_rate": 0.0002997198532607629,
      "loss": 0.9649,
      "step": 5659
    },
    {
      "epoch": 7.546666666666667,
      "grad_norm": 0.03146999701857567,
      "learning_rate": 0.00029941164600210936,
      "loss": 1.0644,
      "step": 5660
    },
    {
      "epoch": 7.548,
      "grad_norm": 0.0287385955452919,
      "learning_rate": 0.0002991035693896178,
      "loss": 0.844,
      "step": 5661
    },
    {
      "epoch": 7.549333333333333,
      "grad_norm": 0.026141716167330742,
      "learning_rate": 0.00029879562348073896,
      "loss": 1.1277,
      "step": 5662
    },
    {
      "epoch": 7.550666666666666,
      "grad_norm": 0.03210445120930672,
      "learning_rate": 0.0002984878083328989,
      "loss": 1.2075,
      "step": 5663
    },
    {
      "epoch": 7.552,
      "grad_norm": 0.02293003909289837,
      "learning_rate": 0.0002981801240034985,
      "loss": 0.7827,
      "step": 5664
    },
    {
      "epoch": 7.553333333333334,
      "grad_norm": 0.026840727776288986,
      "learning_rate": 0.0002978725705499159,
      "loss": 1.0749,
      "step": 5665
    },
    {
      "epoch": 7.554666666666667,
      "grad_norm": 0.026015136390924454,
      "learning_rate": 0.00029756514802950317,
      "loss": 1.0446,
      "step": 5666
    },
    {
      "epoch": 7.556,
      "grad_norm": 0.022920764982700348,
      "learning_rate": 0.0002972578564995889,
      "loss": 1.2213,
      "step": 5667
    },
    {
      "epoch": 7.557333333333333,
      "grad_norm": 0.02214108593761921,
      "learning_rate": 0.00029695069601747725,
      "loss": 1.345,
      "step": 5668
    },
    {
      "epoch": 7.558666666666666,
      "grad_norm": 0.021110404282808304,
      "learning_rate": 0.00029664366664044686,
      "loss": 0.908,
      "step": 5669
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 0.016638709232211113,
      "learning_rate": 0.00029633676842575387,
      "loss": 1.066,
      "step": 5670
    },
    {
      "epoch": 7.561333333333334,
      "grad_norm": 0.0394035167992115,
      "learning_rate": 0.0002960300014306281,
      "loss": 0.8028,
      "step": 5671
    },
    {
      "epoch": 7.562666666666667,
      "grad_norm": 0.02422187104821205,
      "learning_rate": 0.00029572336571227585,
      "loss": 1.001,
      "step": 5672
    },
    {
      "epoch": 7.564,
      "grad_norm": 0.049185000360012054,
      "learning_rate": 0.00029541686132787903,
      "loss": 0.8359,
      "step": 5673
    },
    {
      "epoch": 7.565333333333333,
      "grad_norm": 0.024699557572603226,
      "learning_rate": 0.00029511048833459397,
      "loss": 1.1579,
      "step": 5674
    },
    {
      "epoch": 7.566666666666666,
      "grad_norm": 0.028422342613339424,
      "learning_rate": 0.0002948042467895544,
      "loss": 1.0248,
      "step": 5675
    },
    {
      "epoch": 7.568,
      "grad_norm": 0.017498863860964775,
      "learning_rate": 0.0002944981367498677,
      "loss": 0.8064,
      "step": 5676
    },
    {
      "epoch": 7.569333333333334,
      "grad_norm": 0.026418663561344147,
      "learning_rate": 0.0002941921582726176,
      "loss": 0.8655,
      "step": 5677
    },
    {
      "epoch": 7.570666666666667,
      "grad_norm": 0.022305965423583984,
      "learning_rate": 0.00029388631141486344,
      "loss": 0.9168,
      "step": 5678
    },
    {
      "epoch": 7.572,
      "grad_norm": 0.02499915473163128,
      "learning_rate": 0.00029358059623363895,
      "loss": 1.2121,
      "step": 5679
    },
    {
      "epoch": 7.573333333333333,
      "grad_norm": 0.026092685759067535,
      "learning_rate": 0.00029327501278595525,
      "loss": 1.0562,
      "step": 5680
    },
    {
      "epoch": 7.574666666666666,
      "grad_norm": 0.02304101549088955,
      "learning_rate": 0.00029296956112879693,
      "loss": 0.759,
      "step": 5681
    },
    {
      "epoch": 7.576,
      "grad_norm": 0.03371906280517578,
      "learning_rate": 0.00029266424131912494,
      "loss": 0.9036,
      "step": 5682
    },
    {
      "epoch": 7.577333333333334,
      "grad_norm": 0.030032793059945107,
      "learning_rate": 0.0002923590534138756,
      "loss": 0.853,
      "step": 5683
    },
    {
      "epoch": 7.578666666666667,
      "grad_norm": 0.026808101683855057,
      "learning_rate": 0.00029205399746996043,
      "loss": 1.0172,
      "step": 5684
    },
    {
      "epoch": 7.58,
      "grad_norm": 0.0301621463149786,
      "learning_rate": 0.0002917490735442669,
      "loss": 0.9188,
      "step": 5685
    },
    {
      "epoch": 7.581333333333333,
      "grad_norm": 0.02403303235769272,
      "learning_rate": 0.00029144428169365687,
      "loss": 0.9885,
      "step": 5686
    },
    {
      "epoch": 7.582666666666666,
      "grad_norm": 0.025687580928206444,
      "learning_rate": 0.0002911396219749684,
      "loss": 1.1656,
      "step": 5687
    },
    {
      "epoch": 7.584,
      "grad_norm": 0.021858708932995796,
      "learning_rate": 0.00029083509444501434,
      "loss": 1.0511,
      "step": 5688
    },
    {
      "epoch": 7.585333333333334,
      "grad_norm": 0.023232456296682358,
      "learning_rate": 0.0002905306991605835,
      "loss": 0.9436,
      "step": 5689
    },
    {
      "epoch": 7.586666666666667,
      "grad_norm": 0.031599175184965134,
      "learning_rate": 0.0002902264361784399,
      "loss": 1.0668,
      "step": 5690
    },
    {
      "epoch": 7.588,
      "grad_norm": 0.02917134016752243,
      "learning_rate": 0.00028992230555532206,
      "loss": 0.9599,
      "step": 5691
    },
    {
      "epoch": 7.589333333333333,
      "grad_norm": 0.03711812198162079,
      "learning_rate": 0.0002896183073479448,
      "loss": 0.9349,
      "step": 5692
    },
    {
      "epoch": 7.5906666666666665,
      "grad_norm": 0.025914980098605156,
      "learning_rate": 0.0002893144416129979,
      "loss": 1.1566,
      "step": 5693
    },
    {
      "epoch": 7.592,
      "grad_norm": 0.03179021552205086,
      "learning_rate": 0.0002890107084071465,
      "loss": 1.1236,
      "step": 5694
    },
    {
      "epoch": 7.593333333333334,
      "grad_norm": 0.05882294476032257,
      "learning_rate": 0.000288707107787031,
      "loss": 1.0473,
      "step": 5695
    },
    {
      "epoch": 7.594666666666667,
      "grad_norm": 0.02816472388803959,
      "learning_rate": 0.0002884036398092668,
      "loss": 1.1596,
      "step": 5696
    },
    {
      "epoch": 7.596,
      "grad_norm": 0.02557915821671486,
      "learning_rate": 0.00028810030453044476,
      "loss": 1.0003,
      "step": 5697
    },
    {
      "epoch": 7.597333333333333,
      "grad_norm": 0.02347983606159687,
      "learning_rate": 0.0002877971020071314,
      "loss": 1.0507,
      "step": 5698
    },
    {
      "epoch": 7.5986666666666665,
      "grad_norm": 0.030533958226442337,
      "learning_rate": 0.0002874940322958678,
      "loss": 1.3471,
      "step": 5699
    },
    {
      "epoch": 7.6,
      "grad_norm": 0.02004527486860752,
      "learning_rate": 0.00028719109545317104,
      "loss": 1.0866,
      "step": 5700
    },
    {
      "epoch": 7.601333333333334,
      "grad_norm": 0.025528959929943085,
      "learning_rate": 0.0002868882915355322,
      "loss": 1.1225,
      "step": 5701
    },
    {
      "epoch": 7.602666666666667,
      "grad_norm": 0.01931818202137947,
      "learning_rate": 0.000286585620599419,
      "loss": 0.7923,
      "step": 5702
    },
    {
      "epoch": 7.604,
      "grad_norm": 0.02411622740328312,
      "learning_rate": 0.00028628308270127337,
      "loss": 1.0488,
      "step": 5703
    },
    {
      "epoch": 7.605333333333333,
      "grad_norm": 0.022295836359262466,
      "learning_rate": 0.00028598067789751294,
      "loss": 1.0652,
      "step": 5704
    },
    {
      "epoch": 7.6066666666666665,
      "grad_norm": 0.027687914669513702,
      "learning_rate": 0.00028567840624453057,
      "loss": 1.4709,
      "step": 5705
    },
    {
      "epoch": 7.608,
      "grad_norm": 0.021098436787724495,
      "learning_rate": 0.0002853762677986932,
      "loss": 1.0591,
      "step": 5706
    },
    {
      "epoch": 7.609333333333334,
      "grad_norm": 0.02590252086520195,
      "learning_rate": 0.0002850742626163451,
      "loss": 0.5672,
      "step": 5707
    },
    {
      "epoch": 7.610666666666667,
      "grad_norm": 0.022088011726737022,
      "learning_rate": 0.00028477239075380337,
      "loss": 0.8185,
      "step": 5708
    },
    {
      "epoch": 7.612,
      "grad_norm": 0.022064371034502983,
      "learning_rate": 0.0002844706522673616,
      "loss": 1.0738,
      "step": 5709
    },
    {
      "epoch": 7.613333333333333,
      "grad_norm": 0.02417628839612007,
      "learning_rate": 0.00028416904721328815,
      "loss": 0.9009,
      "step": 5710
    },
    {
      "epoch": 7.6146666666666665,
      "grad_norm": 0.026317855343222618,
      "learning_rate": 0.0002838675756478265,
      "loss": 0.8087,
      "step": 5711
    },
    {
      "epoch": 7.616,
      "grad_norm": 0.020244156941771507,
      "learning_rate": 0.0002835662376271957,
      "loss": 0.9667,
      "step": 5712
    },
    {
      "epoch": 7.617333333333333,
      "grad_norm": 0.03143925592303276,
      "learning_rate": 0.0002832650332075888,
      "loss": 1.247,
      "step": 5713
    },
    {
      "epoch": 7.618666666666667,
      "grad_norm": 0.02423606440424919,
      "learning_rate": 0.0002829639624451747,
      "loss": 0.896,
      "step": 5714
    },
    {
      "epoch": 7.62,
      "grad_norm": 0.1267717331647873,
      "learning_rate": 0.00028266302539609744,
      "loss": 0.7348,
      "step": 5715
    },
    {
      "epoch": 7.621333333333333,
      "grad_norm": 0.02880101278424263,
      "learning_rate": 0.000282362222116476,
      "loss": 0.8542,
      "step": 5716
    },
    {
      "epoch": 7.6226666666666665,
      "grad_norm": 0.02042088285088539,
      "learning_rate": 0.0002820615526624046,
      "loss": 0.7906,
      "step": 5717
    },
    {
      "epoch": 7.624,
      "grad_norm": 0.026255546137690544,
      "learning_rate": 0.00028176101708995174,
      "loss": 0.9235,
      "step": 5718
    },
    {
      "epoch": 7.625333333333334,
      "grad_norm": 0.027264926582574844,
      "learning_rate": 0.00028146061545516153,
      "loss": 0.8982,
      "step": 5719
    },
    {
      "epoch": 7.626666666666667,
      "grad_norm": 0.025548039004206657,
      "learning_rate": 0.0002811603478140534,
      "loss": 1.0078,
      "step": 5720
    },
    {
      "epoch": 7.628,
      "grad_norm": 0.022090919315814972,
      "learning_rate": 0.0002808602142226212,
      "loss": 0.9862,
      "step": 5721
    },
    {
      "epoch": 7.629333333333333,
      "grad_norm": 0.019439952448010445,
      "learning_rate": 0.00028056021473683444,
      "loss": 0.7895,
      "step": 5722
    },
    {
      "epoch": 7.6306666666666665,
      "grad_norm": 0.02434348687529564,
      "learning_rate": 0.0002802603494126367,
      "loss": 0.9969,
      "step": 5723
    },
    {
      "epoch": 7.632,
      "grad_norm": 0.024203229695558548,
      "learning_rate": 0.0002799606183059471,
      "loss": 0.8651,
      "step": 5724
    },
    {
      "epoch": 7.633333333333333,
      "grad_norm": 0.026258785277605057,
      "learning_rate": 0.0002796610214726599,
      "loss": 1.0923,
      "step": 5725
    },
    {
      "epoch": 7.634666666666667,
      "grad_norm": 0.027123788371682167,
      "learning_rate": 0.0002793615589686441,
      "loss": 0.773,
      "step": 5726
    },
    {
      "epoch": 7.636,
      "grad_norm": 0.021070150658488274,
      "learning_rate": 0.000279062230849744,
      "loss": 1.0816,
      "step": 5727
    },
    {
      "epoch": 7.637333333333333,
      "grad_norm": 0.035817768424749374,
      "learning_rate": 0.00027876303717177766,
      "loss": 1.3895,
      "step": 5728
    },
    {
      "epoch": 7.6386666666666665,
      "grad_norm": 0.025173749774694443,
      "learning_rate": 0.00027846397799053947,
      "loss": 1.1012,
      "step": 5729
    },
    {
      "epoch": 7.64,
      "grad_norm": 0.03401301056146622,
      "learning_rate": 0.000278165053361798,
      "loss": 0.8253,
      "step": 5730
    },
    {
      "epoch": 7.641333333333334,
      "grad_norm": 0.01877954602241516,
      "learning_rate": 0.00027786626334129694,
      "loss": 1.0117,
      "step": 5731
    },
    {
      "epoch": 7.642666666666667,
      "grad_norm": 0.032532740384340286,
      "learning_rate": 0.0002775676079847552,
      "loss": 0.8609,
      "step": 5732
    },
    {
      "epoch": 7.644,
      "grad_norm": 0.022715365514159203,
      "learning_rate": 0.0002772690873478656,
      "loss": 0.8538,
      "step": 5733
    },
    {
      "epoch": 7.645333333333333,
      "grad_norm": 0.028656188398599625,
      "learning_rate": 0.00027697070148629676,
      "loss": 0.9785,
      "step": 5734
    },
    {
      "epoch": 7.6466666666666665,
      "grad_norm": 0.021714985370635986,
      "learning_rate": 0.0002766724504556919,
      "loss": 0.8301,
      "step": 5735
    },
    {
      "epoch": 7.648,
      "grad_norm": 0.028100529685616493,
      "learning_rate": 0.000276374334311669,
      "loss": 1.1556,
      "step": 5736
    },
    {
      "epoch": 7.649333333333333,
      "grad_norm": 0.02807893231511116,
      "learning_rate": 0.00027607635310982094,
      "loss": 1.1775,
      "step": 5737
    },
    {
      "epoch": 7.650666666666667,
      "grad_norm": 0.02635885216295719,
      "learning_rate": 0.0002757785069057158,
      "loss": 1.233,
      "step": 5738
    },
    {
      "epoch": 7.652,
      "grad_norm": 0.02279219962656498,
      "learning_rate": 0.0002754807957548955,
      "loss": 1.1255,
      "step": 5739
    },
    {
      "epoch": 7.653333333333333,
      "grad_norm": 0.022249549627304077,
      "learning_rate": 0.00027518321971287774,
      "loss": 1.0318,
      "step": 5740
    },
    {
      "epoch": 7.6546666666666665,
      "grad_norm": 0.026707487180829048,
      "learning_rate": 0.0002748857788351545,
      "loss": 1.009,
      "step": 5741
    },
    {
      "epoch": 7.656,
      "grad_norm": 0.02567862719297409,
      "learning_rate": 0.00027458847317719305,
      "loss": 0.8852,
      "step": 5742
    },
    {
      "epoch": 7.657333333333334,
      "grad_norm": 0.02457880787551403,
      "learning_rate": 0.00027429130279443527,
      "loss": 1.0418,
      "step": 5743
    },
    {
      "epoch": 7.658666666666667,
      "grad_norm": 0.03411615267395973,
      "learning_rate": 0.0002739942677422971,
      "loss": 0.9727,
      "step": 5744
    },
    {
      "epoch": 7.66,
      "grad_norm": 0.02879912219941616,
      "learning_rate": 0.0002736973680761702,
      "loss": 1.0422,
      "step": 5745
    },
    {
      "epoch": 7.661333333333333,
      "grad_norm": 0.033362746238708496,
      "learning_rate": 0.0002734006038514204,
      "loss": 0.8219,
      "step": 5746
    },
    {
      "epoch": 7.6626666666666665,
      "grad_norm": 0.020625893026590347,
      "learning_rate": 0.0002731039751233887,
      "loss": 0.777,
      "step": 5747
    },
    {
      "epoch": 7.664,
      "grad_norm": 0.031253475695848465,
      "learning_rate": 0.0002728074819473908,
      "loss": 1.2481,
      "step": 5748
    },
    {
      "epoch": 7.665333333333333,
      "grad_norm": 0.027930190786719322,
      "learning_rate": 0.0002725111243787164,
      "loss": 1.3203,
      "step": 5749
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 0.02289838343858719,
      "learning_rate": 0.0002722149024726307,
      "loss": 0.8989,
      "step": 5750
    },
    {
      "epoch": 7.668,
      "grad_norm": 0.02814324013888836,
      "learning_rate": 0.00027191881628437333,
      "loss": 1.0799,
      "step": 5751
    },
    {
      "epoch": 7.669333333333333,
      "grad_norm": 0.0728311538696289,
      "learning_rate": 0.0002716228658691587,
      "loss": 1.0697,
      "step": 5752
    },
    {
      "epoch": 7.6706666666666665,
      "grad_norm": 0.032168999314308167,
      "learning_rate": 0.00027132705128217585,
      "loss": 0.9401,
      "step": 5753
    },
    {
      "epoch": 7.672,
      "grad_norm": 0.028445148840546608,
      "learning_rate": 0.00027103137257858867,
      "loss": 1.3591,
      "step": 5754
    },
    {
      "epoch": 7.673333333333334,
      "grad_norm": 0.01974172331392765,
      "learning_rate": 0.00027073582981353497,
      "loss": 1.0585,
      "step": 5755
    },
    {
      "epoch": 7.674666666666667,
      "grad_norm": 0.02094293013215065,
      "learning_rate": 0.000270440423042128,
      "loss": 1.0509,
      "step": 5756
    },
    {
      "epoch": 7.676,
      "grad_norm": 0.03331255912780762,
      "learning_rate": 0.00027014515231945556,
      "loss": 1.1834,
      "step": 5757
    },
    {
      "epoch": 7.677333333333333,
      "grad_norm": 0.03320932388305664,
      "learning_rate": 0.0002698500177005797,
      "loss": 1.094,
      "step": 5758
    },
    {
      "epoch": 7.6786666666666665,
      "grad_norm": 0.026027193292975426,
      "learning_rate": 0.0002695550192405375,
      "loss": 1.0976,
      "step": 5759
    },
    {
      "epoch": 7.68,
      "grad_norm": 0.02406427450478077,
      "learning_rate": 0.0002692601569943407,
      "loss": 1.0874,
      "step": 5760
    },
    {
      "epoch": 7.681333333333333,
      "grad_norm": 0.027241405099630356,
      "learning_rate": 0.0002689654310169748,
      "loss": 1.0467,
      "step": 5761
    },
    {
      "epoch": 7.682666666666667,
      "grad_norm": 0.037034355103969574,
      "learning_rate": 0.0002686708413634007,
      "loss": 0.8612,
      "step": 5762
    },
    {
      "epoch": 7.684,
      "grad_norm": 0.02331644855439663,
      "learning_rate": 0.0002683763880885538,
      "loss": 1.083,
      "step": 5763
    },
    {
      "epoch": 7.685333333333333,
      "grad_norm": 0.048879675567150116,
      "learning_rate": 0.00026808207124734385,
      "loss": 1.1666,
      "step": 5764
    },
    {
      "epoch": 7.6866666666666665,
      "grad_norm": 0.030072703957557678,
      "learning_rate": 0.0002677878908946555,
      "loss": 1.0963,
      "step": 5765
    },
    {
      "epoch": 7.688,
      "grad_norm": 0.02454458177089691,
      "learning_rate": 0.0002674938470853472,
      "loss": 1.0622,
      "step": 5766
    },
    {
      "epoch": 7.689333333333334,
      "grad_norm": 0.024241989478468895,
      "learning_rate": 0.0002671999398742527,
      "loss": 1.0313,
      "step": 5767
    },
    {
      "epoch": 7.690666666666667,
      "grad_norm": 0.019820012152194977,
      "learning_rate": 0.00026690616931617997,
      "loss": 1.0594,
      "step": 5768
    },
    {
      "epoch": 7.692,
      "grad_norm": 0.029882097616791725,
      "learning_rate": 0.0002666125354659116,
      "loss": 0.941,
      "step": 5769
    },
    {
      "epoch": 7.693333333333333,
      "grad_norm": 0.027721188962459564,
      "learning_rate": 0.0002663190383782048,
      "loss": 1.0767,
      "step": 5770
    },
    {
      "epoch": 7.6946666666666665,
      "grad_norm": 0.030032631009817123,
      "learning_rate": 0.00026602567810779054,
      "loss": 0.9006,
      "step": 5771
    },
    {
      "epoch": 7.696,
      "grad_norm": 0.03335653990507126,
      "learning_rate": 0.0002657324547093752,
      "loss": 1.2004,
      "step": 5772
    },
    {
      "epoch": 7.697333333333333,
      "grad_norm": 0.027519401162862778,
      "learning_rate": 0.00026543936823763924,
      "loss": 1.131,
      "step": 5773
    },
    {
      "epoch": 7.698666666666667,
      "grad_norm": 0.061365529894828796,
      "learning_rate": 0.0002651464187472375,
      "loss": 0.8435,
      "step": 5774
    },
    {
      "epoch": 7.7,
      "grad_norm": 0.024186987429857254,
      "learning_rate": 0.0002648536062927999,
      "loss": 0.9082,
      "step": 5775
    },
    {
      "epoch": 7.701333333333333,
      "grad_norm": 0.020258836448192596,
      "learning_rate": 0.00026456093092892943,
      "loss": 0.9096,
      "step": 5776
    },
    {
      "epoch": 7.7026666666666666,
      "grad_norm": 0.03496992588043213,
      "learning_rate": 0.00026426839271020486,
      "loss": 1.0151,
      "step": 5777
    },
    {
      "epoch": 7.704,
      "grad_norm": 0.028943968936800957,
      "learning_rate": 0.00026397599169117883,
      "loss": 1.2052,
      "step": 5778
    },
    {
      "epoch": 7.705333333333334,
      "grad_norm": 0.027595125138759613,
      "learning_rate": 0.00026368372792637853,
      "loss": 0.7701,
      "step": 5779
    },
    {
      "epoch": 7.706666666666667,
      "grad_norm": 0.02851223386824131,
      "learning_rate": 0.0002633916014703057,
      "loss": 1.12,
      "step": 5780
    },
    {
      "epoch": 7.708,
      "grad_norm": 0.027216240763664246,
      "learning_rate": 0.00026309961237743585,
      "loss": 1.2984,
      "step": 5781
    },
    {
      "epoch": 7.709333333333333,
      "grad_norm": 0.026044469326734543,
      "learning_rate": 0.00026280776070221936,
      "loss": 1.0327,
      "step": 5782
    },
    {
      "epoch": 7.710666666666667,
      "grad_norm": 0.02399655431509018,
      "learning_rate": 0.00026251604649908113,
      "loss": 1.1409,
      "step": 5783
    },
    {
      "epoch": 7.712,
      "grad_norm": 0.031786512583494186,
      "learning_rate": 0.00026222446982242,
      "loss": 0.9835,
      "step": 5784
    },
    {
      "epoch": 7.713333333333333,
      "grad_norm": 0.022862814366817474,
      "learning_rate": 0.0002619330307266098,
      "loss": 0.9064,
      "step": 5785
    },
    {
      "epoch": 7.714666666666667,
      "grad_norm": 0.022704362869262695,
      "learning_rate": 0.0002616417292659974,
      "loss": 0.8297,
      "step": 5786
    },
    {
      "epoch": 7.716,
      "grad_norm": 0.022164328023791313,
      "learning_rate": 0.000261350565494906,
      "loss": 1.0635,
      "step": 5787
    },
    {
      "epoch": 7.717333333333333,
      "grad_norm": 0.022425280883908272,
      "learning_rate": 0.00026105953946763117,
      "loss": 0.8754,
      "step": 5788
    },
    {
      "epoch": 7.718666666666667,
      "grad_norm": 0.024533148854970932,
      "learning_rate": 0.00026076865123844385,
      "loss": 1.1526,
      "step": 5789
    },
    {
      "epoch": 7.72,
      "grad_norm": 0.02267465740442276,
      "learning_rate": 0.0002604779008615895,
      "loss": 0.8731,
      "step": 5790
    },
    {
      "epoch": 7.721333333333334,
      "grad_norm": 0.03288419544696808,
      "learning_rate": 0.0002601872883912864,
      "loss": 1.1901,
      "step": 5791
    },
    {
      "epoch": 7.722666666666667,
      "grad_norm": 0.0415957011282444,
      "learning_rate": 0.0002598968138817294,
      "loss": 1.2228,
      "step": 5792
    },
    {
      "epoch": 7.724,
      "grad_norm": 0.028154218569397926,
      "learning_rate": 0.00025960647738708555,
      "loss": 1.3797,
      "step": 5793
    },
    {
      "epoch": 7.725333333333333,
      "grad_norm": 0.022124972194433212,
      "learning_rate": 0.0002593162789614971,
      "loss": 0.8454,
      "step": 5794
    },
    {
      "epoch": 7.726666666666667,
      "grad_norm": 0.02606847882270813,
      "learning_rate": 0.0002590262186590805,
      "loss": 1.0673,
      "step": 5795
    },
    {
      "epoch": 7.728,
      "grad_norm": 0.02146483026444912,
      "learning_rate": 0.0002587362965339265,
      "loss": 0.9547,
      "step": 5796
    },
    {
      "epoch": 7.729333333333333,
      "grad_norm": 0.04258071258664131,
      "learning_rate": 0.0002584465126401002,
      "loss": 0.9811,
      "step": 5797
    },
    {
      "epoch": 7.730666666666667,
      "grad_norm": 0.0332198329269886,
      "learning_rate": 0.0002581568670316401,
      "loss": 0.883,
      "step": 5798
    },
    {
      "epoch": 7.732,
      "grad_norm": 0.02662421017885208,
      "learning_rate": 0.0002578673597625597,
      "loss": 1.0194,
      "step": 5799
    },
    {
      "epoch": 7.733333333333333,
      "grad_norm": 0.025456547737121582,
      "learning_rate": 0.0002575779908868465,
      "loss": 0.9488,
      "step": 5800
    },
    {
      "epoch": 7.734666666666667,
      "grad_norm": 0.024117598310112953,
      "learning_rate": 0.0002572887604584624,
      "loss": 0.955,
      "step": 5801
    },
    {
      "epoch": 7.736,
      "grad_norm": 0.027630914002656937,
      "learning_rate": 0.0002569996685313434,
      "loss": 1.1712,
      "step": 5802
    },
    {
      "epoch": 7.737333333333333,
      "grad_norm": 0.03233041614294052,
      "learning_rate": 0.00025671071515939893,
      "loss": 1.0503,
      "step": 5803
    },
    {
      "epoch": 7.738666666666667,
      "grad_norm": 0.028262224048376083,
      "learning_rate": 0.0002564219003965136,
      "loss": 0.8519,
      "step": 5804
    },
    {
      "epoch": 7.74,
      "grad_norm": 0.034994225949048996,
      "learning_rate": 0.0002561332242965457,
      "loss": 1.0056,
      "step": 5805
    },
    {
      "epoch": 7.741333333333333,
      "grad_norm": 0.02687215991318226,
      "learning_rate": 0.00025584468691332773,
      "loss": 0.8923,
      "step": 5806
    },
    {
      "epoch": 7.742666666666667,
      "grad_norm": 0.022325506433844566,
      "learning_rate": 0.0002555562883006667,
      "loss": 1.0657,
      "step": 5807
    },
    {
      "epoch": 7.744,
      "grad_norm": 0.029554087668657303,
      "learning_rate": 0.00025526802851234267,
      "loss": 0.7658,
      "step": 5808
    },
    {
      "epoch": 7.745333333333333,
      "grad_norm": 0.02217346802353859,
      "learning_rate": 0.0002549799076021109,
      "loss": 1.0363,
      "step": 5809
    },
    {
      "epoch": 7.746666666666667,
      "grad_norm": 0.03404920548200607,
      "learning_rate": 0.0002546919256237002,
      "loss": 1.023,
      "step": 5810
    },
    {
      "epoch": 7.748,
      "grad_norm": 0.025335881859064102,
      "learning_rate": 0.0002544040826308138,
      "loss": 0.8439,
      "step": 5811
    },
    {
      "epoch": 7.749333333333333,
      "grad_norm": 0.020677272230386734,
      "learning_rate": 0.00025411637867712913,
      "loss": 0.8887,
      "step": 5812
    },
    {
      "epoch": 7.750666666666667,
      "grad_norm": 0.035294272005558014,
      "learning_rate": 0.0002538288138162965,
      "loss": 0.9179,
      "step": 5813
    },
    {
      "epoch": 7.752,
      "grad_norm": 0.02781171165406704,
      "learning_rate": 0.00025354138810194227,
      "loss": 0.9878,
      "step": 5814
    },
    {
      "epoch": 7.753333333333333,
      "grad_norm": 0.026261748746037483,
      "learning_rate": 0.00025325410158766505,
      "loss": 0.9724,
      "step": 5815
    },
    {
      "epoch": 7.754666666666667,
      "grad_norm": 0.027306020259857178,
      "learning_rate": 0.0002529669543270383,
      "loss": 0.9673,
      "step": 5816
    },
    {
      "epoch": 7.756,
      "grad_norm": 0.024835802614688873,
      "learning_rate": 0.0002526799463736099,
      "loss": 1.02,
      "step": 5817
    },
    {
      "epoch": 7.757333333333333,
      "grad_norm": 0.029024992138147354,
      "learning_rate": 0.0002523930777809004,
      "loss": 1.2311,
      "step": 5818
    },
    {
      "epoch": 7.758666666666667,
      "grad_norm": 0.029321394860744476,
      "learning_rate": 0.0002521063486024062,
      "loss": 1.0063,
      "step": 5819
    },
    {
      "epoch": 7.76,
      "grad_norm": 0.023259326815605164,
      "learning_rate": 0.0002518197588915961,
      "loss": 1.1333,
      "step": 5820
    },
    {
      "epoch": 7.761333333333333,
      "grad_norm": 0.024601265788078308,
      "learning_rate": 0.0002515333087019137,
      "loss": 1.0749,
      "step": 5821
    },
    {
      "epoch": 7.762666666666667,
      "grad_norm": 0.02349766343832016,
      "learning_rate": 0.00025124699808677675,
      "loss": 1.1604,
      "step": 5822
    },
    {
      "epoch": 7.764,
      "grad_norm": 0.03123682737350464,
      "learning_rate": 0.0002509608270995758,
      "loss": 1.0229,
      "step": 5823
    },
    {
      "epoch": 7.765333333333333,
      "grad_norm": 0.028537148609757423,
      "learning_rate": 0.00025067479579367726,
      "loss": 1.2388,
      "step": 5824
    },
    {
      "epoch": 7.766666666666667,
      "grad_norm": 0.028928060084581375,
      "learning_rate": 0.00025038890422241953,
      "loss": 1.1865,
      "step": 5825
    },
    {
      "epoch": 7.768,
      "grad_norm": 0.025683676823973656,
      "learning_rate": 0.0002501031524391163,
      "loss": 1.1078,
      "step": 5826
    },
    {
      "epoch": 7.769333333333333,
      "grad_norm": 0.030678825452923775,
      "learning_rate": 0.0002498175404970548,
      "loss": 0.9409,
      "step": 5827
    },
    {
      "epoch": 7.770666666666667,
      "grad_norm": 0.02223898284137249,
      "learning_rate": 0.0002495320684494956,
      "loss": 0.9008,
      "step": 5828
    },
    {
      "epoch": 7.772,
      "grad_norm": 0.03277602419257164,
      "learning_rate": 0.00024924673634967465,
      "loss": 1.1803,
      "step": 5829
    },
    {
      "epoch": 7.773333333333333,
      "grad_norm": 0.030015185475349426,
      "learning_rate": 0.0002489615442508,
      "loss": 0.9914,
      "step": 5830
    },
    {
      "epoch": 7.774666666666667,
      "grad_norm": 0.027329210191965103,
      "learning_rate": 0.0002486764922060547,
      "loss": 1.0802,
      "step": 5831
    },
    {
      "epoch": 7.776,
      "grad_norm": 0.030539272353053093,
      "learning_rate": 0.00024839158026859586,
      "loss": 0.901,
      "step": 5832
    },
    {
      "epoch": 7.777333333333333,
      "grad_norm": 0.03167909383773804,
      "learning_rate": 0.0002481068084915531,
      "loss": 0.7767,
      "step": 5833
    },
    {
      "epoch": 7.778666666666666,
      "grad_norm": 0.027955379337072372,
      "learning_rate": 0.000247822176928032,
      "loss": 1.0921,
      "step": 5834
    },
    {
      "epoch": 7.78,
      "grad_norm": 0.022504210472106934,
      "learning_rate": 0.00024753768563110965,
      "loss": 0.9832,
      "step": 5835
    },
    {
      "epoch": 7.781333333333333,
      "grad_norm": 0.02994152531027794,
      "learning_rate": 0.0002472533346538394,
      "loss": 0.8452,
      "step": 5836
    },
    {
      "epoch": 7.782666666666667,
      "grad_norm": 0.026266539469361305,
      "learning_rate": 0.00024696912404924623,
      "loss": 1.1684,
      "step": 5837
    },
    {
      "epoch": 7.784,
      "grad_norm": 0.030696803703904152,
      "learning_rate": 0.00024668505387033025,
      "loss": 1.3923,
      "step": 5838
    },
    {
      "epoch": 7.785333333333333,
      "grad_norm": 0.02449595369398594,
      "learning_rate": 0.00024640112417006544,
      "loss": 0.949,
      "step": 5839
    },
    {
      "epoch": 7.786666666666667,
      "grad_norm": 0.02544548735022545,
      "learning_rate": 0.0002461173350013981,
      "loss": 1.3515,
      "step": 5840
    },
    {
      "epoch": 7.788,
      "grad_norm": 0.0249647106975317,
      "learning_rate": 0.0002458336864172508,
      "loss": 1.2812,
      "step": 5841
    },
    {
      "epoch": 7.789333333333333,
      "grad_norm": 0.02868679352104664,
      "learning_rate": 0.00024555017847051753,
      "loss": 1.3255,
      "step": 5842
    },
    {
      "epoch": 7.790666666666667,
      "grad_norm": 0.023165762424468994,
      "learning_rate": 0.00024526681121406733,
      "loss": 1.127,
      "step": 5843
    },
    {
      "epoch": 7.792,
      "grad_norm": 0.021169373765587807,
      "learning_rate": 0.000244983584700743,
      "loss": 0.9555,
      "step": 5844
    },
    {
      "epoch": 7.793333333333333,
      "grad_norm": 0.026182128116488457,
      "learning_rate": 0.0002447004989833599,
      "loss": 0.9792,
      "step": 5845
    },
    {
      "epoch": 7.794666666666666,
      "grad_norm": 0.0235604215413332,
      "learning_rate": 0.00024441755411470933,
      "loss": 0.8941,
      "step": 5846
    },
    {
      "epoch": 7.796,
      "grad_norm": 0.02747480943799019,
      "learning_rate": 0.00024413475014755393,
      "loss": 1.2774,
      "step": 5847
    },
    {
      "epoch": 7.7973333333333334,
      "grad_norm": 0.0328582338988781,
      "learning_rate": 0.0002438520871346317,
      "loss": 1.1376,
      "step": 5848
    },
    {
      "epoch": 7.798666666666667,
      "grad_norm": 0.02277083694934845,
      "learning_rate": 0.0002435695651286539,
      "loss": 0.8869,
      "step": 5849
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.026883302256464958,
      "learning_rate": 0.00024328718418230468,
      "loss": 0.9445,
      "step": 5850
    },
    {
      "epoch": 7.801333333333333,
      "grad_norm": 0.022880205884575844,
      "learning_rate": 0.0002430049443482437,
      "loss": 0.9217,
      "step": 5851
    },
    {
      "epoch": 7.802666666666667,
      "grad_norm": 0.026170160621404648,
      "learning_rate": 0.00024272284567910241,
      "loss": 0.8399,
      "step": 5852
    },
    {
      "epoch": 7.804,
      "grad_norm": 0.03794870898127556,
      "learning_rate": 0.000242440888227487,
      "loss": 1.0207,
      "step": 5853
    },
    {
      "epoch": 7.8053333333333335,
      "grad_norm": 0.02395261824131012,
      "learning_rate": 0.00024215907204597743,
      "loss": 1.3779,
      "step": 5854
    },
    {
      "epoch": 7.806666666666667,
      "grad_norm": 0.021555641666054726,
      "learning_rate": 0.00024187739718712598,
      "loss": 0.7539,
      "step": 5855
    },
    {
      "epoch": 7.808,
      "grad_norm": 0.03014896996319294,
      "learning_rate": 0.00024159586370346088,
      "loss": 1.1235,
      "step": 5856
    },
    {
      "epoch": 7.809333333333333,
      "grad_norm": 0.026966502889990807,
      "learning_rate": 0.00024131447164748156,
      "loss": 0.8914,
      "step": 5857
    },
    {
      "epoch": 7.810666666666666,
      "grad_norm": 0.02185998484492302,
      "learning_rate": 0.0002410332210716627,
      "loss": 1.1749,
      "step": 5858
    },
    {
      "epoch": 7.812,
      "grad_norm": 0.02031257562339306,
      "learning_rate": 0.00024075211202845227,
      "loss": 0.9968,
      "step": 5859
    },
    {
      "epoch": 7.8133333333333335,
      "grad_norm": 0.02049499936401844,
      "learning_rate": 0.00024047114457027087,
      "loss": 0.8596,
      "step": 5860
    },
    {
      "epoch": 7.814666666666667,
      "grad_norm": 0.023501859977841377,
      "learning_rate": 0.0002401903187495146,
      "loss": 1.051,
      "step": 5861
    },
    {
      "epoch": 7.816,
      "grad_norm": 0.02011442743241787,
      "learning_rate": 0.00023990963461855075,
      "loss": 0.8872,
      "step": 5862
    },
    {
      "epoch": 7.817333333333333,
      "grad_norm": 0.024313006550073624,
      "learning_rate": 0.00023962909222972296,
      "loss": 0.9359,
      "step": 5863
    },
    {
      "epoch": 7.818666666666667,
      "grad_norm": 0.031748633831739426,
      "learning_rate": 0.0002393486916353458,
      "loss": 0.8841,
      "step": 5864
    },
    {
      "epoch": 7.82,
      "grad_norm": 0.022750860080122948,
      "learning_rate": 0.00023906843288770886,
      "loss": 0.9255,
      "step": 5865
    },
    {
      "epoch": 7.8213333333333335,
      "grad_norm": 0.02619623951613903,
      "learning_rate": 0.00023878831603907537,
      "loss": 1.0853,
      "step": 5866
    },
    {
      "epoch": 7.822666666666667,
      "grad_norm": 0.03384382277727127,
      "learning_rate": 0.00023850834114168086,
      "loss": 1.1269,
      "step": 5867
    },
    {
      "epoch": 7.824,
      "grad_norm": 0.02337169088423252,
      "learning_rate": 0.00023822850824773624,
      "loss": 1.0341,
      "step": 5868
    },
    {
      "epoch": 7.825333333333333,
      "grad_norm": 0.02863284945487976,
      "learning_rate": 0.0002379488174094242,
      "loss": 0.879,
      "step": 5869
    },
    {
      "epoch": 7.826666666666666,
      "grad_norm": 0.03158124163746834,
      "learning_rate": 0.00023766926867890183,
      "loss": 1.0002,
      "step": 5870
    },
    {
      "epoch": 7.828,
      "grad_norm": 0.021141456440091133,
      "learning_rate": 0.00023738986210829994,
      "loss": 1.1394,
      "step": 5871
    },
    {
      "epoch": 7.8293333333333335,
      "grad_norm": 0.027076061815023422,
      "learning_rate": 0.00023711059774972142,
      "loss": 1.0265,
      "step": 5872
    },
    {
      "epoch": 7.830666666666667,
      "grad_norm": 0.02845529280602932,
      "learning_rate": 0.0002368314756552451,
      "loss": 0.972,
      "step": 5873
    },
    {
      "epoch": 7.832,
      "grad_norm": 0.02225305885076523,
      "learning_rate": 0.00023655249587692072,
      "loss": 0.9071,
      "step": 5874
    },
    {
      "epoch": 7.833333333333333,
      "grad_norm": 0.03291712701320648,
      "learning_rate": 0.00023627365846677306,
      "loss": 0.9399,
      "step": 5875
    },
    {
      "epoch": 7.834666666666667,
      "grad_norm": 0.03257765248417854,
      "learning_rate": 0.00023599496347680006,
      "loss": 1.0759,
      "step": 5876
    },
    {
      "epoch": 7.836,
      "grad_norm": 0.03188462182879448,
      "learning_rate": 0.00023571641095897222,
      "loss": 1.1147,
      "step": 5877
    },
    {
      "epoch": 7.8373333333333335,
      "grad_norm": 0.020821303129196167,
      "learning_rate": 0.00023543800096523517,
      "loss": 1.0438,
      "step": 5878
    },
    {
      "epoch": 7.838666666666667,
      "grad_norm": 0.027004143223166466,
      "learning_rate": 0.0002351597335475063,
      "loss": 0.8496,
      "step": 5879
    },
    {
      "epoch": 7.84,
      "grad_norm": 0.027977652847766876,
      "learning_rate": 0.00023488160875767718,
      "loss": 1.0801,
      "step": 5880
    },
    {
      "epoch": 7.841333333333333,
      "grad_norm": 0.022984493523836136,
      "learning_rate": 0.00023460362664761314,
      "loss": 1.0636,
      "step": 5881
    },
    {
      "epoch": 7.842666666666666,
      "grad_norm": 0.025600293651223183,
      "learning_rate": 0.00023432578726915166,
      "loss": 0.8098,
      "step": 5882
    },
    {
      "epoch": 7.844,
      "grad_norm": 0.030936172232031822,
      "learning_rate": 0.00023404809067410526,
      "loss": 0.9073,
      "step": 5883
    },
    {
      "epoch": 7.8453333333333335,
      "grad_norm": 0.025924013927578926,
      "learning_rate": 0.00023377053691425842,
      "loss": 1.0346,
      "step": 5884
    },
    {
      "epoch": 7.846666666666667,
      "grad_norm": 0.030249768868088722,
      "learning_rate": 0.00023349312604136973,
      "loss": 1.0966,
      "step": 5885
    },
    {
      "epoch": 7.848,
      "grad_norm": 0.024777403101325035,
      "learning_rate": 0.00023321585810717117,
      "loss": 1.2523,
      "step": 5886
    },
    {
      "epoch": 7.849333333333333,
      "grad_norm": 0.02945263683795929,
      "learning_rate": 0.00023293873316336712,
      "loss": 1.0323,
      "step": 5887
    },
    {
      "epoch": 7.850666666666667,
      "grad_norm": 0.020939888432621956,
      "learning_rate": 0.0002326617512616369,
      "loss": 0.9074,
      "step": 5888
    },
    {
      "epoch": 7.852,
      "grad_norm": 0.032704561948776245,
      "learning_rate": 0.00023238491245363148,
      "loss": 1.1039,
      "step": 5889
    },
    {
      "epoch": 7.8533333333333335,
      "grad_norm": 0.03892885148525238,
      "learning_rate": 0.0002321082167909767,
      "loss": 1.1308,
      "step": 5890
    },
    {
      "epoch": 7.854666666666667,
      "grad_norm": 0.03796297684311867,
      "learning_rate": 0.00023183166432527046,
      "loss": 1.1675,
      "step": 5891
    },
    {
      "epoch": 7.856,
      "grad_norm": 0.030710546299815178,
      "learning_rate": 0.0002315552551080845,
      "loss": 0.9441,
      "step": 5892
    },
    {
      "epoch": 7.857333333333333,
      "grad_norm": 0.029453150928020477,
      "learning_rate": 0.00023127898919096413,
      "loss": 0.9048,
      "step": 5893
    },
    {
      "epoch": 7.858666666666666,
      "grad_norm": 0.02731095440685749,
      "learning_rate": 0.00023100286662542668,
      "loss": 0.9234,
      "step": 5894
    },
    {
      "epoch": 7.86,
      "grad_norm": 0.026963135227560997,
      "learning_rate": 0.00023072688746296488,
      "loss": 0.8972,
      "step": 5895
    },
    {
      "epoch": 7.8613333333333335,
      "grad_norm": 0.027809835970401764,
      "learning_rate": 0.00023045105175504256,
      "loss": 1.1354,
      "step": 5896
    },
    {
      "epoch": 7.862666666666667,
      "grad_norm": 0.026434961706399918,
      "learning_rate": 0.0002301753595530981,
      "loss": 1.2088,
      "step": 5897
    },
    {
      "epoch": 7.864,
      "grad_norm": 0.02902836538851261,
      "learning_rate": 0.00022989981090854305,
      "loss": 0.7927,
      "step": 5898
    },
    {
      "epoch": 7.865333333333333,
      "grad_norm": 0.023165855556726456,
      "learning_rate": 0.00022962440587276101,
      "loss": 0.9201,
      "step": 5899
    },
    {
      "epoch": 7.866666666666667,
      "grad_norm": 0.037945132702589035,
      "learning_rate": 0.00022934914449711087,
      "loss": 1.0475,
      "step": 5900
    },
    {
      "epoch": 7.868,
      "grad_norm": 0.026335256174206734,
      "learning_rate": 0.00022907402683292267,
      "loss": 0.8517,
      "step": 5901
    },
    {
      "epoch": 7.8693333333333335,
      "grad_norm": 0.02557942643761635,
      "learning_rate": 0.00022879905293150072,
      "loss": 1.0732,
      "step": 5902
    },
    {
      "epoch": 7.870666666666667,
      "grad_norm": 0.02117079123854637,
      "learning_rate": 0.00022852422284412265,
      "loss": 1.0575,
      "step": 5903
    },
    {
      "epoch": 7.872,
      "grad_norm": 0.025243770331144333,
      "learning_rate": 0.00022824953662203828,
      "loss": 0.9969,
      "step": 5904
    },
    {
      "epoch": 7.873333333333333,
      "grad_norm": 0.021533749997615814,
      "learning_rate": 0.00022797499431647219,
      "loss": 1.0406,
      "step": 5905
    },
    {
      "epoch": 7.874666666666666,
      "grad_norm": 0.028495904058218002,
      "learning_rate": 0.00022770059597862036,
      "loss": 1.1279,
      "step": 5906
    },
    {
      "epoch": 7.876,
      "grad_norm": 0.02733766660094261,
      "learning_rate": 0.00022742634165965315,
      "loss": 1.0069,
      "step": 5907
    },
    {
      "epoch": 7.8773333333333335,
      "grad_norm": 0.02471177652478218,
      "learning_rate": 0.00022715223141071394,
      "loss": 0.9761,
      "step": 5908
    },
    {
      "epoch": 7.878666666666667,
      "grad_norm": 0.030472956597805023,
      "learning_rate": 0.0002268782652829181,
      "loss": 1.2081,
      "step": 5909
    },
    {
      "epoch": 7.88,
      "grad_norm": 0.027694500982761383,
      "learning_rate": 0.0002266044433273562,
      "loss": 1.103,
      "step": 5910
    },
    {
      "epoch": 7.881333333333333,
      "grad_norm": 0.02559603750705719,
      "learning_rate": 0.0002263307655950898,
      "loss": 0.9968,
      "step": 5911
    },
    {
      "epoch": 7.882666666666667,
      "grad_norm": 0.025253139436244965,
      "learning_rate": 0.0002260572321371549,
      "loss": 1.1259,
      "step": 5912
    },
    {
      "epoch": 7.884,
      "grad_norm": 0.02520136535167694,
      "learning_rate": 0.0002257838430045601,
      "loss": 1.0599,
      "step": 5913
    },
    {
      "epoch": 7.8853333333333335,
      "grad_norm": 0.03751464933156967,
      "learning_rate": 0.00022551059824828712,
      "loss": 1.1977,
      "step": 5914
    },
    {
      "epoch": 7.886666666666667,
      "grad_norm": 0.027454564347863197,
      "learning_rate": 0.00022523749791929126,
      "loss": 1.0769,
      "step": 5915
    },
    {
      "epoch": 7.888,
      "grad_norm": 0.021929606795310974,
      "learning_rate": 0.0002249645420684998,
      "loss": 0.9706,
      "step": 5916
    },
    {
      "epoch": 7.889333333333333,
      "grad_norm": 0.023954031988978386,
      "learning_rate": 0.00022469173074681392,
      "loss": 1.1328,
      "step": 5917
    },
    {
      "epoch": 7.890666666666666,
      "grad_norm": 0.03170151636004448,
      "learning_rate": 0.0002244190640051078,
      "loss": 1.0806,
      "step": 5918
    },
    {
      "epoch": 7.892,
      "grad_norm": 0.029828466475009918,
      "learning_rate": 0.00022414654189422846,
      "loss": 0.9389,
      "step": 5919
    },
    {
      "epoch": 7.8933333333333335,
      "grad_norm": 0.028827501460909843,
      "learning_rate": 0.0002238741644649962,
      "loss": 1.2804,
      "step": 5920
    },
    {
      "epoch": 7.894666666666667,
      "grad_norm": 0.022325556725263596,
      "learning_rate": 0.00022360193176820364,
      "loss": 1.1105,
      "step": 5921
    },
    {
      "epoch": 7.896,
      "grad_norm": 0.032349422574043274,
      "learning_rate": 0.00022332984385461718,
      "loss": 0.9124,
      "step": 5922
    },
    {
      "epoch": 7.897333333333333,
      "grad_norm": 0.019895829260349274,
      "learning_rate": 0.0002230579007749759,
      "loss": 1.1814,
      "step": 5923
    },
    {
      "epoch": 7.898666666666666,
      "grad_norm": 0.025459909811615944,
      "learning_rate": 0.0002227861025799921,
      "loss": 0.9922,
      "step": 5924
    },
    {
      "epoch": 7.9,
      "grad_norm": 0.0307770948857069,
      "learning_rate": 0.0002225144493203509,
      "loss": 1.0468,
      "step": 5925
    },
    {
      "epoch": 7.9013333333333335,
      "grad_norm": 0.019963938742876053,
      "learning_rate": 0.00022224294104670995,
      "loss": 0.9479,
      "step": 5926
    },
    {
      "epoch": 7.902666666666667,
      "grad_norm": 0.02413083054125309,
      "learning_rate": 0.00022197157780970056,
      "loss": 0.641,
      "step": 5927
    },
    {
      "epoch": 7.904,
      "grad_norm": 0.023466553539037704,
      "learning_rate": 0.00022170035965992675,
      "loss": 0.7926,
      "step": 5928
    },
    {
      "epoch": 7.905333333333333,
      "grad_norm": 0.028403781354427338,
      "learning_rate": 0.00022142928664796536,
      "loss": 1.0113,
      "step": 5929
    },
    {
      "epoch": 7.906666666666666,
      "grad_norm": 0.028641076758503914,
      "learning_rate": 0.0002211583588243665,
      "loss": 1.072,
      "step": 5930
    },
    {
      "epoch": 7.908,
      "grad_norm": 0.023876382037997246,
      "learning_rate": 0.0002208875762396526,
      "loss": 1.2534,
      "step": 5931
    },
    {
      "epoch": 7.9093333333333335,
      "grad_norm": 0.027224155142903328,
      "learning_rate": 0.00022061693894431957,
      "loss": 0.9214,
      "step": 5932
    },
    {
      "epoch": 7.910666666666667,
      "grad_norm": 0.028079085052013397,
      "learning_rate": 0.00022034644698883598,
      "loss": 1.1237,
      "step": 5933
    },
    {
      "epoch": 7.912,
      "grad_norm": 0.029174285009503365,
      "learning_rate": 0.00022007610042364334,
      "loss": 1.0762,
      "step": 5934
    },
    {
      "epoch": 7.913333333333333,
      "grad_norm": 0.02748657949268818,
      "learning_rate": 0.0002198058992991564,
      "loss": 1.1026,
      "step": 5935
    },
    {
      "epoch": 7.914666666666666,
      "grad_norm": 0.033284351229667664,
      "learning_rate": 0.00021953584366576152,
      "loss": 1.0343,
      "step": 5936
    },
    {
      "epoch": 7.916,
      "grad_norm": 0.02310032770037651,
      "learning_rate": 0.00021926593357381996,
      "loss": 0.9447,
      "step": 5937
    },
    {
      "epoch": 7.917333333333334,
      "grad_norm": 0.028869058936834335,
      "learning_rate": 0.00021899616907366393,
      "loss": 1.0226,
      "step": 5938
    },
    {
      "epoch": 7.918666666666667,
      "grad_norm": 0.0272572822868824,
      "learning_rate": 0.00021872655021559962,
      "loss": 1.1455,
      "step": 5939
    },
    {
      "epoch": 7.92,
      "grad_norm": 0.025384310632944107,
      "learning_rate": 0.00021845707704990557,
      "loss": 0.8326,
      "step": 5940
    },
    {
      "epoch": 7.921333333333333,
      "grad_norm": 0.030010972172021866,
      "learning_rate": 0.0002181877496268335,
      "loss": 0.9877,
      "step": 5941
    },
    {
      "epoch": 7.922666666666666,
      "grad_norm": 0.028800588101148605,
      "learning_rate": 0.0002179185679966078,
      "loss": 1.1985,
      "step": 5942
    },
    {
      "epoch": 7.924,
      "grad_norm": 0.023451169952750206,
      "learning_rate": 0.00021764953220942542,
      "loss": 1.0164,
      "step": 5943
    },
    {
      "epoch": 7.925333333333334,
      "grad_norm": 0.03021378442645073,
      "learning_rate": 0.00021738064231545618,
      "loss": 1.0554,
      "step": 5944
    },
    {
      "epoch": 7.926666666666667,
      "grad_norm": 0.024890759959816933,
      "learning_rate": 0.00021711189836484313,
      "loss": 0.9819,
      "step": 5945
    },
    {
      "epoch": 7.928,
      "grad_norm": 0.030114522203803062,
      "learning_rate": 0.00021684330040770184,
      "loss": 1.0467,
      "step": 5946
    },
    {
      "epoch": 7.929333333333333,
      "grad_norm": 0.02650943398475647,
      "learning_rate": 0.00021657484849412067,
      "loss": 0.742,
      "step": 5947
    },
    {
      "epoch": 7.930666666666666,
      "grad_norm": 0.050342902541160583,
      "learning_rate": 0.0002163065426741603,
      "loss": 1.1461,
      "step": 5948
    },
    {
      "epoch": 7.932,
      "grad_norm": 0.030747661367058754,
      "learning_rate": 0.00021603838299785483,
      "loss": 1.3076,
      "step": 5949
    },
    {
      "epoch": 7.933333333333334,
      "grad_norm": 0.02704516239464283,
      "learning_rate": 0.00021577036951521089,
      "loss": 0.9438,
      "step": 5950
    },
    {
      "epoch": 7.934666666666667,
      "grad_norm": 0.026974063366651535,
      "learning_rate": 0.00021550250227620772,
      "loss": 1.1287,
      "step": 5951
    },
    {
      "epoch": 7.936,
      "grad_norm": 0.03525058925151825,
      "learning_rate": 0.00021523478133079777,
      "loss": 1.2631,
      "step": 5952
    },
    {
      "epoch": 7.937333333333333,
      "grad_norm": 0.02841188944876194,
      "learning_rate": 0.00021496720672890512,
      "loss": 1.1015,
      "step": 5953
    },
    {
      "epoch": 7.938666666666666,
      "grad_norm": 0.023218365386128426,
      "learning_rate": 0.00021469977852042754,
      "loss": 1.1129,
      "step": 5954
    },
    {
      "epoch": 7.9399999999999995,
      "grad_norm": 0.02524639666080475,
      "learning_rate": 0.00021443249675523536,
      "loss": 0.8091,
      "step": 5955
    },
    {
      "epoch": 7.941333333333334,
      "grad_norm": 0.02238023653626442,
      "learning_rate": 0.00021416536148317135,
      "loss": 1.1223,
      "step": 5956
    },
    {
      "epoch": 7.942666666666667,
      "grad_norm": 0.025339577347040176,
      "learning_rate": 0.00021389837275405122,
      "loss": 1.2881,
      "step": 5957
    },
    {
      "epoch": 7.944,
      "grad_norm": 0.022093316540122032,
      "learning_rate": 0.00021363153061766294,
      "loss": 1.0487,
      "step": 5958
    },
    {
      "epoch": 7.945333333333333,
      "grad_norm": 0.023286806419491768,
      "learning_rate": 0.00021336483512376735,
      "loss": 1.1143,
      "step": 5959
    },
    {
      "epoch": 7.946666666666666,
      "grad_norm": 0.022456055507063866,
      "learning_rate": 0.00021309828632209827,
      "loss": 1.0385,
      "step": 5960
    },
    {
      "epoch": 7.948,
      "grad_norm": 0.021321602165699005,
      "learning_rate": 0.00021283188426236176,
      "loss": 0.9832,
      "step": 5961
    },
    {
      "epoch": 7.949333333333334,
      "grad_norm": 0.028560811653733253,
      "learning_rate": 0.0002125656289942369,
      "loss": 0.9975,
      "step": 5962
    },
    {
      "epoch": 7.950666666666667,
      "grad_norm": 0.024366319179534912,
      "learning_rate": 0.00021229952056737468,
      "loss": 0.9137,
      "step": 5963
    },
    {
      "epoch": 7.952,
      "grad_norm": 0.01906650885939598,
      "learning_rate": 0.0002120335590313993,
      "loss": 1.1665,
      "step": 5964
    },
    {
      "epoch": 7.953333333333333,
      "grad_norm": 0.030661966651678085,
      "learning_rate": 0.00021176774443590763,
      "loss": 0.7607,
      "step": 5965
    },
    {
      "epoch": 7.954666666666666,
      "grad_norm": 0.02357570081949234,
      "learning_rate": 0.00021150207683046885,
      "loss": 0.9818,
      "step": 5966
    },
    {
      "epoch": 7.9559999999999995,
      "grad_norm": 0.02644355036318302,
      "learning_rate": 0.00021123655626462478,
      "loss": 1.2204,
      "step": 5967
    },
    {
      "epoch": 7.957333333333334,
      "grad_norm": 0.03362349420785904,
      "learning_rate": 0.0002109711827878902,
      "loss": 1.0466,
      "step": 5968
    },
    {
      "epoch": 7.958666666666667,
      "grad_norm": 0.03069026954472065,
      "learning_rate": 0.00021070595644975143,
      "loss": 1.0631,
      "step": 5969
    },
    {
      "epoch": 7.96,
      "grad_norm": 0.02340557426214218,
      "learning_rate": 0.00021044087729966854,
      "loss": 1.2979,
      "step": 5970
    },
    {
      "epoch": 7.961333333333333,
      "grad_norm": 0.031679410487413406,
      "learning_rate": 0.00021017594538707364,
      "loss": 0.764,
      "step": 5971
    },
    {
      "epoch": 7.962666666666666,
      "grad_norm": 0.030245305970311165,
      "learning_rate": 0.0002099111607613712,
      "loss": 0.9934,
      "step": 5972
    },
    {
      "epoch": 7.964,
      "grad_norm": 0.02495070919394493,
      "learning_rate": 0.0002096465234719389,
      "loss": 1.153,
      "step": 5973
    },
    {
      "epoch": 7.965333333333334,
      "grad_norm": 0.022632911801338196,
      "learning_rate": 0.0002093820335681258,
      "loss": 1.018,
      "step": 5974
    },
    {
      "epoch": 7.966666666666667,
      "grad_norm": 0.02875789813697338,
      "learning_rate": 0.0002091176910992545,
      "loss": 0.8826,
      "step": 5975
    },
    {
      "epoch": 7.968,
      "grad_norm": 0.024790432304143906,
      "learning_rate": 0.00020885349611461968,
      "loss": 0.9753,
      "step": 5976
    },
    {
      "epoch": 7.969333333333333,
      "grad_norm": 0.023093339055776596,
      "learning_rate": 0.00020858944866348873,
      "loss": 0.9345,
      "step": 5977
    },
    {
      "epoch": 7.970666666666666,
      "grad_norm": 0.029115520417690277,
      "learning_rate": 0.00020832554879510111,
      "loss": 1.0136,
      "step": 5978
    },
    {
      "epoch": 7.9719999999999995,
      "grad_norm": 0.027103444561362267,
      "learning_rate": 0.00020806179655866964,
      "loss": 1.2022,
      "step": 5979
    },
    {
      "epoch": 7.973333333333334,
      "grad_norm": 0.025968585163354874,
      "learning_rate": 0.00020779819200337824,
      "loss": 0.9442,
      "step": 5980
    },
    {
      "epoch": 7.974666666666667,
      "grad_norm": 0.040770094841718674,
      "learning_rate": 0.0002075347351783844,
      "loss": 1.3307,
      "step": 5981
    },
    {
      "epoch": 7.976,
      "grad_norm": 0.022777054458856583,
      "learning_rate": 0.00020727142613281767,
      "loss": 1.0081,
      "step": 5982
    },
    {
      "epoch": 7.977333333333333,
      "grad_norm": 0.024012180045247078,
      "learning_rate": 0.00020700826491578018,
      "loss": 0.9307,
      "step": 5983
    },
    {
      "epoch": 7.978666666666666,
      "grad_norm": 0.025698665529489517,
      "learning_rate": 0.0002067452515763466,
      "loss": 0.7742,
      "step": 5984
    },
    {
      "epoch": 7.98,
      "grad_norm": 0.020367594435811043,
      "learning_rate": 0.0002064823861635633,
      "loss": 1.2501,
      "step": 5985
    },
    {
      "epoch": 7.981333333333334,
      "grad_norm": 0.02727498859167099,
      "learning_rate": 0.0002062196687264498,
      "loss": 0.9638,
      "step": 5986
    },
    {
      "epoch": 7.982666666666667,
      "grad_norm": 0.026440534740686417,
      "learning_rate": 0.00020595709931399796,
      "loss": 1.5657,
      "step": 5987
    },
    {
      "epoch": 7.984,
      "grad_norm": 0.02766217477619648,
      "learning_rate": 0.00020569467797517172,
      "loss": 1.0506,
      "step": 5988
    },
    {
      "epoch": 7.985333333333333,
      "grad_norm": 0.025532839819788933,
      "learning_rate": 0.00020543240475890778,
      "loss": 0.8917,
      "step": 5989
    },
    {
      "epoch": 7.986666666666666,
      "grad_norm": 0.02301843650639057,
      "learning_rate": 0.00020517027971411506,
      "loss": 0.9874,
      "step": 5990
    },
    {
      "epoch": 7.9879999999999995,
      "grad_norm": 0.028589027002453804,
      "learning_rate": 0.00020490830288967443,
      "loss": 1.2232,
      "step": 5991
    },
    {
      "epoch": 7.989333333333334,
      "grad_norm": 0.026354411616921425,
      "learning_rate": 0.00020464647433443973,
      "loss": 0.9723,
      "step": 5992
    },
    {
      "epoch": 7.990666666666667,
      "grad_norm": 0.03155313804745674,
      "learning_rate": 0.00020438479409723686,
      "loss": 0.9618,
      "step": 5993
    },
    {
      "epoch": 7.992,
      "grad_norm": 0.026320794597268105,
      "learning_rate": 0.00020412326222686418,
      "loss": 1.0067,
      "step": 5994
    },
    {
      "epoch": 7.993333333333333,
      "grad_norm": 0.026785992085933685,
      "learning_rate": 0.0002038618787720925,
      "loss": 1.1067,
      "step": 5995
    },
    {
      "epoch": 7.994666666666666,
      "grad_norm": 0.025614701211452484,
      "learning_rate": 0.0002036006437816643,
      "loss": 1.2712,
      "step": 5996
    },
    {
      "epoch": 7.996,
      "grad_norm": 0.034887343645095825,
      "learning_rate": 0.00020333955730429521,
      "loss": 1.2267,
      "step": 5997
    },
    {
      "epoch": 7.997333333333334,
      "grad_norm": 0.023671744391322136,
      "learning_rate": 0.00020307861938867266,
      "loss": 1.1905,
      "step": 5998
    },
    {
      "epoch": 7.998666666666667,
      "grad_norm": 0.03336021304130554,
      "learning_rate": 0.00020281783008345655,
      "loss": 0.7837,
      "step": 5999
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.022966906428337097,
      "learning_rate": 0.0002025571894372794,
      "loss": 1.2403,
      "step": 6000
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.03153395652771,
      "eval_runtime": 23.7948,
      "eval_samples_per_second": 21.013,
      "eval_steps_per_second": 2.648,
      "step": 6000
    }
  ],
  "logging_steps": 1,
  "max_steps": 7500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 1500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.4765368917884928e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
