{
  "best_metric": 1.0540913343429565,
  "best_model_checkpoint": "llama31_test/checkpoint-3000",
  "epoch": 4.0,
  "eval_steps": 1500,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0013333333333333333,
      "grad_norm": 0.3555504083633423,
      "learning_rate": 8.888888888888888e-06,
      "loss": 6.939,
      "step": 1
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 0.2820369005203247,
      "learning_rate": 1.7777777777777777e-05,
      "loss": 7.1212,
      "step": 2
    },
    {
      "epoch": 0.004,
      "grad_norm": 0.3040475845336914,
      "learning_rate": 2.666666666666667e-05,
      "loss": 7.2469,
      "step": 3
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 0.3130744695663452,
      "learning_rate": 3.555555555555555e-05,
      "loss": 7.1404,
      "step": 4
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 0.3134247660636902,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 7.2064,
      "step": 5
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.33323732018470764,
      "learning_rate": 5.333333333333334e-05,
      "loss": 6.659,
      "step": 6
    },
    {
      "epoch": 0.009333333333333334,
      "grad_norm": 0.3007800579071045,
      "learning_rate": 6.222222222222222e-05,
      "loss": 6.9849,
      "step": 7
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.2976188063621521,
      "learning_rate": 7.11111111111111e-05,
      "loss": 7.0471,
      "step": 8
    },
    {
      "epoch": 0.012,
      "grad_norm": 0.32053157687187195,
      "learning_rate": 8e-05,
      "loss": 6.9614,
      "step": 9
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.328104704618454,
      "learning_rate": 8.888888888888889e-05,
      "loss": 6.9047,
      "step": 10
    },
    {
      "epoch": 0.014666666666666666,
      "grad_norm": 0.3572377562522888,
      "learning_rate": 9.777777777777778e-05,
      "loss": 7.0393,
      "step": 11
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.3350681662559509,
      "learning_rate": 0.00010666666666666668,
      "loss": 6.9802,
      "step": 12
    },
    {
      "epoch": 0.017333333333333333,
      "grad_norm": 0.3303508162498474,
      "learning_rate": 0.00011555555555555555,
      "loss": 6.9706,
      "step": 13
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.32809674739837646,
      "learning_rate": 0.00012444444444444444,
      "loss": 6.9325,
      "step": 14
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.3702910840511322,
      "learning_rate": 0.00013333333333333334,
      "loss": 6.7135,
      "step": 15
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.32899269461631775,
      "learning_rate": 0.0001422222222222222,
      "loss": 6.8394,
      "step": 16
    },
    {
      "epoch": 0.02266666666666667,
      "grad_norm": 0.30374035239219666,
      "learning_rate": 0.0001511111111111111,
      "loss": 6.8322,
      "step": 17
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.366574764251709,
      "learning_rate": 0.00016,
      "loss": 6.7342,
      "step": 18
    },
    {
      "epoch": 0.025333333333333333,
      "grad_norm": 0.34597158432006836,
      "learning_rate": 0.00016888888888888889,
      "loss": 7.1006,
      "step": 19
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.3574230968952179,
      "learning_rate": 0.00017777777777777779,
      "loss": 7.0246,
      "step": 20
    },
    {
      "epoch": 0.028,
      "grad_norm": 0.32099682092666626,
      "learning_rate": 0.0001866666666666667,
      "loss": 6.9404,
      "step": 21
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 0.3596108555793762,
      "learning_rate": 0.00019555555555555556,
      "loss": 6.9141,
      "step": 22
    },
    {
      "epoch": 0.030666666666666665,
      "grad_norm": 0.323851078748703,
      "learning_rate": 0.00020444444444444446,
      "loss": 6.7382,
      "step": 23
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.290274977684021,
      "learning_rate": 0.00021333333333333336,
      "loss": 6.8497,
      "step": 24
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.3392907679080963,
      "learning_rate": 0.0002222222222222222,
      "loss": 6.7182,
      "step": 25
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 0.3558824360370636,
      "learning_rate": 0.0002311111111111111,
      "loss": 6.5068,
      "step": 26
    },
    {
      "epoch": 0.036,
      "grad_norm": 0.3710341453552246,
      "learning_rate": 0.00024,
      "loss": 6.4784,
      "step": 27
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 0.3727741241455078,
      "learning_rate": 0.0002488888888888889,
      "loss": 6.5286,
      "step": 28
    },
    {
      "epoch": 0.03866666666666667,
      "grad_norm": 0.3603370189666748,
      "learning_rate": 0.0002577777777777778,
      "loss": 6.6161,
      "step": 29
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.38295966386795044,
      "learning_rate": 0.0002666666666666667,
      "loss": 6.5721,
      "step": 30
    },
    {
      "epoch": 0.04133333333333333,
      "grad_norm": 0.41009706258773804,
      "learning_rate": 0.0002755555555555556,
      "loss": 6.6032,
      "step": 31
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.30817705392837524,
      "learning_rate": 0.0002844444444444444,
      "loss": 6.6088,
      "step": 32
    },
    {
      "epoch": 0.044,
      "grad_norm": 0.4053432047367096,
      "learning_rate": 0.0002933333333333333,
      "loss": 6.3758,
      "step": 33
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 0.3585493266582489,
      "learning_rate": 0.0003022222222222222,
      "loss": 6.5408,
      "step": 34
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.3294336497783661,
      "learning_rate": 0.0003111111111111111,
      "loss": 6.4788,
      "step": 35
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.38775748014450073,
      "learning_rate": 0.00032,
      "loss": 5.8985,
      "step": 36
    },
    {
      "epoch": 0.04933333333333333,
      "grad_norm": 0.5096060037612915,
      "learning_rate": 0.0003288888888888889,
      "loss": 5.8477,
      "step": 37
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.30484312772750854,
      "learning_rate": 0.00033777777777777777,
      "loss": 6.2385,
      "step": 38
    },
    {
      "epoch": 0.052,
      "grad_norm": 0.36814066767692566,
      "learning_rate": 0.00034666666666666667,
      "loss": 6.1029,
      "step": 39
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.35196802020072937,
      "learning_rate": 0.00035555555555555557,
      "loss": 6.3254,
      "step": 40
    },
    {
      "epoch": 0.05466666666666667,
      "grad_norm": 0.36300069093704224,
      "learning_rate": 0.00036444444444444447,
      "loss": 5.6745,
      "step": 41
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.3681320250034332,
      "learning_rate": 0.0003733333333333334,
      "loss": 5.7915,
      "step": 42
    },
    {
      "epoch": 0.05733333333333333,
      "grad_norm": 0.40314555168151855,
      "learning_rate": 0.0003822222222222223,
      "loss": 5.9162,
      "step": 43
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.39633047580718994,
      "learning_rate": 0.0003911111111111111,
      "loss": 5.7487,
      "step": 44
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.39122119545936584,
      "learning_rate": 0.0004,
      "loss": 6.0053,
      "step": 45
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.3738775849342346,
      "learning_rate": 0.0004088888888888889,
      "loss": 5.8295,
      "step": 46
    },
    {
      "epoch": 0.06266666666666666,
      "grad_norm": 0.4502626359462738,
      "learning_rate": 0.0004177777777777778,
      "loss": 5.602,
      "step": 47
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.41406819224357605,
      "learning_rate": 0.0004266666666666667,
      "loss": 5.5885,
      "step": 48
    },
    {
      "epoch": 0.06533333333333333,
      "grad_norm": 0.3728400766849518,
      "learning_rate": 0.0004355555555555555,
      "loss": 5.3816,
      "step": 49
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.43621519207954407,
      "learning_rate": 0.0004444444444444444,
      "loss": 5.5194,
      "step": 50
    },
    {
      "epoch": 0.068,
      "grad_norm": 0.40215903520584106,
      "learning_rate": 0.0004533333333333333,
      "loss": 5.3399,
      "step": 51
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.35502320528030396,
      "learning_rate": 0.0004622222222222222,
      "loss": 5.5855,
      "step": 52
    },
    {
      "epoch": 0.07066666666666667,
      "grad_norm": 0.3777894079685211,
      "learning_rate": 0.0004711111111111111,
      "loss": 5.02,
      "step": 53
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.40208253264427185,
      "learning_rate": 0.00048,
      "loss": 5.239,
      "step": 54
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.4758330285549164,
      "learning_rate": 0.0004888888888888889,
      "loss": 4.995,
      "step": 55
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.4006824195384979,
      "learning_rate": 0.0004977777777777778,
      "loss": 4.9496,
      "step": 56
    },
    {
      "epoch": 0.076,
      "grad_norm": 0.34312161803245544,
      "learning_rate": 0.0005066666666666668,
      "loss": 4.6876,
      "step": 57
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.390020489692688,
      "learning_rate": 0.0005155555555555556,
      "loss": 4.8512,
      "step": 58
    },
    {
      "epoch": 0.07866666666666666,
      "grad_norm": 0.3633885085582733,
      "learning_rate": 0.0005244444444444445,
      "loss": 5.0354,
      "step": 59
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.33825743198394775,
      "learning_rate": 0.0005333333333333334,
      "loss": 5.0089,
      "step": 60
    },
    {
      "epoch": 0.08133333333333333,
      "grad_norm": 0.27596643567085266,
      "learning_rate": 0.0005422222222222223,
      "loss": 4.5417,
      "step": 61
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.2744942903518677,
      "learning_rate": 0.0005511111111111112,
      "loss": 4.5155,
      "step": 62
    },
    {
      "epoch": 0.084,
      "grad_norm": 0.2609971761703491,
      "learning_rate": 0.0005600000000000001,
      "loss": 4.3623,
      "step": 63
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.25846701860427856,
      "learning_rate": 0.0005688888888888889,
      "loss": 4.539,
      "step": 64
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.24574969708919525,
      "learning_rate": 0.0005777777777777778,
      "loss": 4.2871,
      "step": 65
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.2544957995414734,
      "learning_rate": 0.0005866666666666667,
      "loss": 4.4581,
      "step": 66
    },
    {
      "epoch": 0.08933333333333333,
      "grad_norm": 0.2602943480014801,
      "learning_rate": 0.0005955555555555556,
      "loss": 4.2923,
      "step": 67
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.21910735964775085,
      "learning_rate": 0.0006044444444444445,
      "loss": 4.001,
      "step": 68
    },
    {
      "epoch": 0.092,
      "grad_norm": 0.2860652506351471,
      "learning_rate": 0.0006133333333333334,
      "loss": 4.2471,
      "step": 69
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.23105306923389435,
      "learning_rate": 0.0006222222222222223,
      "loss": 4.1282,
      "step": 70
    },
    {
      "epoch": 0.09466666666666666,
      "grad_norm": 0.2389366626739502,
      "learning_rate": 0.000631111111111111,
      "loss": 3.8047,
      "step": 71
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.21928375959396362,
      "learning_rate": 0.00064,
      "loss": 4.0391,
      "step": 72
    },
    {
      "epoch": 0.09733333333333333,
      "grad_norm": 0.24404151737689972,
      "learning_rate": 0.0006488888888888888,
      "loss": 3.9932,
      "step": 73
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.2796590328216553,
      "learning_rate": 0.0006577777777777779,
      "loss": 4.1413,
      "step": 74
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.29691430926322937,
      "learning_rate": 0.0006666666666666666,
      "loss": 4.0533,
      "step": 75
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.2289622575044632,
      "learning_rate": 0.0006755555555555555,
      "loss": 3.9604,
      "step": 76
    },
    {
      "epoch": 0.10266666666666667,
      "grad_norm": 0.25216829776763916,
      "learning_rate": 0.0006844444444444444,
      "loss": 4.0024,
      "step": 77
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.18387772142887115,
      "learning_rate": 0.0006933333333333333,
      "loss": 3.7311,
      "step": 78
    },
    {
      "epoch": 0.10533333333333333,
      "grad_norm": 0.2159779965877533,
      "learning_rate": 0.0007022222222222222,
      "loss": 4.0338,
      "step": 79
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.23165352642536163,
      "learning_rate": 0.0007111111111111111,
      "loss": 3.6967,
      "step": 80
    },
    {
      "epoch": 0.108,
      "grad_norm": 0.25378698110580444,
      "learning_rate": 0.0007199999999999999,
      "loss": 3.5939,
      "step": 81
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.20034092664718628,
      "learning_rate": 0.0007288888888888889,
      "loss": 3.8341,
      "step": 82
    },
    {
      "epoch": 0.11066666666666666,
      "grad_norm": 0.17457611858844757,
      "learning_rate": 0.0007377777777777777,
      "loss": 3.5018,
      "step": 83
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.2083585560321808,
      "learning_rate": 0.0007466666666666667,
      "loss": 3.81,
      "step": 84
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.2119496464729309,
      "learning_rate": 0.0007555555555555555,
      "loss": 3.8609,
      "step": 85
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.22008116543293,
      "learning_rate": 0.0007644444444444445,
      "loss": 3.6746,
      "step": 86
    },
    {
      "epoch": 0.116,
      "grad_norm": 0.21630693972110748,
      "learning_rate": 0.0007733333333333333,
      "loss": 3.5259,
      "step": 87
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.18918000161647797,
      "learning_rate": 0.0007822222222222222,
      "loss": 3.417,
      "step": 88
    },
    {
      "epoch": 0.11866666666666667,
      "grad_norm": 0.24622909724712372,
      "learning_rate": 0.0007911111111111111,
      "loss": 3.504,
      "step": 89
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.2659607231616974,
      "learning_rate": 0.0008,
      "loss": 3.6211,
      "step": 90
    },
    {
      "epoch": 0.12133333333333333,
      "grad_norm": 0.29008498787879944,
      "learning_rate": 0.0008088888888888889,
      "loss": 3.4069,
      "step": 91
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.2224079668521881,
      "learning_rate": 0.0008177777777777778,
      "loss": 3.2265,
      "step": 92
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.19913670420646667,
      "learning_rate": 0.0008266666666666666,
      "loss": 3.1952,
      "step": 93
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.2076563686132431,
      "learning_rate": 0.0008355555555555556,
      "loss": 3.2571,
      "step": 94
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.1831824779510498,
      "learning_rate": 0.0008444444444444444,
      "loss": 3.2619,
      "step": 95
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.23425628244876862,
      "learning_rate": 0.0008533333333333334,
      "loss": 3.0934,
      "step": 96
    },
    {
      "epoch": 0.12933333333333333,
      "grad_norm": 0.25713077187538147,
      "learning_rate": 0.0008622222222222222,
      "loss": 3.4453,
      "step": 97
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.19652040302753448,
      "learning_rate": 0.000871111111111111,
      "loss": 3.0896,
      "step": 98
    },
    {
      "epoch": 0.132,
      "grad_norm": 0.1827600747346878,
      "learning_rate": 0.00088,
      "loss": 2.8709,
      "step": 99
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.20682170987129211,
      "learning_rate": 0.0008888888888888888,
      "loss": 3.0675,
      "step": 100
    },
    {
      "epoch": 0.13466666666666666,
      "grad_norm": 0.1917726695537567,
      "learning_rate": 0.0008977777777777778,
      "loss": 3.0282,
      "step": 101
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.18286015093326569,
      "learning_rate": 0.0009066666666666666,
      "loss": 3.3707,
      "step": 102
    },
    {
      "epoch": 0.13733333333333334,
      "grad_norm": 0.18966540694236755,
      "learning_rate": 0.0009155555555555556,
      "loss": 3.1299,
      "step": 103
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.18103714287281036,
      "learning_rate": 0.0009244444444444444,
      "loss": 2.9937,
      "step": 104
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.17885956168174744,
      "learning_rate": 0.0009333333333333333,
      "loss": 3.2555,
      "step": 105
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.35943517088890076,
      "learning_rate": 0.0009422222222222222,
      "loss": 3.0403,
      "step": 106
    },
    {
      "epoch": 0.14266666666666666,
      "grad_norm": 0.20058420300483704,
      "learning_rate": 0.0009511111111111111,
      "loss": 2.9642,
      "step": 107
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.2611333727836609,
      "learning_rate": 0.00096,
      "loss": 3.096,
      "step": 108
    },
    {
      "epoch": 0.14533333333333334,
      "grad_norm": 0.3253711760044098,
      "learning_rate": 0.0009688888888888889,
      "loss": 3.0214,
      "step": 109
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.2833825945854187,
      "learning_rate": 0.0009777777777777777,
      "loss": 3.0148,
      "step": 110
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.15768861770629883,
      "learning_rate": 0.0009866666666666667,
      "loss": 3.2658,
      "step": 111
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.19665218889713287,
      "learning_rate": 0.0009955555555555555,
      "loss": 2.9948,
      "step": 112
    },
    {
      "epoch": 0.15066666666666667,
      "grad_norm": 0.22823019325733185,
      "learning_rate": 0.0010044444444444445,
      "loss": 2.7265,
      "step": 113
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.15567077696323395,
      "learning_rate": 0.0010133333333333335,
      "loss": 2.8005,
      "step": 114
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.2153978943824768,
      "learning_rate": 0.0010222222222222221,
      "loss": 2.7751,
      "step": 115
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.43959227204322815,
      "learning_rate": 0.0010311111111111111,
      "loss": 2.7895,
      "step": 116
    },
    {
      "epoch": 0.156,
      "grad_norm": 0.19013641774654388,
      "learning_rate": 0.0010400000000000001,
      "loss": 2.774,
      "step": 117
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.21319551765918732,
      "learning_rate": 0.001048888888888889,
      "loss": 2.9346,
      "step": 118
    },
    {
      "epoch": 0.15866666666666668,
      "grad_norm": 0.3194159269332886,
      "learning_rate": 0.0010577777777777777,
      "loss": 2.753,
      "step": 119
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.24583186209201813,
      "learning_rate": 0.0010666666666666667,
      "loss": 3.0352,
      "step": 120
    },
    {
      "epoch": 0.16133333333333333,
      "grad_norm": 0.2755632996559143,
      "learning_rate": 0.0010755555555555557,
      "loss": 2.6773,
      "step": 121
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.1570361852645874,
      "learning_rate": 0.0010844444444444445,
      "loss": 2.895,
      "step": 122
    },
    {
      "epoch": 0.164,
      "grad_norm": 0.31439799070358276,
      "learning_rate": 0.0010933333333333333,
      "loss": 2.8218,
      "step": 123
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.1796349436044693,
      "learning_rate": 0.0011022222222222223,
      "loss": 2.6005,
      "step": 124
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.1848248541355133,
      "learning_rate": 0.0011111111111111111,
      "loss": 2.715,
      "step": 125
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.19730344414710999,
      "learning_rate": 0.0011200000000000001,
      "loss": 2.7502,
      "step": 126
    },
    {
      "epoch": 0.16933333333333334,
      "grad_norm": 0.23708689212799072,
      "learning_rate": 0.001128888888888889,
      "loss": 2.4887,
      "step": 127
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.1518843024969101,
      "learning_rate": 0.0011377777777777777,
      "loss": 2.9067,
      "step": 128
    },
    {
      "epoch": 0.172,
      "grad_norm": 0.23372532427310944,
      "learning_rate": 0.0011466666666666667,
      "loss": 2.5531,
      "step": 129
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.19489368796348572,
      "learning_rate": 0.0011555555555555555,
      "loss": 2.5173,
      "step": 130
    },
    {
      "epoch": 0.17466666666666666,
      "grad_norm": 0.17375832796096802,
      "learning_rate": 0.0011644444444444445,
      "loss": 2.7289,
      "step": 131
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.17223307490348816,
      "learning_rate": 0.0011733333333333333,
      "loss": 2.3688,
      "step": 132
    },
    {
      "epoch": 0.17733333333333334,
      "grad_norm": 0.1609000563621521,
      "learning_rate": 0.0011822222222222223,
      "loss": 2.6588,
      "step": 133
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.2193676382303238,
      "learning_rate": 0.001191111111111111,
      "loss": 2.407,
      "step": 134
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.2354118674993515,
      "learning_rate": 0.0012,
      "loss": 2.5228,
      "step": 135
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.2277151644229889,
      "learning_rate": 0.001208888888888889,
      "loss": 2.5359,
      "step": 136
    },
    {
      "epoch": 0.18266666666666667,
      "grad_norm": 0.2764875888824463,
      "learning_rate": 0.001217777777777778,
      "loss": 2.8204,
      "step": 137
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.18017089366912842,
      "learning_rate": 0.0012266666666666667,
      "loss": 2.3479,
      "step": 138
    },
    {
      "epoch": 0.18533333333333332,
      "grad_norm": 0.2732390761375427,
      "learning_rate": 0.0012355555555555555,
      "loss": 2.6582,
      "step": 139
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.24082329869270325,
      "learning_rate": 0.0012444444444444445,
      "loss": 2.1702,
      "step": 140
    },
    {
      "epoch": 0.188,
      "grad_norm": 0.23642857372760773,
      "learning_rate": 0.0012533333333333335,
      "loss": 2.6012,
      "step": 141
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.22302553057670593,
      "learning_rate": 0.001262222222222222,
      "loss": 2.5584,
      "step": 142
    },
    {
      "epoch": 0.19066666666666668,
      "grad_norm": 0.21631298959255219,
      "learning_rate": 0.001271111111111111,
      "loss": 2.4059,
      "step": 143
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.23782074451446533,
      "learning_rate": 0.00128,
      "loss": 2.4692,
      "step": 144
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.1809307038784027,
      "learning_rate": 0.001288888888888889,
      "loss": 2.442,
      "step": 145
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.22575511038303375,
      "learning_rate": 0.0012977777777777777,
      "loss": 2.3062,
      "step": 146
    },
    {
      "epoch": 0.196,
      "grad_norm": 0.22840678691864014,
      "learning_rate": 0.0013066666666666667,
      "loss": 2.4491,
      "step": 147
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.2556595504283905,
      "learning_rate": 0.0013155555555555557,
      "loss": 2.309,
      "step": 148
    },
    {
      "epoch": 0.19866666666666666,
      "grad_norm": 0.25059255957603455,
      "learning_rate": 0.0013244444444444445,
      "loss": 2.2907,
      "step": 149
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.2540203332901001,
      "learning_rate": 0.0013333333333333333,
      "loss": 2.4229,
      "step": 150
    },
    {
      "epoch": 0.20133333333333334,
      "grad_norm": 0.3189737796783447,
      "learning_rate": 0.0013422222222222223,
      "loss": 2.3595,
      "step": 151
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.2814345061779022,
      "learning_rate": 0.001351111111111111,
      "loss": 2.3265,
      "step": 152
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.23742927610874176,
      "learning_rate": 0.00136,
      "loss": 2.2254,
      "step": 153
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.4587046205997467,
      "learning_rate": 0.0013688888888888889,
      "loss": 2.1579,
      "step": 154
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.30864301323890686,
      "learning_rate": 0.001377777777777778,
      "loss": 2.0703,
      "step": 155
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.22583816945552826,
      "learning_rate": 0.0013866666666666667,
      "loss": 2.3132,
      "step": 156
    },
    {
      "epoch": 0.20933333333333334,
      "grad_norm": 0.22097212076187134,
      "learning_rate": 0.0013955555555555557,
      "loss": 2.3189,
      "step": 157
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.5596842765808105,
      "learning_rate": 0.0014044444444444445,
      "loss": 2.3481,
      "step": 158
    },
    {
      "epoch": 0.212,
      "grad_norm": 0.24860230088233948,
      "learning_rate": 0.0014133333333333333,
      "loss": 2.2695,
      "step": 159
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.5702952742576599,
      "learning_rate": 0.0014222222222222223,
      "loss": 2.2506,
      "step": 160
    },
    {
      "epoch": 0.21466666666666667,
      "grad_norm": 0.2053312063217163,
      "learning_rate": 0.001431111111111111,
      "loss": 2.2972,
      "step": 161
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.26408371329307556,
      "learning_rate": 0.0014399999999999999,
      "loss": 2.1839,
      "step": 162
    },
    {
      "epoch": 0.21733333333333332,
      "grad_norm": 0.21351370215415955,
      "learning_rate": 0.0014488888888888889,
      "loss": 2.1841,
      "step": 163
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.25986039638519287,
      "learning_rate": 0.0014577777777777779,
      "loss": 1.887,
      "step": 164
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.2630249559879303,
      "learning_rate": 0.0014666666666666667,
      "loss": 2.1449,
      "step": 165
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.2603936493396759,
      "learning_rate": 0.0014755555555555555,
      "loss": 1.8729,
      "step": 166
    },
    {
      "epoch": 0.22266666666666668,
      "grad_norm": 0.22913362085819244,
      "learning_rate": 0.0014844444444444445,
      "loss": 2.0419,
      "step": 167
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.2890835106372833,
      "learning_rate": 0.0014933333333333335,
      "loss": 2.0237,
      "step": 168
    },
    {
      "epoch": 0.22533333333333333,
      "grad_norm": 0.24949640035629272,
      "learning_rate": 0.001502222222222222,
      "loss": 2.1428,
      "step": 169
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.2959970235824585,
      "learning_rate": 0.001511111111111111,
      "loss": 2.3783,
      "step": 170
    },
    {
      "epoch": 0.228,
      "grad_norm": 0.3019074499607086,
      "learning_rate": 0.00152,
      "loss": 2.5129,
      "step": 171
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.37460583448410034,
      "learning_rate": 0.001528888888888889,
      "loss": 1.7151,
      "step": 172
    },
    {
      "epoch": 0.23066666666666666,
      "grad_norm": 0.3367548882961273,
      "learning_rate": 0.0015377777777777777,
      "loss": 1.77,
      "step": 173
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.27657562494277954,
      "learning_rate": 0.0015466666666666667,
      "loss": 2.1734,
      "step": 174
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.25605666637420654,
      "learning_rate": 0.0015555555555555557,
      "loss": 1.8297,
      "step": 175
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.32427042722702026,
      "learning_rate": 0.0015644444444444445,
      "loss": 1.9176,
      "step": 176
    },
    {
      "epoch": 0.236,
      "grad_norm": 0.30497390031814575,
      "learning_rate": 0.0015733333333333333,
      "loss": 1.9386,
      "step": 177
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.24644477665424347,
      "learning_rate": 0.0015822222222222223,
      "loss": 1.7461,
      "step": 178
    },
    {
      "epoch": 0.23866666666666667,
      "grad_norm": 0.26502078771591187,
      "learning_rate": 0.001591111111111111,
      "loss": 1.8744,
      "step": 179
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.4013954699039459,
      "learning_rate": 0.0016,
      "loss": 1.7072,
      "step": 180
    },
    {
      "epoch": 0.24133333333333334,
      "grad_norm": 0.26181158423423767,
      "learning_rate": 0.0016088888888888889,
      "loss": 1.9893,
      "step": 181
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.38677167892456055,
      "learning_rate": 0.0016177777777777779,
      "loss": 1.7762,
      "step": 182
    },
    {
      "epoch": 0.244,
      "grad_norm": 0.2490021288394928,
      "learning_rate": 0.0016266666666666667,
      "loss": 2.2498,
      "step": 183
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.3483845293521881,
      "learning_rate": 0.0016355555555555557,
      "loss": 1.7619,
      "step": 184
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.21299581229686737,
      "learning_rate": 0.0016444444444444445,
      "loss": 1.8468,
      "step": 185
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.2674828767776489,
      "learning_rate": 0.0016533333333333333,
      "loss": 2.0769,
      "step": 186
    },
    {
      "epoch": 0.24933333333333332,
      "grad_norm": 0.18275614082813263,
      "learning_rate": 0.0016622222222222223,
      "loss": 1.816,
      "step": 187
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.357870489358902,
      "learning_rate": 0.0016711111111111113,
      "loss": 1.9828,
      "step": 188
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.22154709696769714,
      "learning_rate": 0.00168,
      "loss": 1.327,
      "step": 189
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.1553424596786499,
      "learning_rate": 0.0016888888888888889,
      "loss": 1.7697,
      "step": 190
    },
    {
      "epoch": 0.25466666666666665,
      "grad_norm": 0.35465845465660095,
      "learning_rate": 0.0016977777777777779,
      "loss": 2.1192,
      "step": 191
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.16351741552352905,
      "learning_rate": 0.0017066666666666669,
      "loss": 1.5972,
      "step": 192
    },
    {
      "epoch": 0.25733333333333336,
      "grad_norm": 0.16911204159259796,
      "learning_rate": 0.0017155555555555555,
      "loss": 1.4392,
      "step": 193
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 0.3353917896747589,
      "learning_rate": 0.0017244444444444445,
      "loss": 1.453,
      "step": 194
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.18812192976474762,
      "learning_rate": 0.0017333333333333335,
      "loss": 1.6527,
      "step": 195
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.13612323999404907,
      "learning_rate": 0.001742222222222222,
      "loss": 1.7609,
      "step": 196
    },
    {
      "epoch": 0.26266666666666666,
      "grad_norm": 0.18409249186515808,
      "learning_rate": 0.001751111111111111,
      "loss": 1.731,
      "step": 197
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.20915021002292633,
      "learning_rate": 0.00176,
      "loss": 1.3184,
      "step": 198
    },
    {
      "epoch": 0.2653333333333333,
      "grad_norm": 0.14540532231330872,
      "learning_rate": 0.001768888888888889,
      "loss": 1.7068,
      "step": 199
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.1447267383337021,
      "learning_rate": 0.0017777777777777776,
      "loss": 1.4373,
      "step": 200
    },
    {
      "epoch": 0.268,
      "grad_norm": 0.15977557003498077,
      "learning_rate": 0.0017866666666666667,
      "loss": 1.5344,
      "step": 201
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.23101578652858734,
      "learning_rate": 0.0017955555555555557,
      "loss": 1.8091,
      "step": 202
    },
    {
      "epoch": 0.27066666666666667,
      "grad_norm": 0.11719998717308044,
      "learning_rate": 0.0018044444444444445,
      "loss": 1.7493,
      "step": 203
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.14386355876922607,
      "learning_rate": 0.0018133333333333332,
      "loss": 1.6227,
      "step": 204
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.29026785492897034,
      "learning_rate": 0.0018222222222222223,
      "loss": 1.7728,
      "step": 205
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.12349116802215576,
      "learning_rate": 0.0018311111111111113,
      "loss": 1.5605,
      "step": 206
    },
    {
      "epoch": 0.276,
      "grad_norm": 0.2713688611984253,
      "learning_rate": 0.00184,
      "loss": 1.6615,
      "step": 207
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.14579862356185913,
      "learning_rate": 0.0018488888888888888,
      "loss": 1.4276,
      "step": 208
    },
    {
      "epoch": 0.2786666666666667,
      "grad_norm": 0.15123353898525238,
      "learning_rate": 0.0018577777777777779,
      "loss": 1.2556,
      "step": 209
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.2123190015554428,
      "learning_rate": 0.0018666666666666666,
      "loss": 1.4817,
      "step": 210
    },
    {
      "epoch": 0.2813333333333333,
      "grad_norm": 0.09837790578603745,
      "learning_rate": 0.0018755555555555557,
      "loss": 1.509,
      "step": 211
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.13006064295768738,
      "learning_rate": 0.0018844444444444444,
      "loss": 1.4932,
      "step": 212
    },
    {
      "epoch": 0.284,
      "grad_norm": 0.1544894278049469,
      "learning_rate": 0.0018933333333333335,
      "loss": 1.8282,
      "step": 213
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.1085115373134613,
      "learning_rate": 0.0019022222222222222,
      "loss": 1.364,
      "step": 214
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 0.2004687786102295,
      "learning_rate": 0.0019111111111111113,
      "loss": 1.8523,
      "step": 215
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.1133243516087532,
      "learning_rate": 0.00192,
      "loss": 1.1968,
      "step": 216
    },
    {
      "epoch": 0.28933333333333333,
      "grad_norm": 0.14336636662483215,
      "learning_rate": 0.0019288888888888888,
      "loss": 1.2636,
      "step": 217
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.10528384894132614,
      "learning_rate": 0.0019377777777777778,
      "loss": 1.2506,
      "step": 218
    },
    {
      "epoch": 0.292,
      "grad_norm": 0.11885785311460495,
      "learning_rate": 0.0019466666666666669,
      "loss": 1.528,
      "step": 219
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.10311006009578705,
      "learning_rate": 0.0019555555555555554,
      "loss": 1.4176,
      "step": 220
    },
    {
      "epoch": 0.2946666666666667,
      "grad_norm": 0.08208838850259781,
      "learning_rate": 0.0019644444444444444,
      "loss": 1.3646,
      "step": 221
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.0704248771071434,
      "learning_rate": 0.0019733333333333334,
      "loss": 1.3744,
      "step": 222
    },
    {
      "epoch": 0.29733333333333334,
      "grad_norm": 0.17308077216148376,
      "learning_rate": 0.0019822222222222225,
      "loss": 1.6596,
      "step": 223
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.09888805449008942,
      "learning_rate": 0.001991111111111111,
      "loss": 1.2529,
      "step": 224
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.11442390829324722,
      "learning_rate": 0.002,
      "loss": 1.59,
      "step": 225
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.08426883816719055,
      "learning_rate": 0.001999999906759682,
      "loss": 1.3271,
      "step": 226
    },
    {
      "epoch": 0.30266666666666664,
      "grad_norm": 0.4455508589744568,
      "learning_rate": 0.001999999627038744,
      "loss": 1.9235,
      "step": 227
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.23622238636016846,
      "learning_rate": 0.001999999160837239,
      "loss": 1.5459,
      "step": 228
    },
    {
      "epoch": 0.30533333333333335,
      "grad_norm": 0.15035676956176758,
      "learning_rate": 0.001999998508155254,
      "loss": 1.5617,
      "step": 229
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.1432712823152542,
      "learning_rate": 0.001999997668992911,
      "loss": 1.3752,
      "step": 230
    },
    {
      "epoch": 0.308,
      "grad_norm": 0.19954468309879303,
      "learning_rate": 0.001999996643350365,
      "loss": 1.2875,
      "step": 231
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.2520114779472351,
      "learning_rate": 0.0019999954312278087,
      "loss": 1.7282,
      "step": 232
    },
    {
      "epoch": 0.31066666666666665,
      "grad_norm": 0.15023937821388245,
      "learning_rate": 0.0019999940326254676,
      "loss": 1.3115,
      "step": 233
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.1070207878947258,
      "learning_rate": 0.001999992447543603,
      "loss": 1.1875,
      "step": 234
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.17872443795204163,
      "learning_rate": 0.0019999906759825097,
      "loss": 1.3943,
      "step": 235
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.11692877113819122,
      "learning_rate": 0.0019999887179425187,
      "loss": 1.5984,
      "step": 236
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.10689838975667953,
      "learning_rate": 0.0019999865734239945,
      "loss": 1.4206,
      "step": 237
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.09283257275819778,
      "learning_rate": 0.001999984242427338,
      "loss": 1.3056,
      "step": 238
    },
    {
      "epoch": 0.31866666666666665,
      "grad_norm": 0.20318013429641724,
      "learning_rate": 0.0019999817249529827,
      "loss": 1.4931,
      "step": 239
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.09454657882452011,
      "learning_rate": 0.001999979021001399,
      "loss": 1.3915,
      "step": 240
    },
    {
      "epoch": 0.32133333333333336,
      "grad_norm": 0.0849783793091774,
      "learning_rate": 0.00199997613057309,
      "loss": 1.5463,
      "step": 241
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.19206766784191132,
      "learning_rate": 0.0019999730536685964,
      "loss": 1.7484,
      "step": 242
    },
    {
      "epoch": 0.324,
      "grad_norm": 0.10975363105535507,
      "learning_rate": 0.0019999697902884908,
      "loss": 1.5411,
      "step": 243
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.1025867611169815,
      "learning_rate": 0.001999966340433382,
      "loss": 1.5015,
      "step": 244
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 0.07621404528617859,
      "learning_rate": 0.0019999627041039133,
      "loss": 1.4361,
      "step": 245
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.11332632601261139,
      "learning_rate": 0.0019999588813007633,
      "loss": 1.5896,
      "step": 246
    },
    {
      "epoch": 0.3293333333333333,
      "grad_norm": 0.08630890399217606,
      "learning_rate": 0.001999954872024644,
      "loss": 1.1898,
      "step": 247
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.10983987897634506,
      "learning_rate": 0.0019999506762763035,
      "loss": 1.4508,
      "step": 248
    },
    {
      "epoch": 0.332,
      "grad_norm": 0.06788527965545654,
      "learning_rate": 0.001999946294056524,
      "loss": 1.1999,
      "step": 249
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.5273227691650391,
      "learning_rate": 0.0019999417253661234,
      "loss": 1.8123,
      "step": 250
    },
    {
      "epoch": 0.33466666666666667,
      "grad_norm": 0.08027791976928711,
      "learning_rate": 0.0019999369702059532,
      "loss": 1.443,
      "step": 251
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.1597403585910797,
      "learning_rate": 0.0019999320285769,
      "loss": 1.5052,
      "step": 252
    },
    {
      "epoch": 0.3373333333333333,
      "grad_norm": 0.11737503856420517,
      "learning_rate": 0.001999926900479885,
      "loss": 1.5635,
      "step": 253
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.14986412227153778,
      "learning_rate": 0.0019999215859158657,
      "loss": 1.4726,
      "step": 254
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.17985229194164276,
      "learning_rate": 0.001999916084885832,
      "loss": 1.5511,
      "step": 255
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.13349486887454987,
      "learning_rate": 0.00199991039739081,
      "loss": 1.1446,
      "step": 256
    },
    {
      "epoch": 0.3426666666666667,
      "grad_norm": 0.26438799500465393,
      "learning_rate": 0.0019999045234318606,
      "loss": 1.4797,
      "step": 257
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.09764478355646133,
      "learning_rate": 0.001999898463010079,
      "loss": 1.1688,
      "step": 258
    },
    {
      "epoch": 0.3453333333333333,
      "grad_norm": 0.07612636685371399,
      "learning_rate": 0.001999892216126596,
      "loss": 1.4874,
      "step": 259
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.14608581364154816,
      "learning_rate": 0.001999885782782575,
      "loss": 1.393,
      "step": 260
    },
    {
      "epoch": 0.348,
      "grad_norm": 0.21489006280899048,
      "learning_rate": 0.001999879162979217,
      "loss": 1.3894,
      "step": 261
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 0.06065966188907623,
      "learning_rate": 0.0019998723567177562,
      "loss": 1.2482,
      "step": 262
    },
    {
      "epoch": 0.3506666666666667,
      "grad_norm": 0.0650487095117569,
      "learning_rate": 0.001999865363999461,
      "loss": 1.432,
      "step": 263
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.05423147231340408,
      "learning_rate": 0.0019998581848256368,
      "loss": 1.3507,
      "step": 264
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.07804792374372482,
      "learning_rate": 0.0019998508191976217,
      "loss": 1.3087,
      "step": 265
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.09763214737176895,
      "learning_rate": 0.001999843267116789,
      "loss": 1.2827,
      "step": 266
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.0960591584444046,
      "learning_rate": 0.0019998355285845474,
      "loss": 1.531,
      "step": 267
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.13922002911567688,
      "learning_rate": 0.0019998276036023396,
      "loss": 1.5412,
      "step": 268
    },
    {
      "epoch": 0.3586666666666667,
      "grad_norm": 0.05023603141307831,
      "learning_rate": 0.001999819492171644,
      "loss": 1.4064,
      "step": 269
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.10569024085998535,
      "learning_rate": 0.0019998111942939726,
      "loss": 1.3646,
      "step": 270
    },
    {
      "epoch": 0.36133333333333334,
      "grad_norm": 0.11148208379745483,
      "learning_rate": 0.0019998027099708735,
      "loss": 1.293,
      "step": 271
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.10627737641334534,
      "learning_rate": 0.001999794039203928,
      "loss": 1.4043,
      "step": 272
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.08687829971313477,
      "learning_rate": 0.0019997851819947535,
      "loss": 1.4107,
      "step": 273
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.07988125085830688,
      "learning_rate": 0.001999776138345002,
      "loss": 1.259,
      "step": 274
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.1479395031929016,
      "learning_rate": 0.0019997669082563595,
      "loss": 1.595,
      "step": 275
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.05802853778004646,
      "learning_rate": 0.001999757491730548,
      "loss": 1.4499,
      "step": 276
    },
    {
      "epoch": 0.36933333333333335,
      "grad_norm": 0.06045597046613693,
      "learning_rate": 0.001999747888769322,
      "loss": 1.5376,
      "step": 277
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.07695658504962921,
      "learning_rate": 0.001999738099374474,
      "loss": 1.3521,
      "step": 278
    },
    {
      "epoch": 0.372,
      "grad_norm": 0.08334402740001678,
      "learning_rate": 0.001999728123547828,
      "loss": 1.3981,
      "step": 279
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.06757953017950058,
      "learning_rate": 0.001999717961291245,
      "loss": 1.3287,
      "step": 280
    },
    {
      "epoch": 0.37466666666666665,
      "grad_norm": 0.04772596433758736,
      "learning_rate": 0.0019997076126066207,
      "loss": 1.3087,
      "step": 281
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.05686959996819496,
      "learning_rate": 0.0019996970774958838,
      "loss": 1.2932,
      "step": 282
    },
    {
      "epoch": 0.37733333333333335,
      "grad_norm": 0.05310272052884102,
      "learning_rate": 0.001999686355960999,
      "loss": 1.5018,
      "step": 283
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.05681496486067772,
      "learning_rate": 0.0019996754480039666,
      "loss": 1.234,
      "step": 284
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.05258401110768318,
      "learning_rate": 0.00199966435362682,
      "loss": 1.3291,
      "step": 285
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.07269256561994553,
      "learning_rate": 0.001999653072831629,
      "loss": 1.2992,
      "step": 286
    },
    {
      "epoch": 0.38266666666666665,
      "grad_norm": 0.04860762506723404,
      "learning_rate": 0.0019996416056204955,
      "loss": 1.4232,
      "step": 287
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.044220492243766785,
      "learning_rate": 0.001999629951995559,
      "loss": 1.4881,
      "step": 288
    },
    {
      "epoch": 0.38533333333333336,
      "grad_norm": 0.14887695014476776,
      "learning_rate": 0.0019996181119589927,
      "loss": 1.0483,
      "step": 289
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.09845087677240372,
      "learning_rate": 0.001999606085513005,
      "loss": 1.713,
      "step": 290
    },
    {
      "epoch": 0.388,
      "grad_norm": 0.07102132588624954,
      "learning_rate": 0.0019995938726598372,
      "loss": 1.4657,
      "step": 291
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 0.06169552728533745,
      "learning_rate": 0.001999581473401768,
      "loss": 1.3369,
      "step": 292
    },
    {
      "epoch": 0.39066666666666666,
      "grad_norm": 0.052081745117902756,
      "learning_rate": 0.001999568887741109,
      "loss": 1.3564,
      "step": 293
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.05409561097621918,
      "learning_rate": 0.0019995561156802076,
      "loss": 1.3551,
      "step": 294
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.05737118050456047,
      "learning_rate": 0.0019995431572214454,
      "loss": 1.4825,
      "step": 295
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.12405510246753693,
      "learning_rate": 0.001999530012367239,
      "loss": 1.2869,
      "step": 296
    },
    {
      "epoch": 0.396,
      "grad_norm": 0.04846273735165596,
      "learning_rate": 0.001999516681120039,
      "loss": 1.5018,
      "step": 297
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.07069501280784607,
      "learning_rate": 0.001999503163482332,
      "loss": 1.5463,
      "step": 298
    },
    {
      "epoch": 0.39866666666666667,
      "grad_norm": 0.07002240419387817,
      "learning_rate": 0.001999489459456639,
      "loss": 1.1883,
      "step": 299
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.036256637424230576,
      "learning_rate": 0.0019994755690455153,
      "loss": 1.2057,
      "step": 300
    },
    {
      "epoch": 0.4013333333333333,
      "grad_norm": 0.07686804234981537,
      "learning_rate": 0.0019994614922515504,
      "loss": 1.2535,
      "step": 301
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.06085726246237755,
      "learning_rate": 0.0019994472290773705,
      "loss": 1.3783,
      "step": 302
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.04615631699562073,
      "learning_rate": 0.0019994327795256352,
      "loss": 1.2315,
      "step": 303
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.07021823525428772,
      "learning_rate": 0.0019994181435990382,
      "loss": 1.5601,
      "step": 304
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.06546615064144135,
      "learning_rate": 0.0019994033213003104,
      "loss": 1.2778,
      "step": 305
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.053704507648944855,
      "learning_rate": 0.001999388312632214,
      "loss": 1.3663,
      "step": 306
    },
    {
      "epoch": 0.4093333333333333,
      "grad_norm": 0.045590389519929886,
      "learning_rate": 0.0019993731175975494,
      "loss": 1.1747,
      "step": 307
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.08280221372842789,
      "learning_rate": 0.001999357736199149,
      "loss": 1.2012,
      "step": 308
    },
    {
      "epoch": 0.412,
      "grad_norm": 0.03712066262960434,
      "learning_rate": 0.001999342168439882,
      "loss": 1.4598,
      "step": 309
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.09471578896045685,
      "learning_rate": 0.0019993264143226513,
      "loss": 1.3591,
      "step": 310
    },
    {
      "epoch": 0.4146666666666667,
      "grad_norm": 0.2632363736629486,
      "learning_rate": 0.0019993104738503945,
      "loss": 1.2627,
      "step": 311
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.06657509505748749,
      "learning_rate": 0.001999294347026084,
      "loss": 1.3266,
      "step": 312
    },
    {
      "epoch": 0.41733333333333333,
      "grad_norm": 0.06597900390625,
      "learning_rate": 0.0019992780338527276,
      "loss": 1.4617,
      "step": 313
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.0662798061966896,
      "learning_rate": 0.0019992615343333675,
      "loss": 1.1541,
      "step": 314
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.09077456593513489,
      "learning_rate": 0.0019992448484710797,
      "loss": 1.1156,
      "step": 315
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.049744147807359695,
      "learning_rate": 0.001999227976268977,
      "loss": 1.2986,
      "step": 316
    },
    {
      "epoch": 0.4226666666666667,
      "grad_norm": 0.07648691534996033,
      "learning_rate": 0.0019992109177302043,
      "loss": 1.3802,
      "step": 317
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.30873656272888184,
      "learning_rate": 0.0019991936728579436,
      "loss": 1.7303,
      "step": 318
    },
    {
      "epoch": 0.42533333333333334,
      "grad_norm": 0.10793206095695496,
      "learning_rate": 0.001999176241655411,
      "loss": 1.2319,
      "step": 319
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.04792880639433861,
      "learning_rate": 0.0019991586241258565,
      "loss": 1.2385,
      "step": 320
    },
    {
      "epoch": 0.428,
      "grad_norm": 0.10951211303472519,
      "learning_rate": 0.0019991408202725655,
      "loss": 1.4573,
      "step": 321
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.0815192237496376,
      "learning_rate": 0.0019991228300988585,
      "loss": 1.1636,
      "step": 322
    },
    {
      "epoch": 0.43066666666666664,
      "grad_norm": 0.09811433404684067,
      "learning_rate": 0.0019991046536080898,
      "loss": 1.4158,
      "step": 323
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.07855795323848724,
      "learning_rate": 0.0019990862908036487,
      "loss": 1.4849,
      "step": 324
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.06553905457258224,
      "learning_rate": 0.0019990677416889605,
      "loss": 1.2736,
      "step": 325
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.05087639391422272,
      "learning_rate": 0.001999049006267484,
      "loss": 1.406,
      "step": 326
    },
    {
      "epoch": 0.436,
      "grad_norm": 0.06218847632408142,
      "learning_rate": 0.0019990300845427124,
      "loss": 1.5146,
      "step": 327
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.06691920012235641,
      "learning_rate": 0.0019990109765181743,
      "loss": 1.4756,
      "step": 328
    },
    {
      "epoch": 0.43866666666666665,
      "grad_norm": 0.05301818996667862,
      "learning_rate": 0.0019989916821974334,
      "loss": 1.5027,
      "step": 329
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.05837976187467575,
      "learning_rate": 0.001998972201584088,
      "loss": 1.2203,
      "step": 330
    },
    {
      "epoch": 0.44133333333333336,
      "grad_norm": 0.05512513220310211,
      "learning_rate": 0.00199895253468177,
      "loss": 1.294,
      "step": 331
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.07397747039794922,
      "learning_rate": 0.0019989326814941473,
      "loss": 1.3471,
      "step": 332
    },
    {
      "epoch": 0.444,
      "grad_norm": 0.06325335800647736,
      "learning_rate": 0.0019989126420249218,
      "loss": 1.3156,
      "step": 333
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.05998796969652176,
      "learning_rate": 0.001998892416277831,
      "loss": 1.1595,
      "step": 334
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.10149934887886047,
      "learning_rate": 0.0019988720042566467,
      "loss": 1.3689,
      "step": 335
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.05460279434919357,
      "learning_rate": 0.001998851405965175,
      "loss": 1.6546,
      "step": 336
    },
    {
      "epoch": 0.4493333333333333,
      "grad_norm": 0.054483331739902496,
      "learning_rate": 0.0019988306214072573,
      "loss": 1.2249,
      "step": 337
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.08378994464874268,
      "learning_rate": 0.001998809650586769,
      "loss": 1.8498,
      "step": 338
    },
    {
      "epoch": 0.452,
      "grad_norm": 0.09979964047670364,
      "learning_rate": 0.001998788493507621,
      "loss": 1.2252,
      "step": 339
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.04588304087519646,
      "learning_rate": 0.001998767150173759,
      "loss": 1.5296,
      "step": 340
    },
    {
      "epoch": 0.45466666666666666,
      "grad_norm": 0.0771678239107132,
      "learning_rate": 0.001998745620589163,
      "loss": 1.3201,
      "step": 341
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.057599280029535294,
      "learning_rate": 0.001998723904757848,
      "loss": 1.2344,
      "step": 342
    },
    {
      "epoch": 0.4573333333333333,
      "grad_norm": 0.04583011195063591,
      "learning_rate": 0.0019987020026838633,
      "loss": 1.3051,
      "step": 343
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.03659690544009209,
      "learning_rate": 0.001998679914371293,
      "loss": 1.1183,
      "step": 344
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.0475444570183754,
      "learning_rate": 0.0019986576398242565,
      "loss": 1.1643,
      "step": 345
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.09210855513811111,
      "learning_rate": 0.0019986351790469074,
      "loss": 1.1875,
      "step": 346
    },
    {
      "epoch": 0.46266666666666667,
      "grad_norm": 0.050942420959472656,
      "learning_rate": 0.0019986125320434344,
      "loss": 1.2574,
      "step": 347
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.08606415241956711,
      "learning_rate": 0.0019985896988180605,
      "loss": 1.652,
      "step": 348
    },
    {
      "epoch": 0.4653333333333333,
      "grad_norm": 0.08358950912952423,
      "learning_rate": 0.001998566679375044,
      "loss": 1.562,
      "step": 349
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.05088535696268082,
      "learning_rate": 0.001998543473718677,
      "loss": 1.456,
      "step": 350
    },
    {
      "epoch": 0.468,
      "grad_norm": 0.0559992715716362,
      "learning_rate": 0.0019985200818532873,
      "loss": 1.5776,
      "step": 351
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.05187581852078438,
      "learning_rate": 0.001998496503783237,
      "loss": 1.3837,
      "step": 352
    },
    {
      "epoch": 0.4706666666666667,
      "grad_norm": 0.03263884037733078,
      "learning_rate": 0.0019984727395129234,
      "loss": 1.2556,
      "step": 353
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.04913047328591347,
      "learning_rate": 0.001998448789046777,
      "loss": 1.3417,
      "step": 354
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.05219447240233421,
      "learning_rate": 0.001998424652389265,
      "loss": 1.4745,
      "step": 355
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.057174138724803925,
      "learning_rate": 0.001998400329544888,
      "loss": 1.2246,
      "step": 356
    },
    {
      "epoch": 0.476,
      "grad_norm": 0.041574083268642426,
      "learning_rate": 0.001998375820518182,
      "loss": 1.3928,
      "step": 357
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.047312237322330475,
      "learning_rate": 0.001998351125313717,
      "loss": 1.065,
      "step": 358
    },
    {
      "epoch": 0.4786666666666667,
      "grad_norm": 0.03565726429224014,
      "learning_rate": 0.001998326243936099,
      "loss": 1.1635,
      "step": 359
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.09561602771282196,
      "learning_rate": 0.0019983011763899674,
      "loss": 1.3387,
      "step": 360
    },
    {
      "epoch": 0.48133333333333334,
      "grad_norm": 0.04501534253358841,
      "learning_rate": 0.0019982759226799965,
      "loss": 1.2516,
      "step": 361
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.05453461408615112,
      "learning_rate": 0.001998250482810896,
      "loss": 1.5275,
      "step": 362
    },
    {
      "epoch": 0.484,
      "grad_norm": 0.038464803248643875,
      "learning_rate": 0.0019982248567874095,
      "loss": 1.2146,
      "step": 363
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.04511315003037453,
      "learning_rate": 0.001998199044614317,
      "loss": 1.4316,
      "step": 364
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.0429406613111496,
      "learning_rate": 0.0019981730462964305,
      "loss": 1.4252,
      "step": 365
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.03547189384698868,
      "learning_rate": 0.001998146861838599,
      "loss": 1.4203,
      "step": 366
    },
    {
      "epoch": 0.48933333333333334,
      "grad_norm": 0.07850974053144455,
      "learning_rate": 0.0019981204912457046,
      "loss": 1.2955,
      "step": 367
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.04389988258481026,
      "learning_rate": 0.001998093934522666,
      "loss": 1.2423,
      "step": 368
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.046507686376571655,
      "learning_rate": 0.001998067191674435,
      "loss": 1.0064,
      "step": 369
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.051280561834573746,
      "learning_rate": 0.001998040262705999,
      "loss": 1.4747,
      "step": 370
    },
    {
      "epoch": 0.49466666666666664,
      "grad_norm": 0.04559962823987007,
      "learning_rate": 0.001998013147622379,
      "loss": 1.4199,
      "step": 371
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.04297352582216263,
      "learning_rate": 0.0019979858464286315,
      "loss": 1.1705,
      "step": 372
    },
    {
      "epoch": 0.49733333333333335,
      "grad_norm": 0.04963921383023262,
      "learning_rate": 0.0019979583591298485,
      "loss": 1.1705,
      "step": 373
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.18203385174274445,
      "learning_rate": 0.0019979306857311548,
      "loss": 1.5593,
      "step": 374
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.05946153402328491,
      "learning_rate": 0.0019979028262377117,
      "loss": 1.4933,
      "step": 375
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.04380752146244049,
      "learning_rate": 0.0019978747806547142,
      "loss": 1.206,
      "step": 376
    },
    {
      "epoch": 0.5026666666666667,
      "grad_norm": 0.04074501991271973,
      "learning_rate": 0.001997846548987392,
      "loss": 1.2303,
      "step": 377
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.04231489077210426,
      "learning_rate": 0.0019978181312410104,
      "loss": 1.2513,
      "step": 378
    },
    {
      "epoch": 0.5053333333333333,
      "grad_norm": 0.05422556772828102,
      "learning_rate": 0.001997789527420868,
      "loss": 1.3631,
      "step": 379
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.0528741180896759,
      "learning_rate": 0.0019977607375323,
      "loss": 1.2854,
      "step": 380
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.044870343059301376,
      "learning_rate": 0.0019977317615806735,
      "loss": 1.3378,
      "step": 381
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.044257208704948425,
      "learning_rate": 0.001997702599571393,
      "loss": 1.1967,
      "step": 382
    },
    {
      "epoch": 0.5106666666666667,
      "grad_norm": 0.04860832914710045,
      "learning_rate": 0.001997673251509897,
      "loss": 1.3478,
      "step": 383
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.045245107263326645,
      "learning_rate": 0.001997643717401657,
      "loss": 1.285,
      "step": 384
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.04363589733839035,
      "learning_rate": 0.001997613997252182,
      "loss": 1.0023,
      "step": 385
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.05787007510662079,
      "learning_rate": 0.001997584091067014,
      "loss": 1.4132,
      "step": 386
    },
    {
      "epoch": 0.516,
      "grad_norm": 0.03477425128221512,
      "learning_rate": 0.0019975539988517288,
      "loss": 1.1194,
      "step": 387
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.05705839768052101,
      "learning_rate": 0.001997523720611939,
      "loss": 1.2487,
      "step": 388
    },
    {
      "epoch": 0.5186666666666667,
      "grad_norm": 0.05525148659944534,
      "learning_rate": 0.0019974932563532905,
      "loss": 0.8382,
      "step": 389
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.05530788376927376,
      "learning_rate": 0.0019974626060814647,
      "loss": 1.3131,
      "step": 390
    },
    {
      "epoch": 0.5213333333333333,
      "grad_norm": 0.05661793425679207,
      "learning_rate": 0.001997431769802177,
      "loss": 1.0936,
      "step": 391
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.05634351447224617,
      "learning_rate": 0.0019974007475211776,
      "loss": 1.3485,
      "step": 392
    },
    {
      "epoch": 0.524,
      "grad_norm": 0.04908985644578934,
      "learning_rate": 0.001997369539244252,
      "loss": 1.1945,
      "step": 393
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.14249204099178314,
      "learning_rate": 0.0019973381449772194,
      "loss": 1.6922,
      "step": 394
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.04738152399659157,
      "learning_rate": 0.0019973065647259348,
      "loss": 0.9895,
      "step": 395
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.0476241409778595,
      "learning_rate": 0.0019972747984962867,
      "loss": 1.2069,
      "step": 396
    },
    {
      "epoch": 0.5293333333333333,
      "grad_norm": 0.15888147056102753,
      "learning_rate": 0.0019972428462941994,
      "loss": 1.3894,
      "step": 397
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.046857211738824844,
      "learning_rate": 0.0019972107081256316,
      "loss": 1.4062,
      "step": 398
    },
    {
      "epoch": 0.532,
      "grad_norm": 0.034172605723142624,
      "learning_rate": 0.0019971783839965755,
      "loss": 1.3365,
      "step": 399
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.041829273104667664,
      "learning_rate": 0.0019971458739130596,
      "loss": 0.9306,
      "step": 400
    },
    {
      "epoch": 0.5346666666666666,
      "grad_norm": 0.14157475531101227,
      "learning_rate": 0.0019971131778811465,
      "loss": 1.2573,
      "step": 401
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.05568951368331909,
      "learning_rate": 0.0019970802959069327,
      "loss": 1.3749,
      "step": 402
    },
    {
      "epoch": 0.5373333333333333,
      "grad_norm": 0.05192892253398895,
      "learning_rate": 0.0019970472279965505,
      "loss": 1.3734,
      "step": 403
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.05479362979531288,
      "learning_rate": 0.001997013974156167,
      "loss": 1.3693,
      "step": 404
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.04695793241262436,
      "learning_rate": 0.001996980534391982,
      "loss": 1.4303,
      "step": 405
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 0.049125052988529205,
      "learning_rate": 0.0019969469087102324,
      "loss": 1.3034,
      "step": 406
    },
    {
      "epoch": 0.5426666666666666,
      "grad_norm": 0.04361307993531227,
      "learning_rate": 0.0019969130971171888,
      "loss": 1.1984,
      "step": 407
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.050563666969537735,
      "learning_rate": 0.001996879099619156,
      "loss": 1.2095,
      "step": 408
    },
    {
      "epoch": 0.5453333333333333,
      "grad_norm": 0.05336029827594757,
      "learning_rate": 0.001996844916222474,
      "loss": 1.4589,
      "step": 409
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.04923446476459503,
      "learning_rate": 0.001996810546933517,
      "loss": 1.0218,
      "step": 410
    },
    {
      "epoch": 0.548,
      "grad_norm": 0.4374910891056061,
      "learning_rate": 0.0019967759917586952,
      "loss": 1.3898,
      "step": 411
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.0457625612616539,
      "learning_rate": 0.001996741250704451,
      "loss": 1.2638,
      "step": 412
    },
    {
      "epoch": 0.5506666666666666,
      "grad_norm": 0.07485470920801163,
      "learning_rate": 0.001996706323777264,
      "loss": 1.1842,
      "step": 413
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.16584523022174835,
      "learning_rate": 0.0019966712109836474,
      "loss": 1.2966,
      "step": 414
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.24083121120929718,
      "learning_rate": 0.001996635912330149,
      "loss": 1.1396,
      "step": 415
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.04897521436214447,
      "learning_rate": 0.001996600427823351,
      "loss": 1.1033,
      "step": 416
    },
    {
      "epoch": 0.556,
      "grad_norm": 0.04386454448103905,
      "learning_rate": 0.0019965647574698704,
      "loss": 1.3412,
      "step": 417
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.17673587799072266,
      "learning_rate": 0.0019965289012763596,
      "loss": 1.2012,
      "step": 418
    },
    {
      "epoch": 0.5586666666666666,
      "grad_norm": 0.09123574942350388,
      "learning_rate": 0.0019964928592495043,
      "loss": 1.3724,
      "step": 419
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.30419105291366577,
      "learning_rate": 0.0019964566313960264,
      "loss": 1.2557,
      "step": 420
    },
    {
      "epoch": 0.5613333333333334,
      "grad_norm": 0.06432117521762848,
      "learning_rate": 0.001996420217722682,
      "loss": 1.3414,
      "step": 421
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 0.11227492988109589,
      "learning_rate": 0.0019963836182362604,
      "loss": 1.2183,
      "step": 422
    },
    {
      "epoch": 0.564,
      "grad_norm": 0.2097117155790329,
      "learning_rate": 0.001996346832943587,
      "loss": 1.5035,
      "step": 423
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.06525377184152603,
      "learning_rate": 0.0019963098618515224,
      "loss": 1.2455,
      "step": 424
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.11905170977115631,
      "learning_rate": 0.00199627270496696,
      "loss": 1.4152,
      "step": 425
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.08599259704351425,
      "learning_rate": 0.0019962353622968295,
      "loss": 1.4766,
      "step": 426
    },
    {
      "epoch": 0.5693333333333334,
      "grad_norm": 0.05159797891974449,
      "learning_rate": 0.001996197833848094,
      "loss": 1.4331,
      "step": 427
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.05167139321565628,
      "learning_rate": 0.0019961601196277524,
      "loss": 1.0232,
      "step": 428
    },
    {
      "epoch": 0.572,
      "grad_norm": 0.06547737866640091,
      "learning_rate": 0.0019961222196428377,
      "loss": 1.3607,
      "step": 429
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.04885708540678024,
      "learning_rate": 0.001996084133900417,
      "loss": 1.3514,
      "step": 430
    },
    {
      "epoch": 0.5746666666666667,
      "grad_norm": 0.06885898113250732,
      "learning_rate": 0.001996045862407593,
      "loss": 1.2983,
      "step": 431
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.11762596666812897,
      "learning_rate": 0.001996007405171502,
      "loss": 1.4115,
      "step": 432
    },
    {
      "epoch": 0.5773333333333334,
      "grad_norm": 0.045557085424661636,
      "learning_rate": 0.001995968762199316,
      "loss": 1.2843,
      "step": 433
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.0447065569460392,
      "learning_rate": 0.0019959299334982414,
      "loss": 1.126,
      "step": 434
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.06316620856523514,
      "learning_rate": 0.0019958909190755185,
      "loss": 1.2184,
      "step": 435
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.17869415879249573,
      "learning_rate": 0.001995851718938423,
      "loss": 1.335,
      "step": 436
    },
    {
      "epoch": 0.5826666666666667,
      "grad_norm": 0.041679441928863525,
      "learning_rate": 0.001995812333094265,
      "loss": 1.2453,
      "step": 437
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.06389506906270981,
      "learning_rate": 0.0019957727615503885,
      "loss": 1.4884,
      "step": 438
    },
    {
      "epoch": 0.5853333333333334,
      "grad_norm": 0.05041440948843956,
      "learning_rate": 0.001995733004314174,
      "loss": 1.2995,
      "step": 439
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.039798665791749954,
      "learning_rate": 0.001995693061393035,
      "loss": 1.2662,
      "step": 440
    },
    {
      "epoch": 0.588,
      "grad_norm": 0.05408851057291031,
      "learning_rate": 0.00199565293279442,
      "loss": 1.3753,
      "step": 441
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.039038997143507004,
      "learning_rate": 0.0019956126185258116,
      "loss": 1.2177,
      "step": 442
    },
    {
      "epoch": 0.5906666666666667,
      "grad_norm": 0.04923113062977791,
      "learning_rate": 0.0019955721185947284,
      "loss": 1.2576,
      "step": 443
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.05296563357114792,
      "learning_rate": 0.0019955314330087222,
      "loss": 1.2748,
      "step": 444
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.04187644645571709,
      "learning_rate": 0.0019954905617753814,
      "loss": 1.2104,
      "step": 445
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.12507914006710052,
      "learning_rate": 0.0019954495049023264,
      "loss": 1.2816,
      "step": 446
    },
    {
      "epoch": 0.596,
      "grad_norm": 0.03879881650209427,
      "learning_rate": 0.001995408262397214,
      "loss": 1.3522,
      "step": 447
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.03624623641371727,
      "learning_rate": 0.001995366834267735,
      "loss": 1.1835,
      "step": 448
    },
    {
      "epoch": 0.5986666666666667,
      "grad_norm": 0.08685246109962463,
      "learning_rate": 0.001995325220521615,
      "loss": 1.1703,
      "step": 449
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.05578622221946716,
      "learning_rate": 0.0019952834211666138,
      "loss": 1.3275,
      "step": 450
    },
    {
      "epoch": 0.6013333333333334,
      "grad_norm": 0.042471833527088165,
      "learning_rate": 0.001995241436210527,
      "loss": 1.2373,
      "step": 451
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 0.05162429064512253,
      "learning_rate": 0.0019951992656611836,
      "loss": 1.178,
      "step": 452
    },
    {
      "epoch": 0.604,
      "grad_norm": 0.05223008617758751,
      "learning_rate": 0.0019951569095264473,
      "loss": 1.15,
      "step": 453
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 0.03955993056297302,
      "learning_rate": 0.0019951143678142167,
      "loss": 1.3227,
      "step": 454
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.05032728984951973,
      "learning_rate": 0.0019950716405324254,
      "loss": 1.3173,
      "step": 455
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.0402766689658165,
      "learning_rate": 0.0019950287276890412,
      "loss": 1.067,
      "step": 456
    },
    {
      "epoch": 0.6093333333333333,
      "grad_norm": 0.053399451076984406,
      "learning_rate": 0.001994985629292066,
      "loss": 1.3012,
      "step": 457
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.06857909262180328,
      "learning_rate": 0.001994942345349537,
      "loss": 1.129,
      "step": 458
    },
    {
      "epoch": 0.612,
      "grad_norm": 0.03352987393736839,
      "learning_rate": 0.0019948988758695264,
      "loss": 1.2819,
      "step": 459
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.07027451694011688,
      "learning_rate": 0.00199485522086014,
      "loss": 1.6333,
      "step": 460
    },
    {
      "epoch": 0.6146666666666667,
      "grad_norm": 0.05211083963513374,
      "learning_rate": 0.001994811380329518,
      "loss": 1.5878,
      "step": 461
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.040701575577259064,
      "learning_rate": 0.0019947673542858365,
      "loss": 1.1966,
      "step": 462
    },
    {
      "epoch": 0.6173333333333333,
      "grad_norm": 0.039525751024484634,
      "learning_rate": 0.0019947231427373062,
      "loss": 1.5503,
      "step": 463
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.04832125082612038,
      "learning_rate": 0.00199467874569217,
      "loss": 1.2978,
      "step": 464
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.04553849995136261,
      "learning_rate": 0.0019946341631587087,
      "loss": 1.2427,
      "step": 465
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.32125380635261536,
      "learning_rate": 0.001994589395145235,
      "loss": 1.4024,
      "step": 466
    },
    {
      "epoch": 0.6226666666666667,
      "grad_norm": 0.040016330778598785,
      "learning_rate": 0.001994544441660098,
      "loss": 1.2246,
      "step": 467
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.10011456906795502,
      "learning_rate": 0.0019944993027116798,
      "loss": 1.4486,
      "step": 468
    },
    {
      "epoch": 0.6253333333333333,
      "grad_norm": 0.03368661180138588,
      "learning_rate": 0.0019944539783083985,
      "loss": 1.279,
      "step": 469
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.04721704125404358,
      "learning_rate": 0.001994408468458706,
      "loss": 1.0057,
      "step": 470
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.14364473521709442,
      "learning_rate": 0.0019943627731710896,
      "loss": 1.7496,
      "step": 471
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.03712965548038483,
      "learning_rate": 0.00199431689245407,
      "loss": 0.95,
      "step": 472
    },
    {
      "epoch": 0.6306666666666667,
      "grad_norm": 0.06311935931444168,
      "learning_rate": 0.001994270826316203,
      "loss": 1.3062,
      "step": 473
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.15321768820285797,
      "learning_rate": 0.0019942245747660795,
      "loss": 1.422,
      "step": 474
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.0938134640455246,
      "learning_rate": 0.001994178137812324,
      "loss": 1.3303,
      "step": 475
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.04027548059821129,
      "learning_rate": 0.0019941315154635973,
      "loss": 1.4462,
      "step": 476
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.043631475418806076,
      "learning_rate": 0.0019940847077285916,
      "loss": 1.2526,
      "step": 477
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.07198604196310043,
      "learning_rate": 0.001994037714616037,
      "loss": 1.0239,
      "step": 478
    },
    {
      "epoch": 0.6386666666666667,
      "grad_norm": 0.07211945205926895,
      "learning_rate": 0.0019939905361346963,
      "loss": 1.4472,
      "step": 479
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.04374998062849045,
      "learning_rate": 0.0019939431722933677,
      "loss": 1.3215,
      "step": 480
    },
    {
      "epoch": 0.6413333333333333,
      "grad_norm": 0.04757560417056084,
      "learning_rate": 0.0019938956231008837,
      "loss": 1.3205,
      "step": 481
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.037524498999118805,
      "learning_rate": 0.001993847888566111,
      "loss": 1.3772,
      "step": 482
    },
    {
      "epoch": 0.644,
      "grad_norm": 0.03711952269077301,
      "learning_rate": 0.001993799968697951,
      "loss": 1.3041,
      "step": 483
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 0.05570259690284729,
      "learning_rate": 0.0019937518635053404,
      "loss": 1.5562,
      "step": 484
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.04483720660209656,
      "learning_rate": 0.0019937035729972494,
      "loss": 1.0846,
      "step": 485
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.05575212463736534,
      "learning_rate": 0.0019936550971826833,
      "loss": 1.6606,
      "step": 486
    },
    {
      "epoch": 0.6493333333333333,
      "grad_norm": 0.07995740324258804,
      "learning_rate": 0.001993606436070682,
      "loss": 1.205,
      "step": 487
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.05766502395272255,
      "learning_rate": 0.0019935575896703204,
      "loss": 1.5778,
      "step": 488
    },
    {
      "epoch": 0.652,
      "grad_norm": 0.16146396100521088,
      "learning_rate": 0.0019935085579907063,
      "loss": 1.3767,
      "step": 489
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.05064406991004944,
      "learning_rate": 0.001993459341040984,
      "loss": 1.3276,
      "step": 490
    },
    {
      "epoch": 0.6546666666666666,
      "grad_norm": 0.05369731783866882,
      "learning_rate": 0.001993409938830331,
      "loss": 1.4342,
      "step": 491
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.03992399945855141,
      "learning_rate": 0.0019933603513679603,
      "loss": 1.2062,
      "step": 492
    },
    {
      "epoch": 0.6573333333333333,
      "grad_norm": 0.032426804304122925,
      "learning_rate": 0.001993310578663119,
      "loss": 1.4297,
      "step": 493
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.04238004609942436,
      "learning_rate": 0.0019932606207250883,
      "loss": 1.0547,
      "step": 494
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.04586634784936905,
      "learning_rate": 0.0019932104775631843,
      "loss": 1.0463,
      "step": 495
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.047575145959854126,
      "learning_rate": 0.0019931601491867588,
      "loss": 1.4064,
      "step": 496
    },
    {
      "epoch": 0.6626666666666666,
      "grad_norm": 0.05401758849620819,
      "learning_rate": 0.001993109635605196,
      "loss": 1.2379,
      "step": 497
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.07208560407161713,
      "learning_rate": 0.001993058936827916,
      "loss": 1.2841,
      "step": 498
    },
    {
      "epoch": 0.6653333333333333,
      "grad_norm": 0.03680965304374695,
      "learning_rate": 0.001993008052864373,
      "loss": 1.195,
      "step": 499
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.03922390192747116,
      "learning_rate": 0.0019929569837240564,
      "loss": 1.4405,
      "step": 500
    },
    {
      "epoch": 0.668,
      "grad_norm": 0.11053036898374557,
      "learning_rate": 0.001992905729416489,
      "loss": 1.2263,
      "step": 501
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.32040610909461975,
      "learning_rate": 0.0019928542899512293,
      "loss": 1.4134,
      "step": 502
    },
    {
      "epoch": 0.6706666666666666,
      "grad_norm": 0.04465348646044731,
      "learning_rate": 0.001992802665337869,
      "loss": 1.2449,
      "step": 503
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.05368734896183014,
      "learning_rate": 0.001992750855586036,
      "loss": 1.3804,
      "step": 504
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.13195988535881042,
      "learning_rate": 0.0019926988607053913,
      "loss": 1.3817,
      "step": 505
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.10302291810512543,
      "learning_rate": 0.0019926466807056306,
      "loss": 1.221,
      "step": 506
    },
    {
      "epoch": 0.676,
      "grad_norm": 0.05810856074094772,
      "learning_rate": 0.0019925943155964855,
      "loss": 1.1166,
      "step": 507
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 0.035961881279945374,
      "learning_rate": 0.0019925417653877202,
      "loss": 1.2398,
      "step": 508
    },
    {
      "epoch": 0.6786666666666666,
      "grad_norm": 0.15068960189819336,
      "learning_rate": 0.0019924890300891344,
      "loss": 1.2033,
      "step": 509
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.15369467437267303,
      "learning_rate": 0.0019924361097105625,
      "loss": 1.5533,
      "step": 510
    },
    {
      "epoch": 0.6813333333333333,
      "grad_norm": 0.03356549143791199,
      "learning_rate": 0.001992383004261873,
      "loss": 1.3505,
      "step": 511
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.060285672545433044,
      "learning_rate": 0.0019923297137529688,
      "loss": 1.2064,
      "step": 512
    },
    {
      "epoch": 0.684,
      "grad_norm": 0.03869315981864929,
      "learning_rate": 0.001992276238193788,
      "loss": 1.1933,
      "step": 513
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 0.0386049710214138,
      "learning_rate": 0.0019922225775943023,
      "loss": 1.0469,
      "step": 514
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.10605252534151077,
      "learning_rate": 0.0019921687319645184,
      "loss": 1.1665,
      "step": 515
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.05392318591475487,
      "learning_rate": 0.001992114701314478,
      "loss": 0.9972,
      "step": 516
    },
    {
      "epoch": 0.6893333333333334,
      "grad_norm": 0.049436092376708984,
      "learning_rate": 0.0019920604856542563,
      "loss": 1.3673,
      "step": 517
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 0.10797630995512009,
      "learning_rate": 0.0019920060849939634,
      "loss": 1.3112,
      "step": 518
    },
    {
      "epoch": 0.692,
      "grad_norm": 0.2239043116569519,
      "learning_rate": 0.0019919514993437444,
      "loss": 1.6899,
      "step": 519
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.09764652699232101,
      "learning_rate": 0.0019918967287137785,
      "loss": 1.4775,
      "step": 520
    },
    {
      "epoch": 0.6946666666666667,
      "grad_norm": 0.07788337022066116,
      "learning_rate": 0.0019918417731142786,
      "loss": 1.3581,
      "step": 521
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.05877821892499924,
      "learning_rate": 0.001991786632555494,
      "loss": 1.2085,
      "step": 522
    },
    {
      "epoch": 0.6973333333333334,
      "grad_norm": 0.03842935338616371,
      "learning_rate": 0.0019917313070477055,
      "loss": 1.1956,
      "step": 523
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.08199071139097214,
      "learning_rate": 0.0019916757966012325,
      "loss": 1.178,
      "step": 524
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.04760304465889931,
      "learning_rate": 0.001991620101226425,
      "loss": 1.2226,
      "step": 525
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.039904844015836716,
      "learning_rate": 0.00199156422093367,
      "loss": 1.1394,
      "step": 526
    },
    {
      "epoch": 0.7026666666666667,
      "grad_norm": 0.0473652258515358,
      "learning_rate": 0.0019915081557333875,
      "loss": 1.2372,
      "step": 527
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.04281729459762573,
      "learning_rate": 0.001991451905636033,
      "loss": 1.191,
      "step": 528
    },
    {
      "epoch": 0.7053333333333334,
      "grad_norm": 0.05050241947174072,
      "learning_rate": 0.001991395470652096,
      "loss": 1.1128,
      "step": 529
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.2780245244503021,
      "learning_rate": 0.0019913388507921,
      "loss": 1.4138,
      "step": 530
    },
    {
      "epoch": 0.708,
      "grad_norm": 0.06240695342421532,
      "learning_rate": 0.0019912820460666046,
      "loss": 1.379,
      "step": 531
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 0.07397565245628357,
      "learning_rate": 0.0019912250564862017,
      "loss": 1.3243,
      "step": 532
    },
    {
      "epoch": 0.7106666666666667,
      "grad_norm": 0.06275905668735504,
      "learning_rate": 0.0019911678820615187,
      "loss": 1.2253,
      "step": 533
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.07825533300638199,
      "learning_rate": 0.0019911105228032186,
      "loss": 1.266,
      "step": 534
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.0673811212182045,
      "learning_rate": 0.0019910529787219968,
      "loss": 1.2804,
      "step": 535
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.04868144541978836,
      "learning_rate": 0.0019909952498285846,
      "loss": 1.3076,
      "step": 536
    },
    {
      "epoch": 0.716,
      "grad_norm": 0.06706177443265915,
      "learning_rate": 0.0019909373361337475,
      "loss": 1.399,
      "step": 537
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 0.055421821773052216,
      "learning_rate": 0.001990879237648285,
      "loss": 1.1078,
      "step": 538
    },
    {
      "epoch": 0.7186666666666667,
      "grad_norm": 0.05965227261185646,
      "learning_rate": 0.0019908209543830313,
      "loss": 1.263,
      "step": 539
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.035955995321273804,
      "learning_rate": 0.001990762486348855,
      "loss": 0.9878,
      "step": 540
    },
    {
      "epoch": 0.7213333333333334,
      "grad_norm": 0.03754599392414093,
      "learning_rate": 0.0019907038335566594,
      "loss": 1.2353,
      "step": 541
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.05093695968389511,
      "learning_rate": 0.001990644996017382,
      "loss": 1.2934,
      "step": 542
    },
    {
      "epoch": 0.724,
      "grad_norm": 0.04218876734375954,
      "learning_rate": 0.0019905859737419955,
      "loss": 1.2224,
      "step": 543
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.04036647453904152,
      "learning_rate": 0.0019905267667415056,
      "loss": 1.1762,
      "step": 544
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.07038184255361557,
      "learning_rate": 0.0019904673750269536,
      "loss": 1.1477,
      "step": 545
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.03646077215671539,
      "learning_rate": 0.001990407798609415,
      "loss": 1.0942,
      "step": 546
    },
    {
      "epoch": 0.7293333333333333,
      "grad_norm": 0.05545599013566971,
      "learning_rate": 0.0019903480374999995,
      "loss": 1.147,
      "step": 547
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.04763709753751755,
      "learning_rate": 0.0019902880917098513,
      "loss": 1.1576,
      "step": 548
    },
    {
      "epoch": 0.732,
      "grad_norm": 0.04583637788891792,
      "learning_rate": 0.001990227961250149,
      "loss": 1.0715,
      "step": 549
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.04283996298909187,
      "learning_rate": 0.0019901676461321067,
      "loss": 1.195,
      "step": 550
    },
    {
      "epoch": 0.7346666666666667,
      "grad_norm": 0.030183978378772736,
      "learning_rate": 0.001990107146366971,
      "loss": 1.2644,
      "step": 551
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.03771253675222397,
      "learning_rate": 0.001990046461966024,
      "loss": 1.105,
      "step": 552
    },
    {
      "epoch": 0.7373333333333333,
      "grad_norm": 0.03785443305969238,
      "learning_rate": 0.0019899855929405826,
      "loss": 1.2575,
      "step": 553
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 0.17935176193714142,
      "learning_rate": 0.0019899245393019977,
      "loss": 1.5944,
      "step": 554
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.05186612159013748,
      "learning_rate": 0.001989863301061654,
      "loss": 1.3438,
      "step": 555
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.04307100921869278,
      "learning_rate": 0.001989801878230972,
      "loss": 1.3061,
      "step": 556
    },
    {
      "epoch": 0.7426666666666667,
      "grad_norm": 0.03372262418270111,
      "learning_rate": 0.0019897402708214055,
      "loss": 1.0965,
      "step": 557
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.04772927612066269,
      "learning_rate": 0.0019896784788444428,
      "loss": 1.1738,
      "step": 558
    },
    {
      "epoch": 0.7453333333333333,
      "grad_norm": 0.044674813747406006,
      "learning_rate": 0.0019896165023116077,
      "loss": 1.1048,
      "step": 559
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.05813051760196686,
      "learning_rate": 0.0019895543412344566,
      "loss": 1.2387,
      "step": 560
    },
    {
      "epoch": 0.748,
      "grad_norm": 0.10255257040262222,
      "learning_rate": 0.0019894919956245827,
      "loss": 1.369,
      "step": 561
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.038473162800073624,
      "learning_rate": 0.0019894294654936107,
      "loss": 1.279,
      "step": 562
    },
    {
      "epoch": 0.7506666666666667,
      "grad_norm": 0.0671471357345581,
      "learning_rate": 0.0019893667508532023,
      "loss": 1.4185,
      "step": 563
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.04327027499675751,
      "learning_rate": 0.001989303851715052,
      "loss": 0.8439,
      "step": 564
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.04634663090109825,
      "learning_rate": 0.00198924076809089,
      "loss": 1.1487,
      "step": 565
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 0.03611162677407265,
      "learning_rate": 0.0019891774999924797,
      "loss": 1.0885,
      "step": 566
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.04367527738213539,
      "learning_rate": 0.0019891140474316196,
      "loss": 1.5282,
      "step": 567
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.04957663640379906,
      "learning_rate": 0.0019890504104201415,
      "loss": 1.2081,
      "step": 568
    },
    {
      "epoch": 0.7586666666666667,
      "grad_norm": 0.04294877499341965,
      "learning_rate": 0.0019889865889699135,
      "loss": 1.066,
      "step": 569
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.04970566928386688,
      "learning_rate": 0.0019889225830928363,
      "loss": 1.3773,
      "step": 570
    },
    {
      "epoch": 0.7613333333333333,
      "grad_norm": 0.04009956121444702,
      "learning_rate": 0.0019888583928008466,
      "loss": 1.4071,
      "step": 571
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.05686815828084946,
      "learning_rate": 0.0019887940181059142,
      "loss": 1.2829,
      "step": 572
    },
    {
      "epoch": 0.764,
      "grad_norm": 0.042250555008649826,
      "learning_rate": 0.0019887294590200436,
      "loss": 1.3649,
      "step": 573
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.04666374251246452,
      "learning_rate": 0.001988664715555274,
      "loss": 1.3635,
      "step": 574
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.03796734660863876,
      "learning_rate": 0.0019885997877236786,
      "loss": 1.2567,
      "step": 575
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.042796675115823746,
      "learning_rate": 0.0019885346755373658,
      "loss": 1.202,
      "step": 576
    },
    {
      "epoch": 0.7693333333333333,
      "grad_norm": 0.038811858743429184,
      "learning_rate": 0.001988469379008477,
      "loss": 1.1965,
      "step": 577
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.04945554956793785,
      "learning_rate": 0.001988403898149189,
      "loss": 1.2736,
      "step": 578
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.04024607688188553,
      "learning_rate": 0.0019883382329717128,
      "loss": 1.0842,
      "step": 579
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.09316089749336243,
      "learning_rate": 0.0019882723834882933,
      "loss": 1.3453,
      "step": 580
    },
    {
      "epoch": 0.7746666666666666,
      "grad_norm": 0.0477818064391613,
      "learning_rate": 0.001988206349711211,
      "loss": 1.2358,
      "step": 581
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.0430501364171505,
      "learning_rate": 0.0019881401316527796,
      "loss": 1.2793,
      "step": 582
    },
    {
      "epoch": 0.7773333333333333,
      "grad_norm": 0.04995362088084221,
      "learning_rate": 0.001988073729325347,
      "loss": 1.1499,
      "step": 583
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.0571594201028347,
      "learning_rate": 0.0019880071427412957,
      "loss": 1.3315,
      "step": 584
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.04272359237074852,
      "learning_rate": 0.001987940371913044,
      "loss": 1.1035,
      "step": 585
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 0.09237006306648254,
      "learning_rate": 0.0019878734168530428,
      "loss": 1.171,
      "step": 586
    },
    {
      "epoch": 0.7826666666666666,
      "grad_norm": 0.06027572974562645,
      "learning_rate": 0.0019878062775737777,
      "loss": 1.2347,
      "step": 587
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.07006745785474777,
      "learning_rate": 0.0019877389540877686,
      "loss": 1.2659,
      "step": 588
    },
    {
      "epoch": 0.7853333333333333,
      "grad_norm": 0.034680936485528946,
      "learning_rate": 0.001987671446407571,
      "loss": 1.31,
      "step": 589
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.03421400114893913,
      "learning_rate": 0.001987603754545773,
      "loss": 1.0262,
      "step": 590
    },
    {
      "epoch": 0.788,
      "grad_norm": 0.035117678344249725,
      "learning_rate": 0.001987535878514998,
      "loss": 1.1246,
      "step": 591
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.0343889556825161,
      "learning_rate": 0.001987467818327904,
      "loss": 1.1339,
      "step": 592
    },
    {
      "epoch": 0.7906666666666666,
      "grad_norm": 0.0352587066590786,
      "learning_rate": 0.0019873995739971818,
      "loss": 1.199,
      "step": 593
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.048659373074769974,
      "learning_rate": 0.001987331145535559,
      "loss": 1.277,
      "step": 594
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.05149490758776665,
      "learning_rate": 0.0019872625329557954,
      "loss": 1.4934,
      "step": 595
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.21961577236652374,
      "learning_rate": 0.001987193736270686,
      "loss": 1.3801,
      "step": 596
    },
    {
      "epoch": 0.796,
      "grad_norm": 0.061457935720682144,
      "learning_rate": 0.00198712475549306,
      "loss": 1.2059,
      "step": 597
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 0.07134959101676941,
      "learning_rate": 0.001987055590635781,
      "loss": 1.1302,
      "step": 598
    },
    {
      "epoch": 0.7986666666666666,
      "grad_norm": 0.04010030999779701,
      "learning_rate": 0.0019869862417117475,
      "loss": 1.0476,
      "step": 599
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.057877711951732635,
      "learning_rate": 0.0019869167087338906,
      "loss": 1.2614,
      "step": 600
    },
    {
      "epoch": 0.8013333333333333,
      "grad_norm": 0.0511474646627903,
      "learning_rate": 0.001986846991715178,
      "loss": 1.0175,
      "step": 601
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 0.07443512976169586,
      "learning_rate": 0.0019867770906686097,
      "loss": 0.989,
      "step": 602
    },
    {
      "epoch": 0.804,
      "grad_norm": 0.050948869436979294,
      "learning_rate": 0.0019867070056072216,
      "loss": 1.1243,
      "step": 603
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.08675958216190338,
      "learning_rate": 0.0019866367365440826,
      "loss": 1.3732,
      "step": 604
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.05105762556195259,
      "learning_rate": 0.001986566283492297,
      "loss": 1.456,
      "step": 605
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.03602202981710434,
      "learning_rate": 0.0019864956464650026,
      "loss": 1.0652,
      "step": 606
    },
    {
      "epoch": 0.8093333333333333,
      "grad_norm": 0.042252231389284134,
      "learning_rate": 0.0019864248254753717,
      "loss": 1.549,
      "step": 607
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.038269586861133575,
      "learning_rate": 0.0019863538205366115,
      "loss": 1.2007,
      "step": 608
    },
    {
      "epoch": 0.812,
      "grad_norm": 0.041908953338861465,
      "learning_rate": 0.001986282631661963,
      "loss": 1.5537,
      "step": 609
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.04403570666909218,
      "learning_rate": 0.0019862112588647013,
      "loss": 1.1792,
      "step": 610
    },
    {
      "epoch": 0.8146666666666667,
      "grad_norm": 0.048691533505916595,
      "learning_rate": 0.001986139702158136,
      "loss": 1.3752,
      "step": 611
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.03814046084880829,
      "learning_rate": 0.001986067961555611,
      "loss": 1.337,
      "step": 612
    },
    {
      "epoch": 0.8173333333333334,
      "grad_norm": 0.04022346809506416,
      "learning_rate": 0.001985996037070505,
      "loss": 1.1274,
      "step": 613
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.047556325793266296,
      "learning_rate": 0.00198592392871623,
      "loss": 1.3994,
      "step": 614
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.03753335401415825,
      "learning_rate": 0.0019858516365062334,
      "loss": 1.0339,
      "step": 615
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.04924526438117027,
      "learning_rate": 0.0019857791604539956,
      "loss": 1.238,
      "step": 616
    },
    {
      "epoch": 0.8226666666666667,
      "grad_norm": 0.0611661821603775,
      "learning_rate": 0.0019857065005730325,
      "loss": 1.3735,
      "step": 617
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.06356005370616913,
      "learning_rate": 0.0019856336568768933,
      "loss": 1.2529,
      "step": 618
    },
    {
      "epoch": 0.8253333333333334,
      "grad_norm": 0.040762778371572495,
      "learning_rate": 0.0019855606293791624,
      "loss": 1.0775,
      "step": 619
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.04204663261771202,
      "learning_rate": 0.001985487418093458,
      "loss": 1.2484,
      "step": 620
    },
    {
      "epoch": 0.828,
      "grad_norm": 0.043720610439777374,
      "learning_rate": 0.001985414023033432,
      "loss": 1.351,
      "step": 621
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 0.04349161311984062,
      "learning_rate": 0.001985340444212772,
      "loss": 1.2079,
      "step": 622
    },
    {
      "epoch": 0.8306666666666667,
      "grad_norm": 0.04948198050260544,
      "learning_rate": 0.0019852666816451985,
      "loss": 1.4174,
      "step": 623
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.03907691314816475,
      "learning_rate": 0.001985192735344467,
      "loss": 1.2594,
      "step": 624
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.036606643348932266,
      "learning_rate": 0.0019851186053243667,
      "loss": 1.3409,
      "step": 625
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.03999806568026543,
      "learning_rate": 0.0019850442915987213,
      "loss": 1.3813,
      "step": 626
    },
    {
      "epoch": 0.836,
      "grad_norm": 0.05849708616733551,
      "learning_rate": 0.00198496979418139,
      "loss": 1.3855,
      "step": 627
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.030042242258787155,
      "learning_rate": 0.001984895113086264,
      "loss": 1.2365,
      "step": 628
    },
    {
      "epoch": 0.8386666666666667,
      "grad_norm": 0.0543988011777401,
      "learning_rate": 0.0019848202483272698,
      "loss": 1.3074,
      "step": 629
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.03862643614411354,
      "learning_rate": 0.001984745199918369,
      "loss": 1.3165,
      "step": 630
    },
    {
      "epoch": 0.8413333333333334,
      "grad_norm": 0.033345337957143784,
      "learning_rate": 0.0019846699678735566,
      "loss": 1.0515,
      "step": 631
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.04215315356850624,
      "learning_rate": 0.0019845945522068615,
      "loss": 1.1388,
      "step": 632
    },
    {
      "epoch": 0.844,
      "grad_norm": 0.04496956616640091,
      "learning_rate": 0.0019845189529323474,
      "loss": 1.2424,
      "step": 633
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 0.05219142884016037,
      "learning_rate": 0.001984443170064112,
      "loss": 1.2956,
      "step": 634
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 0.035296279937028885,
      "learning_rate": 0.001984367203616288,
      "loss": 1.159,
      "step": 635
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.039884909987449646,
      "learning_rate": 0.00198429105360304,
      "loss": 1.286,
      "step": 636
    },
    {
      "epoch": 0.8493333333333334,
      "grad_norm": 0.034350570291280746,
      "learning_rate": 0.0019842147200385706,
      "loss": 1.1479,
      "step": 637
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 0.03566160053014755,
      "learning_rate": 0.001984138202937113,
      "loss": 1.4377,
      "step": 638
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.04362429305911064,
      "learning_rate": 0.001984061502312937,
      "loss": 1.2657,
      "step": 639
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.04142385348677635,
      "learning_rate": 0.0019839846181803457,
      "loss": 1.1858,
      "step": 640
    },
    {
      "epoch": 0.8546666666666667,
      "grad_norm": 0.03652351722121239,
      "learning_rate": 0.001983907550553676,
      "loss": 0.834,
      "step": 641
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.040226809680461884,
      "learning_rate": 0.0019838302994472997,
      "loss": 0.955,
      "step": 642
    },
    {
      "epoch": 0.8573333333333333,
      "grad_norm": 0.030205493792891502,
      "learning_rate": 0.001983752864875623,
      "loss": 1.1313,
      "step": 643
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.042249441146850586,
      "learning_rate": 0.0019836752468530856,
      "loss": 1.1725,
      "step": 644
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.07298334687948227,
      "learning_rate": 0.0019835974453941622,
      "loss": 1.2121,
      "step": 645
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.046084094792604446,
      "learning_rate": 0.00198351946051336,
      "loss": 1.3234,
      "step": 646
    },
    {
      "epoch": 0.8626666666666667,
      "grad_norm": 0.0505850613117218,
      "learning_rate": 0.0019834412922252235,
      "loss": 1.4608,
      "step": 647
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.03963499888777733,
      "learning_rate": 0.0019833629405443284,
      "loss": 1.2604,
      "step": 648
    },
    {
      "epoch": 0.8653333333333333,
      "grad_norm": 0.0329366996884346,
      "learning_rate": 0.001983284405485286,
      "loss": 0.9693,
      "step": 649
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.04918372631072998,
      "learning_rate": 0.001983205687062742,
      "loss": 1.3107,
      "step": 650
    },
    {
      "epoch": 0.868,
      "grad_norm": 0.1054145097732544,
      "learning_rate": 0.001983126785291375,
      "loss": 1.3209,
      "step": 651
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.04114145413041115,
      "learning_rate": 0.001983047700185899,
      "loss": 0.9805,
      "step": 652
    },
    {
      "epoch": 0.8706666666666667,
      "grad_norm": 0.06032363697886467,
      "learning_rate": 0.001982968431761062,
      "loss": 1.2847,
      "step": 653
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.030844679102301598,
      "learning_rate": 0.0019828889800316465,
      "loss": 1.055,
      "step": 654
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 0.04572083055973053,
      "learning_rate": 0.0019828093450124677,
      "loss": 1.459,
      "step": 655
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.05414131283760071,
      "learning_rate": 0.0019827295267183767,
      "loss": 1.3006,
      "step": 656
    },
    {
      "epoch": 0.876,
      "grad_norm": 0.03188403695821762,
      "learning_rate": 0.001982649525164258,
      "loss": 1.1935,
      "step": 657
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.047899000346660614,
      "learning_rate": 0.00198256934036503,
      "loss": 1.357,
      "step": 658
    },
    {
      "epoch": 0.8786666666666667,
      "grad_norm": 0.03193974867463112,
      "learning_rate": 0.0019824889723356457,
      "loss": 1.2908,
      "step": 659
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.05039690062403679,
      "learning_rate": 0.0019824084210910923,
      "loss": 1.2561,
      "step": 660
    },
    {
      "epoch": 0.8813333333333333,
      "grad_norm": 0.15111958980560303,
      "learning_rate": 0.001982327686646391,
      "loss": 1.4664,
      "step": 661
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.039017800241708755,
      "learning_rate": 0.0019822467690165974,
      "loss": 0.9607,
      "step": 662
    },
    {
      "epoch": 0.884,
      "grad_norm": 0.04465925693511963,
      "learning_rate": 0.001982165668216801,
      "loss": 1.3703,
      "step": 663
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.09993806481361389,
      "learning_rate": 0.001982084384262126,
      "loss": 1.3284,
      "step": 664
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 0.04884537309408188,
      "learning_rate": 0.0019820029171677286,
      "loss": 1.3242,
      "step": 665
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.034920789301395416,
      "learning_rate": 0.0019819212669488027,
      "loss": 1.2328,
      "step": 666
    },
    {
      "epoch": 0.8893333333333333,
      "grad_norm": 0.04159178584814072,
      "learning_rate": 0.001981839433620573,
      "loss": 1.3912,
      "step": 667
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.054168060421943665,
      "learning_rate": 0.0019817574171983013,
      "loss": 1.2546,
      "step": 668
    },
    {
      "epoch": 0.892,
      "grad_norm": 0.035854071378707886,
      "learning_rate": 0.0019816752176972812,
      "loss": 1.0169,
      "step": 669
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.05354318767786026,
      "learning_rate": 0.0019815928351328413,
      "loss": 1.3071,
      "step": 670
    },
    {
      "epoch": 0.8946666666666667,
      "grad_norm": 0.03891652822494507,
      "learning_rate": 0.001981510269520345,
      "loss": 1.423,
      "step": 671
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.058840181678533554,
      "learning_rate": 0.001981427520875188,
      "loss": 1.2855,
      "step": 672
    },
    {
      "epoch": 0.8973333333333333,
      "grad_norm": 0.043568678200244904,
      "learning_rate": 0.0019813445892128026,
      "loss": 1.058,
      "step": 673
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.03873732313513756,
      "learning_rate": 0.001981261474548653,
      "loss": 1.0552,
      "step": 674
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.043549519032239914,
      "learning_rate": 0.0019811781768982392,
      "loss": 1.5924,
      "step": 675
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.04407823830842972,
      "learning_rate": 0.001981094696277094,
      "loss": 1.1208,
      "step": 676
    },
    {
      "epoch": 0.9026666666666666,
      "grad_norm": 0.040066592395305634,
      "learning_rate": 0.001981011032700785,
      "loss": 1.1359,
      "step": 677
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.042482078075408936,
      "learning_rate": 0.0019809271861849147,
      "loss": 1.2618,
      "step": 678
    },
    {
      "epoch": 0.9053333333333333,
      "grad_norm": 0.04104184731841087,
      "learning_rate": 0.0019808431567451177,
      "loss": 0.9655,
      "step": 679
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.043194301426410675,
      "learning_rate": 0.001980758944397064,
      "loss": 1.0895,
      "step": 680
    },
    {
      "epoch": 0.908,
      "grad_norm": 0.04229630529880524,
      "learning_rate": 0.0019806745491564586,
      "loss": 0.9725,
      "step": 681
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.05327345058321953,
      "learning_rate": 0.0019805899710390383,
      "loss": 1.3442,
      "step": 682
    },
    {
      "epoch": 0.9106666666666666,
      "grad_norm": 0.03620363771915436,
      "learning_rate": 0.001980505210060576,
      "loss": 1.3453,
      "step": 683
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.04740983992815018,
      "learning_rate": 0.001980420266236878,
      "loss": 1.1521,
      "step": 684
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 0.03394995257258415,
      "learning_rate": 0.001980335139583785,
      "loss": 1.2801,
      "step": 685
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.03685692697763443,
      "learning_rate": 0.00198024983011717,
      "loss": 1.1639,
      "step": 686
    },
    {
      "epoch": 0.916,
      "grad_norm": 0.038006071001291275,
      "learning_rate": 0.001980164337852943,
      "loss": 1.3461,
      "step": 687
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.03821340203285217,
      "learning_rate": 0.0019800786628070464,
      "loss": 1.2299,
      "step": 688
    },
    {
      "epoch": 0.9186666666666666,
      "grad_norm": 0.028552325442433357,
      "learning_rate": 0.0019799928049954564,
      "loss": 1.1825,
      "step": 689
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.03532811999320984,
      "learning_rate": 0.0019799067644341844,
      "loss": 1.5049,
      "step": 690
    },
    {
      "epoch": 0.9213333333333333,
      "grad_norm": 0.1984838843345642,
      "learning_rate": 0.001979820541139275,
      "loss": 1.0872,
      "step": 691
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.05993962287902832,
      "learning_rate": 0.0019797341351268072,
      "loss": 1.2646,
      "step": 692
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.0827043280005455,
      "learning_rate": 0.001979647546412894,
      "loss": 1.415,
      "step": 693
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.03462410345673561,
      "learning_rate": 0.001979560775013683,
      "loss": 1.1991,
      "step": 694
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 0.04694166034460068,
      "learning_rate": 0.0019794738209453545,
      "loss": 1.0894,
      "step": 695
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.10221926867961884,
      "learning_rate": 0.0019793866842241245,
      "loss": 1.3328,
      "step": 696
    },
    {
      "epoch": 0.9293333333333333,
      "grad_norm": 0.040791817009449005,
      "learning_rate": 0.0019792993648662417,
      "loss": 1.0221,
      "step": 697
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 0.04611221328377724,
      "learning_rate": 0.0019792118628879905,
      "loss": 1.0564,
      "step": 698
    },
    {
      "epoch": 0.932,
      "grad_norm": 0.0477466955780983,
      "learning_rate": 0.001979124178305687,
      "loss": 1.1124,
      "step": 699
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.04596417024731636,
      "learning_rate": 0.0019790363111356836,
      "loss": 1.2323,
      "step": 700
    },
    {
      "epoch": 0.9346666666666666,
      "grad_norm": 0.029860856011509895,
      "learning_rate": 0.0019789482613943657,
      "loss": 0.8308,
      "step": 701
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.039923761039972305,
      "learning_rate": 0.0019788600290981525,
      "loss": 1.1521,
      "step": 702
    },
    {
      "epoch": 0.9373333333333334,
      "grad_norm": 0.04907752200961113,
      "learning_rate": 0.001978771614263498,
      "loss": 0.9647,
      "step": 703
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.036198344081640244,
      "learning_rate": 0.0019786830169068893,
      "loss": 1.5854,
      "step": 704
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.051110878586769104,
      "learning_rate": 0.0019785942370448488,
      "loss": 1.2361,
      "step": 705
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.050387296825647354,
      "learning_rate": 0.001978505274693932,
      "loss": 1.2785,
      "step": 706
    },
    {
      "epoch": 0.9426666666666667,
      "grad_norm": 0.03628532588481903,
      "learning_rate": 0.001978416129870728,
      "loss": 1.0033,
      "step": 707
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.04510756582021713,
      "learning_rate": 0.0019783268025918618,
      "loss": 1.1307,
      "step": 708
    },
    {
      "epoch": 0.9453333333333334,
      "grad_norm": 0.05902193859219551,
      "learning_rate": 0.0019782372928739906,
      "loss": 1.4857,
      "step": 709
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.02775685489177704,
      "learning_rate": 0.0019781476007338056,
      "loss": 1.1896,
      "step": 710
    },
    {
      "epoch": 0.948,
      "grad_norm": 0.049294859170913696,
      "learning_rate": 0.0019780577261880334,
      "loss": 1.1861,
      "step": 711
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.04747086763381958,
      "learning_rate": 0.001977967669253434,
      "loss": 1.446,
      "step": 712
    },
    {
      "epoch": 0.9506666666666667,
      "grad_norm": 0.040599673986434937,
      "learning_rate": 0.0019778774299468007,
      "loss": 1.3227,
      "step": 713
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.061495207250118256,
      "learning_rate": 0.0019777870082849616,
      "loss": 1.276,
      "step": 714
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 0.04132350534200668,
      "learning_rate": 0.001977696404284779,
      "loss": 1.0075,
      "step": 715
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.0538661926984787,
      "learning_rate": 0.0019776056179631484,
      "loss": 1.2371,
      "step": 716
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.04239802062511444,
      "learning_rate": 0.001977514649336999,
      "loss": 1.3505,
      "step": 717
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.03769887238740921,
      "learning_rate": 0.0019774234984232964,
      "loss": 1.1395,
      "step": 718
    },
    {
      "epoch": 0.9586666666666667,
      "grad_norm": 0.05681364983320236,
      "learning_rate": 0.001977332165239037,
      "loss": 1.5656,
      "step": 719
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.05129548907279968,
      "learning_rate": 0.001977240649801253,
      "loss": 1.6277,
      "step": 720
    },
    {
      "epoch": 0.9613333333333334,
      "grad_norm": 0.2924656569957733,
      "learning_rate": 0.0019771489521270107,
      "loss": 1.407,
      "step": 721
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 0.0376729890704155,
      "learning_rate": 0.0019770570722334093,
      "loss": 1.4534,
      "step": 722
    },
    {
      "epoch": 0.964,
      "grad_norm": 0.05404624715447426,
      "learning_rate": 0.0019769650101375837,
      "loss": 0.9892,
      "step": 723
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 0.16851240396499634,
      "learning_rate": 0.0019768727658567003,
      "loss": 1.3257,
      "step": 724
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.06689499318599701,
      "learning_rate": 0.0019767803394079614,
      "loss": 1.1741,
      "step": 725
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.060719843953847885,
      "learning_rate": 0.0019766877308086037,
      "loss": 1.0683,
      "step": 726
    },
    {
      "epoch": 0.9693333333333334,
      "grad_norm": 0.048348911106586456,
      "learning_rate": 0.0019765949400758955,
      "loss": 1.225,
      "step": 727
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.17444469034671783,
      "learning_rate": 0.0019765019672271416,
      "loss": 1.3784,
      "step": 728
    },
    {
      "epoch": 0.972,
      "grad_norm": 0.04657571762800217,
      "learning_rate": 0.0019764088122796782,
      "loss": 1.2849,
      "step": 729
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.05217059329152107,
      "learning_rate": 0.0019763154752508783,
      "loss": 1.2152,
      "step": 730
    },
    {
      "epoch": 0.9746666666666667,
      "grad_norm": 0.09551148116588593,
      "learning_rate": 0.0019762219561581472,
      "loss": 0.9818,
      "step": 731
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.042320579290390015,
      "learning_rate": 0.001976128255018924,
      "loss": 1.5143,
      "step": 732
    },
    {
      "epoch": 0.9773333333333334,
      "grad_norm": 0.043850623071193695,
      "learning_rate": 0.001976034371850682,
      "loss": 1.0459,
      "step": 733
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.06457269191741943,
      "learning_rate": 0.0019759403066709294,
      "loss": 1.1412,
      "step": 734
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.05029945820569992,
      "learning_rate": 0.001975846059497207,
      "loss": 1.2979,
      "step": 735
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.036278240382671356,
      "learning_rate": 0.0019757516303470896,
      "loss": 1.3369,
      "step": 736
    },
    {
      "epoch": 0.9826666666666667,
      "grad_norm": 0.050299856811761856,
      "learning_rate": 0.0019756570192381875,
      "loss": 1.1386,
      "step": 737
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.057707980275154114,
      "learning_rate": 0.0019755622261881426,
      "loss": 1.0933,
      "step": 738
    },
    {
      "epoch": 0.9853333333333333,
      "grad_norm": 0.05284956470131874,
      "learning_rate": 0.001975467251214633,
      "loss": 1.2255,
      "step": 739
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.030747197568416595,
      "learning_rate": 0.0019753720943353694,
      "loss": 1.1293,
      "step": 740
    },
    {
      "epoch": 0.988,
      "grad_norm": 0.038681916892528534,
      "learning_rate": 0.0019752767555680966,
      "loss": 1.2917,
      "step": 741
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 0.03715824335813522,
      "learning_rate": 0.001975181234930593,
      "loss": 1.1178,
      "step": 742
    },
    {
      "epoch": 0.9906666666666667,
      "grad_norm": 0.04513728618621826,
      "learning_rate": 0.0019750855324406724,
      "loss": 1.1234,
      "step": 743
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.05069280043244362,
      "learning_rate": 0.001974989648116181,
      "loss": 1.4125,
      "step": 744
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 0.03868097439408302,
      "learning_rate": 0.0019748935819749987,
      "loss": 1.2644,
      "step": 745
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 0.05064034461975098,
      "learning_rate": 0.001974797334035041,
      "loss": 1.2755,
      "step": 746
    },
    {
      "epoch": 0.996,
      "grad_norm": 0.052486125379800797,
      "learning_rate": 0.0019747009043142552,
      "loss": 1.2067,
      "step": 747
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.0350789949297905,
      "learning_rate": 0.0019746042928306244,
      "loss": 1.1133,
      "step": 748
    },
    {
      "epoch": 0.9986666666666667,
      "grad_norm": 0.061461661010980606,
      "learning_rate": 0.0019745074996021647,
      "loss": 1.1749,
      "step": 749
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.04764922335743904,
      "learning_rate": 0.001974410524646926,
      "loss": 1.0886,
      "step": 750
    },
    {
      "epoch": 1.0013333333333334,
      "grad_norm": 0.05492081493139267,
      "learning_rate": 0.0019743133679829923,
      "loss": 1.1155,
      "step": 751
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 0.028692753985524178,
      "learning_rate": 0.001974216029628481,
      "loss": 1.0918,
      "step": 752
    },
    {
      "epoch": 1.004,
      "grad_norm": 0.045952919870615005,
      "learning_rate": 0.001974118509601545,
      "loss": 1.3218,
      "step": 753
    },
    {
      "epoch": 1.0053333333333334,
      "grad_norm": 0.03732681646943092,
      "learning_rate": 0.001974020807920368,
      "loss": 1.2281,
      "step": 754
    },
    {
      "epoch": 1.0066666666666666,
      "grad_norm": 0.039693862199783325,
      "learning_rate": 0.0019739229246031717,
      "loss": 0.9007,
      "step": 755
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.03235549479722977,
      "learning_rate": 0.0019738248596682076,
      "loss": 0.9669,
      "step": 756
    },
    {
      "epoch": 1.0093333333333334,
      "grad_norm": 0.028382059186697006,
      "learning_rate": 0.0019737266131337637,
      "loss": 1.3491,
      "step": 757
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.04543567821383476,
      "learning_rate": 0.0019736281850181612,
      "loss": 1.3708,
      "step": 758
    },
    {
      "epoch": 1.012,
      "grad_norm": 0.035272035747766495,
      "learning_rate": 0.001973529575339755,
      "loss": 1.0629,
      "step": 759
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.032112959772348404,
      "learning_rate": 0.0019734307841169337,
      "loss": 1.1489,
      "step": 760
    },
    {
      "epoch": 1.0146666666666666,
      "grad_norm": 0.04895009100437164,
      "learning_rate": 0.00197333181136812,
      "loss": 1.1928,
      "step": 761
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.033490847796201706,
      "learning_rate": 0.00197323265711177,
      "loss": 1.2498,
      "step": 762
    },
    {
      "epoch": 1.0173333333333334,
      "grad_norm": 0.03367787227034569,
      "learning_rate": 0.0019731333213663747,
      "loss": 1.2191,
      "step": 763
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 0.030910920351743698,
      "learning_rate": 0.0019730338041504584,
      "loss": 1.2605,
      "step": 764
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.055230170488357544,
      "learning_rate": 0.0019729341054825784,
      "loss": 1.3101,
      "step": 765
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.04118988662958145,
      "learning_rate": 0.0019728342253813266,
      "loss": 1.0732,
      "step": 766
    },
    {
      "epoch": 1.0226666666666666,
      "grad_norm": 0.0421115942299366,
      "learning_rate": 0.0019727341638653296,
      "loss": 1.4456,
      "step": 767
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.044679395854473114,
      "learning_rate": 0.001972633920953246,
      "loss": 1.351,
      "step": 768
    },
    {
      "epoch": 1.0253333333333334,
      "grad_norm": 0.04334598407149315,
      "learning_rate": 0.00197253349666377,
      "loss": 1.3485,
      "step": 769
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.0557689443230629,
      "learning_rate": 0.001972432891015628,
      "loss": 1.1754,
      "step": 770
    },
    {
      "epoch": 1.028,
      "grad_norm": 0.054303061217069626,
      "learning_rate": 0.0019723321040275815,
      "loss": 1.1276,
      "step": 771
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 0.04731139540672302,
      "learning_rate": 0.001972231135718425,
      "loss": 1.0513,
      "step": 772
    },
    {
      "epoch": 1.0306666666666666,
      "grad_norm": 0.03875308111310005,
      "learning_rate": 0.0019721299861069873,
      "loss": 1.301,
      "step": 773
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.03325248509645462,
      "learning_rate": 0.001972028655212131,
      "loss": 1.0212,
      "step": 774
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.19474375247955322,
      "learning_rate": 0.001971927143052752,
      "loss": 1.3438,
      "step": 775
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 0.04760857671499252,
      "learning_rate": 0.0019718254496477803,
      "loss": 1.2521,
      "step": 776
    },
    {
      "epoch": 1.036,
      "grad_norm": 0.04879898205399513,
      "learning_rate": 0.001971723575016181,
      "loss": 1.0142,
      "step": 777
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 0.04724428802728653,
      "learning_rate": 0.00197162151917695,
      "loss": 1.3684,
      "step": 778
    },
    {
      "epoch": 1.0386666666666666,
      "grad_norm": 0.03401247039437294,
      "learning_rate": 0.001971519282149119,
      "loss": 1.1758,
      "step": 779
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.08395465463399887,
      "learning_rate": 0.001971416863951754,
      "loss": 1.4199,
      "step": 780
    },
    {
      "epoch": 1.0413333333333332,
      "grad_norm": 0.06653392314910889,
      "learning_rate": 0.0019713142646039543,
      "loss": 1.147,
      "step": 781
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.043024055659770966,
      "learning_rate": 0.001971211484124852,
      "loss": 1.143,
      "step": 782
    },
    {
      "epoch": 1.044,
      "grad_norm": 0.12867620587348938,
      "learning_rate": 0.0019711085225336132,
      "loss": 1.3113,
      "step": 783
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 0.05742860957980156,
      "learning_rate": 0.001971005379849439,
      "loss": 1.1678,
      "step": 784
    },
    {
      "epoch": 1.0466666666666666,
      "grad_norm": 0.043350595980882645,
      "learning_rate": 0.0019709020560915638,
      "loss": 1.4979,
      "step": 785
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.06436274945735931,
      "learning_rate": 0.001970798551279254,
      "loss": 1.2417,
      "step": 786
    },
    {
      "epoch": 1.0493333333333332,
      "grad_norm": 0.0517854206264019,
      "learning_rate": 0.0019706948654318133,
      "loss": 1.0394,
      "step": 787
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 0.0455281138420105,
      "learning_rate": 0.0019705909985685754,
      "loss": 1.17,
      "step": 788
    },
    {
      "epoch": 1.052,
      "grad_norm": 0.03233114257454872,
      "learning_rate": 0.0019704869507089105,
      "loss": 1.2047,
      "step": 789
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.11199016124010086,
      "learning_rate": 0.001970382721872221,
      "loss": 1.0844,
      "step": 790
    },
    {
      "epoch": 1.0546666666666666,
      "grad_norm": 0.046638764441013336,
      "learning_rate": 0.0019702783120779436,
      "loss": 1.3331,
      "step": 791
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.06628764420747757,
      "learning_rate": 0.0019701737213455492,
      "loss": 1.1167,
      "step": 792
    },
    {
      "epoch": 1.0573333333333332,
      "grad_norm": 0.04928062856197357,
      "learning_rate": 0.0019700689496945406,
      "loss": 1.2975,
      "step": 793
    },
    {
      "epoch": 1.0586666666666666,
      "grad_norm": 0.03600689396262169,
      "learning_rate": 0.0019699639971444576,
      "loss": 1.2342,
      "step": 794
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.044804465025663376,
      "learning_rate": 0.00196985886371487,
      "loss": 1.2783,
      "step": 795
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 0.046508245170116425,
      "learning_rate": 0.001969753549425385,
      "loss": 1.1335,
      "step": 796
    },
    {
      "epoch": 1.0626666666666666,
      "grad_norm": 0.04667240008711815,
      "learning_rate": 0.0019696480542956397,
      "loss": 1.0865,
      "step": 797
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.06626936048269272,
      "learning_rate": 0.0019695423783453085,
      "loss": 1.221,
      "step": 798
    },
    {
      "epoch": 1.0653333333333332,
      "grad_norm": 0.037824466824531555,
      "learning_rate": 0.0019694365215940967,
      "loss": 1.3444,
      "step": 799
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.08075495809316635,
      "learning_rate": 0.0019693304840617456,
      "loss": 1.5071,
      "step": 800
    },
    {
      "epoch": 1.068,
      "grad_norm": 0.041821498423814774,
      "learning_rate": 0.0019692242657680286,
      "loss": 1.2076,
      "step": 801
    },
    {
      "epoch": 1.0693333333333332,
      "grad_norm": 0.042703695595264435,
      "learning_rate": 0.0019691178667327535,
      "loss": 1.2563,
      "step": 802
    },
    {
      "epoch": 1.0706666666666667,
      "grad_norm": 0.04531577229499817,
      "learning_rate": 0.001969011286975761,
      "loss": 1.3219,
      "step": 803
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.05538894236087799,
      "learning_rate": 0.0019689045265169273,
      "loss": 1.1357,
      "step": 804
    },
    {
      "epoch": 1.0733333333333333,
      "grad_norm": 0.033032774925231934,
      "learning_rate": 0.0019687975853761603,
      "loss": 1.2057,
      "step": 805
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.03953256085515022,
      "learning_rate": 0.0019686904635734027,
      "loss": 1.1293,
      "step": 806
    },
    {
      "epoch": 1.076,
      "grad_norm": 0.03795613721013069,
      "learning_rate": 0.001968583161128631,
      "loss": 1.1765,
      "step": 807
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 0.04781962186098099,
      "learning_rate": 0.001968475678061855,
      "loss": 1.1039,
      "step": 808
    },
    {
      "epoch": 1.0786666666666667,
      "grad_norm": 0.04316357895731926,
      "learning_rate": 0.001968368014393117,
      "loss": 1.2861,
      "step": 809
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.029855255037546158,
      "learning_rate": 0.001968260170142496,
      "loss": 1.2248,
      "step": 810
    },
    {
      "epoch": 1.0813333333333333,
      "grad_norm": 0.033655691891908646,
      "learning_rate": 0.0019681521453301016,
      "loss": 1.2381,
      "step": 811
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 0.035517774522304535,
      "learning_rate": 0.0019680439399760784,
      "loss": 1.0373,
      "step": 812
    },
    {
      "epoch": 1.084,
      "grad_norm": 0.04572390392422676,
      "learning_rate": 0.0019679355541006053,
      "loss": 0.9502,
      "step": 813
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.04740644246339798,
      "learning_rate": 0.0019678269877238938,
      "loss": 0.9651,
      "step": 814
    },
    {
      "epoch": 1.0866666666666667,
      "grad_norm": 0.040754277259111404,
      "learning_rate": 0.0019677182408661892,
      "loss": 1.3105,
      "step": 815
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.034917715936899185,
      "learning_rate": 0.0019676093135477715,
      "loss": 1.3288,
      "step": 816
    },
    {
      "epoch": 1.0893333333333333,
      "grad_norm": 0.03089931793510914,
      "learning_rate": 0.0019675002057889523,
      "loss": 1.3395,
      "step": 817
    },
    {
      "epoch": 1.0906666666666667,
      "grad_norm": 0.031500186771154404,
      "learning_rate": 0.0019673909176100785,
      "loss": 1.0815,
      "step": 818
    },
    {
      "epoch": 1.092,
      "grad_norm": 0.041324492543935776,
      "learning_rate": 0.001967281449031531,
      "loss": 1.3037,
      "step": 819
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.03816927224397659,
      "learning_rate": 0.0019671718000737227,
      "loss": 1.4001,
      "step": 820
    },
    {
      "epoch": 1.0946666666666667,
      "grad_norm": 0.0363711453974247,
      "learning_rate": 0.0019670619707571014,
      "loss": 1.304,
      "step": 821
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.03685825318098068,
      "learning_rate": 0.0019669519611021486,
      "loss": 1.2014,
      "step": 822
    },
    {
      "epoch": 1.0973333333333333,
      "grad_norm": 0.05055256560444832,
      "learning_rate": 0.001966841771129378,
      "loss": 1.1765,
      "step": 823
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 0.062012407928705215,
      "learning_rate": 0.001966731400859338,
      "loss": 1.0428,
      "step": 824
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.04109656810760498,
      "learning_rate": 0.001966620850312611,
      "loss": 1.3648,
      "step": 825
    },
    {
      "epoch": 1.1013333333333333,
      "grad_norm": 0.08261837810277939,
      "learning_rate": 0.001966510119509813,
      "loss": 1.3917,
      "step": 826
    },
    {
      "epoch": 1.1026666666666667,
      "grad_norm": 0.04505375772714615,
      "learning_rate": 0.0019663992084715917,
      "loss": 1.5472,
      "step": 827
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.03917964547872543,
      "learning_rate": 0.001966288117218631,
      "loss": 1.1403,
      "step": 828
    },
    {
      "epoch": 1.1053333333333333,
      "grad_norm": 0.054943814873695374,
      "learning_rate": 0.001966176845771647,
      "loss": 1.3214,
      "step": 829
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.03743620216846466,
      "learning_rate": 0.00196606539415139,
      "loss": 1.124,
      "step": 830
    },
    {
      "epoch": 1.108,
      "grad_norm": 0.03882941976189613,
      "learning_rate": 0.0019659537623786427,
      "loss": 1.0724,
      "step": 831
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 0.0449087917804718,
      "learning_rate": 0.001965841950474223,
      "loss": 1.1441,
      "step": 832
    },
    {
      "epoch": 1.1106666666666667,
      "grad_norm": 0.03611493483185768,
      "learning_rate": 0.0019657299584589817,
      "loss": 1.2189,
      "step": 833
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.21207331120967865,
      "learning_rate": 0.0019656177863538026,
      "loss": 1.076,
      "step": 834
    },
    {
      "epoch": 1.1133333333333333,
      "grad_norm": 0.0416511632502079,
      "learning_rate": 0.0019655054341796035,
      "loss": 1.3412,
      "step": 835
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 0.046583060175180435,
      "learning_rate": 0.0019653929019573368,
      "loss": 1.3777,
      "step": 836
    },
    {
      "epoch": 1.116,
      "grad_norm": 0.03714112937450409,
      "learning_rate": 0.001965280189707987,
      "loss": 1.0972,
      "step": 837
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 0.04214714094996452,
      "learning_rate": 0.0019651672974525724,
      "loss": 1.0115,
      "step": 838
    },
    {
      "epoch": 1.1186666666666667,
      "grad_norm": 0.04557332769036293,
      "learning_rate": 0.001965054225212146,
      "loss": 1.1312,
      "step": 839
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.05759608373045921,
      "learning_rate": 0.0019649409730077933,
      "loss": 1.1488,
      "step": 840
    },
    {
      "epoch": 1.1213333333333333,
      "grad_norm": 0.040716059505939484,
      "learning_rate": 0.0019648275408606338,
      "loss": 0.9685,
      "step": 841
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 0.15011951327323914,
      "learning_rate": 0.00196471392879182,
      "loss": 1.3555,
      "step": 842
    },
    {
      "epoch": 1.124,
      "grad_norm": 0.029683878645300865,
      "learning_rate": 0.001964600136822538,
      "loss": 1.1814,
      "step": 843
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 0.033196765929460526,
      "learning_rate": 0.0019644861649740085,
      "loss": 1.3367,
      "step": 844
    },
    {
      "epoch": 1.1266666666666667,
      "grad_norm": 0.04839568957686424,
      "learning_rate": 0.0019643720132674855,
      "loss": 1.2071,
      "step": 845
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.04349754750728607,
      "learning_rate": 0.001964257681724255,
      "loss": 1.0548,
      "step": 846
    },
    {
      "epoch": 1.1293333333333333,
      "grad_norm": 0.038985058665275574,
      "learning_rate": 0.001964143170365638,
      "loss": 1.1327,
      "step": 847
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 0.04016222059726715,
      "learning_rate": 0.0019640284792129888,
      "loss": 1.2203,
      "step": 848
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 0.0388663075864315,
      "learning_rate": 0.0019639136082876952,
      "loss": 1.0207,
      "step": 849
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.05538376420736313,
      "learning_rate": 0.001963798557611178,
      "loss": 1.2075,
      "step": 850
    },
    {
      "epoch": 1.1346666666666667,
      "grad_norm": 0.05111375451087952,
      "learning_rate": 0.001963683327204892,
      "loss": 1.2315,
      "step": 851
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.07650972157716751,
      "learning_rate": 0.001963567917090326,
      "loss": 1.5605,
      "step": 852
    },
    {
      "epoch": 1.1373333333333333,
      "grad_norm": 0.034140679985284805,
      "learning_rate": 0.001963452327289001,
      "loss": 0.9406,
      "step": 853
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.06674176454544067,
      "learning_rate": 0.0019633365578224727,
      "loss": 1.097,
      "step": 854
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.043116141110658646,
      "learning_rate": 0.0019632206087123296,
      "loss": 1.2446,
      "step": 855
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 0.042417071759700775,
      "learning_rate": 0.0019631044799801943,
      "loss": 1.3021,
      "step": 856
    },
    {
      "epoch": 1.1426666666666667,
      "grad_norm": 0.29406219720840454,
      "learning_rate": 0.0019629881716477222,
      "loss": 1.6659,
      "step": 857
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.040381766855716705,
      "learning_rate": 0.0019628716837366026,
      "loss": 1.0983,
      "step": 858
    },
    {
      "epoch": 1.1453333333333333,
      "grad_norm": 0.0932982787489891,
      "learning_rate": 0.0019627550162685586,
      "loss": 1.1384,
      "step": 859
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.1289646178483963,
      "learning_rate": 0.001962638169265346,
      "loss": 1.2801,
      "step": 860
    },
    {
      "epoch": 1.148,
      "grad_norm": 0.06896897405385971,
      "learning_rate": 0.0019625211427487547,
      "loss": 1.2081,
      "step": 861
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 0.04520745202898979,
      "learning_rate": 0.001962403936740608,
      "loss": 1.3,
      "step": 862
    },
    {
      "epoch": 1.1506666666666667,
      "grad_norm": 0.057265505194664,
      "learning_rate": 0.001962286551262762,
      "loss": 1.1457,
      "step": 863
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.06256968528032303,
      "learning_rate": 0.001962168986337108,
      "loss": 1.2759,
      "step": 864
    },
    {
      "epoch": 1.1533333333333333,
      "grad_norm": 0.23656372725963593,
      "learning_rate": 0.001962051241985568,
      "loss": 1.2796,
      "step": 865
    },
    {
      "epoch": 1.1546666666666667,
      "grad_norm": 0.03691737353801727,
      "learning_rate": 0.0019619333182301006,
      "loss": 1.1457,
      "step": 866
    },
    {
      "epoch": 1.156,
      "grad_norm": 0.04464427009224892,
      "learning_rate": 0.0019618152150926954,
      "loss": 1.2173,
      "step": 867
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 0.2768774926662445,
      "learning_rate": 0.0019616969325953765,
      "loss": 1.1512,
      "step": 868
    },
    {
      "epoch": 1.1586666666666667,
      "grad_norm": 0.12546859681606293,
      "learning_rate": 0.001961578470760201,
      "loss": 1.3206,
      "step": 869
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.04840635880827904,
      "learning_rate": 0.0019614598296092602,
      "loss": 1.0681,
      "step": 870
    },
    {
      "epoch": 1.1613333333333333,
      "grad_norm": 0.03620665520429611,
      "learning_rate": 0.001961341009164678,
      "loss": 0.9532,
      "step": 871
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 0.050271548330783844,
      "learning_rate": 0.0019612220094486123,
      "loss": 1.1742,
      "step": 872
    },
    {
      "epoch": 1.164,
      "grad_norm": 0.05934023857116699,
      "learning_rate": 0.0019611028304832544,
      "loss": 1.145,
      "step": 873
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 0.11515268683433533,
      "learning_rate": 0.001960983472290829,
      "loss": 1.4425,
      "step": 874
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.07152783125638962,
      "learning_rate": 0.0019608639348935937,
      "loss": 1.2687,
      "step": 875
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.041697029024362564,
      "learning_rate": 0.00196074421831384,
      "loss": 1.3197,
      "step": 876
    },
    {
      "epoch": 1.1693333333333333,
      "grad_norm": 0.03429299220442772,
      "learning_rate": 0.0019606243225738927,
      "loss": 1.1547,
      "step": 877
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.044424984604120255,
      "learning_rate": 0.00196050424769611,
      "loss": 1.1934,
      "step": 878
    },
    {
      "epoch": 1.172,
      "grad_norm": 0.37057796120643616,
      "learning_rate": 0.001960383993702884,
      "loss": 1.076,
      "step": 879
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.037166111171245575,
      "learning_rate": 0.001960263560616639,
      "loss": 1.2215,
      "step": 880
    },
    {
      "epoch": 1.1746666666666667,
      "grad_norm": 0.11643952131271362,
      "learning_rate": 0.001960142948459834,
      "loss": 1.2489,
      "step": 881
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.338459849357605,
      "learning_rate": 0.0019600221572549604,
      "loss": 1.1654,
      "step": 882
    },
    {
      "epoch": 1.1773333333333333,
      "grad_norm": 0.04853470250964165,
      "learning_rate": 0.0019599011870245443,
      "loss": 1.1954,
      "step": 883
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 0.04907139390707016,
      "learning_rate": 0.001959780037791143,
      "loss": 1.3119,
      "step": 884
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.041177451610565186,
      "learning_rate": 0.0019596587095773495,
      "loss": 1.1804,
      "step": 885
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 0.07074406743049622,
      "learning_rate": 0.0019595372024057887,
      "loss": 1.1682,
      "step": 886
    },
    {
      "epoch": 1.1826666666666668,
      "grad_norm": 0.21728043258190155,
      "learning_rate": 0.0019594155162991193,
      "loss": 1.0897,
      "step": 887
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.20473513007164001,
      "learning_rate": 0.001959293651280034,
      "loss": 1.1523,
      "step": 888
    },
    {
      "epoch": 1.1853333333333333,
      "grad_norm": 0.07304471731185913,
      "learning_rate": 0.0019591716073712575,
      "loss": 1.0748,
      "step": 889
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.22039680182933807,
      "learning_rate": 0.001959049384595549,
      "loss": 1.0558,
      "step": 890
    },
    {
      "epoch": 1.188,
      "grad_norm": 0.040471937507390976,
      "learning_rate": 0.001958926982975701,
      "loss": 1.1499,
      "step": 891
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 0.0521916039288044,
      "learning_rate": 0.001958804402534538,
      "loss": 1.1655,
      "step": 892
    },
    {
      "epoch": 1.1906666666666668,
      "grad_norm": 0.07169001549482346,
      "learning_rate": 0.00195868164329492,
      "loss": 1.2261,
      "step": 893
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.04304784536361694,
      "learning_rate": 0.0019585587052797387,
      "loss": 1.174,
      "step": 894
    },
    {
      "epoch": 1.1933333333333334,
      "grad_norm": 0.034983307123184204,
      "learning_rate": 0.0019584355885119194,
      "loss": 1.2627,
      "step": 895
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 0.04538155719637871,
      "learning_rate": 0.001958312293014422,
      "loss": 0.977,
      "step": 896
    },
    {
      "epoch": 1.196,
      "grad_norm": 0.038281772285699844,
      "learning_rate": 0.0019581888188102375,
      "loss": 1.1638,
      "step": 897
    },
    {
      "epoch": 1.1973333333333334,
      "grad_norm": 0.04049857705831528,
      "learning_rate": 0.0019580651659223923,
      "loss": 1.1048,
      "step": 898
    },
    {
      "epoch": 1.1986666666666665,
      "grad_norm": 0.05089913681149483,
      "learning_rate": 0.0019579413343739447,
      "loss": 1.1091,
      "step": 899
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.032131582498550415,
      "learning_rate": 0.001957817324187987,
      "loss": 1.1479,
      "step": 900
    },
    {
      "epoch": 1.2013333333333334,
      "grad_norm": 0.04219766706228256,
      "learning_rate": 0.0019576931353876455,
      "loss": 1.0884,
      "step": 901
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.034969180822372437,
      "learning_rate": 0.0019575687679960776,
      "loss": 1.3627,
      "step": 902
    },
    {
      "epoch": 1.204,
      "grad_norm": 0.032337967306375504,
      "learning_rate": 0.0019574442220364765,
      "loss": 0.9053,
      "step": 903
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 0.03637808933854103,
      "learning_rate": 0.001957319497532067,
      "loss": 1.3538,
      "step": 904
    },
    {
      "epoch": 1.2066666666666666,
      "grad_norm": 0.039203744381666183,
      "learning_rate": 0.0019571945945061086,
      "loss": 1.2707,
      "step": 905
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.036725353449583054,
      "learning_rate": 0.0019570695129818927,
      "loss": 1.07,
      "step": 906
    },
    {
      "epoch": 1.2093333333333334,
      "grad_norm": 0.03589681163430214,
      "learning_rate": 0.0019569442529827445,
      "loss": 1.0078,
      "step": 907
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 0.052406907081604004,
      "learning_rate": 0.0019568188145320225,
      "loss": 1.4642,
      "step": 908
    },
    {
      "epoch": 1.212,
      "grad_norm": 0.030685564503073692,
      "learning_rate": 0.001956693197653119,
      "loss": 1.1603,
      "step": 909
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.08401060104370117,
      "learning_rate": 0.0019565674023694587,
      "loss": 1.2702,
      "step": 910
    },
    {
      "epoch": 1.2146666666666666,
      "grad_norm": 0.0396094024181366,
      "learning_rate": 0.0019564414287045005,
      "loss": 1.0741,
      "step": 911
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.053863316774368286,
      "learning_rate": 0.0019563152766817354,
      "loss": 0.7975,
      "step": 912
    },
    {
      "epoch": 1.2173333333333334,
      "grad_norm": 0.043405681848526,
      "learning_rate": 0.0019561889463246885,
      "loss": 1.1493,
      "step": 913
    },
    {
      "epoch": 1.2186666666666666,
      "grad_norm": 0.02939494140446186,
      "learning_rate": 0.0019560624376569187,
      "loss": 1.1243,
      "step": 914
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.043171703815460205,
      "learning_rate": 0.001955935750702016,
      "loss": 1.1387,
      "step": 915
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 0.03859351575374603,
      "learning_rate": 0.0019558088854836064,
      "loss": 0.8999,
      "step": 916
    },
    {
      "epoch": 1.2226666666666666,
      "grad_norm": 0.04007256403565407,
      "learning_rate": 0.001955681842025347,
      "loss": 1.2114,
      "step": 917
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.030193137004971504,
      "learning_rate": 0.0019555546203509295,
      "loss": 1.3066,
      "step": 918
    },
    {
      "epoch": 1.2253333333333334,
      "grad_norm": 0.043922893702983856,
      "learning_rate": 0.0019554272204840776,
      "loss": 0.9868,
      "step": 919
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.028725994750857353,
      "learning_rate": 0.00195529964244855,
      "loss": 1.2071,
      "step": 920
    },
    {
      "epoch": 1.228,
      "grad_norm": 0.037407130002975464,
      "learning_rate": 0.001955171886268136,
      "loss": 1.2809,
      "step": 921
    },
    {
      "epoch": 1.2293333333333334,
      "grad_norm": 0.03820015862584114,
      "learning_rate": 0.001955043951966661,
      "loss": 1.3424,
      "step": 922
    },
    {
      "epoch": 1.2306666666666666,
      "grad_norm": 0.026921585202217102,
      "learning_rate": 0.0019549158395679818,
      "loss": 0.9847,
      "step": 923
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.06583460420370102,
      "learning_rate": 0.0019547875490959882,
      "loss": 1.3747,
      "step": 924
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 0.032380588352680206,
      "learning_rate": 0.0019546590805746052,
      "loss": 0.9501,
      "step": 925
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.058085694909095764,
      "learning_rate": 0.001954530434027789,
      "loss": 1.2671,
      "step": 926
    },
    {
      "epoch": 1.236,
      "grad_norm": 0.025331571698188782,
      "learning_rate": 0.0019544016094795295,
      "loss": 0.9797,
      "step": 927
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 0.034199487417936325,
      "learning_rate": 0.0019542726069538504,
      "loss": 1.1158,
      "step": 928
    },
    {
      "epoch": 1.2386666666666666,
      "grad_norm": 0.03000166453421116,
      "learning_rate": 0.0019541434264748075,
      "loss": 1.1205,
      "step": 929
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.09325883537530899,
      "learning_rate": 0.0019540140680664913,
      "loss": 1.4931,
      "step": 930
    },
    {
      "epoch": 1.2413333333333334,
      "grad_norm": 0.035580094903707504,
      "learning_rate": 0.0019538845317530243,
      "loss": 0.9834,
      "step": 931
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 0.032512106001377106,
      "learning_rate": 0.0019537548175585623,
      "loss": 1.1614,
      "step": 932
    },
    {
      "epoch": 1.244,
      "grad_norm": 0.030432922765612602,
      "learning_rate": 0.0019536249255072947,
      "loss": 1.3405,
      "step": 933
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 0.05341111123561859,
      "learning_rate": 0.001953494855623444,
      "loss": 1.1252,
      "step": 934
    },
    {
      "epoch": 1.2466666666666666,
      "grad_norm": 0.058438364416360855,
      "learning_rate": 0.0019533646079312656,
      "loss": 1.1084,
      "step": 935
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.08664575219154358,
      "learning_rate": 0.001953234182455048,
      "loss": 1.4712,
      "step": 936
    },
    {
      "epoch": 1.2493333333333334,
      "grad_norm": 0.03525920212268829,
      "learning_rate": 0.0019531035792191126,
      "loss": 1.2232,
      "step": 937
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 0.059603240340948105,
      "learning_rate": 0.0019529727982478154,
      "loss": 1.0196,
      "step": 938
    },
    {
      "epoch": 1.252,
      "grad_norm": 0.043205082416534424,
      "learning_rate": 0.0019528418395655441,
      "loss": 1.1563,
      "step": 939
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.040585629642009735,
      "learning_rate": 0.0019527107031967197,
      "loss": 1.179,
      "step": 940
    },
    {
      "epoch": 1.2546666666666666,
      "grad_norm": 0.07096292078495026,
      "learning_rate": 0.001952579389165797,
      "loss": 1.2439,
      "step": 941
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.29361552000045776,
      "learning_rate": 0.001952447897497263,
      "loss": 1.3428,
      "step": 942
    },
    {
      "epoch": 1.2573333333333334,
      "grad_norm": 0.04388132318854332,
      "learning_rate": 0.0019523162282156388,
      "loss": 1.4457,
      "step": 943
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 0.06553230434656143,
      "learning_rate": 0.001952184381345478,
      "loss": 1.2623,
      "step": 944
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.0411115437746048,
      "learning_rate": 0.0019520523569113678,
      "loss": 1.3557,
      "step": 945
    },
    {
      "epoch": 1.2613333333333334,
      "grad_norm": 0.07150384783744812,
      "learning_rate": 0.0019519201549379273,
      "loss": 1.2459,
      "step": 946
    },
    {
      "epoch": 1.2626666666666666,
      "grad_norm": 0.10820503532886505,
      "learning_rate": 0.0019517877754498107,
      "loss": 1.0689,
      "step": 947
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.18272580206394196,
      "learning_rate": 0.0019516552184717037,
      "loss": 1.227,
      "step": 948
    },
    {
      "epoch": 1.2653333333333334,
      "grad_norm": 0.04019171744585037,
      "learning_rate": 0.0019515224840283255,
      "loss": 1.486,
      "step": 949
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.035130444914102554,
      "learning_rate": 0.0019513895721444286,
      "loss": 1.308,
      "step": 950
    },
    {
      "epoch": 1.268,
      "grad_norm": 0.06054501608014107,
      "learning_rate": 0.0019512564828447988,
      "loss": 1.2732,
      "step": 951
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 0.05069601163268089,
      "learning_rate": 0.0019511232161542541,
      "loss": 1.0948,
      "step": 952
    },
    {
      "epoch": 1.2706666666666666,
      "grad_norm": 0.12024369090795517,
      "learning_rate": 0.0019509897720976466,
      "loss": 1.2654,
      "step": 953
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.03756503760814667,
      "learning_rate": 0.001950856150699861,
      "loss": 1.1607,
      "step": 954
    },
    {
      "epoch": 1.2733333333333334,
      "grad_norm": 0.06278540939092636,
      "learning_rate": 0.0019507223519858146,
      "loss": 1.4081,
      "step": 955
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 0.051109179854393005,
      "learning_rate": 0.001950588375980459,
      "loss": 1.1908,
      "step": 956
    },
    {
      "epoch": 1.276,
      "grad_norm": 0.02911866270005703,
      "learning_rate": 0.0019504542227087778,
      "loss": 1.2778,
      "step": 957
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.041811563074588776,
      "learning_rate": 0.001950319892195788,
      "loss": 1.187,
      "step": 958
    },
    {
      "epoch": 1.2786666666666666,
      "grad_norm": 0.05014164373278618,
      "learning_rate": 0.0019501853844665399,
      "loss": 1.1465,
      "step": 959
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.036165930330753326,
      "learning_rate": 0.0019500506995461157,
      "loss": 1.094,
      "step": 960
    },
    {
      "epoch": 1.2813333333333334,
      "grad_norm": 0.033391740173101425,
      "learning_rate": 0.0019499158374596327,
      "loss": 0.868,
      "step": 961
    },
    {
      "epoch": 1.2826666666666666,
      "grad_norm": 0.030474362894892693,
      "learning_rate": 0.0019497807982322393,
      "loss": 1.04,
      "step": 962
    },
    {
      "epoch": 1.284,
      "grad_norm": 0.03946472704410553,
      "learning_rate": 0.0019496455818891181,
      "loss": 1.0843,
      "step": 963
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 0.038722872734069824,
      "learning_rate": 0.0019495101884554839,
      "loss": 1.244,
      "step": 964
    },
    {
      "epoch": 1.2866666666666666,
      "grad_norm": 0.03766161948442459,
      "learning_rate": 0.0019493746179565852,
      "loss": 1.3545,
      "step": 965
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.034053198993206024,
      "learning_rate": 0.0019492388704177035,
      "loss": 0.9666,
      "step": 966
    },
    {
      "epoch": 1.2893333333333334,
      "grad_norm": 0.04746085777878761,
      "learning_rate": 0.0019491029458641527,
      "loss": 1.3431,
      "step": 967
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 0.03471178933978081,
      "learning_rate": 0.0019489668443212805,
      "loss": 1.1371,
      "step": 968
    },
    {
      "epoch": 1.292,
      "grad_norm": 0.03200376406311989,
      "learning_rate": 0.0019488305658144666,
      "loss": 1.2418,
      "step": 969
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.03146272525191307,
      "learning_rate": 0.0019486941103691246,
      "loss": 1.3092,
      "step": 970
    },
    {
      "epoch": 1.2946666666666666,
      "grad_norm": 0.033365488052368164,
      "learning_rate": 0.0019485574780107014,
      "loss": 1.2712,
      "step": 971
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.03075232543051243,
      "learning_rate": 0.0019484206687646753,
      "loss": 1.2443,
      "step": 972
    },
    {
      "epoch": 1.2973333333333334,
      "grad_norm": 0.051488567143678665,
      "learning_rate": 0.0019482836826565594,
      "loss": 1.1158,
      "step": 973
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.030538994818925858,
      "learning_rate": 0.0019481465197118983,
      "loss": 1.1642,
      "step": 974
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.038301169872283936,
      "learning_rate": 0.0019480091799562705,
      "loss": 1.359,
      "step": 975
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 0.06107578054070473,
      "learning_rate": 0.0019478716634152872,
      "loss": 0.9968,
      "step": 976
    },
    {
      "epoch": 1.3026666666666666,
      "grad_norm": 0.0440947525203228,
      "learning_rate": 0.001947733970114593,
      "loss": 1.2118,
      "step": 977
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.02929622121155262,
      "learning_rate": 0.0019475961000798643,
      "loss": 0.9592,
      "step": 978
    },
    {
      "epoch": 1.3053333333333335,
      "grad_norm": 0.049520161002874374,
      "learning_rate": 0.0019474580533368115,
      "loss": 1.2133,
      "step": 979
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.026915408670902252,
      "learning_rate": 0.0019473198299111778,
      "loss": 1.25,
      "step": 980
    },
    {
      "epoch": 1.308,
      "grad_norm": 0.03466947004199028,
      "learning_rate": 0.0019471814298287389,
      "loss": 1.3719,
      "step": 981
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.037364665418863297,
      "learning_rate": 0.0019470428531153038,
      "loss": 1.0986,
      "step": 982
    },
    {
      "epoch": 1.3106666666666666,
      "grad_norm": 0.04226904362440109,
      "learning_rate": 0.001946904099796715,
      "loss": 1.3798,
      "step": 983
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.03864710405468941,
      "learning_rate": 0.0019467651698988461,
      "loss": 1.3063,
      "step": 984
    },
    {
      "epoch": 1.3133333333333335,
      "grad_norm": 0.048180874437093735,
      "learning_rate": 0.001946626063447606,
      "loss": 0.9279,
      "step": 985
    },
    {
      "epoch": 1.3146666666666667,
      "grad_norm": 0.04228246212005615,
      "learning_rate": 0.0019464867804689348,
      "loss": 1.1569,
      "step": 986
    },
    {
      "epoch": 1.316,
      "grad_norm": 0.02623417228460312,
      "learning_rate": 0.001946347320988806,
      "loss": 1.225,
      "step": 987
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 0.047404587268829346,
      "learning_rate": 0.0019462076850332265,
      "loss": 1.1843,
      "step": 988
    },
    {
      "epoch": 1.3186666666666667,
      "grad_norm": 0.03544633090496063,
      "learning_rate": 0.001946067872628235,
      "loss": 1.2119,
      "step": 989
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.05128188803792,
      "learning_rate": 0.0019459278837999046,
      "loss": 1.449,
      "step": 990
    },
    {
      "epoch": 1.3213333333333335,
      "grad_norm": 0.030097145587205887,
      "learning_rate": 0.0019457877185743403,
      "loss": 1.1071,
      "step": 991
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 0.031008301302790642,
      "learning_rate": 0.0019456473769776795,
      "loss": 1.2859,
      "step": 992
    },
    {
      "epoch": 1.324,
      "grad_norm": 0.034020811319351196,
      "learning_rate": 0.0019455068590360943,
      "loss": 1.0749,
      "step": 993
    },
    {
      "epoch": 1.3253333333333333,
      "grad_norm": 0.03134119510650635,
      "learning_rate": 0.0019453661647757877,
      "loss": 1.0457,
      "step": 994
    },
    {
      "epoch": 1.3266666666666667,
      "grad_norm": 0.04797649011015892,
      "learning_rate": 0.001945225294222997,
      "loss": 1.3599,
      "step": 995
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.0311111006885767,
      "learning_rate": 0.0019450842474039913,
      "loss": 1.29,
      "step": 996
    },
    {
      "epoch": 1.3293333333333333,
      "grad_norm": 0.03706010803580284,
      "learning_rate": 0.0019449430243450737,
      "loss": 1.1249,
      "step": 997
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.038652338087558746,
      "learning_rate": 0.0019448016250725791,
      "loss": 0.921,
      "step": 998
    },
    {
      "epoch": 1.332,
      "grad_norm": 0.034926656633615494,
      "learning_rate": 0.0019446600496128758,
      "loss": 1.1532,
      "step": 999
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.027791153639554977,
      "learning_rate": 0.0019445182979923655,
      "loss": 1.1127,
      "step": 1000
    },
    {
      "epoch": 1.3346666666666667,
      "grad_norm": 0.0369306243956089,
      "learning_rate": 0.0019443763702374811,
      "loss": 1.2877,
      "step": 1001
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.03968014568090439,
      "learning_rate": 0.00194423426637469,
      "loss": 1.1902,
      "step": 1002
    },
    {
      "epoch": 1.3373333333333333,
      "grad_norm": 0.041777413338422775,
      "learning_rate": 0.0019440919864304918,
      "loss": 1.1582,
      "step": 1003
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 0.03023817017674446,
      "learning_rate": 0.0019439495304314188,
      "loss": 1.1592,
      "step": 1004
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.03844117000699043,
      "learning_rate": 0.0019438068984040365,
      "loss": 1.0502,
      "step": 1005
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.033100344240665436,
      "learning_rate": 0.0019436640903749427,
      "loss": 1.2267,
      "step": 1006
    },
    {
      "epoch": 1.3426666666666667,
      "grad_norm": 0.03463374078273773,
      "learning_rate": 0.0019435211063707687,
      "loss": 1.0481,
      "step": 1007
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.053173381835222244,
      "learning_rate": 0.0019433779464181778,
      "loss": 1.2908,
      "step": 1008
    },
    {
      "epoch": 1.3453333333333333,
      "grad_norm": 0.03280322253704071,
      "learning_rate": 0.001943234610543867,
      "loss": 0.8925,
      "step": 1009
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.025117285549640656,
      "learning_rate": 0.0019430910987745654,
      "loss": 1.1763,
      "step": 1010
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 0.0350940078496933,
      "learning_rate": 0.001942947411137035,
      "loss": 1.1532,
      "step": 1011
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 0.03816450014710426,
      "learning_rate": 0.0019428035476580713,
      "loss": 1.0659,
      "step": 1012
    },
    {
      "epoch": 1.3506666666666667,
      "grad_norm": 0.031671345233917236,
      "learning_rate": 0.0019426595083645015,
      "loss": 1.4603,
      "step": 1013
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.08946898579597473,
      "learning_rate": 0.001942515293283187,
      "loss": 1.324,
      "step": 1014
    },
    {
      "epoch": 1.3533333333333333,
      "grad_norm": 0.03596218675374985,
      "learning_rate": 0.0019423709024410196,
      "loss": 1.2267,
      "step": 1015
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 0.04250172898173332,
      "learning_rate": 0.0019422263358649268,
      "loss": 1.0925,
      "step": 1016
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 0.06566435098648071,
      "learning_rate": 0.0019420815935818673,
      "loss": 1.4852,
      "step": 1017
    },
    {
      "epoch": 1.3573333333333333,
      "grad_norm": 0.03196272999048233,
      "learning_rate": 0.0019419366756188317,
      "loss": 1.0055,
      "step": 1018
    },
    {
      "epoch": 1.3586666666666667,
      "grad_norm": 0.036296598613262177,
      "learning_rate": 0.0019417915820028457,
      "loss": 0.8954,
      "step": 1019
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.03516979143023491,
      "learning_rate": 0.0019416463127609656,
      "loss": 1.1029,
      "step": 1020
    },
    {
      "epoch": 1.3613333333333333,
      "grad_norm": 0.03265445679426193,
      "learning_rate": 0.0019415008679202815,
      "loss": 1.2619,
      "step": 1021
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.04200386628508568,
      "learning_rate": 0.0019413552475079161,
      "loss": 1.0716,
      "step": 1022
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 0.03689655289053917,
      "learning_rate": 0.0019412094515510248,
      "loss": 1.3979,
      "step": 1023
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 0.03170574828982353,
      "learning_rate": 0.0019410634800767958,
      "loss": 1.1837,
      "step": 1024
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.04890434443950653,
      "learning_rate": 0.0019409173331124499,
      "loss": 0.9876,
      "step": 1025
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.033238139003515244,
      "learning_rate": 0.0019407710106852402,
      "loss": 1.2522,
      "step": 1026
    },
    {
      "epoch": 1.3693333333333333,
      "grad_norm": 0.03388666361570358,
      "learning_rate": 0.001940624512822454,
      "loss": 0.9276,
      "step": 1027
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 0.03537699580192566,
      "learning_rate": 0.0019404778395514094,
      "loss": 1.0985,
      "step": 1028
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 0.04850810021162033,
      "learning_rate": 0.0019403309908994587,
      "loss": 1.1092,
      "step": 1029
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.24810157716274261,
      "learning_rate": 0.001940183966893986,
      "loss": 1.0112,
      "step": 1030
    },
    {
      "epoch": 1.3746666666666667,
      "grad_norm": 0.03380877152085304,
      "learning_rate": 0.0019400367675624084,
      "loss": 0.9848,
      "step": 1031
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.036490194499492645,
      "learning_rate": 0.0019398893929321761,
      "loss": 1.3524,
      "step": 1032
    },
    {
      "epoch": 1.3773333333333333,
      "grad_norm": 0.044986024498939514,
      "learning_rate": 0.0019397418430307713,
      "loss": 1.2282,
      "step": 1033
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 0.10466448217630386,
      "learning_rate": 0.0019395941178857092,
      "loss": 1.2598,
      "step": 1034
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.04024912044405937,
      "learning_rate": 0.001939446217524538,
      "loss": 1.1575,
      "step": 1035
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 0.2530352473258972,
      "learning_rate": 0.0019392981419748376,
      "loss": 1.143,
      "step": 1036
    },
    {
      "epoch": 1.3826666666666667,
      "grad_norm": 0.03773501142859459,
      "learning_rate": 0.001939149891264222,
      "loss": 1.1443,
      "step": 1037
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.0643220916390419,
      "learning_rate": 0.0019390014654203367,
      "loss": 1.1808,
      "step": 1038
    },
    {
      "epoch": 1.3853333333333333,
      "grad_norm": 0.03536034747958183,
      "learning_rate": 0.0019388528644708602,
      "loss": 1.1469,
      "step": 1039
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.06130973622202873,
      "learning_rate": 0.0019387040884435037,
      "loss": 1.109,
      "step": 1040
    },
    {
      "epoch": 1.388,
      "grad_norm": 0.1922924816608429,
      "learning_rate": 0.0019385551373660112,
      "loss": 1.5175,
      "step": 1041
    },
    {
      "epoch": 1.3893333333333333,
      "grad_norm": 0.048888418823480606,
      "learning_rate": 0.001938406011266159,
      "loss": 1.3275,
      "step": 1042
    },
    {
      "epoch": 1.3906666666666667,
      "grad_norm": 0.07075276225805283,
      "learning_rate": 0.0019382567101717564,
      "loss": 1.1408,
      "step": 1043
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.06521464884281158,
      "learning_rate": 0.0019381072341106453,
      "loss": 1.248,
      "step": 1044
    },
    {
      "epoch": 1.3933333333333333,
      "grad_norm": 0.051839422434568405,
      "learning_rate": 0.0019379575831106994,
      "loss": 1.2363,
      "step": 1045
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.042511679232120514,
      "learning_rate": 0.0019378077571998264,
      "loss": 1.2944,
      "step": 1046
    },
    {
      "epoch": 1.396,
      "grad_norm": 0.07609397917985916,
      "learning_rate": 0.001937657756405966,
      "loss": 1.1261,
      "step": 1047
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 0.06251794099807739,
      "learning_rate": 0.00193750758075709,
      "loss": 1.5067,
      "step": 1048
    },
    {
      "epoch": 1.3986666666666667,
      "grad_norm": 0.04129412770271301,
      "learning_rate": 0.0019373572302812034,
      "loss": 1.3242,
      "step": 1049
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.029453624039888382,
      "learning_rate": 0.0019372067050063438,
      "loss": 1.1113,
      "step": 1050
    },
    {
      "epoch": 1.4013333333333333,
      "grad_norm": 0.06899867951869965,
      "learning_rate": 0.001937056004960581,
      "loss": 1.1747,
      "step": 1051
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 0.047641996294260025,
      "learning_rate": 0.0019369051301720177,
      "loss": 1.2007,
      "step": 1052
    },
    {
      "epoch": 1.404,
      "grad_norm": 0.05185546725988388,
      "learning_rate": 0.0019367540806687893,
      "loss": 1.1742,
      "step": 1053
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.031130235642194748,
      "learning_rate": 0.0019366028564790634,
      "loss": 1.207,
      "step": 1054
    },
    {
      "epoch": 1.4066666666666667,
      "grad_norm": 0.03203819692134857,
      "learning_rate": 0.0019364514576310408,
      "loss": 1.2248,
      "step": 1055
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.033899154514074326,
      "learning_rate": 0.001936299884152954,
      "loss": 1.324,
      "step": 1056
    },
    {
      "epoch": 1.4093333333333333,
      "grad_norm": 0.03956972062587738,
      "learning_rate": 0.0019361481360730686,
      "loss": 1.2148,
      "step": 1057
    },
    {
      "epoch": 1.4106666666666667,
      "grad_norm": 0.04931002855300903,
      "learning_rate": 0.0019359962134196827,
      "loss": 1.1771,
      "step": 1058
    },
    {
      "epoch": 1.412,
      "grad_norm": 0.03282837197184563,
      "learning_rate": 0.0019358441162211268,
      "loss": 1.4793,
      "step": 1059
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.02552546001970768,
      "learning_rate": 0.0019356918445057648,
      "loss": 1.3109,
      "step": 1060
    },
    {
      "epoch": 1.4146666666666667,
      "grad_norm": 0.02982131391763687,
      "learning_rate": 0.0019355393983019912,
      "loss": 1.1649,
      "step": 1061
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.034890398383140564,
      "learning_rate": 0.0019353867776382354,
      "loss": 1.1318,
      "step": 1062
    },
    {
      "epoch": 1.4173333333333333,
      "grad_norm": 0.036309607326984406,
      "learning_rate": 0.001935233982542958,
      "loss": 1.254,
      "step": 1063
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 0.027689358219504356,
      "learning_rate": 0.0019350810130446515,
      "loss": 1.1954,
      "step": 1064
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.0335623137652874,
      "learning_rate": 0.0019349278691718427,
      "loss": 1.315,
      "step": 1065
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 0.03918091207742691,
      "learning_rate": 0.0019347745509530894,
      "loss": 1.3466,
      "step": 1066
    },
    {
      "epoch": 1.4226666666666667,
      "grad_norm": 0.04572087153792381,
      "learning_rate": 0.0019346210584169827,
      "loss": 0.8661,
      "step": 1067
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.03770998865365982,
      "learning_rate": 0.0019344673915921461,
      "loss": 1.2371,
      "step": 1068
    },
    {
      "epoch": 1.4253333333333333,
      "grad_norm": 0.03576504811644554,
      "learning_rate": 0.0019343135505072349,
      "loss": 1.3754,
      "step": 1069
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.03012077324092388,
      "learning_rate": 0.0019341595351909384,
      "loss": 1.3323,
      "step": 1070
    },
    {
      "epoch": 1.428,
      "grad_norm": 0.0364651121199131,
      "learning_rate": 0.0019340053456719769,
      "loss": 0.9957,
      "step": 1071
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 0.033979207277297974,
      "learning_rate": 0.0019338509819791037,
      "loss": 0.9219,
      "step": 1072
    },
    {
      "epoch": 1.4306666666666668,
      "grad_norm": 0.05296367406845093,
      "learning_rate": 0.001933696444141105,
      "loss": 1.309,
      "step": 1073
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.029917173087596893,
      "learning_rate": 0.0019335417321867988,
      "loss": 1.3027,
      "step": 1074
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.15252470970153809,
      "learning_rate": 0.0019333868461450358,
      "loss": 1.1495,
      "step": 1075
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 0.04634843021631241,
      "learning_rate": 0.0019332317860446997,
      "loss": 1.3094,
      "step": 1076
    },
    {
      "epoch": 1.436,
      "grad_norm": 0.029510192573070526,
      "learning_rate": 0.0019330765519147057,
      "loss": 0.8929,
      "step": 1077
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 0.03733333945274353,
      "learning_rate": 0.0019329211437840025,
      "loss": 0.9797,
      "step": 1078
    },
    {
      "epoch": 1.4386666666666668,
      "grad_norm": 0.04122123494744301,
      "learning_rate": 0.0019327655616815705,
      "loss": 1.34,
      "step": 1079
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.0380447693169117,
      "learning_rate": 0.0019326098056364222,
      "loss": 1.3913,
      "step": 1080
    },
    {
      "epoch": 1.4413333333333334,
      "grad_norm": 0.03763454034924507,
      "learning_rate": 0.001932453875677604,
      "loss": 1.037,
      "step": 1081
    },
    {
      "epoch": 1.4426666666666668,
      "grad_norm": 0.09115712344646454,
      "learning_rate": 0.0019322977718341933,
      "loss": 1.1692,
      "step": 1082
    },
    {
      "epoch": 1.444,
      "grad_norm": 0.03941146284341812,
      "learning_rate": 0.0019321414941353005,
      "loss": 1.1363,
      "step": 1083
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 0.09977391362190247,
      "learning_rate": 0.001931985042610068,
      "loss": 1.1467,
      "step": 1084
    },
    {
      "epoch": 1.4466666666666668,
      "grad_norm": 0.025791233405470848,
      "learning_rate": 0.0019318284172876719,
      "loss": 1.3123,
      "step": 1085
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.03638454154133797,
      "learning_rate": 0.001931671618197319,
      "loss": 0.8348,
      "step": 1086
    },
    {
      "epoch": 1.4493333333333334,
      "grad_norm": 0.03779686242341995,
      "learning_rate": 0.0019315146453682496,
      "loss": 1.2207,
      "step": 1087
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 0.036933474242687225,
      "learning_rate": 0.0019313574988297358,
      "loss": 1.2993,
      "step": 1088
    },
    {
      "epoch": 1.452,
      "grad_norm": 0.06168394163250923,
      "learning_rate": 0.0019312001786110828,
      "loss": 1.434,
      "step": 1089
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.05353248119354248,
      "learning_rate": 0.0019310426847416275,
      "loss": 1.2748,
      "step": 1090
    },
    {
      "epoch": 1.4546666666666668,
      "grad_norm": 0.06427458673715591,
      "learning_rate": 0.0019308850172507397,
      "loss": 1.3133,
      "step": 1091
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.03762117400765419,
      "learning_rate": 0.0019307271761678214,
      "loss": 1.1852,
      "step": 1092
    },
    {
      "epoch": 1.4573333333333334,
      "grad_norm": 0.039141394197940826,
      "learning_rate": 0.0019305691615223064,
      "loss": 1.3403,
      "step": 1093
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.02479461021721363,
      "learning_rate": 0.0019304109733436616,
      "loss": 1.0433,
      "step": 1094
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.038292624056339264,
      "learning_rate": 0.0019302526116613864,
      "loss": 1.37,
      "step": 1095
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 0.03812011331319809,
      "learning_rate": 0.0019300940765050113,
      "loss": 1.5786,
      "step": 1096
    },
    {
      "epoch": 1.4626666666666668,
      "grad_norm": 0.047531358897686005,
      "learning_rate": 0.001929935367904101,
      "loss": 0.9266,
      "step": 1097
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.06007589399814606,
      "learning_rate": 0.0019297764858882513,
      "loss": 1.0865,
      "step": 1098
    },
    {
      "epoch": 1.4653333333333334,
      "grad_norm": 0.03907125070691109,
      "learning_rate": 0.0019296174304870906,
      "loss": 1.1008,
      "step": 1099
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.03853308781981468,
      "learning_rate": 0.0019294582017302796,
      "loss": 1.3794,
      "step": 1100
    },
    {
      "epoch": 1.468,
      "grad_norm": 0.0294423159211874,
      "learning_rate": 0.001929298799647511,
      "loss": 1.2066,
      "step": 1101
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 0.042536910623311996,
      "learning_rate": 0.0019291392242685107,
      "loss": 0.9692,
      "step": 1102
    },
    {
      "epoch": 1.4706666666666668,
      "grad_norm": 0.06810896843671799,
      "learning_rate": 0.0019289794756230363,
      "loss": 1.6728,
      "step": 1103
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.03829686716198921,
      "learning_rate": 0.0019288195537408778,
      "loss": 1.3638,
      "step": 1104
    },
    {
      "epoch": 1.4733333333333334,
      "grad_norm": 0.03567301854491234,
      "learning_rate": 0.0019286594586518575,
      "loss": 1.3868,
      "step": 1105
    },
    {
      "epoch": 1.4746666666666668,
      "grad_norm": 0.032131630927324295,
      "learning_rate": 0.0019284991903858301,
      "loss": 1.2609,
      "step": 1106
    },
    {
      "epoch": 1.476,
      "grad_norm": 0.03357576206326485,
      "learning_rate": 0.0019283387489726826,
      "loss": 0.8685,
      "step": 1107
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 0.026589877903461456,
      "learning_rate": 0.0019281781344423342,
      "loss": 0.905,
      "step": 1108
    },
    {
      "epoch": 1.4786666666666668,
      "grad_norm": 0.027862684801220894,
      "learning_rate": 0.0019280173468247362,
      "loss": 1.062,
      "step": 1109
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.032053142786026,
      "learning_rate": 0.0019278563861498724,
      "loss": 1.2809,
      "step": 1110
    },
    {
      "epoch": 1.4813333333333334,
      "grad_norm": 0.06968472898006439,
      "learning_rate": 0.0019276952524477592,
      "loss": 1.0491,
      "step": 1111
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 0.030142201110720634,
      "learning_rate": 0.0019275339457484443,
      "loss": 0.964,
      "step": 1112
    },
    {
      "epoch": 1.484,
      "grad_norm": 0.05732380971312523,
      "learning_rate": 0.0019273724660820086,
      "loss": 1.129,
      "step": 1113
    },
    {
      "epoch": 1.4853333333333334,
      "grad_norm": 0.025522222742438316,
      "learning_rate": 0.0019272108134785655,
      "loss": 1.0828,
      "step": 1114
    },
    {
      "epoch": 1.4866666666666668,
      "grad_norm": 0.03420479968190193,
      "learning_rate": 0.001927048987968259,
      "loss": 1.2274,
      "step": 1115
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.03673067316412926,
      "learning_rate": 0.0019268869895812672,
      "loss": 1.3353,
      "step": 1116
    },
    {
      "epoch": 1.4893333333333334,
      "grad_norm": 0.03361811861395836,
      "learning_rate": 0.0019267248183477993,
      "loss": 1.0382,
      "step": 1117
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.036503035575151443,
      "learning_rate": 0.0019265624742980977,
      "loss": 1.2459,
      "step": 1118
    },
    {
      "epoch": 1.492,
      "grad_norm": 0.04614037647843361,
      "learning_rate": 0.0019263999574624356,
      "loss": 1.2337,
      "step": 1119
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.032770320773124695,
      "learning_rate": 0.0019262372678711195,
      "loss": 1.2042,
      "step": 1120
    },
    {
      "epoch": 1.4946666666666666,
      "grad_norm": 0.03707144409418106,
      "learning_rate": 0.001926074405554488,
      "loss": 1.2283,
      "step": 1121
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.03890113905072212,
      "learning_rate": 0.0019259113705429119,
      "loss": 1.1419,
      "step": 1122
    },
    {
      "epoch": 1.4973333333333334,
      "grad_norm": 0.04665500298142433,
      "learning_rate": 0.001925748162866794,
      "loss": 1.3614,
      "step": 1123
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 0.029024560004472733,
      "learning_rate": 0.0019255847825565687,
      "loss": 1.0714,
      "step": 1124
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.027847202494740486,
      "learning_rate": 0.0019254212296427042,
      "loss": 1.066,
      "step": 1125
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.036507509648799896,
      "learning_rate": 0.0019252575041556997,
      "loss": 1.2496,
      "step": 1126
    },
    {
      "epoch": 1.5026666666666668,
      "grad_norm": 0.037839457392692566,
      "learning_rate": 0.0019250936061260865,
      "loss": 1.2916,
      "step": 1127
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.031323038041591644,
      "learning_rate": 0.0019249295355844285,
      "loss": 1.1231,
      "step": 1128
    },
    {
      "epoch": 1.5053333333333332,
      "grad_norm": 0.0476144477725029,
      "learning_rate": 0.0019247652925613221,
      "loss": 1.2372,
      "step": 1129
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.03580286726355553,
      "learning_rate": 0.0019246008770873952,
      "loss": 1.2995,
      "step": 1130
    },
    {
      "epoch": 1.508,
      "grad_norm": 0.053861796855926514,
      "learning_rate": 0.0019244362891933076,
      "loss": 0.9273,
      "step": 1131
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 0.07648592442274094,
      "learning_rate": 0.0019242715289097526,
      "loss": 1.2355,
      "step": 1132
    },
    {
      "epoch": 1.5106666666666668,
      "grad_norm": 0.0389610230922699,
      "learning_rate": 0.0019241065962674541,
      "loss": 1.4008,
      "step": 1133
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.029886845499277115,
      "learning_rate": 0.0019239414912971696,
      "loss": 1.2381,
      "step": 1134
    },
    {
      "epoch": 1.5133333333333332,
      "grad_norm": 0.03296320512890816,
      "learning_rate": 0.0019237762140296873,
      "loss": 1.3392,
      "step": 1135
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 0.043230537325143814,
      "learning_rate": 0.0019236107644958283,
      "loss": 1.5222,
      "step": 1136
    },
    {
      "epoch": 1.516,
      "grad_norm": 0.036820344626903534,
      "learning_rate": 0.0019234451427264459,
      "loss": 1.2157,
      "step": 1137
    },
    {
      "epoch": 1.5173333333333332,
      "grad_norm": 0.038154181092977524,
      "learning_rate": 0.0019232793487524256,
      "loss": 1.2014,
      "step": 1138
    },
    {
      "epoch": 1.5186666666666668,
      "grad_norm": 0.038935672491788864,
      "learning_rate": 0.0019231133826046842,
      "loss": 1.3046,
      "step": 1139
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.08492407947778702,
      "learning_rate": 0.0019229472443141717,
      "loss": 0.9978,
      "step": 1140
    },
    {
      "epoch": 1.5213333333333332,
      "grad_norm": 0.03445436432957649,
      "learning_rate": 0.001922780933911869,
      "loss": 1.2715,
      "step": 1141
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.03141622617840767,
      "learning_rate": 0.0019226144514287906,
      "loss": 1.4054,
      "step": 1142
    },
    {
      "epoch": 1.524,
      "grad_norm": 0.03992561623454094,
      "learning_rate": 0.001922447796895982,
      "loss": 1.1322,
      "step": 1143
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 0.0334891602396965,
      "learning_rate": 0.0019222809703445206,
      "loss": 1.0604,
      "step": 1144
    },
    {
      "epoch": 1.5266666666666666,
      "grad_norm": 0.06263735145330429,
      "learning_rate": 0.001922113971805517,
      "loss": 1.1775,
      "step": 1145
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.08937094360589981,
      "learning_rate": 0.0019219468013101121,
      "loss": 1.0979,
      "step": 1146
    },
    {
      "epoch": 1.5293333333333332,
      "grad_norm": 0.0393209345638752,
      "learning_rate": 0.0019217794588894815,
      "loss": 1.2765,
      "step": 1147
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 0.06908867508172989,
      "learning_rate": 0.0019216119445748302,
      "loss": 1.2652,
      "step": 1148
    },
    {
      "epoch": 1.532,
      "grad_norm": 0.04567311704158783,
      "learning_rate": 0.0019214442583973966,
      "loss": 1.2852,
      "step": 1149
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.03834959492087364,
      "learning_rate": 0.001921276400388451,
      "loss": 1.2297,
      "step": 1150
    },
    {
      "epoch": 1.5346666666666666,
      "grad_norm": 0.06769412755966187,
      "learning_rate": 0.0019211083705792957,
      "loss": 1.2351,
      "step": 1151
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.0361933633685112,
      "learning_rate": 0.0019209401690012651,
      "loss": 1.1698,
      "step": 1152
    },
    {
      "epoch": 1.5373333333333332,
      "grad_norm": 0.039950497448444366,
      "learning_rate": 0.0019207717956857254,
      "loss": 1.3037,
      "step": 1153
    },
    {
      "epoch": 1.5386666666666666,
      "grad_norm": 0.0356692336499691,
      "learning_rate": 0.0019206032506640747,
      "loss": 1.4069,
      "step": 1154
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.030030788853764534,
      "learning_rate": 0.001920434533967744,
      "loss": 1.1806,
      "step": 1155
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 0.04032178595662117,
      "learning_rate": 0.0019202656456281952,
      "loss": 1.2436,
      "step": 1156
    },
    {
      "epoch": 1.5426666666666666,
      "grad_norm": 0.0379839763045311,
      "learning_rate": 0.001920096585676923,
      "loss": 1.1481,
      "step": 1157
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.05142031982541084,
      "learning_rate": 0.0019199273541454537,
      "loss": 1.4854,
      "step": 1158
    },
    {
      "epoch": 1.5453333333333332,
      "grad_norm": 0.04140099883079529,
      "learning_rate": 0.0019197579510653456,
      "loss": 1.2322,
      "step": 1159
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.03596137836575508,
      "learning_rate": 0.0019195883764681892,
      "loss": 1.121,
      "step": 1160
    },
    {
      "epoch": 1.548,
      "grad_norm": 0.029852155596017838,
      "learning_rate": 0.0019194186303856068,
      "loss": 0.9737,
      "step": 1161
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 0.04490175470709801,
      "learning_rate": 0.0019192487128492529,
      "loss": 1.0951,
      "step": 1162
    },
    {
      "epoch": 1.5506666666666666,
      "grad_norm": 0.03802970051765442,
      "learning_rate": 0.0019190786238908136,
      "loss": 1.1863,
      "step": 1163
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.033025212585926056,
      "learning_rate": 0.0019189083635420075,
      "loss": 1.5652,
      "step": 1164
    },
    {
      "epoch": 1.5533333333333332,
      "grad_norm": 0.03308124467730522,
      "learning_rate": 0.0019187379318345845,
      "loss": 1.0581,
      "step": 1165
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.04073697328567505,
      "learning_rate": 0.0019185673288003274,
      "loss": 1.2326,
      "step": 1166
    },
    {
      "epoch": 1.556,
      "grad_norm": 0.028796609491109848,
      "learning_rate": 0.0019183965544710495,
      "loss": 1.1706,
      "step": 1167
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 0.032067958265542984,
      "learning_rate": 0.0019182256088785977,
      "loss": 0.9782,
      "step": 1168
    },
    {
      "epoch": 1.5586666666666666,
      "grad_norm": 0.04135400429368019,
      "learning_rate": 0.0019180544920548495,
      "loss": 1.0871,
      "step": 1169
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.05263848230242729,
      "learning_rate": 0.0019178832040317154,
      "loss": 1.3031,
      "step": 1170
    },
    {
      "epoch": 1.5613333333333332,
      "grad_norm": 0.03904447704553604,
      "learning_rate": 0.0019177117448411366,
      "loss": 1.232,
      "step": 1171
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 0.03162356838583946,
      "learning_rate": 0.0019175401145150874,
      "loss": 1.2634,
      "step": 1172
    },
    {
      "epoch": 1.564,
      "grad_norm": 0.03250851482152939,
      "learning_rate": 0.0019173683130855738,
      "loss": 1.1357,
      "step": 1173
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.02929721400141716,
      "learning_rate": 0.0019171963405846327,
      "loss": 1.1657,
      "step": 1174
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.03873145207762718,
      "learning_rate": 0.0019170241970443342,
      "loss": 1.3201,
      "step": 1175
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.03734424710273743,
      "learning_rate": 0.0019168518824967797,
      "loss": 1.3253,
      "step": 1176
    },
    {
      "epoch": 1.5693333333333332,
      "grad_norm": 0.050296295434236526,
      "learning_rate": 0.0019166793969741024,
      "loss": 1.3498,
      "step": 1177
    },
    {
      "epoch": 1.5706666666666667,
      "grad_norm": 0.05395993962883949,
      "learning_rate": 0.0019165067405084672,
      "loss": 1.3635,
      "step": 1178
    },
    {
      "epoch": 1.572,
      "grad_norm": 0.08466081321239471,
      "learning_rate": 0.0019163339131320716,
      "loss": 1.1913,
      "step": 1179
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.040522653609514236,
      "learning_rate": 0.0019161609148771444,
      "loss": 1.4815,
      "step": 1180
    },
    {
      "epoch": 1.5746666666666667,
      "grad_norm": 0.031109554693102837,
      "learning_rate": 0.0019159877457759467,
      "loss": 0.9617,
      "step": 1181
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.05868156999349594,
      "learning_rate": 0.0019158144058607706,
      "loss": 1.4265,
      "step": 1182
    },
    {
      "epoch": 1.5773333333333333,
      "grad_norm": 0.039943210780620575,
      "learning_rate": 0.0019156408951639414,
      "loss": 1.078,
      "step": 1183
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 0.04538954421877861,
      "learning_rate": 0.0019154672137178148,
      "loss": 1.1227,
      "step": 1184
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.032892413437366486,
      "learning_rate": 0.0019152933615547796,
      "loss": 1.0505,
      "step": 1185
    },
    {
      "epoch": 1.5813333333333333,
      "grad_norm": 0.039090242236852646,
      "learning_rate": 0.0019151193387072555,
      "loss": 0.9823,
      "step": 1186
    },
    {
      "epoch": 1.5826666666666667,
      "grad_norm": 0.03311231732368469,
      "learning_rate": 0.0019149451452076943,
      "loss": 1.0706,
      "step": 1187
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.03480307012796402,
      "learning_rate": 0.0019147707810885798,
      "loss": 1.1343,
      "step": 1188
    },
    {
      "epoch": 1.5853333333333333,
      "grad_norm": 0.03385277837514877,
      "learning_rate": 0.0019145962463824279,
      "loss": 1.1136,
      "step": 1189
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.041126929223537445,
      "learning_rate": 0.001914421541121785,
      "loss": 1.1069,
      "step": 1190
    },
    {
      "epoch": 1.588,
      "grad_norm": 0.040245819836854935,
      "learning_rate": 0.0019142466653392317,
      "loss": 1.1831,
      "step": 1191
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 0.03378815948963165,
      "learning_rate": 0.0019140716190673777,
      "loss": 1.4464,
      "step": 1192
    },
    {
      "epoch": 1.5906666666666667,
      "grad_norm": 0.07238516956567764,
      "learning_rate": 0.0019138964023388662,
      "loss": 1.118,
      "step": 1193
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.03839018568396568,
      "learning_rate": 0.001913721015186372,
      "loss": 1.1486,
      "step": 1194
    },
    {
      "epoch": 1.5933333333333333,
      "grad_norm": 0.02827485091984272,
      "learning_rate": 0.001913545457642601,
      "loss": 0.9859,
      "step": 1195
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 0.03224482014775276,
      "learning_rate": 0.0019133697297402912,
      "loss": 1.1681,
      "step": 1196
    },
    {
      "epoch": 1.596,
      "grad_norm": 0.035218946635723114,
      "learning_rate": 0.0019131938315122131,
      "loss": 1.3026,
      "step": 1197
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.04090793803334236,
      "learning_rate": 0.0019130177629911674,
      "loss": 1.1723,
      "step": 1198
    },
    {
      "epoch": 1.5986666666666667,
      "grad_norm": 0.06373558193445206,
      "learning_rate": 0.0019128415242099881,
      "loss": 1.2005,
      "step": 1199
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.027570605278015137,
      "learning_rate": 0.0019126651152015402,
      "loss": 1.0651,
      "step": 1200
    },
    {
      "epoch": 1.6013333333333333,
      "grad_norm": 0.03094179928302765,
      "learning_rate": 0.0019124885359987206,
      "loss": 1.3301,
      "step": 1201
    },
    {
      "epoch": 1.6026666666666667,
      "grad_norm": 0.039830271154642105,
      "learning_rate": 0.0019123117866344575,
      "loss": 1.3648,
      "step": 1202
    },
    {
      "epoch": 1.604,
      "grad_norm": 0.033579643815755844,
      "learning_rate": 0.0019121348671417117,
      "loss": 1.2406,
      "step": 1203
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 0.036577798426151276,
      "learning_rate": 0.0019119577775534755,
      "loss": 1.1239,
      "step": 1204
    },
    {
      "epoch": 1.6066666666666667,
      "grad_norm": 0.028768688440322876,
      "learning_rate": 0.0019117805179027722,
      "loss": 1.1041,
      "step": 1205
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.03136226907372475,
      "learning_rate": 0.001911603088222657,
      "loss": 1.398,
      "step": 1206
    },
    {
      "epoch": 1.6093333333333333,
      "grad_norm": 0.029745595529675484,
      "learning_rate": 0.0019114254885462176,
      "loss": 1.0693,
      "step": 1207
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 0.0319194495677948,
      "learning_rate": 0.001911247718906573,
      "loss": 1.2434,
      "step": 1208
    },
    {
      "epoch": 1.612,
      "grad_norm": 0.04638802260160446,
      "learning_rate": 0.0019110697793368733,
      "loss": 1.2395,
      "step": 1209
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.029794272035360336,
      "learning_rate": 0.0019108916698703014,
      "loss": 1.1865,
      "step": 1210
    },
    {
      "epoch": 1.6146666666666667,
      "grad_norm": 0.03279958665370941,
      "learning_rate": 0.0019107133905400709,
      "loss": 1.4016,
      "step": 1211
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.031919293105602264,
      "learning_rate": 0.001910534941379427,
      "loss": 1.1496,
      "step": 1212
    },
    {
      "epoch": 1.6173333333333333,
      "grad_norm": 0.25600144267082214,
      "learning_rate": 0.001910356322421648,
      "loss": 1.222,
      "step": 1213
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.05811659246683121,
      "learning_rate": 0.0019101775337000422,
      "loss": 1.2157,
      "step": 1214
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.3031785488128662,
      "learning_rate": 0.0019099985752479506,
      "loss": 1.0924,
      "step": 1215
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 0.028796110302209854,
      "learning_rate": 0.0019098194470987448,
      "loss": 1.1603,
      "step": 1216
    },
    {
      "epoch": 1.6226666666666667,
      "grad_norm": 0.038465362042188644,
      "learning_rate": 0.0019096401492858298,
      "loss": 1.0193,
      "step": 1217
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.07102926820516586,
      "learning_rate": 0.0019094606818426403,
      "loss": 1.1355,
      "step": 1218
    },
    {
      "epoch": 1.6253333333333333,
      "grad_norm": 0.13549208641052246,
      "learning_rate": 0.0019092810448026439,
      "loss": 1.3408,
      "step": 1219
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.03507966175675392,
      "learning_rate": 0.001909101238199339,
      "loss": 1.0847,
      "step": 1220
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 0.041864726692438126,
      "learning_rate": 0.0019089212620662568,
      "loss": 1.2147,
      "step": 1221
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 0.2200983762741089,
      "learning_rate": 0.0019087411164369589,
      "loss": 1.1567,
      "step": 1222
    },
    {
      "epoch": 1.6306666666666667,
      "grad_norm": 0.05468539148569107,
      "learning_rate": 0.0019085608013450388,
      "loss": 1.1195,
      "step": 1223
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.07808142155408859,
      "learning_rate": 0.0019083803168241224,
      "loss": 1.1464,
      "step": 1224
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.058709025382995605,
      "learning_rate": 0.0019081996629078655,
      "loss": 1.2303,
      "step": 1225
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 0.3061031997203827,
      "learning_rate": 0.0019080188396299576,
      "loss": 1.1295,
      "step": 1226
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 0.25929364562034607,
      "learning_rate": 0.0019078378470241183,
      "loss": 1.2193,
      "step": 1227
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 0.03649763762950897,
      "learning_rate": 0.0019076566851240994,
      "loss": 0.8796,
      "step": 1228
    },
    {
      "epoch": 1.6386666666666667,
      "grad_norm": 0.050126027315855026,
      "learning_rate": 0.0019074753539636835,
      "loss": 1.2626,
      "step": 1229
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.09182040393352509,
      "learning_rate": 0.0019072938535766863,
      "loss": 1.034,
      "step": 1230
    },
    {
      "epoch": 1.6413333333333333,
      "grad_norm": 0.16734810173511505,
      "learning_rate": 0.0019071121839969534,
      "loss": 1.0777,
      "step": 1231
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 0.046076979488134384,
      "learning_rate": 0.0019069303452583627,
      "loss": 1.3502,
      "step": 1232
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 0.04674212634563446,
      "learning_rate": 0.0019067483373948243,
      "loss": 1.0992,
      "step": 1233
    },
    {
      "epoch": 1.6453333333333333,
      "grad_norm": 0.04751649498939514,
      "learning_rate": 0.0019065661604402782,
      "loss": 1.0023,
      "step": 1234
    },
    {
      "epoch": 1.6466666666666665,
      "grad_norm": 0.2590172290802002,
      "learning_rate": 0.0019063838144286974,
      "loss": 1.3586,
      "step": 1235
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.03021070919930935,
      "learning_rate": 0.0019062012993940859,
      "loss": 1.0303,
      "step": 1236
    },
    {
      "epoch": 1.6493333333333333,
      "grad_norm": 0.04040015861392021,
      "learning_rate": 0.0019060186153704787,
      "loss": 1.2953,
      "step": 1237
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.032560091465711594,
      "learning_rate": 0.0019058357623919437,
      "loss": 1.1655,
      "step": 1238
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 0.03935252130031586,
      "learning_rate": 0.0019056527404925788,
      "loss": 1.1003,
      "step": 1239
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.0936133936047554,
      "learning_rate": 0.0019054695497065142,
      "loss": 0.8688,
      "step": 1240
    },
    {
      "epoch": 1.6546666666666665,
      "grad_norm": 0.06002434343099594,
      "learning_rate": 0.0019052861900679115,
      "loss": 1.3319,
      "step": 1241
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.03721441701054573,
      "learning_rate": 0.0019051026616109636,
      "loss": 1.2336,
      "step": 1242
    },
    {
      "epoch": 1.6573333333333333,
      "grad_norm": 0.03649011254310608,
      "learning_rate": 0.0019049189643698952,
      "loss": 1.1829,
      "step": 1243
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 0.029769908636808395,
      "learning_rate": 0.0019047350983789622,
      "loss": 1.0471,
      "step": 1244
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.09674405306577682,
      "learning_rate": 0.0019045510636724518,
      "loss": 1.3474,
      "step": 1245
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.03294941037893295,
      "learning_rate": 0.0019043668602846833,
      "loss": 1.1993,
      "step": 1246
    },
    {
      "epoch": 1.6626666666666665,
      "grad_norm": 0.06474320590496063,
      "learning_rate": 0.001904182488250007,
      "loss": 1.0684,
      "step": 1247
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.06734839081764221,
      "learning_rate": 0.001903997947602804,
      "loss": 1.1451,
      "step": 1248
    },
    {
      "epoch": 1.6653333333333333,
      "grad_norm": 0.03365247696638107,
      "learning_rate": 0.0019038132383774888,
      "loss": 1.1108,
      "step": 1249
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.04782658442854881,
      "learning_rate": 0.0019036283606085054,
      "loss": 1.4607,
      "step": 1250
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 0.025025110691785812,
      "learning_rate": 0.0019034433143303299,
      "loss": 0.9268,
      "step": 1251
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 0.04126270115375519,
      "learning_rate": 0.0019032580995774697,
      "loss": 1.172,
      "step": 1252
    },
    {
      "epoch": 1.6706666666666665,
      "grad_norm": 0.05352315679192543,
      "learning_rate": 0.0019030727163844641,
      "loss": 1.2918,
      "step": 1253
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.03063308261334896,
      "learning_rate": 0.0019028871647858833,
      "loss": 1.1123,
      "step": 1254
    },
    {
      "epoch": 1.6733333333333333,
      "grad_norm": 0.078941710293293,
      "learning_rate": 0.0019027014448163295,
      "loss": 1.1963,
      "step": 1255
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 0.028724735602736473,
      "learning_rate": 0.0019025155565104352,
      "loss": 1.3837,
      "step": 1256
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 0.03329942002892494,
      "learning_rate": 0.0019023294999028653,
      "loss": 1.2551,
      "step": 1257
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 0.025968533009290695,
      "learning_rate": 0.001902143275028316,
      "loss": 1.3551,
      "step": 1258
    },
    {
      "epoch": 1.6786666666666665,
      "grad_norm": 0.02595193311572075,
      "learning_rate": 0.001901956881921514,
      "loss": 1.0747,
      "step": 1259
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.03346455469727516,
      "learning_rate": 0.0019017703206172186,
      "loss": 1.0206,
      "step": 1260
    },
    {
      "epoch": 1.6813333333333333,
      "grad_norm": 0.0357394702732563,
      "learning_rate": 0.0019015835911502196,
      "loss": 1.3811,
      "step": 1261
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.03815038874745369,
      "learning_rate": 0.0019013966935553385,
      "loss": 1.0889,
      "step": 1262
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 0.03146841749548912,
      "learning_rate": 0.001901209627867428,
      "loss": 1.0898,
      "step": 1263
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 0.028193963691592216,
      "learning_rate": 0.0019010223941213723,
      "loss": 1.1958,
      "step": 1264
    },
    {
      "epoch": 1.6866666666666665,
      "grad_norm": 0.033551715314388275,
      "learning_rate": 0.001900834992352087,
      "loss": 0.9596,
      "step": 1265
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.04016895219683647,
      "learning_rate": 0.0019006474225945187,
      "loss": 1.1662,
      "step": 1266
    },
    {
      "epoch": 1.6893333333333334,
      "grad_norm": 0.031781382858753204,
      "learning_rate": 0.0019004596848836454,
      "loss": 1.3564,
      "step": 1267
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 0.023324353620409966,
      "learning_rate": 0.001900271779254477,
      "loss": 1.1682,
      "step": 1268
    },
    {
      "epoch": 1.692,
      "grad_norm": 0.03504998981952667,
      "learning_rate": 0.0019000837057420539,
      "loss": 1.3668,
      "step": 1269
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.029398400336503983,
      "learning_rate": 0.0018998954643814484,
      "loss": 1.2265,
      "step": 1270
    },
    {
      "epoch": 1.6946666666666665,
      "grad_norm": 0.04528603330254555,
      "learning_rate": 0.0018997070552077635,
      "loss": 1.2289,
      "step": 1271
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.03444022312760353,
      "learning_rate": 0.0018995184782561343,
      "loss": 0.7848,
      "step": 1272
    },
    {
      "epoch": 1.6973333333333334,
      "grad_norm": 0.026356151327490807,
      "learning_rate": 0.0018993297335617263,
      "loss": 1.129,
      "step": 1273
    },
    {
      "epoch": 1.6986666666666665,
      "grad_norm": 0.039837796241045,
      "learning_rate": 0.0018991408211597371,
      "loss": 1.2663,
      "step": 1274
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.029256027191877365,
      "learning_rate": 0.0018989517410853954,
      "loss": 1.2285,
      "step": 1275
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 0.030302047729492188,
      "learning_rate": 0.0018987624933739604,
      "loss": 0.9946,
      "step": 1276
    },
    {
      "epoch": 1.7026666666666666,
      "grad_norm": 0.03322473168373108,
      "learning_rate": 0.0018985730780607237,
      "loss": 1.2188,
      "step": 1277
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.029829341918230057,
      "learning_rate": 0.0018983834951810069,
      "loss": 1.0724,
      "step": 1278
    },
    {
      "epoch": 1.7053333333333334,
      "grad_norm": 0.030723629519343376,
      "learning_rate": 0.0018981937447701638,
      "loss": 1.0318,
      "step": 1279
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.028177203610539436,
      "learning_rate": 0.0018980038268635794,
      "loss": 1.104,
      "step": 1280
    },
    {
      "epoch": 1.708,
      "grad_norm": 0.027784399688243866,
      "learning_rate": 0.0018978137414966698,
      "loss": 1.2739,
      "step": 1281
    },
    {
      "epoch": 1.7093333333333334,
      "grad_norm": 0.023871077224612236,
      "learning_rate": 0.001897623488704882,
      "loss": 1.2207,
      "step": 1282
    },
    {
      "epoch": 1.7106666666666666,
      "grad_norm": 0.033844079822301865,
      "learning_rate": 0.0018974330685236945,
      "loss": 1.2293,
      "step": 1283
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.052604105323553085,
      "learning_rate": 0.001897242480988617,
      "loss": 1.1445,
      "step": 1284
    },
    {
      "epoch": 1.7133333333333334,
      "grad_norm": 0.03092321753501892,
      "learning_rate": 0.00189705172613519,
      "loss": 1.1699,
      "step": 1285
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.03373512625694275,
      "learning_rate": 0.0018968608039989865,
      "loss": 1.2784,
      "step": 1286
    },
    {
      "epoch": 1.716,
      "grad_norm": 0.03810304030776024,
      "learning_rate": 0.001896669714615609,
      "loss": 1.2912,
      "step": 1287
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 0.035578105598688126,
      "learning_rate": 0.0018964784580206922,
      "loss": 1.0598,
      "step": 1288
    },
    {
      "epoch": 1.7186666666666666,
      "grad_norm": 0.03400229662656784,
      "learning_rate": 0.001896287034249902,
      "loss": 1.2402,
      "step": 1289
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.02596878819167614,
      "learning_rate": 0.0018960954433389346,
      "loss": 0.9581,
      "step": 1290
    },
    {
      "epoch": 1.7213333333333334,
      "grad_norm": 0.03508739173412323,
      "learning_rate": 0.0018959036853235186,
      "loss": 1.2146,
      "step": 1291
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 0.03211157023906708,
      "learning_rate": 0.0018957117602394129,
      "loss": 1.1856,
      "step": 1292
    },
    {
      "epoch": 1.724,
      "grad_norm": 0.031712885946035385,
      "learning_rate": 0.001895519668122408,
      "loss": 1.1832,
      "step": 1293
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.03060387633740902,
      "learning_rate": 0.001895327409008325,
      "loss": 1.1561,
      "step": 1294
    },
    {
      "epoch": 1.7266666666666666,
      "grad_norm": 0.026773180812597275,
      "learning_rate": 0.0018951349829330166,
      "loss": 1.0478,
      "step": 1295
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.03134344145655632,
      "learning_rate": 0.001894942389932367,
      "loss": 1.2289,
      "step": 1296
    },
    {
      "epoch": 1.7293333333333334,
      "grad_norm": 0.03330351412296295,
      "learning_rate": 0.0018947496300422903,
      "loss": 1.1096,
      "step": 1297
    },
    {
      "epoch": 1.7306666666666666,
      "grad_norm": 0.02788815274834633,
      "learning_rate": 0.0018945567032987332,
      "loss": 1.1562,
      "step": 1298
    },
    {
      "epoch": 1.732,
      "grad_norm": 0.03094436414539814,
      "learning_rate": 0.0018943636097376727,
      "loss": 1.1661,
      "step": 1299
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.04924730584025383,
      "learning_rate": 0.0018941703493951163,
      "loss": 1.2369,
      "step": 1300
    },
    {
      "epoch": 1.7346666666666666,
      "grad_norm": 0.034816328436136246,
      "learning_rate": 0.0018939769223071043,
      "loss": 1.0322,
      "step": 1301
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.0391564704477787,
      "learning_rate": 0.0018937833285097066,
      "loss": 1.3479,
      "step": 1302
    },
    {
      "epoch": 1.7373333333333334,
      "grad_norm": 0.032706983387470245,
      "learning_rate": 0.0018935895680390242,
      "loss": 1.0646,
      "step": 1303
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 0.03629806637763977,
      "learning_rate": 0.0018933956409311907,
      "loss": 0.7184,
      "step": 1304
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.036405935883522034,
      "learning_rate": 0.0018932015472223692,
      "loss": 1.0584,
      "step": 1305
    },
    {
      "epoch": 1.7413333333333334,
      "grad_norm": 0.041054125875234604,
      "learning_rate": 0.0018930072869487544,
      "loss": 1.2518,
      "step": 1306
    },
    {
      "epoch": 1.7426666666666666,
      "grad_norm": 0.03304671123623848,
      "learning_rate": 0.0018928128601465723,
      "loss": 1.254,
      "step": 1307
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.03576839715242386,
      "learning_rate": 0.0018926182668520793,
      "loss": 0.8299,
      "step": 1308
    },
    {
      "epoch": 1.7453333333333334,
      "grad_norm": 0.04643280431628227,
      "learning_rate": 0.0018924235071015637,
      "loss": 1.0034,
      "step": 1309
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.03677359223365784,
      "learning_rate": 0.0018922285809313442,
      "loss": 1.409,
      "step": 1310
    },
    {
      "epoch": 1.748,
      "grad_norm": 0.10000596940517426,
      "learning_rate": 0.0018920334883777707,
      "loss": 1.4515,
      "step": 1311
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 0.033641062676906586,
      "learning_rate": 0.0018918382294772246,
      "loss": 0.9019,
      "step": 1312
    },
    {
      "epoch": 1.7506666666666666,
      "grad_norm": 0.02857152372598648,
      "learning_rate": 0.0018916428042661177,
      "loss": 0.9846,
      "step": 1313
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.044123005121946335,
      "learning_rate": 0.0018914472127808928,
      "loss": 1.2375,
      "step": 1314
    },
    {
      "epoch": 1.7533333333333334,
      "grad_norm": 0.03313343971967697,
      "learning_rate": 0.0018912514550580243,
      "loss": 1.3124,
      "step": 1315
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 0.02761625498533249,
      "learning_rate": 0.001891055531134017,
      "loss": 1.2075,
      "step": 1316
    },
    {
      "epoch": 1.756,
      "grad_norm": 0.04096516966819763,
      "learning_rate": 0.0018908594410454067,
      "loss": 1.1835,
      "step": 1317
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.033655934035778046,
      "learning_rate": 0.0018906631848287605,
      "loss": 1.2573,
      "step": 1318
    },
    {
      "epoch": 1.7586666666666666,
      "grad_norm": 0.03689945116639137,
      "learning_rate": 0.0018904667625206767,
      "loss": 1.2539,
      "step": 1319
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.034489117562770844,
      "learning_rate": 0.0018902701741577842,
      "loss": 1.0822,
      "step": 1320
    },
    {
      "epoch": 1.7613333333333334,
      "grad_norm": 0.028308356180787086,
      "learning_rate": 0.0018900734197767424,
      "loss": 1.1201,
      "step": 1321
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 0.031972646713256836,
      "learning_rate": 0.001889876499414243,
      "loss": 1.3334,
      "step": 1322
    },
    {
      "epoch": 1.764,
      "grad_norm": 0.03217318281531334,
      "learning_rate": 0.0018896794131070072,
      "loss": 1.0036,
      "step": 1323
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 0.07792829722166061,
      "learning_rate": 0.001889482160891788,
      "loss": 0.9451,
      "step": 1324
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.04310810565948486,
      "learning_rate": 0.0018892847428053693,
      "loss": 1.1754,
      "step": 1325
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.03274562209844589,
      "learning_rate": 0.001889087158884565,
      "loss": 1.2035,
      "step": 1326
    },
    {
      "epoch": 1.7693333333333334,
      "grad_norm": 0.028197236359119415,
      "learning_rate": 0.0018888894091662217,
      "loss": 0.9115,
      "step": 1327
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 0.03760989382863045,
      "learning_rate": 0.0018886914936872153,
      "loss": 1.227,
      "step": 1328
    },
    {
      "epoch": 1.772,
      "grad_norm": 0.03211091458797455,
      "learning_rate": 0.0018884934124844533,
      "loss": 1.04,
      "step": 1329
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.04761089012026787,
      "learning_rate": 0.001888295165594874,
      "loss": 1.2624,
      "step": 1330
    },
    {
      "epoch": 1.7746666666666666,
      "grad_norm": 0.035953789949417114,
      "learning_rate": 0.001888096753055447,
      "loss": 1.2851,
      "step": 1331
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.12217722833156586,
      "learning_rate": 0.0018878981749031716,
      "loss": 1.2275,
      "step": 1332
    },
    {
      "epoch": 1.7773333333333334,
      "grad_norm": 0.027957692742347717,
      "learning_rate": 0.0018876994311750792,
      "loss": 1.1839,
      "step": 1333
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 0.030590467154979706,
      "learning_rate": 0.001887500521908232,
      "loss": 0.9701,
      "step": 1334
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.04305307939648628,
      "learning_rate": 0.0018873014471397222,
      "loss": 1.0544,
      "step": 1335
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 0.027654387056827545,
      "learning_rate": 0.0018871022069066737,
      "loss": 1.015,
      "step": 1336
    },
    {
      "epoch": 1.7826666666666666,
      "grad_norm": 0.031566549092531204,
      "learning_rate": 0.0018869028012462408,
      "loss": 1.1429,
      "step": 1337
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.03159976750612259,
      "learning_rate": 0.0018867032301956089,
      "loss": 0.9927,
      "step": 1338
    },
    {
      "epoch": 1.7853333333333334,
      "grad_norm": 0.04417363926768303,
      "learning_rate": 0.0018865034937919937,
      "loss": 1.2067,
      "step": 1339
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.027856070548295975,
      "learning_rate": 0.001886303592072643,
      "loss": 1.089,
      "step": 1340
    },
    {
      "epoch": 1.788,
      "grad_norm": 0.040572457015514374,
      "learning_rate": 0.0018861035250748342,
      "loss": 1.3645,
      "step": 1341
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.0314355194568634,
      "learning_rate": 0.0018859032928358755,
      "loss": 1.2355,
      "step": 1342
    },
    {
      "epoch": 1.7906666666666666,
      "grad_norm": 0.039292823523283005,
      "learning_rate": 0.0018857028953931068,
      "loss": 1.2262,
      "step": 1343
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.05921813100576401,
      "learning_rate": 0.0018855023327838984,
      "loss": 1.2787,
      "step": 1344
    },
    {
      "epoch": 1.7933333333333334,
      "grad_norm": 0.10428109019994736,
      "learning_rate": 0.001885301605045651,
      "loss": 0.9102,
      "step": 1345
    },
    {
      "epoch": 1.7946666666666666,
      "grad_norm": 0.029718823730945587,
      "learning_rate": 0.0018851007122157966,
      "loss": 1.0667,
      "step": 1346
    },
    {
      "epoch": 1.796,
      "grad_norm": 0.025827350094914436,
      "learning_rate": 0.001884899654331798,
      "loss": 1.2169,
      "step": 1347
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 0.02552994154393673,
      "learning_rate": 0.0018846984314311484,
      "loss": 0.9912,
      "step": 1348
    },
    {
      "epoch": 1.7986666666666666,
      "grad_norm": 0.040677592158317566,
      "learning_rate": 0.0018844970435513719,
      "loss": 1.076,
      "step": 1349
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.03254977613687515,
      "learning_rate": 0.0018842954907300237,
      "loss": 0.9481,
      "step": 1350
    },
    {
      "epoch": 1.8013333333333335,
      "grad_norm": 0.03555314615368843,
      "learning_rate": 0.0018840937730046892,
      "loss": 1.0393,
      "step": 1351
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 0.03102840669453144,
      "learning_rate": 0.001883891890412985,
      "loss": 1.0495,
      "step": 1352
    },
    {
      "epoch": 1.804,
      "grad_norm": 0.041418544948101044,
      "learning_rate": 0.0018836898429925584,
      "loss": 1.1912,
      "step": 1353
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 0.04032829776406288,
      "learning_rate": 0.0018834876307810876,
      "loss": 1.0502,
      "step": 1354
    },
    {
      "epoch": 1.8066666666666666,
      "grad_norm": 0.03677399829030037,
      "learning_rate": 0.0018832852538162804,
      "loss": 1.1741,
      "step": 1355
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.032436199486255646,
      "learning_rate": 0.0018830827121358769,
      "loss": 0.9785,
      "step": 1356
    },
    {
      "epoch": 1.8093333333333335,
      "grad_norm": 0.03219539299607277,
      "learning_rate": 0.001882880005777647,
      "loss": 1.0005,
      "step": 1357
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 0.05093623325228691,
      "learning_rate": 0.0018826771347793911,
      "loss": 0.9879,
      "step": 1358
    },
    {
      "epoch": 1.812,
      "grad_norm": 0.03988831490278244,
      "learning_rate": 0.0018824740991789416,
      "loss": 1.2959,
      "step": 1359
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.03173240274190903,
      "learning_rate": 0.00188227089901416,
      "loss": 1.1432,
      "step": 1360
    },
    {
      "epoch": 1.8146666666666667,
      "grad_norm": 0.053757261484861374,
      "learning_rate": 0.0018820675343229395,
      "loss": 1.0454,
      "step": 1361
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.031653717160224915,
      "learning_rate": 0.0018818640051432034,
      "loss": 1.076,
      "step": 1362
    },
    {
      "epoch": 1.8173333333333335,
      "grad_norm": 0.034455813467502594,
      "learning_rate": 0.0018816603115129062,
      "loss": 1.0076,
      "step": 1363
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 0.03756426274776459,
      "learning_rate": 0.001881456453470033,
      "loss": 1.3615,
      "step": 1364
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.03341095894575119,
      "learning_rate": 0.001881252431052599,
      "loss": 1.4133,
      "step": 1365
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.050927259027957916,
      "learning_rate": 0.0018810482442986503,
      "loss": 1.1797,
      "step": 1366
    },
    {
      "epoch": 1.8226666666666667,
      "grad_norm": 0.07957785576581955,
      "learning_rate": 0.0018808438932462642,
      "loss": 1.1293,
      "step": 1367
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.03390580043196678,
      "learning_rate": 0.0018806393779335483,
      "loss": 1.1763,
      "step": 1368
    },
    {
      "epoch": 1.8253333333333335,
      "grad_norm": 0.025584766641259193,
      "learning_rate": 0.0018804346983986402,
      "loss": 1.0189,
      "step": 1369
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.05006355792284012,
      "learning_rate": 0.0018802298546797091,
      "loss": 1.4497,
      "step": 1370
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 0.027011804282665253,
      "learning_rate": 0.0018800248468149544,
      "loss": 1.2305,
      "step": 1371
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 0.035984527319669724,
      "learning_rate": 0.0018798196748426056,
      "loss": 1.0747,
      "step": 1372
    },
    {
      "epoch": 1.8306666666666667,
      "grad_norm": 0.04567969590425491,
      "learning_rate": 0.0018796143388009239,
      "loss": 1.3167,
      "step": 1373
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.028441516682505608,
      "learning_rate": 0.0018794088387282,
      "loss": 1.1614,
      "step": 1374
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.029467375949025154,
      "learning_rate": 0.0018792031746627563,
      "loss": 0.9297,
      "step": 1375
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 0.03096092864871025,
      "learning_rate": 0.0018789973466429447,
      "loss": 1.3011,
      "step": 1376
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 0.04032488167285919,
      "learning_rate": 0.0018787913547071483,
      "loss": 1.405,
      "step": 1377
    },
    {
      "epoch": 1.8373333333333335,
      "grad_norm": 0.04002244397997856,
      "learning_rate": 0.0018785851988937802,
      "loss": 1.1646,
      "step": 1378
    },
    {
      "epoch": 1.8386666666666667,
      "grad_norm": 0.03360747918486595,
      "learning_rate": 0.001878378879241285,
      "loss": 1.1413,
      "step": 1379
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.03476371988654137,
      "learning_rate": 0.001878172395788137,
      "loss": 1.1269,
      "step": 1380
    },
    {
      "epoch": 1.8413333333333335,
      "grad_norm": 0.039157234132289886,
      "learning_rate": 0.001877965748572842,
      "loss": 1.2591,
      "step": 1381
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.04895796626806259,
      "learning_rate": 0.001877758937633935,
      "loss": 1.3431,
      "step": 1382
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 0.0792015865445137,
      "learning_rate": 0.001877551963009982,
      "loss": 1.1679,
      "step": 1383
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 0.03211560100317001,
      "learning_rate": 0.0018773448247395806,
      "loss": 1.3012,
      "step": 1384
    },
    {
      "epoch": 1.8466666666666667,
      "grad_norm": 0.034284643828868866,
      "learning_rate": 0.0018771375228613576,
      "loss": 1.2236,
      "step": 1385
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.027183273807168007,
      "learning_rate": 0.0018769300574139709,
      "loss": 0.9811,
      "step": 1386
    },
    {
      "epoch": 1.8493333333333335,
      "grad_norm": 0.030465709045529366,
      "learning_rate": 0.0018767224284361088,
      "loss": 1.3686,
      "step": 1387
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 0.04073521867394447,
      "learning_rate": 0.0018765146359664899,
      "loss": 1.0492,
      "step": 1388
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 0.030783599242568016,
      "learning_rate": 0.0018763066800438636,
      "loss": 1.1797,
      "step": 1389
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.030743855983018875,
      "learning_rate": 0.0018760985607070098,
      "loss": 1.0134,
      "step": 1390
    },
    {
      "epoch": 1.8546666666666667,
      "grad_norm": 0.031264662742614746,
      "learning_rate": 0.0018758902779947384,
      "loss": 0.9404,
      "step": 1391
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.03574087843298912,
      "learning_rate": 0.0018756818319458906,
      "loss": 1.315,
      "step": 1392
    },
    {
      "epoch": 1.8573333333333333,
      "grad_norm": 0.042973555624485016,
      "learning_rate": 0.0018754732225993369,
      "loss": 1.1153,
      "step": 1393
    },
    {
      "epoch": 1.8586666666666667,
      "grad_norm": 0.02912137471139431,
      "learning_rate": 0.0018752644499939795,
      "loss": 0.8428,
      "step": 1394
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.024553248658776283,
      "learning_rate": 0.00187505551416875,
      "loss": 1.2375,
      "step": 1395
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 0.031365782022476196,
      "learning_rate": 0.001874846415162611,
      "loss": 1.0577,
      "step": 1396
    },
    {
      "epoch": 1.8626666666666667,
      "grad_norm": 0.04311109706759453,
      "learning_rate": 0.0018746371530145556,
      "loss": 0.9856,
      "step": 1397
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.0397888720035553,
      "learning_rate": 0.001874427727763607,
      "loss": 1.2438,
      "step": 1398
    },
    {
      "epoch": 1.8653333333333333,
      "grad_norm": 0.03499394282698631,
      "learning_rate": 0.0018742181394488192,
      "loss": 1.1497,
      "step": 1399
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.030373457819223404,
      "learning_rate": 0.0018740083881092758,
      "loss": 1.4715,
      "step": 1400
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 0.030928129330277443,
      "learning_rate": 0.001873798473784092,
      "loss": 1.0604,
      "step": 1401
    },
    {
      "epoch": 1.8693333333333333,
      "grad_norm": 0.06468886137008667,
      "learning_rate": 0.0018735883965124121,
      "loss": 1.2356,
      "step": 1402
    },
    {
      "epoch": 1.8706666666666667,
      "grad_norm": 0.18336279690265656,
      "learning_rate": 0.0018733781563334121,
      "loss": 1.1076,
      "step": 1403
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.038702357560396194,
      "learning_rate": 0.0018731677532862975,
      "loss": 1.0531,
      "step": 1404
    },
    {
      "epoch": 1.8733333333333333,
      "grad_norm": 0.03170780465006828,
      "learning_rate": 0.001872957187410304,
      "loss": 1.1963,
      "step": 1405
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.03813354671001434,
      "learning_rate": 0.0018727464587446985,
      "loss": 1.079,
      "step": 1406
    },
    {
      "epoch": 1.876,
      "grad_norm": 0.03671356663107872,
      "learning_rate": 0.0018725355673287777,
      "loss": 0.8986,
      "step": 1407
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 0.03545207157731056,
      "learning_rate": 0.0018723245132018689,
      "loss": 0.9366,
      "step": 1408
    },
    {
      "epoch": 1.8786666666666667,
      "grad_norm": 0.15159215033054352,
      "learning_rate": 0.0018721132964033293,
      "loss": 1.4992,
      "step": 1409
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.033975109457969666,
      "learning_rate": 0.001871901916972547,
      "loss": 1.1877,
      "step": 1410
    },
    {
      "epoch": 1.8813333333333333,
      "grad_norm": 0.03172123432159424,
      "learning_rate": 0.00187169037494894,
      "loss": 0.9717,
      "step": 1411
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 0.030028928071260452,
      "learning_rate": 0.0018714786703719572,
      "loss": 1.1786,
      "step": 1412
    },
    {
      "epoch": 1.884,
      "grad_norm": 0.03153274953365326,
      "learning_rate": 0.0018712668032810768,
      "loss": 1.0732,
      "step": 1413
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.042484577745199203,
      "learning_rate": 0.001871054773715808,
      "loss": 1.1957,
      "step": 1414
    },
    {
      "epoch": 1.8866666666666667,
      "grad_norm": 0.040974248200654984,
      "learning_rate": 0.001870842581715691,
      "loss": 0.9419,
      "step": 1415
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.08846309781074524,
      "learning_rate": 0.0018706302273202942,
      "loss": 1.1518,
      "step": 1416
    },
    {
      "epoch": 1.8893333333333333,
      "grad_norm": 0.03399694710969925,
      "learning_rate": 0.0018704177105692186,
      "loss": 1.0013,
      "step": 1417
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 0.03126509487628937,
      "learning_rate": 0.0018702050315020941,
      "loss": 1.1241,
      "step": 1418
    },
    {
      "epoch": 1.892,
      "grad_norm": 0.028887374326586723,
      "learning_rate": 0.0018699921901585812,
      "loss": 1.4328,
      "step": 1419
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.038699761033058167,
      "learning_rate": 0.001869779186578371,
      "loss": 1.0091,
      "step": 1420
    },
    {
      "epoch": 1.8946666666666667,
      "grad_norm": 0.04095184803009033,
      "learning_rate": 0.0018695660208011841,
      "loss": 1.2887,
      "step": 1421
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.04157808795571327,
      "learning_rate": 0.0018693526928667721,
      "loss": 1.2746,
      "step": 1422
    },
    {
      "epoch": 1.8973333333333333,
      "grad_norm": 0.10388131439685822,
      "learning_rate": 0.0018691392028149163,
      "loss": 1.3753,
      "step": 1423
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 0.03579718992114067,
      "learning_rate": 0.0018689255506854287,
      "loss": 1.0806,
      "step": 1424
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.026865215972065926,
      "learning_rate": 0.0018687117365181513,
      "loss": 1.0447,
      "step": 1425
    },
    {
      "epoch": 1.9013333333333333,
      "grad_norm": 0.030520541593432426,
      "learning_rate": 0.0018684977603529557,
      "loss": 0.9882,
      "step": 1426
    },
    {
      "epoch": 1.9026666666666667,
      "grad_norm": 0.027295174077153206,
      "learning_rate": 0.001868283622229745,
      "loss": 1.2894,
      "step": 1427
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.029721692204475403,
      "learning_rate": 0.0018680693221884517,
      "loss": 1.016,
      "step": 1428
    },
    {
      "epoch": 1.9053333333333333,
      "grad_norm": 0.02969093807041645,
      "learning_rate": 0.0018678548602690387,
      "loss": 1.0913,
      "step": 1429
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.034553758800029755,
      "learning_rate": 0.0018676402365114982,
      "loss": 1.2696,
      "step": 1430
    },
    {
      "epoch": 1.908,
      "grad_norm": 0.03832172602415085,
      "learning_rate": 0.0018674254509558543,
      "loss": 1.0985,
      "step": 1431
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 0.03448032587766647,
      "learning_rate": 0.00186721050364216,
      "loss": 1.3501,
      "step": 1432
    },
    {
      "epoch": 1.9106666666666667,
      "grad_norm": 0.04191401228308678,
      "learning_rate": 0.001866995394610499,
      "loss": 1.2344,
      "step": 1433
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.03059355355799198,
      "learning_rate": 0.0018667801239009845,
      "loss": 1.274,
      "step": 1434
    },
    {
      "epoch": 1.9133333333333333,
      "grad_norm": 0.03467783331871033,
      "learning_rate": 0.001866564691553761,
      "loss": 0.9425,
      "step": 1435
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 0.027513697743415833,
      "learning_rate": 0.0018663490976090016,
      "loss": 1.0682,
      "step": 1436
    },
    {
      "epoch": 1.916,
      "grad_norm": 0.026467466726899147,
      "learning_rate": 0.001866133342106911,
      "loss": 1.0783,
      "step": 1437
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.047839466482400894,
      "learning_rate": 0.0018659174250877236,
      "loss": 1.4764,
      "step": 1438
    },
    {
      "epoch": 1.9186666666666667,
      "grad_norm": 0.025585174560546875,
      "learning_rate": 0.0018657013465917032,
      "loss": 1.1768,
      "step": 1439
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.04048372432589531,
      "learning_rate": 0.0018654851066591447,
      "loss": 1.221,
      "step": 1440
    },
    {
      "epoch": 1.9213333333333333,
      "grad_norm": 0.029877396300435066,
      "learning_rate": 0.0018652687053303722,
      "loss": 1.2758,
      "step": 1441
    },
    {
      "epoch": 1.9226666666666667,
      "grad_norm": 0.038539037108421326,
      "learning_rate": 0.0018650521426457406,
      "loss": 1.1292,
      "step": 1442
    },
    {
      "epoch": 1.924,
      "grad_norm": 0.03709743171930313,
      "learning_rate": 0.0018648354186456349,
      "loss": 0.914,
      "step": 1443
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 0.033556193113327026,
      "learning_rate": 0.0018646185333704695,
      "loss": 1.018,
      "step": 1444
    },
    {
      "epoch": 1.9266666666666667,
      "grad_norm": 0.03646198660135269,
      "learning_rate": 0.0018644014868606895,
      "loss": 1.1389,
      "step": 1445
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.029426690191030502,
      "learning_rate": 0.00186418427915677,
      "loss": 1.0026,
      "step": 1446
    },
    {
      "epoch": 1.9293333333333333,
      "grad_norm": 0.029403401538729668,
      "learning_rate": 0.0018639669102992159,
      "loss": 1.1398,
      "step": 1447
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 0.040749043226242065,
      "learning_rate": 0.001863749380328562,
      "loss": 1.2134,
      "step": 1448
    },
    {
      "epoch": 1.932,
      "grad_norm": 0.04176736995577812,
      "learning_rate": 0.0018635316892853739,
      "loss": 1.0495,
      "step": 1449
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.03915014490485191,
      "learning_rate": 0.0018633138372102468,
      "loss": 1.4383,
      "step": 1450
    },
    {
      "epoch": 1.9346666666666668,
      "grad_norm": 0.035090718418359756,
      "learning_rate": 0.0018630958241438052,
      "loss": 1.0151,
      "step": 1451
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.04583970457315445,
      "learning_rate": 0.001862877650126705,
      "loss": 1.4779,
      "step": 1452
    },
    {
      "epoch": 1.9373333333333334,
      "grad_norm": 0.03524994105100632,
      "learning_rate": 0.0018626593151996314,
      "loss": 0.9529,
      "step": 1453
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.02554062008857727,
      "learning_rate": 0.0018624408194032993,
      "loss": 1.1822,
      "step": 1454
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.037093088030815125,
      "learning_rate": 0.0018622221627784539,
      "loss": 0.9309,
      "step": 1455
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 0.034735362976789474,
      "learning_rate": 0.001862003345365871,
      "loss": 0.9868,
      "step": 1456
    },
    {
      "epoch": 1.9426666666666668,
      "grad_norm": 0.034348879009485245,
      "learning_rate": 0.0018617843672063552,
      "loss": 0.9333,
      "step": 1457
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.05417470633983612,
      "learning_rate": 0.0018615652283407419,
      "loss": 1.2609,
      "step": 1458
    },
    {
      "epoch": 1.9453333333333334,
      "grad_norm": 0.039587054401636124,
      "learning_rate": 0.0018613459288098961,
      "loss": 1.2004,
      "step": 1459
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.0480683334171772,
      "learning_rate": 0.0018611264686547134,
      "loss": 1.0641,
      "step": 1460
    },
    {
      "epoch": 1.948,
      "grad_norm": 0.07752077281475067,
      "learning_rate": 0.0018609068479161182,
      "loss": 1.2008,
      "step": 1461
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 0.028380312025547028,
      "learning_rate": 0.0018606870666350661,
      "loss": 0.9766,
      "step": 1462
    },
    {
      "epoch": 1.9506666666666668,
      "grad_norm": 0.029873916879296303,
      "learning_rate": 0.0018604671248525417,
      "loss": 0.8542,
      "step": 1463
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.03763354942202568,
      "learning_rate": 0.0018602470226095603,
      "loss": 1.0896,
      "step": 1464
    },
    {
      "epoch": 1.9533333333333334,
      "grad_norm": 0.04448999464511871,
      "learning_rate": 0.001860026759947166,
      "loss": 1.2345,
      "step": 1465
    },
    {
      "epoch": 1.9546666666666668,
      "grad_norm": 0.032382331788539886,
      "learning_rate": 0.0018598063369064343,
      "loss": 0.9043,
      "step": 1466
    },
    {
      "epoch": 1.956,
      "grad_norm": 0.037869833409786224,
      "learning_rate": 0.001859585753528469,
      "loss": 0.9883,
      "step": 1467
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 0.036326002329587936,
      "learning_rate": 0.0018593650098544052,
      "loss": 0.9629,
      "step": 1468
    },
    {
      "epoch": 1.9586666666666668,
      "grad_norm": 0.03744564950466156,
      "learning_rate": 0.0018591441059254074,
      "loss": 1.2657,
      "step": 1469
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.02835443802177906,
      "learning_rate": 0.0018589230417826697,
      "loss": 1.2481,
      "step": 1470
    },
    {
      "epoch": 1.9613333333333334,
      "grad_norm": 0.20039622485637665,
      "learning_rate": 0.0018587018174674164,
      "loss": 1.3225,
      "step": 1471
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 0.03728345036506653,
      "learning_rate": 0.001858480433020901,
      "loss": 1.1595,
      "step": 1472
    },
    {
      "epoch": 1.964,
      "grad_norm": 0.03157305717468262,
      "learning_rate": 0.0018582588884844084,
      "loss": 1.2043,
      "step": 1473
    },
    {
      "epoch": 1.9653333333333334,
      "grad_norm": 0.22094811499118805,
      "learning_rate": 0.0018580371838992514,
      "loss": 1.0541,
      "step": 1474
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.10128224641084671,
      "learning_rate": 0.0018578153193067745,
      "loss": 1.1698,
      "step": 1475
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.1280946284532547,
      "learning_rate": 0.0018575932947483503,
      "loss": 1.2659,
      "step": 1476
    },
    {
      "epoch": 1.9693333333333334,
      "grad_norm": 0.09836617857217789,
      "learning_rate": 0.0018573711102653824,
      "loss": 1.0374,
      "step": 1477
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.031240783631801605,
      "learning_rate": 0.0018571487658993041,
      "loss": 1.0866,
      "step": 1478
    },
    {
      "epoch": 1.972,
      "grad_norm": 0.12695123255252838,
      "learning_rate": 0.0018569262616915782,
      "loss": 1.5457,
      "step": 1479
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.04127512127161026,
      "learning_rate": 0.0018567035976836974,
      "loss": 0.9624,
      "step": 1480
    },
    {
      "epoch": 1.9746666666666668,
      "grad_norm": 0.039520107209682465,
      "learning_rate": 0.0018564807739171842,
      "loss": 1.0489,
      "step": 1481
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.04577441141009331,
      "learning_rate": 0.001856257790433591,
      "loss": 1.0165,
      "step": 1482
    },
    {
      "epoch": 1.9773333333333334,
      "grad_norm": 0.08957981318235397,
      "learning_rate": 0.0018560346472745,
      "loss": 1.4289,
      "step": 1483
    },
    {
      "epoch": 1.9786666666666668,
      "grad_norm": 0.043472450226545334,
      "learning_rate": 0.001855811344481523,
      "loss": 1.1109,
      "step": 1484
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.07138729095458984,
      "learning_rate": 0.0018555878820963013,
      "loss": 0.9984,
      "step": 1485
    },
    {
      "epoch": 1.9813333333333332,
      "grad_norm": 0.045047372579574585,
      "learning_rate": 0.0018553642601605068,
      "loss": 1.1115,
      "step": 1486
    },
    {
      "epoch": 1.9826666666666668,
      "grad_norm": 0.03237010911107063,
      "learning_rate": 0.0018551404787158403,
      "loss": 1.008,
      "step": 1487
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.055632084608078,
      "learning_rate": 0.0018549165378040327,
      "loss": 1.3125,
      "step": 1488
    },
    {
      "epoch": 1.9853333333333332,
      "grad_norm": 0.09588129818439484,
      "learning_rate": 0.001854692437466845,
      "loss": 1.2365,
      "step": 1489
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.03002503141760826,
      "learning_rate": 0.0018544681777460674,
      "loss": 1.1224,
      "step": 1490
    },
    {
      "epoch": 1.988,
      "grad_norm": 0.04386824369430542,
      "learning_rate": 0.00185424375868352,
      "loss": 1.2378,
      "step": 1491
    },
    {
      "epoch": 1.9893333333333332,
      "grad_norm": 0.02813909202814102,
      "learning_rate": 0.001854019180321053,
      "loss": 1.1875,
      "step": 1492
    },
    {
      "epoch": 1.9906666666666668,
      "grad_norm": 0.03873402252793312,
      "learning_rate": 0.0018537944427005448,
      "loss": 1.1552,
      "step": 1493
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.037198666483163834,
      "learning_rate": 0.0018535695458639055,
      "loss": 1.1678,
      "step": 1494
    },
    {
      "epoch": 1.9933333333333332,
      "grad_norm": 0.031215107068419456,
      "learning_rate": 0.001853344489853074,
      "loss": 1.2746,
      "step": 1495
    },
    {
      "epoch": 1.9946666666666668,
      "grad_norm": 0.03172533959150314,
      "learning_rate": 0.0018531192747100185,
      "loss": 1.0674,
      "step": 1496
    },
    {
      "epoch": 1.996,
      "grad_norm": 0.06409084051847458,
      "learning_rate": 0.0018528939004767377,
      "loss": 1.2441,
      "step": 1497
    },
    {
      "epoch": 1.9973333333333332,
      "grad_norm": 0.05110764876008034,
      "learning_rate": 0.001852668367195259,
      "loss": 1.3827,
      "step": 1498
    },
    {
      "epoch": 1.9986666666666668,
      "grad_norm": 0.0384422168135643,
      "learning_rate": 0.0018524426749076406,
      "loss": 1.0968,
      "step": 1499
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.03537039831280708,
      "learning_rate": 0.0018522168236559692,
      "loss": 1.4937,
      "step": 1500
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.1484078168869019,
      "eval_runtime": 24.0374,
      "eval_samples_per_second": 20.801,
      "eval_steps_per_second": 2.621,
      "step": 1500
    },
    {
      "epoch": 2.001333333333333,
      "grad_norm": 0.03206050395965576,
      "learning_rate": 0.0018519908134823622,
      "loss": 1.4192,
      "step": 1501
    },
    {
      "epoch": 2.002666666666667,
      "grad_norm": 0.03889770433306694,
      "learning_rate": 0.0018517646444289656,
      "loss": 1.2112,
      "step": 1502
    },
    {
      "epoch": 2.004,
      "grad_norm": 0.03255879878997803,
      "learning_rate": 0.001851538316537956,
      "loss": 1.244,
      "step": 1503
    },
    {
      "epoch": 2.005333333333333,
      "grad_norm": 0.03734751418232918,
      "learning_rate": 0.0018513118298515386,
      "loss": 1.1316,
      "step": 1504
    },
    {
      "epoch": 2.006666666666667,
      "grad_norm": 0.042906418442726135,
      "learning_rate": 0.0018510851844119494,
      "loss": 1.0432,
      "step": 1505
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.033721111714839935,
      "learning_rate": 0.0018508583802614531,
      "loss": 1.0901,
      "step": 1506
    },
    {
      "epoch": 2.009333333333333,
      "grad_norm": 0.030224928632378578,
      "learning_rate": 0.0018506314174423443,
      "loss": 0.9542,
      "step": 1507
    },
    {
      "epoch": 2.010666666666667,
      "grad_norm": 0.028082944452762604,
      "learning_rate": 0.0018504042959969472,
      "loss": 1.1559,
      "step": 1508
    },
    {
      "epoch": 2.012,
      "grad_norm": 0.04331367835402489,
      "learning_rate": 0.0018501770159676154,
      "loss": 1.121,
      "step": 1509
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.03412313014268875,
      "learning_rate": 0.0018499495773967325,
      "loss": 0.9142,
      "step": 1510
    },
    {
      "epoch": 2.014666666666667,
      "grad_norm": 0.04463547468185425,
      "learning_rate": 0.0018497219803267112,
      "loss": 1.1781,
      "step": 1511
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.04788892716169357,
      "learning_rate": 0.0018494942247999939,
      "loss": 1.2157,
      "step": 1512
    },
    {
      "epoch": 2.017333333333333,
      "grad_norm": 0.04154293239116669,
      "learning_rate": 0.001849266310859053,
      "loss": 1.2934,
      "step": 1513
    },
    {
      "epoch": 2.018666666666667,
      "grad_norm": 0.027358273044228554,
      "learning_rate": 0.0018490382385463894,
      "loss": 1.088,
      "step": 1514
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.025253716856241226,
      "learning_rate": 0.0018488100079045344,
      "loss": 0.9831,
      "step": 1515
    },
    {
      "epoch": 2.021333333333333,
      "grad_norm": 0.033655378967523575,
      "learning_rate": 0.001848581618976049,
      "loss": 1.1799,
      "step": 1516
    },
    {
      "epoch": 2.022666666666667,
      "grad_norm": 0.0278474148362875,
      "learning_rate": 0.0018483530718035228,
      "loss": 0.9627,
      "step": 1517
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.02975114807486534,
      "learning_rate": 0.001848124366429576,
      "loss": 1.4675,
      "step": 1518
    },
    {
      "epoch": 2.025333333333333,
      "grad_norm": 0.030529076233506203,
      "learning_rate": 0.001847895502896857,
      "loss": 1.0013,
      "step": 1519
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.029047828167676926,
      "learning_rate": 0.0018476664812480446,
      "loss": 1.0786,
      "step": 1520
    },
    {
      "epoch": 2.028,
      "grad_norm": 0.03170185908675194,
      "learning_rate": 0.0018474373015258472,
      "loss": 1.2356,
      "step": 1521
    },
    {
      "epoch": 2.029333333333333,
      "grad_norm": 0.041586920619010925,
      "learning_rate": 0.0018472079637730024,
      "loss": 1.1554,
      "step": 1522
    },
    {
      "epoch": 2.030666666666667,
      "grad_norm": 0.042564209550619125,
      "learning_rate": 0.001846978468032277,
      "loss": 1.0688,
      "step": 1523
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.03285310044884682,
      "learning_rate": 0.001846748814346468,
      "loss": 1.1444,
      "step": 1524
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 0.08501674979925156,
      "learning_rate": 0.0018465190027584005,
      "loss": 1.1888,
      "step": 1525
    },
    {
      "epoch": 2.034666666666667,
      "grad_norm": 0.03264405205845833,
      "learning_rate": 0.0018462890333109303,
      "loss": 1.1598,
      "step": 1526
    },
    {
      "epoch": 2.036,
      "grad_norm": 0.04657170549035072,
      "learning_rate": 0.0018460589060469426,
      "loss": 1.0256,
      "step": 1527
    },
    {
      "epoch": 2.037333333333333,
      "grad_norm": 0.03040378727018833,
      "learning_rate": 0.0018458286210093512,
      "loss": 1.1372,
      "step": 1528
    },
    {
      "epoch": 2.038666666666667,
      "grad_norm": 0.030029231682419777,
      "learning_rate": 0.0018455981782411004,
      "loss": 1.1687,
      "step": 1529
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.043281398713588715,
      "learning_rate": 0.0018453675777851627,
      "loss": 0.9563,
      "step": 1530
    },
    {
      "epoch": 2.041333333333333,
      "grad_norm": 0.033876340836286545,
      "learning_rate": 0.0018451368196845408,
      "loss": 1.1639,
      "step": 1531
    },
    {
      "epoch": 2.042666666666667,
      "grad_norm": 0.05470160394906998,
      "learning_rate": 0.0018449059039822666,
      "loss": 1.4458,
      "step": 1532
    },
    {
      "epoch": 2.044,
      "grad_norm": 0.0533413402736187,
      "learning_rate": 0.0018446748307214017,
      "loss": 1.1314,
      "step": 1533
    },
    {
      "epoch": 2.0453333333333332,
      "grad_norm": 0.03326975181698799,
      "learning_rate": 0.0018444435999450364,
      "loss": 1.0742,
      "step": 1534
    },
    {
      "epoch": 2.046666666666667,
      "grad_norm": 0.028921373188495636,
      "learning_rate": 0.001844212211696291,
      "loss": 1.3089,
      "step": 1535
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.035996608436107635,
      "learning_rate": 0.001843980666018315,
      "loss": 1.2341,
      "step": 1536
    },
    {
      "epoch": 2.0493333333333332,
      "grad_norm": 0.04420936852693558,
      "learning_rate": 0.0018437489629542867,
      "loss": 1.1105,
      "step": 1537
    },
    {
      "epoch": 2.050666666666667,
      "grad_norm": 0.04404274746775627,
      "learning_rate": 0.0018435171025474148,
      "loss": 1.3581,
      "step": 1538
    },
    {
      "epoch": 2.052,
      "grad_norm": 0.033280953764915466,
      "learning_rate": 0.0018432850848409364,
      "loss": 1.1615,
      "step": 1539
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.05460378900170326,
      "learning_rate": 0.0018430529098781188,
      "loss": 1.2031,
      "step": 1540
    },
    {
      "epoch": 2.054666666666667,
      "grad_norm": 0.043018683791160583,
      "learning_rate": 0.0018428205777022573,
      "loss": 1.1804,
      "step": 1541
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.046292755752801895,
      "learning_rate": 0.0018425880883566781,
      "loss": 1.1582,
      "step": 1542
    },
    {
      "epoch": 2.0573333333333332,
      "grad_norm": 0.03400758281350136,
      "learning_rate": 0.001842355441884736,
      "loss": 1.1214,
      "step": 1543
    },
    {
      "epoch": 2.058666666666667,
      "grad_norm": 0.026490231975913048,
      "learning_rate": 0.0018421226383298142,
      "loss": 1.1327,
      "step": 1544
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.03161914274096489,
      "learning_rate": 0.001841889677735327,
      "loss": 1.1238,
      "step": 1545
    },
    {
      "epoch": 2.0613333333333332,
      "grad_norm": 0.02715485915541649,
      "learning_rate": 0.0018416565601447165,
      "loss": 1.2562,
      "step": 1546
    },
    {
      "epoch": 2.062666666666667,
      "grad_norm": 0.02938251756131649,
      "learning_rate": 0.001841423285601455,
      "loss": 1.2371,
      "step": 1547
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.036058422178030014,
      "learning_rate": 0.0018411898541490432,
      "loss": 0.9246,
      "step": 1548
    },
    {
      "epoch": 2.0653333333333332,
      "grad_norm": 0.03724882751703262,
      "learning_rate": 0.0018409562658310119,
      "loss": 0.9194,
      "step": 1549
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.034624651074409485,
      "learning_rate": 0.0018407225206909209,
      "loss": 1.1931,
      "step": 1550
    },
    {
      "epoch": 2.068,
      "grad_norm": 0.04064387455582619,
      "learning_rate": 0.0018404886187723588,
      "loss": 1.182,
      "step": 1551
    },
    {
      "epoch": 2.0693333333333332,
      "grad_norm": 0.03459921106696129,
      "learning_rate": 0.001840254560118944,
      "loss": 1.0921,
      "step": 1552
    },
    {
      "epoch": 2.070666666666667,
      "grad_norm": 0.04496217891573906,
      "learning_rate": 0.0018400203447743236,
      "loss": 1.1375,
      "step": 1553
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.02141891047358513,
      "learning_rate": 0.0018397859727821747,
      "loss": 1.0674,
      "step": 1554
    },
    {
      "epoch": 2.0733333333333333,
      "grad_norm": 0.046565279364585876,
      "learning_rate": 0.0018395514441862026,
      "loss": 0.9269,
      "step": 1555
    },
    {
      "epoch": 2.074666666666667,
      "grad_norm": 0.027784891426563263,
      "learning_rate": 0.001839316759030143,
      "loss": 1.0234,
      "step": 1556
    },
    {
      "epoch": 2.076,
      "grad_norm": 0.026486964896321297,
      "learning_rate": 0.0018390819173577599,
      "loss": 0.8707,
      "step": 1557
    },
    {
      "epoch": 2.0773333333333333,
      "grad_norm": 0.03265489265322685,
      "learning_rate": 0.0018388469192128461,
      "loss": 1.2177,
      "step": 1558
    },
    {
      "epoch": 2.078666666666667,
      "grad_norm": 0.04334312304854393,
      "learning_rate": 0.0018386117646392252,
      "loss": 1.0346,
      "step": 1559
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.03962106257677078,
      "learning_rate": 0.0018383764536807484,
      "loss": 0.9807,
      "step": 1560
    },
    {
      "epoch": 2.0813333333333333,
      "grad_norm": 0.03201364725828171,
      "learning_rate": 0.0018381409863812963,
      "loss": 1.2228,
      "step": 1561
    },
    {
      "epoch": 2.0826666666666664,
      "grad_norm": 0.02751343324780464,
      "learning_rate": 0.0018379053627847798,
      "loss": 1.2541,
      "step": 1562
    },
    {
      "epoch": 2.084,
      "grad_norm": 0.03017442300915718,
      "learning_rate": 0.0018376695829351378,
      "loss": 0.9563,
      "step": 1563
    },
    {
      "epoch": 2.0853333333333333,
      "grad_norm": 0.040118440985679626,
      "learning_rate": 0.0018374336468763384,
      "loss": 1.0588,
      "step": 1564
    },
    {
      "epoch": 2.086666666666667,
      "grad_norm": 0.03816062957048416,
      "learning_rate": 0.0018371975546523794,
      "loss": 1.1888,
      "step": 1565
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.03415035456418991,
      "learning_rate": 0.0018369613063072875,
      "loss": 0.9378,
      "step": 1566
    },
    {
      "epoch": 2.0893333333333333,
      "grad_norm": 0.028993602842092514,
      "learning_rate": 0.0018367249018851179,
      "loss": 1.0204,
      "step": 1567
    },
    {
      "epoch": 2.0906666666666665,
      "grad_norm": 0.042814262211322784,
      "learning_rate": 0.0018364883414299564,
      "loss": 1.3079,
      "step": 1568
    },
    {
      "epoch": 2.092,
      "grad_norm": 0.034047871828079224,
      "learning_rate": 0.0018362516249859162,
      "loss": 1.2425,
      "step": 1569
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.04307588189840317,
      "learning_rate": 0.0018360147525971402,
      "loss": 1.1048,
      "step": 1570
    },
    {
      "epoch": 2.0946666666666665,
      "grad_norm": 0.035420097410678864,
      "learning_rate": 0.0018357777243078015,
      "loss": 1.0951,
      "step": 1571
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.030951665714383125,
      "learning_rate": 0.0018355405401621002,
      "loss": 1.0475,
      "step": 1572
    },
    {
      "epoch": 2.0973333333333333,
      "grad_norm": 0.03390814736485481,
      "learning_rate": 0.0018353032002042672,
      "loss": 1.0997,
      "step": 1573
    },
    {
      "epoch": 2.0986666666666665,
      "grad_norm": 0.04838145524263382,
      "learning_rate": 0.0018350657044785613,
      "loss": 1.4146,
      "step": 1574
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.031868401914834976,
      "learning_rate": 0.0018348280530292712,
      "loss": 1.1362,
      "step": 1575
    },
    {
      "epoch": 2.1013333333333333,
      "grad_norm": 0.03241406008601189,
      "learning_rate": 0.0018345902459007144,
      "loss": 1.1548,
      "step": 1576
    },
    {
      "epoch": 2.1026666666666665,
      "grad_norm": 0.031007323414087296,
      "learning_rate": 0.001834352283137237,
      "loss": 0.9932,
      "step": 1577
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.031687408685684204,
      "learning_rate": 0.0018341141647832146,
      "loss": 0.8802,
      "step": 1578
    },
    {
      "epoch": 2.1053333333333333,
      "grad_norm": 0.034072622656822205,
      "learning_rate": 0.001833875890883052,
      "loss": 1.092,
      "step": 1579
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.030009085312485695,
      "learning_rate": 0.001833637461481182,
      "loss": 1.061,
      "step": 1580
    },
    {
      "epoch": 2.108,
      "grad_norm": 0.03078029304742813,
      "learning_rate": 0.0018333988766220674,
      "loss": 1.3165,
      "step": 1581
    },
    {
      "epoch": 2.1093333333333333,
      "grad_norm": 0.03436454012989998,
      "learning_rate": 0.0018331601363502,
      "loss": 1.392,
      "step": 1582
    },
    {
      "epoch": 2.1106666666666665,
      "grad_norm": 0.04147268086671829,
      "learning_rate": 0.0018329212407100994,
      "loss": 0.8812,
      "step": 1583
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.031422559171915054,
      "learning_rate": 0.0018326821897463159,
      "loss": 1.0731,
      "step": 1584
    },
    {
      "epoch": 2.1133333333333333,
      "grad_norm": 0.03252732753753662,
      "learning_rate": 0.0018324429835034275,
      "loss": 1.1624,
      "step": 1585
    },
    {
      "epoch": 2.1146666666666665,
      "grad_norm": 0.046844761818647385,
      "learning_rate": 0.0018322036220260413,
      "loss": 0.8959,
      "step": 1586
    },
    {
      "epoch": 2.116,
      "grad_norm": 0.045018069446086884,
      "learning_rate": 0.001831964105358794,
      "loss": 1.0941,
      "step": 1587
    },
    {
      "epoch": 2.1173333333333333,
      "grad_norm": 0.04557160288095474,
      "learning_rate": 0.0018317244335463504,
      "loss": 0.9714,
      "step": 1588
    },
    {
      "epoch": 2.1186666666666665,
      "grad_norm": 0.03206801041960716,
      "learning_rate": 0.001831484606633405,
      "loss": 0.9409,
      "step": 1589
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.036468785256147385,
      "learning_rate": 0.0018312446246646808,
      "loss": 1.1421,
      "step": 1590
    },
    {
      "epoch": 2.1213333333333333,
      "grad_norm": 0.03275672346353531,
      "learning_rate": 0.0018310044876849293,
      "loss": 0.9643,
      "step": 1591
    },
    {
      "epoch": 2.1226666666666665,
      "grad_norm": 0.026826292276382446,
      "learning_rate": 0.0018307641957389326,
      "loss": 0.9619,
      "step": 1592
    },
    {
      "epoch": 2.124,
      "grad_norm": 0.05341413617134094,
      "learning_rate": 0.0018305237488714994,
      "loss": 1.3254,
      "step": 1593
    },
    {
      "epoch": 2.1253333333333333,
      "grad_norm": 0.031641166657209396,
      "learning_rate": 0.0018302831471274687,
      "loss": 0.9655,
      "step": 1594
    },
    {
      "epoch": 2.1266666666666665,
      "grad_norm": 0.03449874371290207,
      "learning_rate": 0.0018300423905517078,
      "loss": 0.9793,
      "step": 1595
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.034817103296518326,
      "learning_rate": 0.0018298014791891138,
      "loss": 1.319,
      "step": 1596
    },
    {
      "epoch": 2.1293333333333333,
      "grad_norm": 0.040291670709848404,
      "learning_rate": 0.0018295604130846113,
      "loss": 1.2,
      "step": 1597
    },
    {
      "epoch": 2.1306666666666665,
      "grad_norm": 0.04754216596484184,
      "learning_rate": 0.0018293191922831552,
      "loss": 1.1775,
      "step": 1598
    },
    {
      "epoch": 2.132,
      "grad_norm": 0.028905445709824562,
      "learning_rate": 0.0018290778168297277,
      "loss": 1.1431,
      "step": 1599
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.029329994693398476,
      "learning_rate": 0.0018288362867693413,
      "loss": 0.894,
      "step": 1600
    },
    {
      "epoch": 2.1346666666666665,
      "grad_norm": 0.03736545890569687,
      "learning_rate": 0.0018285946021470362,
      "loss": 1.3753,
      "step": 1601
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.19068540632724762,
      "learning_rate": 0.0018283527630078825,
      "loss": 1.3406,
      "step": 1602
    },
    {
      "epoch": 2.1373333333333333,
      "grad_norm": 0.03317524120211601,
      "learning_rate": 0.0018281107693969777,
      "loss": 1.1353,
      "step": 1603
    },
    {
      "epoch": 2.1386666666666665,
      "grad_norm": 0.029504137113690376,
      "learning_rate": 0.0018278686213594499,
      "loss": 1.3085,
      "step": 1604
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.04874732345342636,
      "learning_rate": 0.001827626318940454,
      "loss": 1.1679,
      "step": 1605
    },
    {
      "epoch": 2.1413333333333333,
      "grad_norm": 0.04854116216301918,
      "learning_rate": 0.0018273838621851756,
      "loss": 1.345,
      "step": 1606
    },
    {
      "epoch": 2.1426666666666665,
      "grad_norm": 0.04465069994330406,
      "learning_rate": 0.0018271412511388272,
      "loss": 1.1151,
      "step": 1607
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.07977046072483063,
      "learning_rate": 0.0018268984858466522,
      "loss": 1.2119,
      "step": 1608
    },
    {
      "epoch": 2.1453333333333333,
      "grad_norm": 0.10774002969264984,
      "learning_rate": 0.0018266555663539207,
      "loss": 1.0146,
      "step": 1609
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.05182772874832153,
      "learning_rate": 0.001826412492705933,
      "loss": 1.3048,
      "step": 1610
    },
    {
      "epoch": 2.148,
      "grad_norm": 0.036410458385944366,
      "learning_rate": 0.0018261692649480174,
      "loss": 1.2641,
      "step": 1611
    },
    {
      "epoch": 2.1493333333333333,
      "grad_norm": 0.05257286876440048,
      "learning_rate": 0.0018259258831255316,
      "loss": 0.8887,
      "step": 1612
    },
    {
      "epoch": 2.1506666666666665,
      "grad_norm": 0.05909044295549393,
      "learning_rate": 0.0018256823472838609,
      "loss": 1.23,
      "step": 1613
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.04131048917770386,
      "learning_rate": 0.0018254386574684206,
      "loss": 1.0294,
      "step": 1614
    },
    {
      "epoch": 2.1533333333333333,
      "grad_norm": 0.0457046739757061,
      "learning_rate": 0.0018251948137246537,
      "loss": 1.1295,
      "step": 1615
    },
    {
      "epoch": 2.1546666666666665,
      "grad_norm": 0.12662458419799805,
      "learning_rate": 0.0018249508160980326,
      "loss": 1.2221,
      "step": 1616
    },
    {
      "epoch": 2.156,
      "grad_norm": 0.03341729938983917,
      "learning_rate": 0.001824706664634058,
      "loss": 0.9157,
      "step": 1617
    },
    {
      "epoch": 2.1573333333333333,
      "grad_norm": 0.06334402412176132,
      "learning_rate": 0.0018244623593782596,
      "loss": 1.0748,
      "step": 1618
    },
    {
      "epoch": 2.1586666666666665,
      "grad_norm": 0.07658596336841583,
      "learning_rate": 0.0018242179003761954,
      "loss": 1.1852,
      "step": 1619
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.08540394902229309,
      "learning_rate": 0.0018239732876734526,
      "loss": 1.1042,
      "step": 1620
    },
    {
      "epoch": 2.1613333333333333,
      "grad_norm": 0.16674180328845978,
      "learning_rate": 0.0018237285213156462,
      "loss": 1.1894,
      "step": 1621
    },
    {
      "epoch": 2.1626666666666665,
      "grad_norm": 0.04831162840127945,
      "learning_rate": 0.0018234836013484208,
      "loss": 1.2278,
      "step": 1622
    },
    {
      "epoch": 2.164,
      "grad_norm": 0.02974887564778328,
      "learning_rate": 0.001823238527817449,
      "loss": 1.167,
      "step": 1623
    },
    {
      "epoch": 2.1653333333333333,
      "grad_norm": 0.06757490336894989,
      "learning_rate": 0.0018229933007684325,
      "loss": 1.1919,
      "step": 1624
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 0.03164302185177803,
      "learning_rate": 0.0018227479202471014,
      "loss": 1.1555,
      "step": 1625
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.027667276561260223,
      "learning_rate": 0.0018225023862992141,
      "loss": 1.0211,
      "step": 1626
    },
    {
      "epoch": 2.1693333333333333,
      "grad_norm": 0.12810708582401276,
      "learning_rate": 0.001822256698970558,
      "loss": 1.0947,
      "step": 1627
    },
    {
      "epoch": 2.1706666666666665,
      "grad_norm": 0.033021487295627594,
      "learning_rate": 0.0018220108583069492,
      "loss": 1.3015,
      "step": 1628
    },
    {
      "epoch": 2.172,
      "grad_norm": 0.027670400217175484,
      "learning_rate": 0.0018217648643542323,
      "loss": 1.1677,
      "step": 1629
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.02913912571966648,
      "learning_rate": 0.00182151871715828,
      "loss": 1.221,
      "step": 1630
    },
    {
      "epoch": 2.1746666666666665,
      "grad_norm": 0.0305685605853796,
      "learning_rate": 0.0018212724167649948,
      "loss": 0.9704,
      "step": 1631
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.04029713198542595,
      "learning_rate": 0.001821025963220306,
      "loss": 1.0245,
      "step": 1632
    },
    {
      "epoch": 2.1773333333333333,
      "grad_norm": 0.04308263957500458,
      "learning_rate": 0.001820779356570173,
      "loss": 1.0676,
      "step": 1633
    },
    {
      "epoch": 2.1786666666666665,
      "grad_norm": 0.04760883376002312,
      "learning_rate": 0.001820532596860583,
      "loss": 1.1344,
      "step": 1634
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.10429349541664124,
      "learning_rate": 0.0018202856841375518,
      "loss": 1.0058,
      "step": 1635
    },
    {
      "epoch": 2.1813333333333333,
      "grad_norm": 0.09797783941030502,
      "learning_rate": 0.001820038618447124,
      "loss": 1.048,
      "step": 1636
    },
    {
      "epoch": 2.1826666666666665,
      "grad_norm": 0.03851134702563286,
      "learning_rate": 0.0018197913998353726,
      "loss": 1.2203,
      "step": 1637
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.043880678713321686,
      "learning_rate": 0.001819544028348399,
      "loss": 0.9562,
      "step": 1638
    },
    {
      "epoch": 2.1853333333333333,
      "grad_norm": 0.037051137536764145,
      "learning_rate": 0.001819296504032333,
      "loss": 1.1029,
      "step": 1639
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.04430435225367546,
      "learning_rate": 0.0018190488269333334,
      "loss": 0.9472,
      "step": 1640
    },
    {
      "epoch": 2.188,
      "grad_norm": 0.043403007090091705,
      "learning_rate": 0.001818800997097587,
      "loss": 1.1952,
      "step": 1641
    },
    {
      "epoch": 2.1893333333333334,
      "grad_norm": 0.04232187196612358,
      "learning_rate": 0.0018185530145713093,
      "loss": 1.3875,
      "step": 1642
    },
    {
      "epoch": 2.1906666666666665,
      "grad_norm": 0.03396100178360939,
      "learning_rate": 0.0018183048794007445,
      "loss": 1.1807,
      "step": 1643
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.11884729564189911,
      "learning_rate": 0.0018180565916321645,
      "loss": 1.1825,
      "step": 1644
    },
    {
      "epoch": 2.1933333333333334,
      "grad_norm": 0.038095589727163315,
      "learning_rate": 0.0018178081513118706,
      "loss": 1.1225,
      "step": 1645
    },
    {
      "epoch": 2.1946666666666665,
      "grad_norm": 0.03028082847595215,
      "learning_rate": 0.001817559558486192,
      "loss": 1.0738,
      "step": 1646
    },
    {
      "epoch": 2.196,
      "grad_norm": 0.03438664227724075,
      "learning_rate": 0.0018173108132014861,
      "loss": 1.2055,
      "step": 1647
    },
    {
      "epoch": 2.1973333333333334,
      "grad_norm": 0.04236558452248573,
      "learning_rate": 0.0018170619155041398,
      "loss": 1.0521,
      "step": 1648
    },
    {
      "epoch": 2.1986666666666665,
      "grad_norm": 0.03687522932887077,
      "learning_rate": 0.001816812865440567,
      "loss": 0.9667,
      "step": 1649
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.04468117654323578,
      "learning_rate": 0.001816563663057211,
      "loss": 1.3664,
      "step": 1650
    },
    {
      "epoch": 2.2013333333333334,
      "grad_norm": 0.04563533142209053,
      "learning_rate": 0.001816314308400543,
      "loss": 0.8148,
      "step": 1651
    },
    {
      "epoch": 2.2026666666666666,
      "grad_norm": 0.034806057810783386,
      "learning_rate": 0.0018160648015170633,
      "loss": 0.9305,
      "step": 1652
    },
    {
      "epoch": 2.204,
      "grad_norm": 0.04529105871915817,
      "learning_rate": 0.0018158151424533,
      "loss": 0.9163,
      "step": 1653
    },
    {
      "epoch": 2.2053333333333334,
      "grad_norm": 0.028095286339521408,
      "learning_rate": 0.0018155653312558093,
      "loss": 1.0459,
      "step": 1654
    },
    {
      "epoch": 2.2066666666666666,
      "grad_norm": 0.04845855012536049,
      "learning_rate": 0.0018153153679711762,
      "loss": 1.0904,
      "step": 1655
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.04408740624785423,
      "learning_rate": 0.0018150652526460146,
      "loss": 1.2121,
      "step": 1656
    },
    {
      "epoch": 2.2093333333333334,
      "grad_norm": 0.029395513236522675,
      "learning_rate": 0.0018148149853269653,
      "loss": 1.0822,
      "step": 1657
    },
    {
      "epoch": 2.2106666666666666,
      "grad_norm": 0.04157914221286774,
      "learning_rate": 0.001814564566060699,
      "loss": 1.0859,
      "step": 1658
    },
    {
      "epoch": 2.212,
      "grad_norm": 0.035049669444561005,
      "learning_rate": 0.0018143139948939135,
      "loss": 1.2082,
      "step": 1659
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.03219219669699669,
      "learning_rate": 0.001814063271873336,
      "loss": 1.1309,
      "step": 1660
    },
    {
      "epoch": 2.2146666666666666,
      "grad_norm": 0.035565271973609924,
      "learning_rate": 0.001813812397045721,
      "loss": 0.9219,
      "step": 1661
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.02358989603817463,
      "learning_rate": 0.0018135613704578523,
      "loss": 1.002,
      "step": 1662
    },
    {
      "epoch": 2.2173333333333334,
      "grad_norm": 0.03082539513707161,
      "learning_rate": 0.0018133101921565414,
      "loss": 0.864,
      "step": 1663
    },
    {
      "epoch": 2.2186666666666666,
      "grad_norm": 0.032392315566539764,
      "learning_rate": 0.001813058862188628,
      "loss": 1.0673,
      "step": 1664
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.044918786734342575,
      "learning_rate": 0.00181280738060098,
      "loss": 1.0278,
      "step": 1665
    },
    {
      "epoch": 2.2213333333333334,
      "grad_norm": 0.028875350952148438,
      "learning_rate": 0.001812555747440494,
      "loss": 1.0543,
      "step": 1666
    },
    {
      "epoch": 2.2226666666666666,
      "grad_norm": 0.03857977315783501,
      "learning_rate": 0.0018123039627540953,
      "loss": 1.349,
      "step": 1667
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.03635900467634201,
      "learning_rate": 0.0018120520265887364,
      "loss": 1.2645,
      "step": 1668
    },
    {
      "epoch": 2.2253333333333334,
      "grad_norm": 0.030431773513555527,
      "learning_rate": 0.0018117999389913983,
      "loss": 1.0617,
      "step": 1669
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 0.052355945110321045,
      "learning_rate": 0.0018115477000090908,
      "loss": 1.2363,
      "step": 1670
    },
    {
      "epoch": 2.228,
      "grad_norm": 0.0353848971426487,
      "learning_rate": 0.0018112953096888517,
      "loss": 0.9955,
      "step": 1671
    },
    {
      "epoch": 2.2293333333333334,
      "grad_norm": 0.04861535131931305,
      "learning_rate": 0.0018110427680777463,
      "loss": 1.2152,
      "step": 1672
    },
    {
      "epoch": 2.2306666666666666,
      "grad_norm": 0.031338170170784,
      "learning_rate": 0.0018107900752228694,
      "loss": 1.0966,
      "step": 1673
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.049957167357206345,
      "learning_rate": 0.001810537231171343,
      "loss": 0.9451,
      "step": 1674
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 0.02995944581925869,
      "learning_rate": 0.0018102842359703176,
      "loss": 1.2084,
      "step": 1675
    },
    {
      "epoch": 2.2346666666666666,
      "grad_norm": 0.03569791093468666,
      "learning_rate": 0.0018100310896669721,
      "loss": 1.1391,
      "step": 1676
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 0.027840400114655495,
      "learning_rate": 0.0018097777923085131,
      "loss": 0.7835,
      "step": 1677
    },
    {
      "epoch": 2.2373333333333334,
      "grad_norm": 0.03857380151748657,
      "learning_rate": 0.0018095243439421758,
      "loss": 1.19,
      "step": 1678
    },
    {
      "epoch": 2.2386666666666666,
      "grad_norm": 0.03823813050985336,
      "learning_rate": 0.0018092707446152234,
      "loss": 1.2959,
      "step": 1679
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.029219556599855423,
      "learning_rate": 0.0018090169943749475,
      "loss": 1.1294,
      "step": 1680
    },
    {
      "epoch": 2.2413333333333334,
      "grad_norm": 0.0483291894197464,
      "learning_rate": 0.0018087630932686674,
      "loss": 1.0216,
      "step": 1681
    },
    {
      "epoch": 2.2426666666666666,
      "grad_norm": 0.03905878961086273,
      "learning_rate": 0.0018085090413437303,
      "loss": 1.3077,
      "step": 1682
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 0.04084055498242378,
      "learning_rate": 0.0018082548386475128,
      "loss": 1.1676,
      "step": 1683
    },
    {
      "epoch": 2.2453333333333334,
      "grad_norm": 0.04271857440471649,
      "learning_rate": 0.001808000485227418,
      "loss": 1.1752,
      "step": 1684
    },
    {
      "epoch": 2.2466666666666666,
      "grad_norm": 0.03370962664484978,
      "learning_rate": 0.0018077459811308785,
      "loss": 1.0173,
      "step": 1685
    },
    {
      "epoch": 2.248,
      "grad_norm": 0.04736679047346115,
      "learning_rate": 0.0018074913264053546,
      "loss": 1.1454,
      "step": 1686
    },
    {
      "epoch": 2.2493333333333334,
      "grad_norm": 0.03593824431300163,
      "learning_rate": 0.0018072365210983336,
      "loss": 0.8273,
      "step": 1687
    },
    {
      "epoch": 2.2506666666666666,
      "grad_norm": 0.04554356634616852,
      "learning_rate": 0.001806981565257332,
      "loss": 1.3934,
      "step": 1688
    },
    {
      "epoch": 2.252,
      "grad_norm": 0.02824927493929863,
      "learning_rate": 0.0018067264589298943,
      "loss": 1.0691,
      "step": 1689
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.06588491797447205,
      "learning_rate": 0.0018064712021635932,
      "loss": 1.0669,
      "step": 1690
    },
    {
      "epoch": 2.2546666666666666,
      "grad_norm": 0.03453746438026428,
      "learning_rate": 0.0018062157950060288,
      "loss": 0.9718,
      "step": 1691
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.0420086607336998,
      "learning_rate": 0.0018059602375048295,
      "loss": 0.9937,
      "step": 1692
    },
    {
      "epoch": 2.2573333333333334,
      "grad_norm": 0.03964634984731674,
      "learning_rate": 0.0018057045297076519,
      "loss": 1.1458,
      "step": 1693
    },
    {
      "epoch": 2.2586666666666666,
      "grad_norm": 0.040629856288433075,
      "learning_rate": 0.0018054486716621808,
      "loss": 0.9532,
      "step": 1694
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.031095270067453384,
      "learning_rate": 0.0018051926634161282,
      "loss": 1.0057,
      "step": 1695
    },
    {
      "epoch": 2.2613333333333334,
      "grad_norm": 0.04059898853302002,
      "learning_rate": 0.0018049365050172355,
      "loss": 1.4087,
      "step": 1696
    },
    {
      "epoch": 2.2626666666666666,
      "grad_norm": 0.034647032618522644,
      "learning_rate": 0.0018046801965132706,
      "loss": 1.205,
      "step": 1697
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.026125410571694374,
      "learning_rate": 0.0018044237379520305,
      "loss": 1.0686,
      "step": 1698
    },
    {
      "epoch": 2.2653333333333334,
      "grad_norm": 0.035278454422950745,
      "learning_rate": 0.0018041671293813394,
      "loss": 1.2284,
      "step": 1699
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.04565658047795296,
      "learning_rate": 0.00180391037084905,
      "loss": 1.2434,
      "step": 1700
    },
    {
      "epoch": 2.268,
      "grad_norm": 0.030469771474599838,
      "learning_rate": 0.0018036534624030428,
      "loss": 1.1417,
      "step": 1701
    },
    {
      "epoch": 2.2693333333333334,
      "grad_norm": 0.04211621358990669,
      "learning_rate": 0.0018033964040912265,
      "loss": 1.2422,
      "step": 1702
    },
    {
      "epoch": 2.2706666666666666,
      "grad_norm": 0.03067358396947384,
      "learning_rate": 0.0018031391959615368,
      "loss": 0.7956,
      "step": 1703
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.027364814653992653,
      "learning_rate": 0.0018028818380619387,
      "loss": 1.0465,
      "step": 1704
    },
    {
      "epoch": 2.2733333333333334,
      "grad_norm": 0.0408007949590683,
      "learning_rate": 0.0018026243304404244,
      "loss": 1.1447,
      "step": 1705
    },
    {
      "epoch": 2.2746666666666666,
      "grad_norm": 0.033113993704319,
      "learning_rate": 0.0018023666731450136,
      "loss": 0.9939,
      "step": 1706
    },
    {
      "epoch": 2.276,
      "grad_norm": 0.03716058284044266,
      "learning_rate": 0.001802108866223755,
      "loss": 0.8654,
      "step": 1707
    },
    {
      "epoch": 2.2773333333333334,
      "grad_norm": 0.03138978034257889,
      "learning_rate": 0.0018018509097247244,
      "loss": 1.033,
      "step": 1708
    },
    {
      "epoch": 2.2786666666666666,
      "grad_norm": 0.03387804701924324,
      "learning_rate": 0.0018015928036960254,
      "loss": 1.4128,
      "step": 1709
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.041250452399253845,
      "learning_rate": 0.00180133454818579,
      "loss": 1.156,
      "step": 1710
    },
    {
      "epoch": 2.2813333333333334,
      "grad_norm": 0.03419585898518562,
      "learning_rate": 0.0018010761432421779,
      "loss": 1.085,
      "step": 1711
    },
    {
      "epoch": 2.2826666666666666,
      "grad_norm": 0.03636215254664421,
      "learning_rate": 0.0018008175889133767,
      "loss": 1.1123,
      "step": 1712
    },
    {
      "epoch": 2.284,
      "grad_norm": 0.037826959043741226,
      "learning_rate": 0.0018005588852476016,
      "loss": 1.24,
      "step": 1713
    },
    {
      "epoch": 2.2853333333333334,
      "grad_norm": 0.03214343637228012,
      "learning_rate": 0.0018003000322930956,
      "loss": 1.0178,
      "step": 1714
    },
    {
      "epoch": 2.2866666666666666,
      "grad_norm": 0.05052069202065468,
      "learning_rate": 0.0018000410300981302,
      "loss": 1.1385,
      "step": 1715
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.03094184584915638,
      "learning_rate": 0.0017997818787110042,
      "loss": 1.2783,
      "step": 1716
    },
    {
      "epoch": 2.2893333333333334,
      "grad_norm": 0.0902366116642952,
      "learning_rate": 0.001799522578180044,
      "loss": 0.8201,
      "step": 1717
    },
    {
      "epoch": 2.2906666666666666,
      "grad_norm": 0.03597113862633705,
      "learning_rate": 0.0017992631285536046,
      "loss": 0.9059,
      "step": 1718
    },
    {
      "epoch": 2.292,
      "grad_norm": 0.03699377551674843,
      "learning_rate": 0.001799003529880068,
      "loss": 1.1312,
      "step": 1719
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.038199834525585175,
      "learning_rate": 0.0017987437822078443,
      "loss": 0.9957,
      "step": 1720
    },
    {
      "epoch": 2.2946666666666666,
      "grad_norm": 0.0351998545229435,
      "learning_rate": 0.0017984838855853718,
      "loss": 1.321,
      "step": 1721
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.04645036906003952,
      "learning_rate": 0.001798223840061116,
      "loss": 1.1695,
      "step": 1722
    },
    {
      "epoch": 2.2973333333333334,
      "grad_norm": 0.06070668622851372,
      "learning_rate": 0.00179796364568357,
      "loss": 1.2401,
      "step": 1723
    },
    {
      "epoch": 2.2986666666666666,
      "grad_norm": 0.03369631618261337,
      "learning_rate": 0.0017977033025012553,
      "loss": 1.2651,
      "step": 1724
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.039812520146369934,
      "learning_rate": 0.0017974428105627207,
      "loss": 1.1387,
      "step": 1725
    },
    {
      "epoch": 2.3013333333333335,
      "grad_norm": 0.09986761957406998,
      "learning_rate": 0.0017971821699165433,
      "loss": 1.1701,
      "step": 1726
    },
    {
      "epoch": 2.3026666666666666,
      "grad_norm": 0.07080382853746414,
      "learning_rate": 0.0017969213806113273,
      "loss": 0.8934,
      "step": 1727
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.029702529311180115,
      "learning_rate": 0.0017966604426957048,
      "loss": 1.1291,
      "step": 1728
    },
    {
      "epoch": 2.3053333333333335,
      "grad_norm": 0.028115401044487953,
      "learning_rate": 0.0017963993562183358,
      "loss": 0.8546,
      "step": 1729
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.04317856952548027,
      "learning_rate": 0.0017961381212279078,
      "loss": 1.2439,
      "step": 1730
    },
    {
      "epoch": 2.308,
      "grad_norm": 0.03333313763141632,
      "learning_rate": 0.0017958767377731358,
      "loss": 1.1956,
      "step": 1731
    },
    {
      "epoch": 2.3093333333333335,
      "grad_norm": 0.030578771606087685,
      "learning_rate": 0.0017956152059027631,
      "loss": 1.1173,
      "step": 1732
    },
    {
      "epoch": 2.3106666666666666,
      "grad_norm": 0.19584159553050995,
      "learning_rate": 0.0017953535256655605,
      "loss": 1.0458,
      "step": 1733
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.031134886667132378,
      "learning_rate": 0.0017950916971103258,
      "loss": 1.1581,
      "step": 1734
    },
    {
      "epoch": 2.3133333333333335,
      "grad_norm": 0.034805722534656525,
      "learning_rate": 0.0017948297202858851,
      "loss": 1.1178,
      "step": 1735
    },
    {
      "epoch": 2.3146666666666667,
      "grad_norm": 0.03567242994904518,
      "learning_rate": 0.0017945675952410923,
      "loss": 1.1185,
      "step": 1736
    },
    {
      "epoch": 2.316,
      "grad_norm": 0.031344834715127945,
      "learning_rate": 0.0017943053220248282,
      "loss": 1.1782,
      "step": 1737
    },
    {
      "epoch": 2.3173333333333335,
      "grad_norm": 0.08595309406518936,
      "learning_rate": 0.0017940429006860022,
      "loss": 1.0594,
      "step": 1738
    },
    {
      "epoch": 2.3186666666666667,
      "grad_norm": 0.038143713027238846,
      "learning_rate": 0.0017937803312735503,
      "loss": 1.2497,
      "step": 1739
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.035372767597436905,
      "learning_rate": 0.0017935176138364369,
      "loss": 1.0617,
      "step": 1740
    },
    {
      "epoch": 2.3213333333333335,
      "grad_norm": 0.04239934682846069,
      "learning_rate": 0.0017932547484236538,
      "loss": 0.9344,
      "step": 1741
    },
    {
      "epoch": 2.3226666666666667,
      "grad_norm": 0.09970785677433014,
      "learning_rate": 0.00179299173508422,
      "loss": 1.2976,
      "step": 1742
    },
    {
      "epoch": 2.324,
      "grad_norm": 0.02823290228843689,
      "learning_rate": 0.0017927285738671824,
      "loss": 1.134,
      "step": 1743
    },
    {
      "epoch": 2.3253333333333335,
      "grad_norm": 0.03258182853460312,
      "learning_rate": 0.0017924652648216158,
      "loss": 1.2484,
      "step": 1744
    },
    {
      "epoch": 2.3266666666666667,
      "grad_norm": 0.030590206384658813,
      "learning_rate": 0.001792201807996622,
      "loss": 0.8827,
      "step": 1745
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.15038266777992249,
      "learning_rate": 0.0017919382034413305,
      "loss": 1.0933,
      "step": 1746
    },
    {
      "epoch": 2.3293333333333335,
      "grad_norm": 0.03026830404996872,
      "learning_rate": 0.0017916744512048988,
      "loss": 1.1623,
      "step": 1747
    },
    {
      "epoch": 2.3306666666666667,
      "grad_norm": 0.037652187049388885,
      "learning_rate": 0.0017914105513365114,
      "loss": 1.0428,
      "step": 1748
    },
    {
      "epoch": 2.332,
      "grad_norm": 0.03560274839401245,
      "learning_rate": 0.0017911465038853804,
      "loss": 0.8626,
      "step": 1749
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.05173017829656601,
      "learning_rate": 0.0017908823089007458,
      "loss": 1.1475,
      "step": 1750
    },
    {
      "epoch": 2.3346666666666667,
      "grad_norm": 0.051743168383836746,
      "learning_rate": 0.0017906179664318742,
      "loss": 1.0132,
      "step": 1751
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.0336705818772316,
      "learning_rate": 0.0017903534765280613,
      "loss": 0.9336,
      "step": 1752
    },
    {
      "epoch": 2.3373333333333335,
      "grad_norm": 0.029300078749656677,
      "learning_rate": 0.0017900888392386287,
      "loss": 0.881,
      "step": 1753
    },
    {
      "epoch": 2.3386666666666667,
      "grad_norm": 0.09040197730064392,
      "learning_rate": 0.0017898240546129266,
      "loss": 1.062,
      "step": 1754
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.029729235917329788,
      "learning_rate": 0.0017895591227003315,
      "loss": 1.087,
      "step": 1755
    },
    {
      "epoch": 2.3413333333333335,
      "grad_norm": 0.036320168524980545,
      "learning_rate": 0.0017892940435502488,
      "loss": 1.4125,
      "step": 1756
    },
    {
      "epoch": 2.3426666666666667,
      "grad_norm": 0.23262295126914978,
      "learning_rate": 0.00178902881721211,
      "loss": 1.3967,
      "step": 1757
    },
    {
      "epoch": 2.344,
      "grad_norm": 0.027778787538409233,
      "learning_rate": 0.0017887634437353753,
      "loss": 0.9664,
      "step": 1758
    },
    {
      "epoch": 2.3453333333333335,
      "grad_norm": 0.03037768043577671,
      "learning_rate": 0.001788497923169531,
      "loss": 1.0879,
      "step": 1759
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.059893619269132614,
      "learning_rate": 0.0017882322555640925,
      "loss": 1.2257,
      "step": 1760
    },
    {
      "epoch": 2.348,
      "grad_norm": 0.12460192292928696,
      "learning_rate": 0.0017879664409686008,
      "loss": 1.4975,
      "step": 1761
    },
    {
      "epoch": 2.3493333333333335,
      "grad_norm": 0.04613211005926132,
      "learning_rate": 0.0017877004794326257,
      "loss": 1.0365,
      "step": 1762
    },
    {
      "epoch": 2.3506666666666667,
      "grad_norm": 0.040938276797533035,
      "learning_rate": 0.0017874343710057633,
      "loss": 1.105,
      "step": 1763
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.16651339828968048,
      "learning_rate": 0.0017871681157376382,
      "loss": 1.3314,
      "step": 1764
    },
    {
      "epoch": 2.3533333333333335,
      "grad_norm": 0.060410432517528534,
      "learning_rate": 0.001786901713677902,
      "loss": 1.2257,
      "step": 1765
    },
    {
      "epoch": 2.3546666666666667,
      "grad_norm": 0.052317097783088684,
      "learning_rate": 0.0017866351648762326,
      "loss": 1.1273,
      "step": 1766
    },
    {
      "epoch": 2.356,
      "grad_norm": 0.0341738685965538,
      "learning_rate": 0.0017863684693823375,
      "loss": 1.0144,
      "step": 1767
    },
    {
      "epoch": 2.3573333333333335,
      "grad_norm": 0.052692994475364685,
      "learning_rate": 0.001786101627245949,
      "loss": 0.9721,
      "step": 1768
    },
    {
      "epoch": 2.3586666666666667,
      "grad_norm": 0.03822941705584526,
      "learning_rate": 0.0017858346385168286,
      "loss": 0.9676,
      "step": 1769
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.09847387671470642,
      "learning_rate": 0.0017855675032447647,
      "loss": 0.8639,
      "step": 1770
    },
    {
      "epoch": 2.3613333333333335,
      "grad_norm": 0.043565038591623306,
      "learning_rate": 0.0017853002214795724,
      "loss": 1.0312,
      "step": 1771
    },
    {
      "epoch": 2.3626666666666667,
      "grad_norm": 0.037510454654693604,
      "learning_rate": 0.001785032793271095,
      "loss": 1.2343,
      "step": 1772
    },
    {
      "epoch": 2.364,
      "grad_norm": 0.036151375621557236,
      "learning_rate": 0.0017847652186692025,
      "loss": 1.0979,
      "step": 1773
    },
    {
      "epoch": 2.3653333333333335,
      "grad_norm": 0.05663544684648514,
      "learning_rate": 0.0017844974977237922,
      "loss": 0.9264,
      "step": 1774
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 0.03927619010210037,
      "learning_rate": 0.0017842296304847892,
      "loss": 1.067,
      "step": 1775
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.06601516902446747,
      "learning_rate": 0.001783961617002145,
      "loss": 1.2388,
      "step": 1776
    },
    {
      "epoch": 2.3693333333333335,
      "grad_norm": 0.03471463918685913,
      "learning_rate": 0.0017836934573258398,
      "loss": 1.2929,
      "step": 1777
    },
    {
      "epoch": 2.3706666666666667,
      "grad_norm": 0.03234587982296944,
      "learning_rate": 0.0017834251515058796,
      "loss": 0.9758,
      "step": 1778
    },
    {
      "epoch": 2.372,
      "grad_norm": 0.0383436493575573,
      "learning_rate": 0.0017831566995922983,
      "loss": 1.1954,
      "step": 1779
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.03241103142499924,
      "learning_rate": 0.001782888101635157,
      "loss": 0.7187,
      "step": 1780
    },
    {
      "epoch": 2.3746666666666667,
      "grad_norm": 0.0412507988512516,
      "learning_rate": 0.0017826193576845437,
      "loss": 1.1048,
      "step": 1781
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.036640431731939316,
      "learning_rate": 0.0017823504677905748,
      "loss": 1.2559,
      "step": 1782
    },
    {
      "epoch": 2.3773333333333335,
      "grad_norm": 0.030063489452004433,
      "learning_rate": 0.0017820814320033925,
      "loss": 1.1776,
      "step": 1783
    },
    {
      "epoch": 2.3786666666666667,
      "grad_norm": 0.031771220266819,
      "learning_rate": 0.0017818122503731667,
      "loss": 1.2755,
      "step": 1784
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.03764480724930763,
      "learning_rate": 0.0017815429229500945,
      "loss": 1.0223,
      "step": 1785
    },
    {
      "epoch": 2.3813333333333335,
      "grad_norm": 0.032228320837020874,
      "learning_rate": 0.0017812734497844006,
      "loss": 1.1901,
      "step": 1786
    },
    {
      "epoch": 2.3826666666666667,
      "grad_norm": 0.03026685304939747,
      "learning_rate": 0.0017810038309263362,
      "loss": 1.152,
      "step": 1787
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.03129875287413597,
      "learning_rate": 0.0017807340664261803,
      "loss": 1.0957,
      "step": 1788
    },
    {
      "epoch": 2.3853333333333335,
      "grad_norm": 0.03441300615668297,
      "learning_rate": 0.0017804641563342384,
      "loss": 1.2054,
      "step": 1789
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.03896637260913849,
      "learning_rate": 0.001780194100700844,
      "loss": 1.0362,
      "step": 1790
    },
    {
      "epoch": 2.388,
      "grad_norm": 0.054559703916311264,
      "learning_rate": 0.0017799238995763566,
      "loss": 1.1567,
      "step": 1791
    },
    {
      "epoch": 2.389333333333333,
      "grad_norm": 0.04548582434654236,
      "learning_rate": 0.0017796535530111642,
      "loss": 1.0237,
      "step": 1792
    },
    {
      "epoch": 2.3906666666666667,
      "grad_norm": 0.021943435072898865,
      "learning_rate": 0.0017793830610556807,
      "loss": 1.0219,
      "step": 1793
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.031209014356136322,
      "learning_rate": 0.0017791124237603476,
      "loss": 1.0604,
      "step": 1794
    },
    {
      "epoch": 2.3933333333333335,
      "grad_norm": 0.03615739941596985,
      "learning_rate": 0.0017788416411756338,
      "loss": 1.214,
      "step": 1795
    },
    {
      "epoch": 2.3946666666666667,
      "grad_norm": 0.060399722307920456,
      "learning_rate": 0.0017785707133520345,
      "loss": 0.7537,
      "step": 1796
    },
    {
      "epoch": 2.396,
      "grad_norm": 0.035668402910232544,
      "learning_rate": 0.0017782996403400736,
      "loss": 1.3504,
      "step": 1797
    },
    {
      "epoch": 2.397333333333333,
      "grad_norm": 0.025548497214913368,
      "learning_rate": 0.0017780284221902995,
      "loss": 0.7873,
      "step": 1798
    },
    {
      "epoch": 2.3986666666666667,
      "grad_norm": 0.028521409258246422,
      "learning_rate": 0.0017777570589532902,
      "loss": 1.4767,
      "step": 1799
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.02875196374952793,
      "learning_rate": 0.0017774855506796495,
      "loss": 1.3289,
      "step": 1800
    },
    {
      "epoch": 2.4013333333333335,
      "grad_norm": 0.03630419448018074,
      "learning_rate": 0.001777213897420008,
      "loss": 1.0493,
      "step": 1801
    },
    {
      "epoch": 2.4026666666666667,
      "grad_norm": 0.028192274272441864,
      "learning_rate": 0.0017769420992250244,
      "loss": 1.0367,
      "step": 1802
    },
    {
      "epoch": 2.404,
      "grad_norm": 0.038275036960840225,
      "learning_rate": 0.001776670156145383,
      "loss": 0.971,
      "step": 1803
    },
    {
      "epoch": 2.405333333333333,
      "grad_norm": 0.04102632403373718,
      "learning_rate": 0.0017763980682317965,
      "loss": 0.9339,
      "step": 1804
    },
    {
      "epoch": 2.4066666666666667,
      "grad_norm": 0.034298546612262726,
      "learning_rate": 0.0017761258355350037,
      "loss": 1.1935,
      "step": 1805
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.06290225684642792,
      "learning_rate": 0.0017758534581057716,
      "loss": 1.0231,
      "step": 1806
    },
    {
      "epoch": 2.4093333333333335,
      "grad_norm": 0.0270040575414896,
      "learning_rate": 0.0017755809359948922,
      "loss": 1.0656,
      "step": 1807
    },
    {
      "epoch": 2.4106666666666667,
      "grad_norm": 0.025817429646849632,
      "learning_rate": 0.0017753082692531861,
      "loss": 1.3573,
      "step": 1808
    },
    {
      "epoch": 2.412,
      "grad_norm": 0.03418901190161705,
      "learning_rate": 0.0017750354579315001,
      "loss": 0.8279,
      "step": 1809
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 0.04308672994375229,
      "learning_rate": 0.001774762502080709,
      "loss": 0.7867,
      "step": 1810
    },
    {
      "epoch": 2.4146666666666667,
      "grad_norm": 0.03089023195207119,
      "learning_rate": 0.0017744894017517128,
      "loss": 0.8859,
      "step": 1811
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.026297254487872124,
      "learning_rate": 0.0017742161569954398,
      "loss": 1.1141,
      "step": 1812
    },
    {
      "epoch": 2.4173333333333336,
      "grad_norm": 0.03513956815004349,
      "learning_rate": 0.0017739427678628453,
      "loss": 1.0223,
      "step": 1813
    },
    {
      "epoch": 2.4186666666666667,
      "grad_norm": 0.03486934304237366,
      "learning_rate": 0.00177366923440491,
      "loss": 1.1967,
      "step": 1814
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.03707578405737877,
      "learning_rate": 0.0017733955566726438,
      "loss": 1.3181,
      "step": 1815
    },
    {
      "epoch": 2.421333333333333,
      "grad_norm": 0.02639029547572136,
      "learning_rate": 0.0017731217347170816,
      "loss": 1.0904,
      "step": 1816
    },
    {
      "epoch": 2.4226666666666667,
      "grad_norm": 0.030451657250523567,
      "learning_rate": 0.0017728477685892865,
      "loss": 0.9375,
      "step": 1817
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.029582930728793144,
      "learning_rate": 0.0017725736583403468,
      "loss": 1.0174,
      "step": 1818
    },
    {
      "epoch": 2.4253333333333336,
      "grad_norm": 0.03533751890063286,
      "learning_rate": 0.0017722994040213797,
      "loss": 0.9878,
      "step": 1819
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.02851272001862526,
      "learning_rate": 0.001772025005683528,
      "loss": 0.919,
      "step": 1820
    },
    {
      "epoch": 2.428,
      "grad_norm": 0.038304027169942856,
      "learning_rate": 0.0017717504633779618,
      "loss": 0.9427,
      "step": 1821
    },
    {
      "epoch": 2.429333333333333,
      "grad_norm": 0.02681264840066433,
      "learning_rate": 0.0017714757771558776,
      "loss": 1.0949,
      "step": 1822
    },
    {
      "epoch": 2.4306666666666668,
      "grad_norm": 0.036532603204250336,
      "learning_rate": 0.0017712009470684993,
      "loss": 1.1866,
      "step": 1823
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.061168231070041656,
      "learning_rate": 0.0017709259731670773,
      "loss": 0.9206,
      "step": 1824
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 0.0319136306643486,
      "learning_rate": 0.0017706508555028894,
      "loss": 1.0082,
      "step": 1825
    },
    {
      "epoch": 2.4346666666666668,
      "grad_norm": 0.034186892211437225,
      "learning_rate": 0.0017703755941272388,
      "loss": 1.156,
      "step": 1826
    },
    {
      "epoch": 2.436,
      "grad_norm": 0.03690517321228981,
      "learning_rate": 0.0017701001890914573,
      "loss": 1.2836,
      "step": 1827
    },
    {
      "epoch": 2.437333333333333,
      "grad_norm": 0.035963237285614014,
      "learning_rate": 0.001769824640446902,
      "loss": 1.3316,
      "step": 1828
    },
    {
      "epoch": 2.4386666666666668,
      "grad_norm": 0.03509199619293213,
      "learning_rate": 0.0017695489482449575,
      "loss": 0.9798,
      "step": 1829
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.04518967494368553,
      "learning_rate": 0.0017692731125370353,
      "loss": 1.0611,
      "step": 1830
    },
    {
      "epoch": 2.4413333333333336,
      "grad_norm": 0.03678644821047783,
      "learning_rate": 0.0017689971333745733,
      "loss": 1.22,
      "step": 1831
    },
    {
      "epoch": 2.4426666666666668,
      "grad_norm": 0.03158094361424446,
      "learning_rate": 0.0017687210108090362,
      "loss": 0.9778,
      "step": 1832
    },
    {
      "epoch": 2.444,
      "grad_norm": 0.05484827980399132,
      "learning_rate": 0.0017684447448919154,
      "loss": 1.6631,
      "step": 1833
    },
    {
      "epoch": 2.445333333333333,
      "grad_norm": 0.030619457364082336,
      "learning_rate": 0.0017681683356747296,
      "loss": 1.1729,
      "step": 1834
    },
    {
      "epoch": 2.4466666666666668,
      "grad_norm": 0.0368243083357811,
      "learning_rate": 0.0017678917832090234,
      "loss": 1.0772,
      "step": 1835
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.04346098005771637,
      "learning_rate": 0.0017676150875463685,
      "loss": 1.0152,
      "step": 1836
    },
    {
      "epoch": 2.449333333333333,
      "grad_norm": 0.031813595443964005,
      "learning_rate": 0.0017673382487383633,
      "loss": 1.2632,
      "step": 1837
    },
    {
      "epoch": 2.4506666666666668,
      "grad_norm": 0.030687116086483,
      "learning_rate": 0.0017670612668366328,
      "loss": 1.1283,
      "step": 1838
    },
    {
      "epoch": 2.452,
      "grad_norm": 0.03923732042312622,
      "learning_rate": 0.001766784141892829,
      "loss": 0.9816,
      "step": 1839
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.03170732408761978,
      "learning_rate": 0.0017665068739586304,
      "loss": 0.9982,
      "step": 1840
    },
    {
      "epoch": 2.4546666666666668,
      "grad_norm": 0.03755863383412361,
      "learning_rate": 0.0017662294630857416,
      "loss": 1.1999,
      "step": 1841
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.02729463018476963,
      "learning_rate": 0.001765951909325895,
      "loss": 1.3778,
      "step": 1842
    },
    {
      "epoch": 2.457333333333333,
      "grad_norm": 0.03161436691880226,
      "learning_rate": 0.0017656742127308482,
      "loss": 1.3041,
      "step": 1843
    },
    {
      "epoch": 2.458666666666667,
      "grad_norm": 0.0338800810277462,
      "learning_rate": 0.0017653963733523872,
      "loss": 1.1494,
      "step": 1844
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.03219587728381157,
      "learning_rate": 0.0017651183912423228,
      "loss": 1.1403,
      "step": 1845
    },
    {
      "epoch": 2.461333333333333,
      "grad_norm": 0.031543340533971786,
      "learning_rate": 0.001764840266452494,
      "loss": 1.0327,
      "step": 1846
    },
    {
      "epoch": 2.462666666666667,
      "grad_norm": 0.03982898220419884,
      "learning_rate": 0.001764561999034765,
      "loss": 1.0378,
      "step": 1847
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.03233126178383827,
      "learning_rate": 0.0017642835890410277,
      "loss": 1.2928,
      "step": 1848
    },
    {
      "epoch": 2.465333333333333,
      "grad_norm": 0.030925976112484932,
      "learning_rate": 0.0017640050365232002,
      "loss": 1.1425,
      "step": 1849
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.02965587191283703,
      "learning_rate": 0.001763726341533227,
      "loss": 1.0215,
      "step": 1850
    },
    {
      "epoch": 2.468,
      "grad_norm": 0.03085949644446373,
      "learning_rate": 0.0017634475041230795,
      "loss": 1.2446,
      "step": 1851
    },
    {
      "epoch": 2.469333333333333,
      "grad_norm": 0.03662377968430519,
      "learning_rate": 0.001763168524344755,
      "loss": 1.1081,
      "step": 1852
    },
    {
      "epoch": 2.470666666666667,
      "grad_norm": 0.05134524777531624,
      "learning_rate": 0.0017628894022502783,
      "loss": 1.0003,
      "step": 1853
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.03492407873272896,
      "learning_rate": 0.0017626101378917004,
      "loss": 1.3946,
      "step": 1854
    },
    {
      "epoch": 2.473333333333333,
      "grad_norm": 0.02809261716902256,
      "learning_rate": 0.0017623307313210984,
      "loss": 1.044,
      "step": 1855
    },
    {
      "epoch": 2.474666666666667,
      "grad_norm": 0.03902648016810417,
      "learning_rate": 0.001762051182590576,
      "loss": 1.0221,
      "step": 1856
    },
    {
      "epoch": 2.476,
      "grad_norm": 0.03338392823934555,
      "learning_rate": 0.0017617714917522638,
      "loss": 1.0463,
      "step": 1857
    },
    {
      "epoch": 2.477333333333333,
      "grad_norm": 0.03520733490586281,
      "learning_rate": 0.001761491658858319,
      "loss": 0.8523,
      "step": 1858
    },
    {
      "epoch": 2.478666666666667,
      "grad_norm": 0.04603138938546181,
      "learning_rate": 0.001761211683960925,
      "loss": 1.0081,
      "step": 1859
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.029111968353390694,
      "learning_rate": 0.0017609315671122912,
      "loss": 0.79,
      "step": 1860
    },
    {
      "epoch": 2.481333333333333,
      "grad_norm": 0.02865162491798401,
      "learning_rate": 0.0017606513083646546,
      "loss": 1.1507,
      "step": 1861
    },
    {
      "epoch": 2.482666666666667,
      "grad_norm": 0.03752179816365242,
      "learning_rate": 0.0017603709077702772,
      "loss": 1.1612,
      "step": 1862
    },
    {
      "epoch": 2.484,
      "grad_norm": 0.0258791521191597,
      "learning_rate": 0.001760090365381449,
      "loss": 0.9639,
      "step": 1863
    },
    {
      "epoch": 2.485333333333333,
      "grad_norm": 0.03794706612825394,
      "learning_rate": 0.0017598096812504855,
      "loss": 0.9213,
      "step": 1864
    },
    {
      "epoch": 2.486666666666667,
      "grad_norm": 0.0318702831864357,
      "learning_rate": 0.0017595288554297292,
      "loss": 1.2821,
      "step": 1865
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.030744798481464386,
      "learning_rate": 0.001759247887971548,
      "loss": 0.9946,
      "step": 1866
    },
    {
      "epoch": 2.489333333333333,
      "grad_norm": 0.03663735091686249,
      "learning_rate": 0.0017589667789283375,
      "loss": 1.0183,
      "step": 1867
    },
    {
      "epoch": 2.490666666666667,
      "grad_norm": 0.040438976138830185,
      "learning_rate": 0.0017586855283525185,
      "loss": 1.1402,
      "step": 1868
    },
    {
      "epoch": 2.492,
      "grad_norm": 0.050160542130470276,
      "learning_rate": 0.0017584041362965395,
      "loss": 1.081,
      "step": 1869
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.04451470822095871,
      "learning_rate": 0.001758122602812874,
      "loss": 0.9779,
      "step": 1870
    },
    {
      "epoch": 2.494666666666667,
      "grad_norm": 0.02749297395348549,
      "learning_rate": 0.0017578409279540227,
      "loss": 0.9873,
      "step": 1871
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.04220397770404816,
      "learning_rate": 0.0017575591117725132,
      "loss": 1.0504,
      "step": 1872
    },
    {
      "epoch": 2.497333333333333,
      "grad_norm": 0.03409822657704353,
      "learning_rate": 0.0017572771543208977,
      "loss": 1.1333,
      "step": 1873
    },
    {
      "epoch": 2.498666666666667,
      "grad_norm": 0.03776745870709419,
      "learning_rate": 0.0017569950556517563,
      "loss": 1.0386,
      "step": 1874
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.027868468314409256,
      "learning_rate": 0.0017567128158176952,
      "loss": 1.025,
      "step": 1875
    },
    {
      "epoch": 2.501333333333333,
      "grad_norm": 0.061672892421483994,
      "learning_rate": 0.0017564304348713464,
      "loss": 0.8546,
      "step": 1876
    },
    {
      "epoch": 2.502666666666667,
      "grad_norm": 0.029656067490577698,
      "learning_rate": 0.0017561479128653686,
      "loss": 1.1196,
      "step": 1877
    },
    {
      "epoch": 2.504,
      "grad_norm": 0.0322490856051445,
      "learning_rate": 0.0017558652498524461,
      "loss": 0.863,
      "step": 1878
    },
    {
      "epoch": 2.505333333333333,
      "grad_norm": 0.03603831306099892,
      "learning_rate": 0.0017555824458852908,
      "loss": 1.1456,
      "step": 1879
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.026717795059084892,
      "learning_rate": 0.0017552995010166402,
      "loss": 1.1255,
      "step": 1880
    },
    {
      "epoch": 2.508,
      "grad_norm": 0.03693490847945213,
      "learning_rate": 0.001755016415299257,
      "loss": 1.254,
      "step": 1881
    },
    {
      "epoch": 2.509333333333333,
      "grad_norm": 0.034742843359708786,
      "learning_rate": 0.0017547331887859325,
      "loss": 0.9058,
      "step": 1882
    },
    {
      "epoch": 2.510666666666667,
      "grad_norm": 0.031570591032505035,
      "learning_rate": 0.0017544498215294826,
      "loss": 0.7967,
      "step": 1883
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.03368833661079407,
      "learning_rate": 0.0017541663135827493,
      "loss": 1.0544,
      "step": 1884
    },
    {
      "epoch": 2.513333333333333,
      "grad_norm": 0.03750072419643402,
      "learning_rate": 0.001753882664998602,
      "loss": 1.0748,
      "step": 1885
    },
    {
      "epoch": 2.514666666666667,
      "grad_norm": 0.039020802825689316,
      "learning_rate": 0.001753598875829935,
      "loss": 1.0343,
      "step": 1886
    },
    {
      "epoch": 2.516,
      "grad_norm": 0.029885658994317055,
      "learning_rate": 0.0017533149461296698,
      "loss": 1.0165,
      "step": 1887
    },
    {
      "epoch": 2.517333333333333,
      "grad_norm": 0.033057644963264465,
      "learning_rate": 0.001753030875950754,
      "loss": 1.0208,
      "step": 1888
    },
    {
      "epoch": 2.518666666666667,
      "grad_norm": 0.23908738791942596,
      "learning_rate": 0.0017527466653461609,
      "loss": 1.2208,
      "step": 1889
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.03492164984345436,
      "learning_rate": 0.0017524623143688903,
      "loss": 0.8785,
      "step": 1890
    },
    {
      "epoch": 2.521333333333333,
      "grad_norm": 0.03373975679278374,
      "learning_rate": 0.0017521778230719682,
      "loss": 1.2371,
      "step": 1891
    },
    {
      "epoch": 2.522666666666667,
      "grad_norm": 0.044494688510894775,
      "learning_rate": 0.001751893191508447,
      "loss": 1.2331,
      "step": 1892
    },
    {
      "epoch": 2.524,
      "grad_norm": 0.03650861606001854,
      "learning_rate": 0.0017516084197314043,
      "loss": 1.0097,
      "step": 1893
    },
    {
      "epoch": 2.525333333333333,
      "grad_norm": 0.03320329263806343,
      "learning_rate": 0.0017513235077939453,
      "loss": 1.1105,
      "step": 1894
    },
    {
      "epoch": 2.5266666666666664,
      "grad_norm": 0.03855917602777481,
      "learning_rate": 0.0017510384557492,
      "loss": 0.9187,
      "step": 1895
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.027209850028157234,
      "learning_rate": 0.0017507532636503256,
      "loss": 0.8389,
      "step": 1896
    },
    {
      "epoch": 2.529333333333333,
      "grad_norm": 0.04307852312922478,
      "learning_rate": 0.0017504679315505044,
      "loss": 0.8599,
      "step": 1897
    },
    {
      "epoch": 2.530666666666667,
      "grad_norm": 0.044224899262189865,
      "learning_rate": 0.0017501824595029453,
      "loss": 0.8954,
      "step": 1898
    },
    {
      "epoch": 2.532,
      "grad_norm": 0.03691231086850166,
      "learning_rate": 0.0017498968475608838,
      "loss": 0.993,
      "step": 1899
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.02693348191678524,
      "learning_rate": 0.0017496110957775808,
      "loss": 0.8688,
      "step": 1900
    },
    {
      "epoch": 2.5346666666666664,
      "grad_norm": 0.027915459126234055,
      "learning_rate": 0.001749325204206323,
      "loss": 0.7346,
      "step": 1901
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.1160968542098999,
      "learning_rate": 0.0017490391729004242,
      "loss": 1.0864,
      "step": 1902
    },
    {
      "epoch": 2.537333333333333,
      "grad_norm": 0.03367406874895096,
      "learning_rate": 0.0017487530019132235,
      "loss": 0.9938,
      "step": 1903
    },
    {
      "epoch": 2.538666666666667,
      "grad_norm": 0.03567896783351898,
      "learning_rate": 0.0017484666912980864,
      "loss": 0.9022,
      "step": 1904
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.029370900243520737,
      "learning_rate": 0.001748180241108404,
      "loss": 0.8843,
      "step": 1905
    },
    {
      "epoch": 2.541333333333333,
      "grad_norm": 0.0331161729991436,
      "learning_rate": 0.001747893651397594,
      "loss": 1.029,
      "step": 1906
    },
    {
      "epoch": 2.5426666666666664,
      "grad_norm": 0.03600897267460823,
      "learning_rate": 0.0017476069222190996,
      "loss": 1.4099,
      "step": 1907
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.04450666159391403,
      "learning_rate": 0.0017473200536263906,
      "loss": 1.2551,
      "step": 1908
    },
    {
      "epoch": 2.5453333333333332,
      "grad_norm": 0.030659791082143784,
      "learning_rate": 0.0017470330456729618,
      "loss": 1.1007,
      "step": 1909
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 0.03088466078042984,
      "learning_rate": 0.001746745898412335,
      "loss": 0.9826,
      "step": 1910
    },
    {
      "epoch": 2.548,
      "grad_norm": 0.04555818811058998,
      "learning_rate": 0.0017464586118980578,
      "loss": 1.112,
      "step": 1911
    },
    {
      "epoch": 2.5493333333333332,
      "grad_norm": 0.031047262251377106,
      "learning_rate": 0.0017461711861837036,
      "loss": 1.0188,
      "step": 1912
    },
    {
      "epoch": 2.5506666666666664,
      "grad_norm": 0.04024512693285942,
      "learning_rate": 0.0017458836213228711,
      "loss": 1.0247,
      "step": 1913
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.03365034982562065,
      "learning_rate": 0.0017455959173691862,
      "loss": 0.9796,
      "step": 1914
    },
    {
      "epoch": 2.5533333333333332,
      "grad_norm": 0.03135377913713455,
      "learning_rate": 0.0017453080743763,
      "loss": 1.19,
      "step": 1915
    },
    {
      "epoch": 2.554666666666667,
      "grad_norm": 0.031916435807943344,
      "learning_rate": 0.0017450200923978894,
      "loss": 1.1818,
      "step": 1916
    },
    {
      "epoch": 2.556,
      "grad_norm": 0.037277717143297195,
      "learning_rate": 0.0017447319714876577,
      "loss": 1.045,
      "step": 1917
    },
    {
      "epoch": 2.5573333333333332,
      "grad_norm": 0.028818249702453613,
      "learning_rate": 0.0017444437116993336,
      "loss": 0.9515,
      "step": 1918
    },
    {
      "epoch": 2.5586666666666664,
      "grad_norm": 0.02838919870555401,
      "learning_rate": 0.0017441553130866723,
      "loss": 1.0253,
      "step": 1919
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.03208693861961365,
      "learning_rate": 0.0017438667757034545,
      "loss": 1.1508,
      "step": 1920
    },
    {
      "epoch": 2.5613333333333332,
      "grad_norm": 0.06879857182502747,
      "learning_rate": 0.0017435780996034867,
      "loss": 1.2878,
      "step": 1921
    },
    {
      "epoch": 2.562666666666667,
      "grad_norm": 0.04105928912758827,
      "learning_rate": 0.0017432892848406013,
      "loss": 1.0844,
      "step": 1922
    },
    {
      "epoch": 2.564,
      "grad_norm": 0.035399388521909714,
      "learning_rate": 0.001743000331468657,
      "loss": 1.1193,
      "step": 1923
    },
    {
      "epoch": 2.5653333333333332,
      "grad_norm": 0.04861779883503914,
      "learning_rate": 0.0017427112395415377,
      "loss": 1.1332,
      "step": 1924
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 0.026199307292699814,
      "learning_rate": 0.0017424220091131536,
      "loss": 1.0483,
      "step": 1925
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.03751998022198677,
      "learning_rate": 0.0017421326402374405,
      "loss": 0.9909,
      "step": 1926
    },
    {
      "epoch": 2.5693333333333332,
      "grad_norm": 0.029388923197984695,
      "learning_rate": 0.0017418431329683601,
      "loss": 1.323,
      "step": 1927
    },
    {
      "epoch": 2.570666666666667,
      "grad_norm": 0.03189375251531601,
      "learning_rate": 0.0017415534873599,
      "loss": 0.9134,
      "step": 1928
    },
    {
      "epoch": 2.572,
      "grad_norm": 0.031694378703832626,
      "learning_rate": 0.0017412637034660734,
      "loss": 1.1099,
      "step": 1929
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 0.0507865808904171,
      "learning_rate": 0.0017409737813409195,
      "loss": 1.3134,
      "step": 1930
    },
    {
      "epoch": 2.5746666666666664,
      "grad_norm": 0.02453240193426609,
      "learning_rate": 0.001740683721038503,
      "loss": 1.0524,
      "step": 1931
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.03554791584610939,
      "learning_rate": 0.0017403935226129148,
      "loss": 1.0314,
      "step": 1932
    },
    {
      "epoch": 2.5773333333333333,
      "grad_norm": 0.03371719270944595,
      "learning_rate": 0.0017401031861182708,
      "loss": 0.9404,
      "step": 1933
    },
    {
      "epoch": 2.578666666666667,
      "grad_norm": 0.030676379799842834,
      "learning_rate": 0.0017398127116087134,
      "loss": 1.1914,
      "step": 1934
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.02802792377769947,
      "learning_rate": 0.0017395220991384109,
      "loss": 1.1473,
      "step": 1935
    },
    {
      "epoch": 2.5813333333333333,
      "grad_norm": 0.02333354949951172,
      "learning_rate": 0.0017392313487615562,
      "loss": 0.9428,
      "step": 1936
    },
    {
      "epoch": 2.5826666666666664,
      "grad_norm": 0.02805374376475811,
      "learning_rate": 0.0017389404605323692,
      "loss": 0.9095,
      "step": 1937
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.03820542246103287,
      "learning_rate": 0.0017386494345050943,
      "loss": 1.2131,
      "step": 1938
    },
    {
      "epoch": 2.5853333333333333,
      "grad_norm": 0.03317071124911308,
      "learning_rate": 0.0017383582707340027,
      "loss": 0.9623,
      "step": 1939
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.03358094394207001,
      "learning_rate": 0.0017380669692733904,
      "loss": 0.982,
      "step": 1940
    },
    {
      "epoch": 2.588,
      "grad_norm": 0.02889959327876568,
      "learning_rate": 0.0017377755301775802,
      "loss": 0.8889,
      "step": 1941
    },
    {
      "epoch": 2.5893333333333333,
      "grad_norm": 0.03693389520049095,
      "learning_rate": 0.001737483953500919,
      "loss": 1.21,
      "step": 1942
    },
    {
      "epoch": 2.5906666666666665,
      "grad_norm": 0.028635825961828232,
      "learning_rate": 0.0017371922392977808,
      "loss": 0.9679,
      "step": 1943
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.03758758679032326,
      "learning_rate": 0.0017369003876225642,
      "loss": 1.0779,
      "step": 1944
    },
    {
      "epoch": 2.5933333333333333,
      "grad_norm": 0.02744816616177559,
      "learning_rate": 0.0017366083985296947,
      "loss": 1.1397,
      "step": 1945
    },
    {
      "epoch": 2.594666666666667,
      "grad_norm": 0.03191674128174782,
      "learning_rate": 0.0017363162720736213,
      "loss": 1.1138,
      "step": 1946
    },
    {
      "epoch": 2.596,
      "grad_norm": 0.0489296056330204,
      "learning_rate": 0.0017360240083088212,
      "loss": 1.0352,
      "step": 1947
    },
    {
      "epoch": 2.5973333333333333,
      "grad_norm": 0.03980083391070366,
      "learning_rate": 0.0017357316072897954,
      "loss": 0.8709,
      "step": 1948
    },
    {
      "epoch": 2.5986666666666665,
      "grad_norm": 0.045689892023801804,
      "learning_rate": 0.0017354390690710706,
      "loss": 1.0905,
      "step": 1949
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.030223501846194267,
      "learning_rate": 0.0017351463937072004,
      "loss": 1.1125,
      "step": 1950
    },
    {
      "epoch": 2.6013333333333333,
      "grad_norm": 0.02479376830160618,
      "learning_rate": 0.0017348535812527627,
      "loss": 0.9081,
      "step": 1951
    },
    {
      "epoch": 2.602666666666667,
      "grad_norm": 0.029660390689969063,
      "learning_rate": 0.0017345606317623607,
      "loss": 0.8906,
      "step": 1952
    },
    {
      "epoch": 2.604,
      "grad_norm": 0.027431577444076538,
      "learning_rate": 0.001734267545290625,
      "loss": 0.8534,
      "step": 1953
    },
    {
      "epoch": 2.6053333333333333,
      "grad_norm": 0.026317572221159935,
      "learning_rate": 0.0017339743218922098,
      "loss": 1.0862,
      "step": 1954
    },
    {
      "epoch": 2.6066666666666665,
      "grad_norm": 0.036150652915239334,
      "learning_rate": 0.0017336809616217955,
      "loss": 1.3648,
      "step": 1955
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.03557697683572769,
      "learning_rate": 0.0017333874645340884,
      "loss": 1.2375,
      "step": 1956
    },
    {
      "epoch": 2.6093333333333333,
      "grad_norm": 0.033129189163446426,
      "learning_rate": 0.0017330938306838201,
      "loss": 1.3654,
      "step": 1957
    },
    {
      "epoch": 2.610666666666667,
      "grad_norm": 0.028644705191254616,
      "learning_rate": 0.0017328000601257475,
      "loss": 1.0062,
      "step": 1958
    },
    {
      "epoch": 2.612,
      "grad_norm": 0.02631392516195774,
      "learning_rate": 0.001732506152914653,
      "loss": 1.2656,
      "step": 1959
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.023919207975268364,
      "learning_rate": 0.0017322121091053447,
      "loss": 1.1096,
      "step": 1960
    },
    {
      "epoch": 2.6146666666666665,
      "grad_norm": 0.029948575422167778,
      "learning_rate": 0.0017319179287526563,
      "loss": 0.9483,
      "step": 1961
    },
    {
      "epoch": 2.616,
      "grad_norm": 0.03331578150391579,
      "learning_rate": 0.0017316236119114463,
      "loss": 1.124,
      "step": 1962
    },
    {
      "epoch": 2.6173333333333333,
      "grad_norm": 0.03279265761375427,
      "learning_rate": 0.0017313291586365995,
      "loss": 0.9874,
      "step": 1963
    },
    {
      "epoch": 2.618666666666667,
      "grad_norm": 0.026867352426052094,
      "learning_rate": 0.0017310345689830254,
      "loss": 1.0451,
      "step": 1964
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.032720897346735,
      "learning_rate": 0.0017307398430056594,
      "loss": 1.2785,
      "step": 1965
    },
    {
      "epoch": 2.6213333333333333,
      "grad_norm": 0.03052125871181488,
      "learning_rate": 0.0017304449807594624,
      "loss": 1.1854,
      "step": 1966
    },
    {
      "epoch": 2.6226666666666665,
      "grad_norm": 0.030474716797471046,
      "learning_rate": 0.0017301499822994202,
      "loss": 1.0471,
      "step": 1967
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.04120847210288048,
      "learning_rate": 0.0017298548476805445,
      "loss": 1.1471,
      "step": 1968
    },
    {
      "epoch": 2.6253333333333333,
      "grad_norm": 0.029449958354234695,
      "learning_rate": 0.001729559576957872,
      "loss": 1.0047,
      "step": 1969
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.025842493399977684,
      "learning_rate": 0.001729264170186465,
      "loss": 1.083,
      "step": 1970
    },
    {
      "epoch": 2.628,
      "grad_norm": 0.051873717457056046,
      "learning_rate": 0.0017289686274214115,
      "loss": 1.2096,
      "step": 1971
    },
    {
      "epoch": 2.6293333333333333,
      "grad_norm": 0.028842223808169365,
      "learning_rate": 0.001728672948717824,
      "loss": 0.8898,
      "step": 1972
    },
    {
      "epoch": 2.6306666666666665,
      "grad_norm": 0.03032749891281128,
      "learning_rate": 0.001728377134130841,
      "loss": 0.9749,
      "step": 1973
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.047939151525497437,
      "learning_rate": 0.0017280811837156267,
      "loss": 1.0602,
      "step": 1974
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 0.16585077345371246,
      "learning_rate": 0.0017277850975273696,
      "loss": 1.192,
      "step": 1975
    },
    {
      "epoch": 2.634666666666667,
      "grad_norm": 0.0331089161336422,
      "learning_rate": 0.001727488875621284,
      "loss": 1.0602,
      "step": 1976
    },
    {
      "epoch": 2.636,
      "grad_norm": 0.03391234204173088,
      "learning_rate": 0.0017271925180526093,
      "loss": 1.1445,
      "step": 1977
    },
    {
      "epoch": 2.6373333333333333,
      "grad_norm": 0.024979326874017715,
      "learning_rate": 0.0017268960248766115,
      "loss": 1.0458,
      "step": 1978
    },
    {
      "epoch": 2.6386666666666665,
      "grad_norm": 0.049865689128637314,
      "learning_rate": 0.0017265993961485798,
      "loss": 1.0624,
      "step": 1979
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.2344421148300171,
      "learning_rate": 0.00172630263192383,
      "loss": 0.9843,
      "step": 1980
    },
    {
      "epoch": 2.6413333333333333,
      "grad_norm": 0.04040343314409256,
      "learning_rate": 0.0017260057322577031,
      "loss": 1.3387,
      "step": 1981
    },
    {
      "epoch": 2.642666666666667,
      "grad_norm": 0.03210017457604408,
      "learning_rate": 0.0017257086972055648,
      "loss": 1.38,
      "step": 1982
    },
    {
      "epoch": 2.644,
      "grad_norm": 0.06780732423067093,
      "learning_rate": 0.001725411526822807,
      "loss": 1.0548,
      "step": 1983
    },
    {
      "epoch": 2.6453333333333333,
      "grad_norm": 0.100440613925457,
      "learning_rate": 0.0017251142211648456,
      "loss": 0.9536,
      "step": 1984
    },
    {
      "epoch": 2.6466666666666665,
      "grad_norm": 0.049759045243263245,
      "learning_rate": 0.0017248167802871224,
      "loss": 1.2586,
      "step": 1985
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.056993160396814346,
      "learning_rate": 0.0017245192042451049,
      "loss": 1.0813,
      "step": 1986
    },
    {
      "epoch": 2.6493333333333333,
      "grad_norm": 0.19502931833267212,
      "learning_rate": 0.0017242214930942845,
      "loss": 1.1804,
      "step": 1987
    },
    {
      "epoch": 2.6506666666666665,
      "grad_norm": 0.03506189584732056,
      "learning_rate": 0.0017239236468901788,
      "loss": 1.2308,
      "step": 1988
    },
    {
      "epoch": 2.652,
      "grad_norm": 0.03004157543182373,
      "learning_rate": 0.001723625665688331,
      "loss": 1.0153,
      "step": 1989
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 0.02582607790827751,
      "learning_rate": 0.0017233275495443083,
      "loss": 1.1196,
      "step": 1990
    },
    {
      "epoch": 2.6546666666666665,
      "grad_norm": 0.03473260626196861,
      "learning_rate": 0.0017230292985137032,
      "loss": 1.048,
      "step": 1991
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.03128723055124283,
      "learning_rate": 0.0017227309126521346,
      "loss": 1.2694,
      "step": 1992
    },
    {
      "epoch": 2.6573333333333333,
      "grad_norm": 0.061163391917943954,
      "learning_rate": 0.0017224323920152452,
      "loss": 1.1278,
      "step": 1993
    },
    {
      "epoch": 2.6586666666666665,
      "grad_norm": 0.0666196420788765,
      "learning_rate": 0.0017221337366587028,
      "loss": 1.0615,
      "step": 1994
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.03356173262000084,
      "learning_rate": 0.001721834946638202,
      "loss": 1.201,
      "step": 1995
    },
    {
      "epoch": 2.6613333333333333,
      "grad_norm": 0.034998610615730286,
      "learning_rate": 0.0017215360220094607,
      "loss": 1.0729,
      "step": 1996
    },
    {
      "epoch": 2.6626666666666665,
      "grad_norm": 0.20809976756572723,
      "learning_rate": 0.0017212369628282224,
      "loss": 1.1157,
      "step": 1997
    },
    {
      "epoch": 2.664,
      "grad_norm": 0.2673642635345459,
      "learning_rate": 0.0017209377691502564,
      "loss": 0.8501,
      "step": 1998
    },
    {
      "epoch": 2.6653333333333333,
      "grad_norm": 0.04867381229996681,
      "learning_rate": 0.0017206384410313558,
      "loss": 1.116,
      "step": 1999
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.14093253016471863,
      "learning_rate": 0.00172033897852734,
      "loss": 1.1857,
      "step": 2000
    },
    {
      "epoch": 2.668,
      "grad_norm": 0.04934033378958702,
      "learning_rate": 0.001720039381694053,
      "loss": 0.8924,
      "step": 2001
    },
    {
      "epoch": 2.6693333333333333,
      "grad_norm": 0.09533829987049103,
      "learning_rate": 0.0017197396505873638,
      "loss": 1.0268,
      "step": 2002
    },
    {
      "epoch": 2.6706666666666665,
      "grad_norm": 0.05273430421948433,
      "learning_rate": 0.0017194397852631658,
      "loss": 1.2402,
      "step": 2003
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.030559008941054344,
      "learning_rate": 0.0017191397857773787,
      "loss": 0.8691,
      "step": 2004
    },
    {
      "epoch": 2.6733333333333333,
      "grad_norm": 0.03387072682380676,
      "learning_rate": 0.0017188396521859465,
      "loss": 1.0752,
      "step": 2005
    },
    {
      "epoch": 2.6746666666666665,
      "grad_norm": 0.028690669685602188,
      "learning_rate": 0.0017185393845448385,
      "loss": 1.2717,
      "step": 2006
    },
    {
      "epoch": 2.676,
      "grad_norm": 0.03513453155755997,
      "learning_rate": 0.0017182389829100485,
      "loss": 1.1853,
      "step": 2007
    },
    {
      "epoch": 2.6773333333333333,
      "grad_norm": 0.03785013407468796,
      "learning_rate": 0.0017179384473375956,
      "loss": 0.9724,
      "step": 2008
    },
    {
      "epoch": 2.6786666666666665,
      "grad_norm": 0.0489492192864418,
      "learning_rate": 0.001717637777883524,
      "loss": 1.0388,
      "step": 2009
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.03316517546772957,
      "learning_rate": 0.0017173369746039024,
      "loss": 1.0386,
      "step": 2010
    },
    {
      "epoch": 2.6813333333333333,
      "grad_norm": 0.032570093870162964,
      "learning_rate": 0.0017170360375548253,
      "loss": 1.1129,
      "step": 2011
    },
    {
      "epoch": 2.6826666666666665,
      "grad_norm": 0.04309031739830971,
      "learning_rate": 0.0017167349667924112,
      "loss": 1.0446,
      "step": 2012
    },
    {
      "epoch": 2.684,
      "grad_norm": 0.03181082382798195,
      "learning_rate": 0.0017164337623728043,
      "loss": 1.3203,
      "step": 2013
    },
    {
      "epoch": 2.6853333333333333,
      "grad_norm": 0.02460576593875885,
      "learning_rate": 0.0017161324243521734,
      "loss": 1.0535,
      "step": 2014
    },
    {
      "epoch": 2.6866666666666665,
      "grad_norm": 0.035589586943387985,
      "learning_rate": 0.0017158309527867117,
      "loss": 1.1646,
      "step": 2015
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.04682967811822891,
      "learning_rate": 0.0017155293477326384,
      "loss": 1.0166,
      "step": 2016
    },
    {
      "epoch": 2.6893333333333334,
      "grad_norm": 0.037691835314035416,
      "learning_rate": 0.0017152276092461966,
      "loss": 1.3934,
      "step": 2017
    },
    {
      "epoch": 2.6906666666666665,
      "grad_norm": 0.03317675739526749,
      "learning_rate": 0.0017149257373836552,
      "loss": 1.0606,
      "step": 2018
    },
    {
      "epoch": 2.692,
      "grad_norm": 0.03101968951523304,
      "learning_rate": 0.0017146237322013067,
      "loss": 1.1945,
      "step": 2019
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 0.03179483488202095,
      "learning_rate": 0.0017143215937554697,
      "loss": 1.3917,
      "step": 2020
    },
    {
      "epoch": 2.6946666666666665,
      "grad_norm": 0.03675268962979317,
      "learning_rate": 0.001714019322102487,
      "loss": 0.8715,
      "step": 2021
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.036712389439344406,
      "learning_rate": 0.0017137169172987269,
      "loss": 1.1023,
      "step": 2022
    },
    {
      "epoch": 2.6973333333333334,
      "grad_norm": 0.030668068677186966,
      "learning_rate": 0.0017134143794005814,
      "loss": 1.2298,
      "step": 2023
    },
    {
      "epoch": 2.6986666666666665,
      "grad_norm": 0.038837309926748276,
      "learning_rate": 0.001713111708464468,
      "loss": 1.0209,
      "step": 2024
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.032974373549222946,
      "learning_rate": 0.0017128089045468293,
      "loss": 1.1803,
      "step": 2025
    },
    {
      "epoch": 2.7013333333333334,
      "grad_norm": 0.03275725245475769,
      "learning_rate": 0.0017125059677041321,
      "loss": 1.2616,
      "step": 2026
    },
    {
      "epoch": 2.7026666666666666,
      "grad_norm": 0.03391652554273605,
      "learning_rate": 0.0017122028979928686,
      "loss": 1.1681,
      "step": 2027
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.0403575524687767,
      "learning_rate": 0.0017118996954695552,
      "loss": 0.9883,
      "step": 2028
    },
    {
      "epoch": 2.7053333333333334,
      "grad_norm": 0.03424416854977608,
      "learning_rate": 0.0017115963601907335,
      "loss": 1.0544,
      "step": 2029
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 0.02980235405266285,
      "learning_rate": 0.0017112928922129692,
      "loss": 0.9938,
      "step": 2030
    },
    {
      "epoch": 2.708,
      "grad_norm": 0.025671586394309998,
      "learning_rate": 0.0017109892915928536,
      "loss": 0.8725,
      "step": 2031
    },
    {
      "epoch": 2.7093333333333334,
      "grad_norm": 0.034289754927158356,
      "learning_rate": 0.0017106855583870022,
      "loss": 0.9718,
      "step": 2032
    },
    {
      "epoch": 2.7106666666666666,
      "grad_norm": 0.028375549241900444,
      "learning_rate": 0.0017103816926520553,
      "loss": 1.1043,
      "step": 2033
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.029182562604546547,
      "learning_rate": 0.0017100776944446781,
      "loss": 0.9961,
      "step": 2034
    },
    {
      "epoch": 2.7133333333333334,
      "grad_norm": 0.0296177938580513,
      "learning_rate": 0.0017097735638215603,
      "loss": 0.9604,
      "step": 2035
    },
    {
      "epoch": 2.7146666666666666,
      "grad_norm": 0.02634892426431179,
      "learning_rate": 0.0017094693008394164,
      "loss": 1.0408,
      "step": 2036
    },
    {
      "epoch": 2.716,
      "grad_norm": 0.034140001982450485,
      "learning_rate": 0.0017091649055549856,
      "loss": 1.3482,
      "step": 2037
    },
    {
      "epoch": 2.7173333333333334,
      "grad_norm": 0.02327243983745575,
      "learning_rate": 0.0017088603780250318,
      "loss": 1.1158,
      "step": 2038
    },
    {
      "epoch": 2.7186666666666666,
      "grad_norm": 0.03223568573594093,
      "learning_rate": 0.0017085557183063433,
      "loss": 0.9276,
      "step": 2039
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.03762540966272354,
      "learning_rate": 0.001708250926455733,
      "loss": 1.1997,
      "step": 2040
    },
    {
      "epoch": 2.7213333333333334,
      "grad_norm": 0.03445546329021454,
      "learning_rate": 0.0017079460025300394,
      "loss": 1.2014,
      "step": 2041
    },
    {
      "epoch": 2.7226666666666666,
      "grad_norm": 0.03446699678897858,
      "learning_rate": 0.0017076409465861247,
      "loss": 0.9947,
      "step": 2042
    },
    {
      "epoch": 2.724,
      "grad_norm": 0.03141940012574196,
      "learning_rate": 0.001707335758680875,
      "loss": 1.069,
      "step": 2043
    },
    {
      "epoch": 2.7253333333333334,
      "grad_norm": 0.04520728066563606,
      "learning_rate": 0.0017070304388712035,
      "loss": 1.1207,
      "step": 2044
    },
    {
      "epoch": 2.7266666666666666,
      "grad_norm": 0.027693241834640503,
      "learning_rate": 0.0017067249872140448,
      "loss": 1.1723,
      "step": 2045
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.03063521347939968,
      "learning_rate": 0.0017064194037663609,
      "loss": 1.1132,
      "step": 2046
    },
    {
      "epoch": 2.7293333333333334,
      "grad_norm": 0.031268924474716187,
      "learning_rate": 0.0017061136885851368,
      "loss": 1.111,
      "step": 2047
    },
    {
      "epoch": 2.7306666666666666,
      "grad_norm": 0.027714010328054428,
      "learning_rate": 0.0017058078417273826,
      "loss": 1.1536,
      "step": 2048
    },
    {
      "epoch": 2.732,
      "grad_norm": 0.04158652201294899,
      "learning_rate": 0.0017055018632501325,
      "loss": 1.0998,
      "step": 2049
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.03460021689534187,
      "learning_rate": 0.001705195753210446,
      "loss": 1.1209,
      "step": 2050
    },
    {
      "epoch": 2.7346666666666666,
      "grad_norm": 0.02969503030180931,
      "learning_rate": 0.001704889511665406,
      "loss": 1.0976,
      "step": 2051
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.037438347935676575,
      "learning_rate": 0.0017045831386721213,
      "loss": 0.932,
      "step": 2052
    },
    {
      "epoch": 2.7373333333333334,
      "grad_norm": 0.028125599026679993,
      "learning_rate": 0.0017042766342877244,
      "loss": 1.1059,
      "step": 2053
    },
    {
      "epoch": 2.7386666666666666,
      "grad_norm": 0.037256207317113876,
      "learning_rate": 0.001703969998569372,
      "loss": 1.0108,
      "step": 2054
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.027397317811846733,
      "learning_rate": 0.0017036632315742461,
      "loss": 1.1172,
      "step": 2055
    },
    {
      "epoch": 2.7413333333333334,
      "grad_norm": 0.02697867900133133,
      "learning_rate": 0.0017033563333595531,
      "loss": 1.1107,
      "step": 2056
    },
    {
      "epoch": 2.7426666666666666,
      "grad_norm": 0.03079160302877426,
      "learning_rate": 0.001703049303982523,
      "loss": 0.9345,
      "step": 2057
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.04173128306865692,
      "learning_rate": 0.0017027421435004111,
      "loss": 1.1811,
      "step": 2058
    },
    {
      "epoch": 2.7453333333333334,
      "grad_norm": 0.031500495970249176,
      "learning_rate": 0.001702434851970497,
      "loss": 1.1235,
      "step": 2059
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.026693033054471016,
      "learning_rate": 0.0017021274294500841,
      "loss": 1.0928,
      "step": 2060
    },
    {
      "epoch": 2.748,
      "grad_norm": 0.027827855199575424,
      "learning_rate": 0.0017018198759965015,
      "loss": 1.1029,
      "step": 2061
    },
    {
      "epoch": 2.7493333333333334,
      "grad_norm": 0.033510591834783554,
      "learning_rate": 0.0017015121916671013,
      "loss": 1.1794,
      "step": 2062
    },
    {
      "epoch": 2.7506666666666666,
      "grad_norm": 0.02843673713505268,
      "learning_rate": 0.001701204376519261,
      "loss": 1.0664,
      "step": 2063
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.030112426728010178,
      "learning_rate": 0.0017008964306103823,
      "loss": 0.9972,
      "step": 2064
    },
    {
      "epoch": 2.7533333333333334,
      "grad_norm": 0.047872770577669144,
      "learning_rate": 0.0017005883539978907,
      "loss": 1.3286,
      "step": 2065
    },
    {
      "epoch": 2.7546666666666666,
      "grad_norm": 0.030683279037475586,
      "learning_rate": 0.001700280146739237,
      "loss": 0.9745,
      "step": 2066
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 0.03807344287633896,
      "learning_rate": 0.0016999718088918953,
      "loss": 1.0658,
      "step": 2067
    },
    {
      "epoch": 2.7573333333333334,
      "grad_norm": 0.023799574002623558,
      "learning_rate": 0.0016996633405133655,
      "loss": 1.159,
      "step": 2068
    },
    {
      "epoch": 2.7586666666666666,
      "grad_norm": 0.025288116186857224,
      "learning_rate": 0.0016993547416611703,
      "loss": 0.8737,
      "step": 2069
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.032817404717206955,
      "learning_rate": 0.0016990460123928574,
      "loss": 1.231,
      "step": 2070
    },
    {
      "epoch": 2.7613333333333334,
      "grad_norm": 0.027728982269763947,
      "learning_rate": 0.0016987371527659994,
      "loss": 1.0563,
      "step": 2071
    },
    {
      "epoch": 2.7626666666666666,
      "grad_norm": 0.03325491026043892,
      "learning_rate": 0.0016984281628381918,
      "loss": 1.2032,
      "step": 2072
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 0.033552151173353195,
      "learning_rate": 0.0016981190426670558,
      "loss": 1.0454,
      "step": 2073
    },
    {
      "epoch": 2.7653333333333334,
      "grad_norm": 0.036240726709365845,
      "learning_rate": 0.0016978097923102367,
      "loss": 0.9989,
      "step": 2074
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 0.04960215836763382,
      "learning_rate": 0.001697500411825403,
      "loss": 1.2071,
      "step": 2075
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.02804628200829029,
      "learning_rate": 0.0016971909012702482,
      "loss": 1.0561,
      "step": 2076
    },
    {
      "epoch": 2.7693333333333334,
      "grad_norm": 0.026453105732798576,
      "learning_rate": 0.0016968812607024903,
      "loss": 0.9807,
      "step": 2077
    },
    {
      "epoch": 2.7706666666666666,
      "grad_norm": 0.025326237082481384,
      "learning_rate": 0.0016965714901798713,
      "loss": 1.1016,
      "step": 2078
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 0.023310046643018723,
      "learning_rate": 0.0016962615897601573,
      "loss": 1.1687,
      "step": 2079
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 0.029127145186066628,
      "learning_rate": 0.0016959515595011388,
      "loss": 0.9032,
      "step": 2080
    },
    {
      "epoch": 2.7746666666666666,
      "grad_norm": 0.026633568108081818,
      "learning_rate": 0.00169564139946063,
      "loss": 0.9775,
      "step": 2081
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.10063374042510986,
      "learning_rate": 0.0016953311096964704,
      "loss": 1.1288,
      "step": 2082
    },
    {
      "epoch": 2.7773333333333334,
      "grad_norm": 0.03364305570721626,
      "learning_rate": 0.0016950206902665225,
      "loss": 0.9723,
      "step": 2083
    },
    {
      "epoch": 2.7786666666666666,
      "grad_norm": 0.03733626753091812,
      "learning_rate": 0.0016947101412286743,
      "loss": 0.9299,
      "step": 2084
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 0.033767685294151306,
      "learning_rate": 0.0016943994626408363,
      "loss": 1.0825,
      "step": 2085
    },
    {
      "epoch": 2.7813333333333334,
      "grad_norm": 0.030657511204481125,
      "learning_rate": 0.0016940886545609444,
      "loss": 1.0409,
      "step": 2086
    },
    {
      "epoch": 2.7826666666666666,
      "grad_norm": 0.0309575367718935,
      "learning_rate": 0.001693777717046958,
      "loss": 0.9444,
      "step": 2087
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.028843896463513374,
      "learning_rate": 0.0016934666501568615,
      "loss": 1.0669,
      "step": 2088
    },
    {
      "epoch": 2.7853333333333334,
      "grad_norm": 0.03174426406621933,
      "learning_rate": 0.0016931554539486627,
      "loss": 1.1807,
      "step": 2089
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 0.03522256016731262,
      "learning_rate": 0.0016928441284803934,
      "loss": 1.1396,
      "step": 2090
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 0.0382445827126503,
      "learning_rate": 0.00169253267381011,
      "loss": 1.2009,
      "step": 2091
    },
    {
      "epoch": 2.7893333333333334,
      "grad_norm": 0.021640866994857788,
      "learning_rate": 0.0016922210899958923,
      "loss": 0.9273,
      "step": 2092
    },
    {
      "epoch": 2.7906666666666666,
      "grad_norm": 0.04055514931678772,
      "learning_rate": 0.0016919093770958451,
      "loss": 1.0401,
      "step": 2093
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.03195895627140999,
      "learning_rate": 0.0016915975351680969,
      "loss": 1.3507,
      "step": 2094
    },
    {
      "epoch": 2.7933333333333334,
      "grad_norm": 0.03262104466557503,
      "learning_rate": 0.0016912855642708,
      "loss": 1.1323,
      "step": 2095
    },
    {
      "epoch": 2.7946666666666666,
      "grad_norm": 0.033069856464862823,
      "learning_rate": 0.0016909734644621306,
      "loss": 0.8141,
      "step": 2096
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 0.037650495767593384,
      "learning_rate": 0.0016906612358002899,
      "loss": 1.203,
      "step": 2097
    },
    {
      "epoch": 2.7973333333333334,
      "grad_norm": 0.026346171274781227,
      "learning_rate": 0.0016903488783435023,
      "loss": 1.1525,
      "step": 2098
    },
    {
      "epoch": 2.7986666666666666,
      "grad_norm": 0.025141645222902298,
      "learning_rate": 0.0016900363921500162,
      "loss": 1.0759,
      "step": 2099
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.02343449555337429,
      "learning_rate": 0.0016897237772781045,
      "loss": 1.0777,
      "step": 2100
    },
    {
      "epoch": 2.8013333333333335,
      "grad_norm": 0.02964434213936329,
      "learning_rate": 0.0016894110337860632,
      "loss": 1.1308,
      "step": 2101
    },
    {
      "epoch": 2.8026666666666666,
      "grad_norm": 0.034765392541885376,
      "learning_rate": 0.0016890981617322138,
      "loss": 1.1458,
      "step": 2102
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 0.027675211429595947,
      "learning_rate": 0.0016887851611749004,
      "loss": 0.9139,
      "step": 2103
    },
    {
      "epoch": 2.8053333333333335,
      "grad_norm": 0.045197103172540665,
      "learning_rate": 0.0016884720321724913,
      "loss": 1.0593,
      "step": 2104
    },
    {
      "epoch": 2.8066666666666666,
      "grad_norm": 0.025029940530657768,
      "learning_rate": 0.0016881587747833793,
      "loss": 1.0039,
      "step": 2105
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.03792248293757439,
      "learning_rate": 0.0016878453890659814,
      "loss": 1.1556,
      "step": 2106
    },
    {
      "epoch": 2.8093333333333335,
      "grad_norm": 0.02528727799654007,
      "learning_rate": 0.001687531875078737,
      "loss": 0.9454,
      "step": 2107
    },
    {
      "epoch": 2.8106666666666666,
      "grad_norm": 0.04167123883962631,
      "learning_rate": 0.0016872182328801107,
      "loss": 1.0573,
      "step": 2108
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 0.026222148910164833,
      "learning_rate": 0.0016869044625285907,
      "loss": 1.1156,
      "step": 2109
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 0.05343370512127876,
      "learning_rate": 0.0016865905640826894,
      "loss": 0.7752,
      "step": 2110
    },
    {
      "epoch": 2.8146666666666667,
      "grad_norm": 0.05141665041446686,
      "learning_rate": 0.0016862765376009427,
      "loss": 1.3021,
      "step": 2111
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.029767956584692,
      "learning_rate": 0.00168596238314191,
      "loss": 1.3002,
      "step": 2112
    },
    {
      "epoch": 2.8173333333333335,
      "grad_norm": 0.03189937025308609,
      "learning_rate": 0.0016856481007641755,
      "loss": 1.2978,
      "step": 2113
    },
    {
      "epoch": 2.8186666666666667,
      "grad_norm": 0.030041303485631943,
      "learning_rate": 0.0016853336905263466,
      "loss": 0.8052,
      "step": 2114
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.0308882724493742,
      "learning_rate": 0.0016850191524870547,
      "loss": 1.3346,
      "step": 2115
    },
    {
      "epoch": 2.8213333333333335,
      "grad_norm": 0.040680013597011566,
      "learning_rate": 0.0016847044867049552,
      "loss": 1.1394,
      "step": 2116
    },
    {
      "epoch": 2.8226666666666667,
      "grad_norm": 0.03553124889731407,
      "learning_rate": 0.0016843896932387267,
      "loss": 1.0484,
      "step": 2117
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.03619334101676941,
      "learning_rate": 0.001684074772147073,
      "loss": 0.9638,
      "step": 2118
    },
    {
      "epoch": 2.8253333333333335,
      "grad_norm": 0.03191347047686577,
      "learning_rate": 0.0016837597234887198,
      "loss": 0.9303,
      "step": 2119
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 0.022430045530200005,
      "learning_rate": 0.0016834445473224182,
      "loss": 1.0912,
      "step": 2120
    },
    {
      "epoch": 2.828,
      "grad_norm": 0.04302253574132919,
      "learning_rate": 0.0016831292437069424,
      "loss": 1.2292,
      "step": 2121
    },
    {
      "epoch": 2.8293333333333335,
      "grad_norm": 0.028203662484884262,
      "learning_rate": 0.0016828138127010903,
      "loss": 1.2631,
      "step": 2122
    },
    {
      "epoch": 2.8306666666666667,
      "grad_norm": 0.044472649693489075,
      "learning_rate": 0.0016824982543636833,
      "loss": 1.0203,
      "step": 2123
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.027330053970217705,
      "learning_rate": 0.0016821825687535674,
      "loss": 1.1086,
      "step": 2124
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.04065445438027382,
      "learning_rate": 0.0016818667559296118,
      "loss": 1.048,
      "step": 2125
    },
    {
      "epoch": 2.8346666666666667,
      "grad_norm": 0.03411131352186203,
      "learning_rate": 0.0016815508159507095,
      "loss": 1.2013,
      "step": 2126
    },
    {
      "epoch": 2.836,
      "grad_norm": 0.0335271917283535,
      "learning_rate": 0.0016812347488757773,
      "loss": 1.076,
      "step": 2127
    },
    {
      "epoch": 2.8373333333333335,
      "grad_norm": 0.04863887280225754,
      "learning_rate": 0.0016809185547637551,
      "loss": 0.9435,
      "step": 2128
    },
    {
      "epoch": 2.8386666666666667,
      "grad_norm": 0.03096184879541397,
      "learning_rate": 0.0016806022336736072,
      "loss": 0.9502,
      "step": 2129
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.03180520981550217,
      "learning_rate": 0.0016802857856643215,
      "loss": 1.1657,
      "step": 2130
    },
    {
      "epoch": 2.8413333333333335,
      "grad_norm": 0.026362065225839615,
      "learning_rate": 0.0016799692107949094,
      "loss": 1.0045,
      "step": 2131
    },
    {
      "epoch": 2.8426666666666667,
      "grad_norm": 0.026506422087550163,
      "learning_rate": 0.0016796525091244058,
      "loss": 1.0897,
      "step": 2132
    },
    {
      "epoch": 2.844,
      "grad_norm": 0.021864870563149452,
      "learning_rate": 0.0016793356807118696,
      "loss": 1.0137,
      "step": 2133
    },
    {
      "epoch": 2.8453333333333335,
      "grad_norm": 0.02694203332066536,
      "learning_rate": 0.0016790187256163832,
      "loss": 1.1998,
      "step": 2134
    },
    {
      "epoch": 2.8466666666666667,
      "grad_norm": 0.03404393419623375,
      "learning_rate": 0.0016787016438970525,
      "loss": 1.1206,
      "step": 2135
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.03353532403707504,
      "learning_rate": 0.001678384435613007,
      "loss": 1.5854,
      "step": 2136
    },
    {
      "epoch": 2.8493333333333335,
      "grad_norm": 0.034878987818956375,
      "learning_rate": 0.0016780671008234003,
      "loss": 1.2796,
      "step": 2137
    },
    {
      "epoch": 2.8506666666666667,
      "grad_norm": 0.035059258341789246,
      "learning_rate": 0.0016777496395874087,
      "loss": 1.054,
      "step": 2138
    },
    {
      "epoch": 2.852,
      "grad_norm": 0.02423018403351307,
      "learning_rate": 0.001677432051964233,
      "loss": 1.281,
      "step": 2139
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 0.030760742723941803,
      "learning_rate": 0.0016771143380130966,
      "loss": 1.3064,
      "step": 2140
    },
    {
      "epoch": 2.8546666666666667,
      "grad_norm": 0.03086676076054573,
      "learning_rate": 0.0016767964977932475,
      "loss": 1.0698,
      "step": 2141
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.04736403375864029,
      "learning_rate": 0.0016764785313639569,
      "loss": 1.2101,
      "step": 2142
    },
    {
      "epoch": 2.857333333333333,
      "grad_norm": 0.03809691220521927,
      "learning_rate": 0.0016761604387845186,
      "loss": 1.2404,
      "step": 2143
    },
    {
      "epoch": 2.8586666666666667,
      "grad_norm": 0.030456513166427612,
      "learning_rate": 0.0016758422201142511,
      "loss": 1.1205,
      "step": 2144
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.03305927664041519,
      "learning_rate": 0.0016755238754124963,
      "loss": 1.2568,
      "step": 2145
    },
    {
      "epoch": 2.8613333333333335,
      "grad_norm": 0.0346122570335865,
      "learning_rate": 0.001675205404738619,
      "loss": 0.9099,
      "step": 2146
    },
    {
      "epoch": 2.8626666666666667,
      "grad_norm": 0.029987752437591553,
      "learning_rate": 0.0016748868081520084,
      "loss": 1.5107,
      "step": 2147
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.029430657625198364,
      "learning_rate": 0.0016745680857120754,
      "loss": 1.006,
      "step": 2148
    },
    {
      "epoch": 2.865333333333333,
      "grad_norm": 0.026394207030534744,
      "learning_rate": 0.0016742492374782567,
      "loss": 1.2301,
      "step": 2149
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.030711645260453224,
      "learning_rate": 0.0016739302635100108,
      "loss": 0.9185,
      "step": 2150
    },
    {
      "epoch": 2.868,
      "grad_norm": 0.029481373727321625,
      "learning_rate": 0.0016736111638668203,
      "loss": 1.1503,
      "step": 2151
    },
    {
      "epoch": 2.8693333333333335,
      "grad_norm": 0.03605872020125389,
      "learning_rate": 0.001673291938608191,
      "loss": 1.3511,
      "step": 2152
    },
    {
      "epoch": 2.8706666666666667,
      "grad_norm": 0.06657612323760986,
      "learning_rate": 0.0016729725877936525,
      "loss": 1.2401,
      "step": 2153
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.03753475099802017,
      "learning_rate": 0.0016726531114827571,
      "loss": 1.103,
      "step": 2154
    },
    {
      "epoch": 2.873333333333333,
      "grad_norm": 0.027939658612012863,
      "learning_rate": 0.0016723335097350814,
      "loss": 0.8036,
      "step": 2155
    },
    {
      "epoch": 2.8746666666666667,
      "grad_norm": 0.02446945756673813,
      "learning_rate": 0.0016720137826102246,
      "loss": 0.984,
      "step": 2156
    },
    {
      "epoch": 2.876,
      "grad_norm": 0.030278405174613,
      "learning_rate": 0.0016716939301678099,
      "loss": 0.9672,
      "step": 2157
    },
    {
      "epoch": 2.8773333333333335,
      "grad_norm": 0.030775273218750954,
      "learning_rate": 0.0016713739524674834,
      "loss": 0.9816,
      "step": 2158
    },
    {
      "epoch": 2.8786666666666667,
      "grad_norm": 0.0977621078491211,
      "learning_rate": 0.0016710538495689144,
      "loss": 1.3744,
      "step": 2159
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.03202559053897858,
      "learning_rate": 0.0016707336215317967,
      "loss": 1.1712,
      "step": 2160
    },
    {
      "epoch": 2.881333333333333,
      "grad_norm": 0.03216254338622093,
      "learning_rate": 0.001670413268415846,
      "loss": 0.8732,
      "step": 2161
    },
    {
      "epoch": 2.8826666666666667,
      "grad_norm": 0.025230593979358673,
      "learning_rate": 0.0016700927902808024,
      "loss": 0.7918,
      "step": 2162
    },
    {
      "epoch": 2.884,
      "grad_norm": 0.02734268270432949,
      "learning_rate": 0.0016697721871864284,
      "loss": 0.9456,
      "step": 2163
    },
    {
      "epoch": 2.8853333333333335,
      "grad_norm": 0.032931648194789886,
      "learning_rate": 0.0016694514591925106,
      "loss": 1.252,
      "step": 2164
    },
    {
      "epoch": 2.8866666666666667,
      "grad_norm": 0.031409066170454025,
      "learning_rate": 0.0016691306063588583,
      "loss": 1.2832,
      "step": 2165
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.025559023022651672,
      "learning_rate": 0.0016688096287453046,
      "loss": 1.0691,
      "step": 2166
    },
    {
      "epoch": 2.889333333333333,
      "grad_norm": 0.022565821185708046,
      "learning_rate": 0.0016684885264117053,
      "loss": 1.056,
      "step": 2167
    },
    {
      "epoch": 2.8906666666666667,
      "grad_norm": 0.025213690474629402,
      "learning_rate": 0.0016681672994179402,
      "loss": 0.9716,
      "step": 2168
    },
    {
      "epoch": 2.892,
      "grad_norm": 0.03298121690750122,
      "learning_rate": 0.0016678459478239116,
      "loss": 1.0136,
      "step": 2169
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 0.041112445294857025,
      "learning_rate": 0.0016675244716895454,
      "loss": 0.9623,
      "step": 2170
    },
    {
      "epoch": 2.8946666666666667,
      "grad_norm": 0.02578364685177803,
      "learning_rate": 0.0016672028710747909,
      "loss": 1.0145,
      "step": 2171
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.028488656505942345,
      "learning_rate": 0.0016668811460396201,
      "loss": 0.9108,
      "step": 2172
    },
    {
      "epoch": 2.897333333333333,
      "grad_norm": 0.028791695833206177,
      "learning_rate": 0.0016665592966440283,
      "loss": 1.1392,
      "step": 2173
    },
    {
      "epoch": 2.8986666666666667,
      "grad_norm": 0.025598417967557907,
      "learning_rate": 0.001666237322948035,
      "loss": 1.0611,
      "step": 2174
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.03429477661848068,
      "learning_rate": 0.0016659152250116812,
      "loss": 1.3977,
      "step": 2175
    },
    {
      "epoch": 2.9013333333333335,
      "grad_norm": 0.03685419633984566,
      "learning_rate": 0.001665593002895032,
      "loss": 1.0687,
      "step": 2176
    },
    {
      "epoch": 2.9026666666666667,
      "grad_norm": 0.027557305991649628,
      "learning_rate": 0.0016652706566581763,
      "loss": 0.8656,
      "step": 2177
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.02455511875450611,
      "learning_rate": 0.0016649481863612247,
      "loss": 0.9429,
      "step": 2178
    },
    {
      "epoch": 2.905333333333333,
      "grad_norm": 0.028105491772294044,
      "learning_rate": 0.0016646255920643124,
      "loss": 1.1473,
      "step": 2179
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.026881057769060135,
      "learning_rate": 0.0016643028738275959,
      "loss": 1.0775,
      "step": 2180
    },
    {
      "epoch": 2.908,
      "grad_norm": 0.03895958513021469,
      "learning_rate": 0.0016639800317112567,
      "loss": 0.8634,
      "step": 2181
    },
    {
      "epoch": 2.9093333333333335,
      "grad_norm": 0.028040217235684395,
      "learning_rate": 0.0016636570657754986,
      "loss": 0.9246,
      "step": 2182
    },
    {
      "epoch": 2.9106666666666667,
      "grad_norm": 0.03344236686825752,
      "learning_rate": 0.0016633339760805481,
      "loss": 0.9438,
      "step": 2183
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.03419153764843941,
      "learning_rate": 0.0016630107626866557,
      "loss": 0.7909,
      "step": 2184
    },
    {
      "epoch": 2.913333333333333,
      "grad_norm": 0.03703178092837334,
      "learning_rate": 0.0016626874256540937,
      "loss": 1.2451,
      "step": 2185
    },
    {
      "epoch": 2.9146666666666667,
      "grad_norm": 0.030647223815321922,
      "learning_rate": 0.0016623639650431591,
      "loss": 1.0949,
      "step": 2186
    },
    {
      "epoch": 2.916,
      "grad_norm": 0.04085690900683403,
      "learning_rate": 0.0016620403809141705,
      "loss": 1.0992,
      "step": 2187
    },
    {
      "epoch": 2.9173333333333336,
      "grad_norm": 0.029108494520187378,
      "learning_rate": 0.0016617166733274699,
      "loss": 1.0873,
      "step": 2188
    },
    {
      "epoch": 2.9186666666666667,
      "grad_norm": 0.029377765953540802,
      "learning_rate": 0.001661392842343423,
      "loss": 0.8937,
      "step": 2189
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.0358896367251873,
      "learning_rate": 0.0016610688880224177,
      "loss": 1.2077,
      "step": 2190
    },
    {
      "epoch": 2.921333333333333,
      "grad_norm": 0.03121590055525303,
      "learning_rate": 0.0016607448104248648,
      "loss": 0.944,
      "step": 2191
    },
    {
      "epoch": 2.9226666666666667,
      "grad_norm": 0.03218378126621246,
      "learning_rate": 0.0016604206096111995,
      "loss": 0.9928,
      "step": 2192
    },
    {
      "epoch": 2.924,
      "grad_norm": 0.03160586580634117,
      "learning_rate": 0.0016600962856418782,
      "loss": 0.7924,
      "step": 2193
    },
    {
      "epoch": 2.9253333333333336,
      "grad_norm": 0.04188750684261322,
      "learning_rate": 0.0016597718385773814,
      "loss": 0.9488,
      "step": 2194
    },
    {
      "epoch": 2.9266666666666667,
      "grad_norm": 0.03667214885354042,
      "learning_rate": 0.001659447268478212,
      "loss": 1.4016,
      "step": 2195
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.040883492678403854,
      "learning_rate": 0.0016591225754048961,
      "loss": 1.2023,
      "step": 2196
    },
    {
      "epoch": 2.929333333333333,
      "grad_norm": 0.04800324887037277,
      "learning_rate": 0.0016587977594179825,
      "loss": 1.2528,
      "step": 2197
    },
    {
      "epoch": 2.9306666666666668,
      "grad_norm": 0.03356111794710159,
      "learning_rate": 0.0016584728205780435,
      "loss": 0.9167,
      "step": 2198
    },
    {
      "epoch": 2.932,
      "grad_norm": 0.04765361174941063,
      "learning_rate": 0.0016581477589456735,
      "loss": 1.011,
      "step": 2199
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.031024662777781487,
      "learning_rate": 0.0016578225745814909,
      "loss": 1.1746,
      "step": 2200
    },
    {
      "epoch": 2.9346666666666668,
      "grad_norm": 0.11833731085062027,
      "learning_rate": 0.001657497267546135,
      "loss": 1.0211,
      "step": 2201
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.10052083432674408,
      "learning_rate": 0.0016571718379002705,
      "loss": 1.0527,
      "step": 2202
    },
    {
      "epoch": 2.937333333333333,
      "grad_norm": 0.037267208099365234,
      "learning_rate": 0.001656846285704583,
      "loss": 1.1568,
      "step": 2203
    },
    {
      "epoch": 2.9386666666666668,
      "grad_norm": 0.036114588379859924,
      "learning_rate": 0.001656520611019782,
      "loss": 1.0832,
      "step": 2204
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.035601064562797546,
      "learning_rate": 0.0016561948139065996,
      "loss": 0.944,
      "step": 2205
    },
    {
      "epoch": 2.9413333333333336,
      "grad_norm": 0.02723981998860836,
      "learning_rate": 0.0016558688944257902,
      "loss": 1.2923,
      "step": 2206
    },
    {
      "epoch": 2.9426666666666668,
      "grad_norm": 0.054134175181388855,
      "learning_rate": 0.0016555428526381323,
      "loss": 0.9869,
      "step": 2207
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.04200543835759163,
      "learning_rate": 0.0016552166886044254,
      "loss": 1.1097,
      "step": 2208
    },
    {
      "epoch": 2.945333333333333,
      "grad_norm": 0.03239554539322853,
      "learning_rate": 0.0016548904023854932,
      "loss": 1.2106,
      "step": 2209
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 0.024332625791430473,
      "learning_rate": 0.0016545639940421819,
      "loss": 0.9806,
      "step": 2210
    },
    {
      "epoch": 2.948,
      "grad_norm": 0.03373686224222183,
      "learning_rate": 0.0016542374636353604,
      "loss": 1.0785,
      "step": 2211
    },
    {
      "epoch": 2.9493333333333336,
      "grad_norm": 0.0290727186948061,
      "learning_rate": 0.00165391081122592,
      "loss": 0.8693,
      "step": 2212
    },
    {
      "epoch": 2.9506666666666668,
      "grad_norm": 0.02869122102856636,
      "learning_rate": 0.0016535840368747752,
      "loss": 1.1226,
      "step": 2213
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.02423572912812233,
      "learning_rate": 0.001653257140642863,
      "loss": 0.8516,
      "step": 2214
    },
    {
      "epoch": 2.953333333333333,
      "grad_norm": 0.04644278064370155,
      "learning_rate": 0.0016529301225911431,
      "loss": 0.8499,
      "step": 2215
    },
    {
      "epoch": 2.9546666666666668,
      "grad_norm": 0.02946840599179268,
      "learning_rate": 0.0016526029827805986,
      "loss": 1.0276,
      "step": 2216
    },
    {
      "epoch": 2.956,
      "grad_norm": 0.026463009417057037,
      "learning_rate": 0.0016522757212722344,
      "loss": 0.849,
      "step": 2217
    },
    {
      "epoch": 2.9573333333333336,
      "grad_norm": 0.0431792177259922,
      "learning_rate": 0.0016519483381270779,
      "loss": 1.6877,
      "step": 2218
    },
    {
      "epoch": 2.958666666666667,
      "grad_norm": 0.04342630133032799,
      "learning_rate": 0.0016516208334061806,
      "loss": 0.9664,
      "step": 2219
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.02974679507315159,
      "learning_rate": 0.0016512932071706152,
      "loss": 1.2065,
      "step": 2220
    },
    {
      "epoch": 2.961333333333333,
      "grad_norm": 0.04573775455355644,
      "learning_rate": 0.001650965459481478,
      "loss": 1.2012,
      "step": 2221
    },
    {
      "epoch": 2.962666666666667,
      "grad_norm": 0.029056938365101814,
      "learning_rate": 0.0016506375903998876,
      "loss": 1.0765,
      "step": 2222
    },
    {
      "epoch": 2.964,
      "grad_norm": 0.038847167044878006,
      "learning_rate": 0.0016503095999869848,
      "loss": 0.9019,
      "step": 2223
    },
    {
      "epoch": 2.9653333333333336,
      "grad_norm": 0.033734988421201706,
      "learning_rate": 0.0016499814883039338,
      "loss": 1.0267,
      "step": 2224
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 0.027923185378313065,
      "learning_rate": 0.0016496532554119213,
      "loss": 1.3507,
      "step": 2225
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.03458350896835327,
      "learning_rate": 0.0016493249013721557,
      "loss": 1.3655,
      "step": 2226
    },
    {
      "epoch": 2.969333333333333,
      "grad_norm": 0.030050141736865044,
      "learning_rate": 0.0016489964262458691,
      "loss": 1.0728,
      "step": 2227
    },
    {
      "epoch": 2.970666666666667,
      "grad_norm": 0.03225494176149368,
      "learning_rate": 0.0016486678300943161,
      "loss": 1.0104,
      "step": 2228
    },
    {
      "epoch": 2.972,
      "grad_norm": 0.14536254107952118,
      "learning_rate": 0.0016483391129787726,
      "loss": 0.9894,
      "step": 2229
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 0.03200913593173027,
      "learning_rate": 0.001648010274960539,
      "loss": 1.0865,
      "step": 2230
    },
    {
      "epoch": 2.974666666666667,
      "grad_norm": 0.026379669085144997,
      "learning_rate": 0.0016476813161009365,
      "loss": 1.1653,
      "step": 2231
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.03275229036808014,
      "learning_rate": 0.0016473522364613099,
      "loss": 1.1122,
      "step": 2232
    },
    {
      "epoch": 2.977333333333333,
      "grad_norm": 0.055282022804021835,
      "learning_rate": 0.0016470230361030258,
      "loss": 0.9464,
      "step": 2233
    },
    {
      "epoch": 2.978666666666667,
      "grad_norm": 0.030316080898046494,
      "learning_rate": 0.0016466937150874741,
      "loss": 0.994,
      "step": 2234
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.05141614377498627,
      "learning_rate": 0.0016463642734760669,
      "loss": 1.2082,
      "step": 2235
    },
    {
      "epoch": 2.981333333333333,
      "grad_norm": 0.031790655106306076,
      "learning_rate": 0.0016460347113302382,
      "loss": 1.1428,
      "step": 2236
    },
    {
      "epoch": 2.982666666666667,
      "grad_norm": 0.033395107835531235,
      "learning_rate": 0.0016457050287114452,
      "loss": 1.2728,
      "step": 2237
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.04389146342873573,
      "learning_rate": 0.0016453752256811675,
      "loss": 1.1507,
      "step": 2238
    },
    {
      "epoch": 2.985333333333333,
      "grad_norm": 0.028300154954195023,
      "learning_rate": 0.0016450453023009067,
      "loss": 1.1693,
      "step": 2239
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 0.03334532305598259,
      "learning_rate": 0.001644715258632187,
      "loss": 1.0005,
      "step": 2240
    },
    {
      "epoch": 2.988,
      "grad_norm": 0.04921041056513786,
      "learning_rate": 0.0016443850947365558,
      "loss": 1.5479,
      "step": 2241
    },
    {
      "epoch": 2.989333333333333,
      "grad_norm": 0.0343785397708416,
      "learning_rate": 0.0016440548106755816,
      "loss": 1.1721,
      "step": 2242
    },
    {
      "epoch": 2.990666666666667,
      "grad_norm": 0.03219155967235565,
      "learning_rate": 0.0016437244065108563,
      "loss": 0.876,
      "step": 2243
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.023985540494322777,
      "learning_rate": 0.001643393882303994,
      "loss": 1.2195,
      "step": 2244
    },
    {
      "epoch": 2.993333333333333,
      "grad_norm": 0.029765672981739044,
      "learning_rate": 0.0016430632381166305,
      "loss": 0.9806,
      "step": 2245
    },
    {
      "epoch": 2.994666666666667,
      "grad_norm": 0.023493777960538864,
      "learning_rate": 0.0016427324740104251,
      "loss": 0.7741,
      "step": 2246
    },
    {
      "epoch": 2.996,
      "grad_norm": 0.022160332649946213,
      "learning_rate": 0.0016424015900470589,
      "loss": 0.9733,
      "step": 2247
    },
    {
      "epoch": 2.997333333333333,
      "grad_norm": 0.03327234089374542,
      "learning_rate": 0.0016420705862882348,
      "loss": 1.0625,
      "step": 2248
    },
    {
      "epoch": 2.998666666666667,
      "grad_norm": 0.02747105062007904,
      "learning_rate": 0.0016417394627956793,
      "loss": 1.1508,
      "step": 2249
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.03315483033657074,
      "learning_rate": 0.00164140821963114,
      "loss": 1.1549,
      "step": 2250
    },
    {
      "epoch": 3.001333333333333,
      "grad_norm": 0.02201818861067295,
      "learning_rate": 0.0016410768568563875,
      "loss": 0.9296,
      "step": 2251
    },
    {
      "epoch": 3.002666666666667,
      "grad_norm": 0.03382681682705879,
      "learning_rate": 0.0016407453745332146,
      "loss": 1.2486,
      "step": 2252
    },
    {
      "epoch": 3.004,
      "grad_norm": 0.03193267807364464,
      "learning_rate": 0.0016404137727234365,
      "loss": 0.9823,
      "step": 2253
    },
    {
      "epoch": 3.005333333333333,
      "grad_norm": 0.028958454728126526,
      "learning_rate": 0.0016400820514888901,
      "loss": 1.1274,
      "step": 2254
    },
    {
      "epoch": 3.006666666666667,
      "grad_norm": 0.02540566772222519,
      "learning_rate": 0.0016397502108914353,
      "loss": 0.9422,
      "step": 2255
    },
    {
      "epoch": 3.008,
      "grad_norm": 0.023995349183678627,
      "learning_rate": 0.0016394182509929535,
      "loss": 0.8057,
      "step": 2256
    },
    {
      "epoch": 3.009333333333333,
      "grad_norm": 0.02780074067413807,
      "learning_rate": 0.0016390861718553495,
      "loss": 1.1338,
      "step": 2257
    },
    {
      "epoch": 3.010666666666667,
      "grad_norm": 0.03440787270665169,
      "learning_rate": 0.0016387539735405491,
      "loss": 1.1773,
      "step": 2258
    },
    {
      "epoch": 3.012,
      "grad_norm": 0.03595776855945587,
      "learning_rate": 0.0016384216561105011,
      "loss": 0.9656,
      "step": 2259
    },
    {
      "epoch": 3.013333333333333,
      "grad_norm": 0.02641797624528408,
      "learning_rate": 0.0016380892196271764,
      "loss": 1.21,
      "step": 2260
    },
    {
      "epoch": 3.014666666666667,
      "grad_norm": 0.03101632185280323,
      "learning_rate": 0.0016377566641525673,
      "loss": 0.9221,
      "step": 2261
    },
    {
      "epoch": 3.016,
      "grad_norm": 0.07365258783102036,
      "learning_rate": 0.0016374239897486897,
      "loss": 1.1002,
      "step": 2262
    },
    {
      "epoch": 3.017333333333333,
      "grad_norm": 0.029425906017422676,
      "learning_rate": 0.0016370911964775808,
      "loss": 0.9415,
      "step": 2263
    },
    {
      "epoch": 3.018666666666667,
      "grad_norm": 0.02925816923379898,
      "learning_rate": 0.0016367582844012997,
      "loss": 0.835,
      "step": 2264
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.03603258356451988,
      "learning_rate": 0.0016364252535819282,
      "loss": 1.0834,
      "step": 2265
    },
    {
      "epoch": 3.021333333333333,
      "grad_norm": 0.028746148571372032,
      "learning_rate": 0.0016360921040815704,
      "loss": 1.2157,
      "step": 2266
    },
    {
      "epoch": 3.022666666666667,
      "grad_norm": 0.02923213504254818,
      "learning_rate": 0.0016357588359623518,
      "loss": 1.2099,
      "step": 2267
    },
    {
      "epoch": 3.024,
      "grad_norm": 0.03844751790165901,
      "learning_rate": 0.001635425449286421,
      "loss": 1.2224,
      "step": 2268
    },
    {
      "epoch": 3.025333333333333,
      "grad_norm": 0.04033305123448372,
      "learning_rate": 0.0016350919441159477,
      "loss": 1.0984,
      "step": 2269
    },
    {
      "epoch": 3.026666666666667,
      "grad_norm": 0.03881243243813515,
      "learning_rate": 0.0016347583205131243,
      "loss": 0.9175,
      "step": 2270
    },
    {
      "epoch": 3.028,
      "grad_norm": 0.03453761711716652,
      "learning_rate": 0.0016344245785401652,
      "loss": 1.1304,
      "step": 2271
    },
    {
      "epoch": 3.029333333333333,
      "grad_norm": 0.030447712168097496,
      "learning_rate": 0.0016340907182593066,
      "loss": 1.0367,
      "step": 2272
    },
    {
      "epoch": 3.030666666666667,
      "grad_norm": 0.039583027362823486,
      "learning_rate": 0.0016337567397328072,
      "loss": 1.244,
      "step": 2273
    },
    {
      "epoch": 3.032,
      "grad_norm": 0.031205791980028152,
      "learning_rate": 0.0016334226430229474,
      "loss": 0.9804,
      "step": 2274
    },
    {
      "epoch": 3.033333333333333,
      "grad_norm": 0.03114047273993492,
      "learning_rate": 0.00163308842819203,
      "loss": 0.7762,
      "step": 2275
    },
    {
      "epoch": 3.034666666666667,
      "grad_norm": 0.05856575444340706,
      "learning_rate": 0.0016327540953023792,
      "loss": 1.2586,
      "step": 2276
    },
    {
      "epoch": 3.036,
      "grad_norm": 0.023417966440320015,
      "learning_rate": 0.001632419644416342,
      "loss": 0.6474,
      "step": 2277
    },
    {
      "epoch": 3.037333333333333,
      "grad_norm": 0.16918152570724487,
      "learning_rate": 0.001632085075596287,
      "loss": 1.0804,
      "step": 2278
    },
    {
      "epoch": 3.038666666666667,
      "grad_norm": 0.03254249691963196,
      "learning_rate": 0.0016317503889046044,
      "loss": 0.9352,
      "step": 2279
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.040424417704343796,
      "learning_rate": 0.0016314155844037073,
      "loss": 1.2017,
      "step": 2280
    },
    {
      "epoch": 3.041333333333333,
      "grad_norm": 0.023922564461827278,
      "learning_rate": 0.0016310806621560295,
      "loss": 0.9637,
      "step": 2281
    },
    {
      "epoch": 3.042666666666667,
      "grad_norm": 0.024866243824362755,
      "learning_rate": 0.0016307456222240284,
      "loss": 0.994,
      "step": 2282
    },
    {
      "epoch": 3.044,
      "grad_norm": 0.030618824064731598,
      "learning_rate": 0.0016304104646701818,
      "loss": 1.2371,
      "step": 2283
    },
    {
      "epoch": 3.0453333333333332,
      "grad_norm": 0.146577849984169,
      "learning_rate": 0.0016300751895569907,
      "loss": 1.1071,
      "step": 2284
    },
    {
      "epoch": 3.046666666666667,
      "grad_norm": 0.11519399285316467,
      "learning_rate": 0.0016297397969469772,
      "loss": 0.998,
      "step": 2285
    },
    {
      "epoch": 3.048,
      "grad_norm": 0.020872557535767555,
      "learning_rate": 0.001629404286902685,
      "loss": 0.9712,
      "step": 2286
    },
    {
      "epoch": 3.0493333333333332,
      "grad_norm": 0.025250673294067383,
      "learning_rate": 0.0016290686594866806,
      "loss": 1.0217,
      "step": 2287
    },
    {
      "epoch": 3.050666666666667,
      "grad_norm": 0.04247387498617172,
      "learning_rate": 0.0016287329147615525,
      "loss": 0.8406,
      "step": 2288
    },
    {
      "epoch": 3.052,
      "grad_norm": 0.03769795596599579,
      "learning_rate": 0.00162839705278991,
      "loss": 0.9083,
      "step": 2289
    },
    {
      "epoch": 3.0533333333333332,
      "grad_norm": 0.02173374965786934,
      "learning_rate": 0.0016280610736343847,
      "loss": 0.9662,
      "step": 2290
    },
    {
      "epoch": 3.054666666666667,
      "grad_norm": 0.03408164530992508,
      "learning_rate": 0.0016277249773576307,
      "loss": 1.1005,
      "step": 2291
    },
    {
      "epoch": 3.056,
      "grad_norm": 0.03387819230556488,
      "learning_rate": 0.001627388764022323,
      "loss": 1.222,
      "step": 2292
    },
    {
      "epoch": 3.0573333333333332,
      "grad_norm": 0.044878195971250534,
      "learning_rate": 0.0016270524336911597,
      "loss": 1.1131,
      "step": 2293
    },
    {
      "epoch": 3.058666666666667,
      "grad_norm": 0.028852049261331558,
      "learning_rate": 0.0016267159864268587,
      "loss": 1.0093,
      "step": 2294
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.03263714537024498,
      "learning_rate": 0.001626379422292162,
      "loss": 0.9027,
      "step": 2295
    },
    {
      "epoch": 3.0613333333333332,
      "grad_norm": 0.029844636097550392,
      "learning_rate": 0.0016260427413498319,
      "loss": 1.0352,
      "step": 2296
    },
    {
      "epoch": 3.062666666666667,
      "grad_norm": 0.034320008009672165,
      "learning_rate": 0.0016257059436626522,
      "loss": 1.3137,
      "step": 2297
    },
    {
      "epoch": 3.064,
      "grad_norm": 0.03587692603468895,
      "learning_rate": 0.00162536902929343,
      "loss": 1.2537,
      "step": 2298
    },
    {
      "epoch": 3.0653333333333332,
      "grad_norm": 0.02011966146528721,
      "learning_rate": 0.0016250319983049932,
      "loss": 0.9699,
      "step": 2299
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.028370114043354988,
      "learning_rate": 0.0016246948507601913,
      "loss": 1.1858,
      "step": 2300
    },
    {
      "epoch": 3.068,
      "grad_norm": 0.03497609868645668,
      "learning_rate": 0.0016243575867218958,
      "loss": 1.1321,
      "step": 2301
    },
    {
      "epoch": 3.0693333333333332,
      "grad_norm": 0.027792364358901978,
      "learning_rate": 0.0016240202062530002,
      "loss": 0.9842,
      "step": 2302
    },
    {
      "epoch": 3.070666666666667,
      "grad_norm": 0.02730766125023365,
      "learning_rate": 0.001623682709416419,
      "loss": 1.1692,
      "step": 2303
    },
    {
      "epoch": 3.072,
      "grad_norm": 0.024309365078806877,
      "learning_rate": 0.0016233450962750892,
      "loss": 1.2348,
      "step": 2304
    },
    {
      "epoch": 3.0733333333333333,
      "grad_norm": 0.028886014595627785,
      "learning_rate": 0.0016230073668919692,
      "loss": 1.2561,
      "step": 2305
    },
    {
      "epoch": 3.074666666666667,
      "grad_norm": 0.0318010188639164,
      "learning_rate": 0.0016226695213300381,
      "loss": 1.1774,
      "step": 2306
    },
    {
      "epoch": 3.076,
      "grad_norm": 0.02691299095749855,
      "learning_rate": 0.0016223315596522988,
      "loss": 1.0306,
      "step": 2307
    },
    {
      "epoch": 3.0773333333333333,
      "grad_norm": 0.032502952963113785,
      "learning_rate": 0.0016219934819217737,
      "loss": 1.2317,
      "step": 2308
    },
    {
      "epoch": 3.078666666666667,
      "grad_norm": 0.03170233592391014,
      "learning_rate": 0.001621655288201508,
      "loss": 1.3437,
      "step": 2309
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.027136487886309624,
      "learning_rate": 0.0016213169785545688,
      "loss": 1.0051,
      "step": 2310
    },
    {
      "epoch": 3.0813333333333333,
      "grad_norm": 0.028643356636166573,
      "learning_rate": 0.0016209785530440434,
      "loss": 1.2812,
      "step": 2311
    },
    {
      "epoch": 3.0826666666666664,
      "grad_norm": 0.02991592139005661,
      "learning_rate": 0.0016206400117330425,
      "loss": 1.0872,
      "step": 2312
    },
    {
      "epoch": 3.084,
      "grad_norm": 0.030520278960466385,
      "learning_rate": 0.0016203013546846965,
      "loss": 0.9822,
      "step": 2313
    },
    {
      "epoch": 3.0853333333333333,
      "grad_norm": 0.03446860983967781,
      "learning_rate": 0.001619962581962159,
      "loss": 1.0516,
      "step": 2314
    },
    {
      "epoch": 3.086666666666667,
      "grad_norm": 0.03197808191180229,
      "learning_rate": 0.001619623693628605,
      "loss": 1.0479,
      "step": 2315
    },
    {
      "epoch": 3.088,
      "grad_norm": 0.028749549761414528,
      "learning_rate": 0.0016192846897472297,
      "loss": 1.0989,
      "step": 2316
    },
    {
      "epoch": 3.0893333333333333,
      "grad_norm": 0.0405140146613121,
      "learning_rate": 0.0016189455703812512,
      "loss": 1.0131,
      "step": 2317
    },
    {
      "epoch": 3.0906666666666665,
      "grad_norm": 0.030882062390446663,
      "learning_rate": 0.0016186063355939086,
      "loss": 0.7412,
      "step": 2318
    },
    {
      "epoch": 3.092,
      "grad_norm": 0.039467163383960724,
      "learning_rate": 0.0016182669854484628,
      "loss": 1.0866,
      "step": 2319
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 0.043284397572278976,
      "learning_rate": 0.001617927520008196,
      "loss": 1.3431,
      "step": 2320
    },
    {
      "epoch": 3.0946666666666665,
      "grad_norm": 0.037943463772535324,
      "learning_rate": 0.0016175879393364116,
      "loss": 1.109,
      "step": 2321
    },
    {
      "epoch": 3.096,
      "grad_norm": 0.030784161761403084,
      "learning_rate": 0.0016172482434964352,
      "loss": 0.9644,
      "step": 2322
    },
    {
      "epoch": 3.0973333333333333,
      "grad_norm": 0.038774523884058,
      "learning_rate": 0.0016169084325516133,
      "loss": 1.1237,
      "step": 2323
    },
    {
      "epoch": 3.0986666666666665,
      "grad_norm": 0.03480285778641701,
      "learning_rate": 0.001616568506565314,
      "loss": 0.8635,
      "step": 2324
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.04820620268583298,
      "learning_rate": 0.0016162284656009273,
      "loss": 1.4482,
      "step": 2325
    },
    {
      "epoch": 3.1013333333333333,
      "grad_norm": 0.03230919688940048,
      "learning_rate": 0.001615888309721864,
      "loss": 1.0912,
      "step": 2326
    },
    {
      "epoch": 3.1026666666666665,
      "grad_norm": 0.053422145545482635,
      "learning_rate": 0.0016155480389915561,
      "loss": 0.7465,
      "step": 2327
    },
    {
      "epoch": 3.104,
      "grad_norm": 0.025344572961330414,
      "learning_rate": 0.0016152076534734583,
      "loss": 1.2399,
      "step": 2328
    },
    {
      "epoch": 3.1053333333333333,
      "grad_norm": 0.028936322778463364,
      "learning_rate": 0.0016148671532310456,
      "loss": 1.0431,
      "step": 2329
    },
    {
      "epoch": 3.1066666666666665,
      "grad_norm": 0.026852300390601158,
      "learning_rate": 0.0016145265383278144,
      "loss": 0.9789,
      "step": 2330
    },
    {
      "epoch": 3.108,
      "grad_norm": 0.021957244724035263,
      "learning_rate": 0.0016141858088272835,
      "loss": 0.8924,
      "step": 2331
    },
    {
      "epoch": 3.1093333333333333,
      "grad_norm": 0.03181817755103111,
      "learning_rate": 0.0016138449647929917,
      "loss": 0.9205,
      "step": 2332
    },
    {
      "epoch": 3.1106666666666665,
      "grad_norm": 0.027513276785612106,
      "learning_rate": 0.0016135040062885003,
      "loss": 1.2341,
      "step": 2333
    },
    {
      "epoch": 3.112,
      "grad_norm": 0.0327935554087162,
      "learning_rate": 0.0016131629333773908,
      "loss": 1.2111,
      "step": 2334
    },
    {
      "epoch": 3.1133333333333333,
      "grad_norm": 0.03337598592042923,
      "learning_rate": 0.001612821746123267,
      "loss": 1.1233,
      "step": 2335
    },
    {
      "epoch": 3.1146666666666665,
      "grad_norm": 0.025853391736745834,
      "learning_rate": 0.0016124804445897542,
      "loss": 1.0869,
      "step": 2336
    },
    {
      "epoch": 3.116,
      "grad_norm": 0.04071875661611557,
      "learning_rate": 0.001612139028840498,
      "loss": 0.976,
      "step": 2337
    },
    {
      "epoch": 3.1173333333333333,
      "grad_norm": 0.022651247680187225,
      "learning_rate": 0.0016117974989391657,
      "loss": 1.2395,
      "step": 2338
    },
    {
      "epoch": 3.1186666666666665,
      "grad_norm": 0.03851344808936119,
      "learning_rate": 0.0016114558549494467,
      "loss": 1.1702,
      "step": 2339
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.041822705417871475,
      "learning_rate": 0.0016111140969350501,
      "loss": 1.1929,
      "step": 2340
    },
    {
      "epoch": 3.1213333333333333,
      "grad_norm": 0.03104749694466591,
      "learning_rate": 0.001610772224959708,
      "loss": 1.027,
      "step": 2341
    },
    {
      "epoch": 3.1226666666666665,
      "grad_norm": 0.04554689675569534,
      "learning_rate": 0.0016104302390871723,
      "loss": 1.0279,
      "step": 2342
    },
    {
      "epoch": 3.124,
      "grad_norm": 0.02779427357017994,
      "learning_rate": 0.001610088139381217,
      "loss": 1.0994,
      "step": 2343
    },
    {
      "epoch": 3.1253333333333333,
      "grad_norm": 0.03431810811161995,
      "learning_rate": 0.0016097459259056368,
      "loss": 1.0998,
      "step": 2344
    },
    {
      "epoch": 3.1266666666666665,
      "grad_norm": 0.032478801906108856,
      "learning_rate": 0.0016094035987242483,
      "loss": 1.0492,
      "step": 2345
    },
    {
      "epoch": 3.128,
      "grad_norm": 0.03156214579939842,
      "learning_rate": 0.001609061157900889,
      "loss": 1.1636,
      "step": 2346
    },
    {
      "epoch": 3.1293333333333333,
      "grad_norm": 0.03456081077456474,
      "learning_rate": 0.0016087186034994168,
      "loss": 1.1151,
      "step": 2347
    },
    {
      "epoch": 3.1306666666666665,
      "grad_norm": 0.029416754841804504,
      "learning_rate": 0.001608375935583712,
      "loss": 1.0174,
      "step": 2348
    },
    {
      "epoch": 3.132,
      "grad_norm": 0.03991890326142311,
      "learning_rate": 0.0016080331542176753,
      "loss": 1.0396,
      "step": 2349
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.0238210279494524,
      "learning_rate": 0.001607690259465229,
      "loss": 1.0701,
      "step": 2350
    },
    {
      "epoch": 3.1346666666666665,
      "grad_norm": 0.036639776080846786,
      "learning_rate": 0.001607347251390316,
      "loss": 1.3533,
      "step": 2351
    },
    {
      "epoch": 3.136,
      "grad_norm": 0.041870374232530594,
      "learning_rate": 0.0016070041300569012,
      "loss": 1.1202,
      "step": 2352
    },
    {
      "epoch": 3.1373333333333333,
      "grad_norm": 0.02790934592485428,
      "learning_rate": 0.0016066608955289693,
      "loss": 1.0813,
      "step": 2353
    },
    {
      "epoch": 3.1386666666666665,
      "grad_norm": 0.0314762257039547,
      "learning_rate": 0.0016063175478705277,
      "loss": 0.9794,
      "step": 2354
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.03321545943617821,
      "learning_rate": 0.0016059740871456035,
      "loss": 0.9132,
      "step": 2355
    },
    {
      "epoch": 3.1413333333333333,
      "grad_norm": 0.026199378073215485,
      "learning_rate": 0.0016056305134182459,
      "loss": 1.1801,
      "step": 2356
    },
    {
      "epoch": 3.1426666666666665,
      "grad_norm": 0.042221799492836,
      "learning_rate": 0.0016052868267525246,
      "loss": 1.1024,
      "step": 2357
    },
    {
      "epoch": 3.144,
      "grad_norm": 0.03447648882865906,
      "learning_rate": 0.0016049430272125301,
      "loss": 0.8376,
      "step": 2358
    },
    {
      "epoch": 3.1453333333333333,
      "grad_norm": 0.030333152040839195,
      "learning_rate": 0.001604599114862375,
      "loss": 1.0716,
      "step": 2359
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 0.033913325518369675,
      "learning_rate": 0.0016042550897661918,
      "loss": 0.8823,
      "step": 2360
    },
    {
      "epoch": 3.148,
      "grad_norm": 0.03165041282773018,
      "learning_rate": 0.0016039109519881349,
      "loss": 1.0863,
      "step": 2361
    },
    {
      "epoch": 3.1493333333333333,
      "grad_norm": 0.03663633018732071,
      "learning_rate": 0.001603566701592379,
      "loss": 0.9822,
      "step": 2362
    },
    {
      "epoch": 3.1506666666666665,
      "grad_norm": 0.03279414027929306,
      "learning_rate": 0.0016032223386431206,
      "loss": 1.2889,
      "step": 2363
    },
    {
      "epoch": 3.152,
      "grad_norm": 0.02825789339840412,
      "learning_rate": 0.0016028778632045761,
      "loss": 1.1416,
      "step": 2364
    },
    {
      "epoch": 3.1533333333333333,
      "grad_norm": 0.03851594403386116,
      "learning_rate": 0.0016025332753409841,
      "loss": 1.1684,
      "step": 2365
    },
    {
      "epoch": 3.1546666666666665,
      "grad_norm": 0.025392943993210793,
      "learning_rate": 0.0016021885751166033,
      "loss": 0.8447,
      "step": 2366
    },
    {
      "epoch": 3.156,
      "grad_norm": 0.028521625325083733,
      "learning_rate": 0.0016018437625957135,
      "loss": 0.9073,
      "step": 2367
    },
    {
      "epoch": 3.1573333333333333,
      "grad_norm": 0.037293918430805206,
      "learning_rate": 0.0016014988378426156,
      "loss": 1.3096,
      "step": 2368
    },
    {
      "epoch": 3.1586666666666665,
      "grad_norm": 0.031660426408052444,
      "learning_rate": 0.0016011538009216312,
      "loss": 1.13,
      "step": 2369
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.032650601118803024,
      "learning_rate": 0.0016008086518971036,
      "loss": 1.3917,
      "step": 2370
    },
    {
      "epoch": 3.1613333333333333,
      "grad_norm": 0.030601516366004944,
      "learning_rate": 0.001600463390833396,
      "loss": 1.1389,
      "step": 2371
    },
    {
      "epoch": 3.1626666666666665,
      "grad_norm": 0.04113338887691498,
      "learning_rate": 0.001600118017794893,
      "loss": 1.0637,
      "step": 2372
    },
    {
      "epoch": 3.164,
      "grad_norm": 0.02500620111823082,
      "learning_rate": 0.001599772532846,
      "loss": 0.8134,
      "step": 2373
    },
    {
      "epoch": 3.1653333333333333,
      "grad_norm": 0.02047419548034668,
      "learning_rate": 0.0015994269360511428,
      "loss": 0.9929,
      "step": 2374
    },
    {
      "epoch": 3.1666666666666665,
      "grad_norm": 0.026978135108947754,
      "learning_rate": 0.0015990812274747693,
      "loss": 0.9344,
      "step": 2375
    },
    {
      "epoch": 3.168,
      "grad_norm": 0.02978707104921341,
      "learning_rate": 0.001598735407181347,
      "loss": 0.9699,
      "step": 2376
    },
    {
      "epoch": 3.1693333333333333,
      "grad_norm": 0.026482004672288895,
      "learning_rate": 0.0015983894752353645,
      "loss": 1.0387,
      "step": 2377
    },
    {
      "epoch": 3.1706666666666665,
      "grad_norm": 0.0261706430464983,
      "learning_rate": 0.001598043431701332,
      "loss": 1.1219,
      "step": 2378
    },
    {
      "epoch": 3.172,
      "grad_norm": 0.02744581550359726,
      "learning_rate": 0.0015976972766437794,
      "loss": 1.158,
      "step": 2379
    },
    {
      "epoch": 3.1733333333333333,
      "grad_norm": 0.033714719116687775,
      "learning_rate": 0.001597351010127258,
      "loss": 0.8898,
      "step": 2380
    },
    {
      "epoch": 3.1746666666666665,
      "grad_norm": 0.021076040342450142,
      "learning_rate": 0.0015970046322163402,
      "loss": 0.984,
      "step": 2381
    },
    {
      "epoch": 3.176,
      "grad_norm": 0.025623386725783348,
      "learning_rate": 0.0015966581429756183,
      "loss": 1.2355,
      "step": 2382
    },
    {
      "epoch": 3.1773333333333333,
      "grad_norm": 0.02690540999174118,
      "learning_rate": 0.001596311542469706,
      "loss": 0.8198,
      "step": 2383
    },
    {
      "epoch": 3.1786666666666665,
      "grad_norm": 0.03219997510313988,
      "learning_rate": 0.0015959648307632378,
      "loss": 1.2554,
      "step": 2384
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.031402550637722015,
      "learning_rate": 0.0015956180079208682,
      "loss": 1.2481,
      "step": 2385
    },
    {
      "epoch": 3.1813333333333333,
      "grad_norm": 0.030770208686590195,
      "learning_rate": 0.0015952710740072733,
      "loss": 0.8238,
      "step": 2386
    },
    {
      "epoch": 3.1826666666666665,
      "grad_norm": 0.04094685614109039,
      "learning_rate": 0.0015949240290871495,
      "loss": 0.9654,
      "step": 2387
    },
    {
      "epoch": 3.184,
      "grad_norm": 0.0696566179394722,
      "learning_rate": 0.0015945768732252145,
      "loss": 1.1179,
      "step": 2388
    },
    {
      "epoch": 3.1853333333333333,
      "grad_norm": 0.027888676151633263,
      "learning_rate": 0.001594229606486205,
      "loss": 1.1345,
      "step": 2389
    },
    {
      "epoch": 3.1866666666666665,
      "grad_norm": 0.03430980071425438,
      "learning_rate": 0.0015938822289348802,
      "loss": 0.7379,
      "step": 2390
    },
    {
      "epoch": 3.188,
      "grad_norm": 0.03150368481874466,
      "learning_rate": 0.0015935347406360193,
      "loss": 1.0839,
      "step": 2391
    },
    {
      "epoch": 3.1893333333333334,
      "grad_norm": 0.037834350019693375,
      "learning_rate": 0.0015931871416544222,
      "loss": 1.1257,
      "step": 2392
    },
    {
      "epoch": 3.1906666666666665,
      "grad_norm": 0.04061490669846535,
      "learning_rate": 0.0015928394320549092,
      "loss": 1.1818,
      "step": 2393
    },
    {
      "epoch": 3.192,
      "grad_norm": 0.033057909458875656,
      "learning_rate": 0.0015924916119023213,
      "loss": 1.2485,
      "step": 2394
    },
    {
      "epoch": 3.1933333333333334,
      "grad_norm": 0.0333588682115078,
      "learning_rate": 0.0015921436812615204,
      "loss": 1.0449,
      "step": 2395
    },
    {
      "epoch": 3.1946666666666665,
      "grad_norm": 0.04058796539902687,
      "learning_rate": 0.0015917956401973888,
      "loss": 0.9915,
      "step": 2396
    },
    {
      "epoch": 3.196,
      "grad_norm": 0.03528266400098801,
      "learning_rate": 0.0015914474887748296,
      "loss": 1.2981,
      "step": 2397
    },
    {
      "epoch": 3.1973333333333334,
      "grad_norm": 0.02642429620027542,
      "learning_rate": 0.0015910992270587658,
      "loss": 1.2934,
      "step": 2398
    },
    {
      "epoch": 3.1986666666666665,
      "grad_norm": 0.03836710751056671,
      "learning_rate": 0.0015907508551141422,
      "loss": 1.1517,
      "step": 2399
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.030062664300203323,
      "learning_rate": 0.0015904023730059227,
      "loss": 0.9358,
      "step": 2400
    },
    {
      "epoch": 3.2013333333333334,
      "grad_norm": 0.051980432122945786,
      "learning_rate": 0.0015900537807990928,
      "loss": 0.9354,
      "step": 2401
    },
    {
      "epoch": 3.2026666666666666,
      "grad_norm": 0.023099975660443306,
      "learning_rate": 0.0015897050785586581,
      "loss": 1.0396,
      "step": 2402
    },
    {
      "epoch": 3.204,
      "grad_norm": 0.03210536763072014,
      "learning_rate": 0.0015893562663496451,
      "loss": 1.2459,
      "step": 2403
    },
    {
      "epoch": 3.2053333333333334,
      "grad_norm": 0.02870263159275055,
      "learning_rate": 0.0015890073442371003,
      "loss": 1.2215,
      "step": 2404
    },
    {
      "epoch": 3.2066666666666666,
      "grad_norm": 0.031160004436969757,
      "learning_rate": 0.0015886583122860908,
      "loss": 0.9943,
      "step": 2405
    },
    {
      "epoch": 3.208,
      "grad_norm": 0.03835415840148926,
      "learning_rate": 0.0015883091705617045,
      "loss": 1.031,
      "step": 2406
    },
    {
      "epoch": 3.2093333333333334,
      "grad_norm": 0.024243010208010674,
      "learning_rate": 0.0015879599191290495,
      "loss": 1.2826,
      "step": 2407
    },
    {
      "epoch": 3.2106666666666666,
      "grad_norm": 0.02670104242861271,
      "learning_rate": 0.0015876105580532546,
      "loss": 0.9211,
      "step": 2408
    },
    {
      "epoch": 3.212,
      "grad_norm": 0.021388422697782516,
      "learning_rate": 0.0015872610873994685,
      "loss": 1.1547,
      "step": 2409
    },
    {
      "epoch": 3.2133333333333334,
      "grad_norm": 0.030884861946105957,
      "learning_rate": 0.0015869115072328608,
      "loss": 1.2407,
      "step": 2410
    },
    {
      "epoch": 3.2146666666666666,
      "grad_norm": 0.03482918441295624,
      "learning_rate": 0.001586561817618622,
      "loss": 1.207,
      "step": 2411
    },
    {
      "epoch": 3.216,
      "grad_norm": 0.024487439543008804,
      "learning_rate": 0.0015862120186219612,
      "loss": 1.0289,
      "step": 2412
    },
    {
      "epoch": 3.2173333333333334,
      "grad_norm": 0.036087438464164734,
      "learning_rate": 0.0015858621103081103,
      "loss": 1.373,
      "step": 2413
    },
    {
      "epoch": 3.2186666666666666,
      "grad_norm": 0.033988554030656815,
      "learning_rate": 0.0015855120927423206,
      "loss": 0.8935,
      "step": 2414
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.04793447628617287,
      "learning_rate": 0.0015851619659898624,
      "loss": 1.0997,
      "step": 2415
    },
    {
      "epoch": 3.2213333333333334,
      "grad_norm": 0.0311167873442173,
      "learning_rate": 0.0015848117301160284,
      "loss": 1.0084,
      "step": 2416
    },
    {
      "epoch": 3.2226666666666666,
      "grad_norm": 0.07559961825609207,
      "learning_rate": 0.0015844613851861305,
      "loss": 1.2957,
      "step": 2417
    },
    {
      "epoch": 3.224,
      "grad_norm": 0.03289595991373062,
      "learning_rate": 0.0015841109312655015,
      "loss": 1.1429,
      "step": 2418
    },
    {
      "epoch": 3.2253333333333334,
      "grad_norm": 0.02657989226281643,
      "learning_rate": 0.001583760368419494,
      "loss": 1.1691,
      "step": 2419
    },
    {
      "epoch": 3.2266666666666666,
      "grad_norm": 0.03534427657723427,
      "learning_rate": 0.0015834096967134816,
      "loss": 1.0067,
      "step": 2420
    },
    {
      "epoch": 3.228,
      "grad_norm": 0.03015759587287903,
      "learning_rate": 0.0015830589162128574,
      "loss": 0.9859,
      "step": 2421
    },
    {
      "epoch": 3.2293333333333334,
      "grad_norm": 0.04172728955745697,
      "learning_rate": 0.0015827080269830351,
      "loss": 0.9808,
      "step": 2422
    },
    {
      "epoch": 3.2306666666666666,
      "grad_norm": 0.035861123353242874,
      "learning_rate": 0.001582357029089449,
      "loss": 1.3956,
      "step": 2423
    },
    {
      "epoch": 3.232,
      "grad_norm": 0.04951854795217514,
      "learning_rate": 0.001582005922597553,
      "loss": 1.1465,
      "step": 2424
    },
    {
      "epoch": 3.2333333333333334,
      "grad_norm": 0.025763550773262978,
      "learning_rate": 0.0015816547075728226,
      "loss": 0.9642,
      "step": 2425
    },
    {
      "epoch": 3.2346666666666666,
      "grad_norm": 0.029858488589525223,
      "learning_rate": 0.0015813033840807518,
      "loss": 0.8429,
      "step": 2426
    },
    {
      "epoch": 3.2359999999999998,
      "grad_norm": 0.029611531645059586,
      "learning_rate": 0.0015809519521868558,
      "loss": 1.2328,
      "step": 2427
    },
    {
      "epoch": 3.2373333333333334,
      "grad_norm": 0.041971806436777115,
      "learning_rate": 0.0015806004119566697,
      "loss": 1.0675,
      "step": 2428
    },
    {
      "epoch": 3.2386666666666666,
      "grad_norm": 0.03826865553855896,
      "learning_rate": 0.0015802487634557492,
      "loss": 0.8843,
      "step": 2429
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.04000704362988472,
      "learning_rate": 0.00157989700674967,
      "loss": 1.1849,
      "step": 2430
    },
    {
      "epoch": 3.2413333333333334,
      "grad_norm": 0.04749183729290962,
      "learning_rate": 0.0015795451419040277,
      "loss": 1.3156,
      "step": 2431
    },
    {
      "epoch": 3.2426666666666666,
      "grad_norm": 0.02718333527445793,
      "learning_rate": 0.0015791931689844382,
      "loss": 0.9882,
      "step": 2432
    },
    {
      "epoch": 3.2439999999999998,
      "grad_norm": 0.04511220008134842,
      "learning_rate": 0.0015788410880565379,
      "loss": 0.9692,
      "step": 2433
    },
    {
      "epoch": 3.2453333333333334,
      "grad_norm": 0.07801100611686707,
      "learning_rate": 0.0015784888991859827,
      "loss": 1.2552,
      "step": 2434
    },
    {
      "epoch": 3.2466666666666666,
      "grad_norm": 0.05130777880549431,
      "learning_rate": 0.0015781366024384496,
      "loss": 1.2631,
      "step": 2435
    },
    {
      "epoch": 3.248,
      "grad_norm": 0.030606120824813843,
      "learning_rate": 0.0015777841978796348,
      "loss": 0.9749,
      "step": 2436
    },
    {
      "epoch": 3.2493333333333334,
      "grad_norm": 0.037833765149116516,
      "learning_rate": 0.0015774316855752546,
      "loss": 1.0478,
      "step": 2437
    },
    {
      "epoch": 3.2506666666666666,
      "grad_norm": 0.046919189393520355,
      "learning_rate": 0.0015770790655910462,
      "loss": 0.9603,
      "step": 2438
    },
    {
      "epoch": 3.252,
      "grad_norm": 0.031551793217659,
      "learning_rate": 0.001576726337992766,
      "loss": 1.199,
      "step": 2439
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 0.03881652280688286,
      "learning_rate": 0.0015763735028461914,
      "loss": 0.9861,
      "step": 2440
    },
    {
      "epoch": 3.2546666666666666,
      "grad_norm": 0.03296532481908798,
      "learning_rate": 0.001576020560217119,
      "loss": 0.7305,
      "step": 2441
    },
    {
      "epoch": 3.2560000000000002,
      "grad_norm": 0.030949223786592484,
      "learning_rate": 0.0015756675101713657,
      "loss": 1.0738,
      "step": 2442
    },
    {
      "epoch": 3.2573333333333334,
      "grad_norm": 0.032120369374752045,
      "learning_rate": 0.0015753143527747681,
      "loss": 1.2064,
      "step": 2443
    },
    {
      "epoch": 3.2586666666666666,
      "grad_norm": 0.05047132819890976,
      "learning_rate": 0.0015749610880931844,
      "loss": 0.8185,
      "step": 2444
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.02691749669611454,
      "learning_rate": 0.0015746077161924905,
      "loss": 1.0952,
      "step": 2445
    },
    {
      "epoch": 3.2613333333333334,
      "grad_norm": 0.04127928987145424,
      "learning_rate": 0.0015742542371385841,
      "loss": 1.1455,
      "step": 2446
    },
    {
      "epoch": 3.2626666666666666,
      "grad_norm": 0.029471488669514656,
      "learning_rate": 0.001573900650997382,
      "loss": 0.9453,
      "step": 2447
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 0.03168381378054619,
      "learning_rate": 0.0015735469578348207,
      "loss": 0.9237,
      "step": 2448
    },
    {
      "epoch": 3.2653333333333334,
      "grad_norm": 0.03129103407263756,
      "learning_rate": 0.0015731931577168577,
      "loss": 1.3735,
      "step": 2449
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.02645247057080269,
      "learning_rate": 0.0015728392507094698,
      "loss": 1.006,
      "step": 2450
    },
    {
      "epoch": 3.268,
      "grad_norm": 0.0340455062687397,
      "learning_rate": 0.0015724852368786538,
      "loss": 1.1095,
      "step": 2451
    },
    {
      "epoch": 3.2693333333333334,
      "grad_norm": 0.0373513326048851,
      "learning_rate": 0.001572131116290426,
      "loss": 0.8848,
      "step": 2452
    },
    {
      "epoch": 3.2706666666666666,
      "grad_norm": 0.02731442265212536,
      "learning_rate": 0.0015717768890108235,
      "loss": 1.1297,
      "step": 2453
    },
    {
      "epoch": 3.2720000000000002,
      "grad_norm": 0.02330610901117325,
      "learning_rate": 0.0015714225551059027,
      "loss": 1.0252,
      "step": 2454
    },
    {
      "epoch": 3.2733333333333334,
      "grad_norm": 0.031291525810956955,
      "learning_rate": 0.00157106811464174,
      "loss": 1.0156,
      "step": 2455
    },
    {
      "epoch": 3.2746666666666666,
      "grad_norm": 0.029292384162545204,
      "learning_rate": 0.001570713567684432,
      "loss": 1.0551,
      "step": 2456
    },
    {
      "epoch": 3.276,
      "grad_norm": 0.02778688073158264,
      "learning_rate": 0.001570358914300094,
      "loss": 0.9606,
      "step": 2457
    },
    {
      "epoch": 3.2773333333333334,
      "grad_norm": 0.037094756960868835,
      "learning_rate": 0.0015700041545548627,
      "loss": 1.1873,
      "step": 2458
    },
    {
      "epoch": 3.2786666666666666,
      "grad_norm": 0.03037886507809162,
      "learning_rate": 0.0015696492885148938,
      "loss": 1.157,
      "step": 2459
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.029870638623833656,
      "learning_rate": 0.0015692943162463626,
      "loss": 0.7451,
      "step": 2460
    },
    {
      "epoch": 3.2813333333333334,
      "grad_norm": 0.05937017500400543,
      "learning_rate": 0.0015689392378154653,
      "loss": 1.4976,
      "step": 2461
    },
    {
      "epoch": 3.2826666666666666,
      "grad_norm": 0.029458314180374146,
      "learning_rate": 0.0015685840532884163,
      "loss": 0.8562,
      "step": 2462
    },
    {
      "epoch": 3.284,
      "grad_norm": 0.04551973193883896,
      "learning_rate": 0.0015682287627314514,
      "loss": 1.0387,
      "step": 2463
    },
    {
      "epoch": 3.2853333333333334,
      "grad_norm": 0.0334927998483181,
      "learning_rate": 0.001567873366210825,
      "loss": 1.2356,
      "step": 2464
    },
    {
      "epoch": 3.2866666666666666,
      "grad_norm": 0.029058020561933517,
      "learning_rate": 0.0015675178637928114,
      "loss": 1.1703,
      "step": 2465
    },
    {
      "epoch": 3.288,
      "grad_norm": 0.03343898430466652,
      "learning_rate": 0.0015671622555437053,
      "loss": 1.0158,
      "step": 2466
    },
    {
      "epoch": 3.2893333333333334,
      "grad_norm": 0.0280962772667408,
      "learning_rate": 0.0015668065415298208,
      "loss": 1.3055,
      "step": 2467
    },
    {
      "epoch": 3.2906666666666666,
      "grad_norm": 0.0391867570579052,
      "learning_rate": 0.0015664507218174915,
      "loss": 1.0912,
      "step": 2468
    },
    {
      "epoch": 3.292,
      "grad_norm": 0.028908034786581993,
      "learning_rate": 0.0015660947964730707,
      "loss": 0.9639,
      "step": 2469
    },
    {
      "epoch": 3.2933333333333334,
      "grad_norm": 0.03727992624044418,
      "learning_rate": 0.0015657387655629322,
      "loss": 1.0241,
      "step": 2470
    },
    {
      "epoch": 3.2946666666666666,
      "grad_norm": 0.0337437242269516,
      "learning_rate": 0.0015653826291534684,
      "loss": 1.0286,
      "step": 2471
    },
    {
      "epoch": 3.296,
      "grad_norm": 0.04025668650865555,
      "learning_rate": 0.001565026387311092,
      "loss": 1.4491,
      "step": 2472
    },
    {
      "epoch": 3.2973333333333334,
      "grad_norm": 0.022491183131933212,
      "learning_rate": 0.001564670040102235,
      "loss": 1.0132,
      "step": 2473
    },
    {
      "epoch": 3.2986666666666666,
      "grad_norm": 0.029301991686224937,
      "learning_rate": 0.0015643135875933493,
      "loss": 1.0763,
      "step": 2474
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.031207837164402008,
      "learning_rate": 0.0015639570298509064,
      "loss": 0.9768,
      "step": 2475
    },
    {
      "epoch": 3.3013333333333335,
      "grad_norm": 0.03776101395487785,
      "learning_rate": 0.001563600366941398,
      "loss": 0.7468,
      "step": 2476
    },
    {
      "epoch": 3.3026666666666666,
      "grad_norm": 0.03671630099415779,
      "learning_rate": 0.001563243598931334,
      "loss": 0.8659,
      "step": 2477
    },
    {
      "epoch": 3.304,
      "grad_norm": 0.042604710906744,
      "learning_rate": 0.001562886725887245,
      "loss": 1.1197,
      "step": 2478
    },
    {
      "epoch": 3.3053333333333335,
      "grad_norm": 0.03575126826763153,
      "learning_rate": 0.001562529747875681,
      "loss": 0.878,
      "step": 2479
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 0.03051646798849106,
      "learning_rate": 0.0015621726649632116,
      "loss": 1.163,
      "step": 2480
    },
    {
      "epoch": 3.308,
      "grad_norm": 0.02723105438053608,
      "learning_rate": 0.0015618154772164255,
      "loss": 1.1176,
      "step": 2481
    },
    {
      "epoch": 3.3093333333333335,
      "grad_norm": 0.03136206418275833,
      "learning_rate": 0.0015614581847019316,
      "loss": 1.1611,
      "step": 2482
    },
    {
      "epoch": 3.3106666666666666,
      "grad_norm": 0.030198493972420692,
      "learning_rate": 0.0015611007874863582,
      "loss": 1.1207,
      "step": 2483
    },
    {
      "epoch": 3.312,
      "grad_norm": 0.035837724804878235,
      "learning_rate": 0.0015607432856363523,
      "loss": 1.1507,
      "step": 2484
    },
    {
      "epoch": 3.3133333333333335,
      "grad_norm": 0.024107951670885086,
      "learning_rate": 0.0015603856792185816,
      "loss": 1.1083,
      "step": 2485
    },
    {
      "epoch": 3.3146666666666667,
      "grad_norm": 0.03093414008617401,
      "learning_rate": 0.0015600279682997328,
      "loss": 0.9592,
      "step": 2486
    },
    {
      "epoch": 3.316,
      "grad_norm": 0.022481173276901245,
      "learning_rate": 0.0015596701529465116,
      "loss": 0.9005,
      "step": 2487
    },
    {
      "epoch": 3.3173333333333335,
      "grad_norm": 0.046265240758657455,
      "learning_rate": 0.0015593122332256443,
      "loss": 0.9833,
      "step": 2488
    },
    {
      "epoch": 3.3186666666666667,
      "grad_norm": 0.03052622079849243,
      "learning_rate": 0.0015589542092038751,
      "loss": 0.9426,
      "step": 2489
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.03805726021528244,
      "learning_rate": 0.0015585960809479697,
      "loss": 1.0426,
      "step": 2490
    },
    {
      "epoch": 3.3213333333333335,
      "grad_norm": 0.02625943161547184,
      "learning_rate": 0.001558237848524711,
      "loss": 1.0532,
      "step": 2491
    },
    {
      "epoch": 3.3226666666666667,
      "grad_norm": 0.026508159935474396,
      "learning_rate": 0.001557879512000903,
      "loss": 1.1074,
      "step": 2492
    },
    {
      "epoch": 3.324,
      "grad_norm": 0.03396189957857132,
      "learning_rate": 0.0015575210714433684,
      "loss": 1.0795,
      "step": 2493
    },
    {
      "epoch": 3.3253333333333335,
      "grad_norm": 0.03282664343714714,
      "learning_rate": 0.0015571625269189498,
      "loss": 1.1404,
      "step": 2494
    },
    {
      "epoch": 3.3266666666666667,
      "grad_norm": 0.030214274302124977,
      "learning_rate": 0.0015568038784945079,
      "loss": 1.0356,
      "step": 2495
    },
    {
      "epoch": 3.328,
      "grad_norm": 0.037544384598731995,
      "learning_rate": 0.0015564451262369246,
      "loss": 1.0498,
      "step": 2496
    },
    {
      "epoch": 3.3293333333333335,
      "grad_norm": 0.030074186623096466,
      "learning_rate": 0.0015560862702130997,
      "loss": 1.1749,
      "step": 2497
    },
    {
      "epoch": 3.3306666666666667,
      "grad_norm": 0.022189239040017128,
      "learning_rate": 0.001555727310489953,
      "loss": 1.2424,
      "step": 2498
    },
    {
      "epoch": 3.332,
      "grad_norm": 0.03992554917931557,
      "learning_rate": 0.0015553682471344237,
      "loss": 1.2318,
      "step": 2499
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.027516277506947517,
      "learning_rate": 0.00155500908021347,
      "loss": 0.873,
      "step": 2500
    },
    {
      "epoch": 3.3346666666666667,
      "grad_norm": 0.02751966565847397,
      "learning_rate": 0.0015546498097940697,
      "loss": 0.8206,
      "step": 2501
    },
    {
      "epoch": 3.336,
      "grad_norm": 0.032970257103443146,
      "learning_rate": 0.0015542904359432197,
      "loss": 1.2598,
      "step": 2502
    },
    {
      "epoch": 3.3373333333333335,
      "grad_norm": 0.027595553547143936,
      "learning_rate": 0.0015539309587279365,
      "loss": 1.1091,
      "step": 2503
    },
    {
      "epoch": 3.3386666666666667,
      "grad_norm": 0.02293711155653,
      "learning_rate": 0.0015535713782152552,
      "loss": 1.1512,
      "step": 2504
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.024497611448168755,
      "learning_rate": 0.0015532116944722307,
      "loss": 1.0,
      "step": 2505
    },
    {
      "epoch": 3.3413333333333335,
      "grad_norm": 0.03374919667840004,
      "learning_rate": 0.0015528519075659376,
      "loss": 1.0515,
      "step": 2506
    },
    {
      "epoch": 3.3426666666666667,
      "grad_norm": 0.07426372915506363,
      "learning_rate": 0.0015524920175634684,
      "loss": 1.0105,
      "step": 2507
    },
    {
      "epoch": 3.344,
      "grad_norm": 0.03174735605716705,
      "learning_rate": 0.0015521320245319363,
      "loss": 1.1114,
      "step": 2508
    },
    {
      "epoch": 3.3453333333333335,
      "grad_norm": 0.022116875275969505,
      "learning_rate": 0.0015517719285384724,
      "loss": 1.1248,
      "step": 2509
    },
    {
      "epoch": 3.3466666666666667,
      "grad_norm": 0.0401594415307045,
      "learning_rate": 0.0015514117296502282,
      "loss": 1.0527,
      "step": 2510
    },
    {
      "epoch": 3.348,
      "grad_norm": 0.027611957862973213,
      "learning_rate": 0.0015510514279343734,
      "loss": 0.8824,
      "step": 2511
    },
    {
      "epoch": 3.3493333333333335,
      "grad_norm": 0.025033535435795784,
      "learning_rate": 0.0015506910234580975,
      "loss": 1.1244,
      "step": 2512
    },
    {
      "epoch": 3.3506666666666667,
      "grad_norm": 0.034698694944381714,
      "learning_rate": 0.001550330516288609,
      "loss": 0.9703,
      "step": 2513
    },
    {
      "epoch": 3.352,
      "grad_norm": 0.061758775264024734,
      "learning_rate": 0.0015499699064931353,
      "loss": 0.8947,
      "step": 2514
    },
    {
      "epoch": 3.3533333333333335,
      "grad_norm": 0.0367167703807354,
      "learning_rate": 0.0015496091941389234,
      "loss": 1.021,
      "step": 2515
    },
    {
      "epoch": 3.3546666666666667,
      "grad_norm": 0.04514293000102043,
      "learning_rate": 0.001549248379293239,
      "loss": 1.34,
      "step": 2516
    },
    {
      "epoch": 3.356,
      "grad_norm": 0.02640366181731224,
      "learning_rate": 0.0015488874620233674,
      "loss": 1.0343,
      "step": 2517
    },
    {
      "epoch": 3.3573333333333335,
      "grad_norm": 0.0259980708360672,
      "learning_rate": 0.001548526442396612,
      "loss": 1.0689,
      "step": 2518
    },
    {
      "epoch": 3.3586666666666667,
      "grad_norm": 0.024314777925610542,
      "learning_rate": 0.0015481653204802966,
      "loss": 1.0734,
      "step": 2519
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.034706272184848785,
      "learning_rate": 0.001547804096341763,
      "loss": 1.0274,
      "step": 2520
    },
    {
      "epoch": 3.3613333333333335,
      "grad_norm": 0.22658856213092804,
      "learning_rate": 0.001547442770048373,
      "loss": 1.2501,
      "step": 2521
    },
    {
      "epoch": 3.3626666666666667,
      "grad_norm": 0.023853257298469543,
      "learning_rate": 0.0015470813416675062,
      "loss": 1.1164,
      "step": 2522
    },
    {
      "epoch": 3.364,
      "grad_norm": 0.023285524919629097,
      "learning_rate": 0.001546719811266563,
      "loss": 1.0436,
      "step": 2523
    },
    {
      "epoch": 3.3653333333333335,
      "grad_norm": 0.022638283669948578,
      "learning_rate": 0.0015463581789129613,
      "loss": 0.9166,
      "step": 2524
    },
    {
      "epoch": 3.3666666666666667,
      "grad_norm": 0.02934163808822632,
      "learning_rate": 0.0015459964446741382,
      "loss": 1.1116,
      "step": 2525
    },
    {
      "epoch": 3.368,
      "grad_norm": 0.10960453003644943,
      "learning_rate": 0.0015456346086175508,
      "loss": 0.8396,
      "step": 2526
    },
    {
      "epoch": 3.3693333333333335,
      "grad_norm": 0.06336884200572968,
      "learning_rate": 0.0015452726708106738,
      "loss": 0.885,
      "step": 2527
    },
    {
      "epoch": 3.3706666666666667,
      "grad_norm": 0.024366402998566628,
      "learning_rate": 0.0015449106313210022,
      "loss": 0.9807,
      "step": 2528
    },
    {
      "epoch": 3.372,
      "grad_norm": 0.02573806419968605,
      "learning_rate": 0.0015445484902160491,
      "loss": 0.9541,
      "step": 2529
    },
    {
      "epoch": 3.3733333333333335,
      "grad_norm": 0.02897738479077816,
      "learning_rate": 0.001544186247563347,
      "loss": 1.0493,
      "step": 2530
    },
    {
      "epoch": 3.3746666666666667,
      "grad_norm": 0.03129241615533829,
      "learning_rate": 0.0015438239034304467,
      "loss": 1.1711,
      "step": 2531
    },
    {
      "epoch": 3.376,
      "grad_norm": 0.025472017005085945,
      "learning_rate": 0.0015434614578849187,
      "loss": 1.1078,
      "step": 2532
    },
    {
      "epoch": 3.3773333333333335,
      "grad_norm": 0.034678541123867035,
      "learning_rate": 0.0015430989109943521,
      "loss": 1.0827,
      "step": 2533
    },
    {
      "epoch": 3.3786666666666667,
      "grad_norm": 0.030704062432050705,
      "learning_rate": 0.0015427362628263545,
      "loss": 1.1828,
      "step": 2534
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.020433709025382996,
      "learning_rate": 0.0015423735134485535,
      "loss": 0.8479,
      "step": 2535
    },
    {
      "epoch": 3.3813333333333335,
      "grad_norm": 0.032032184302806854,
      "learning_rate": 0.001542010662928594,
      "loss": 1.0456,
      "step": 2536
    },
    {
      "epoch": 3.3826666666666667,
      "grad_norm": 0.03152830898761749,
      "learning_rate": 0.0015416477113341413,
      "loss": 0.8696,
      "step": 2537
    },
    {
      "epoch": 3.384,
      "grad_norm": 0.02448127046227455,
      "learning_rate": 0.0015412846587328781,
      "loss": 0.8081,
      "step": 2538
    },
    {
      "epoch": 3.3853333333333335,
      "grad_norm": 0.036882489919662476,
      "learning_rate": 0.0015409215051925074,
      "loss": 1.0715,
      "step": 2539
    },
    {
      "epoch": 3.3866666666666667,
      "grad_norm": 0.025204600766301155,
      "learning_rate": 0.0015405582507807504,
      "loss": 1.1502,
      "step": 2540
    },
    {
      "epoch": 3.388,
      "grad_norm": 0.03514653444290161,
      "learning_rate": 0.001540194895565346,
      "loss": 1.1668,
      "step": 2541
    },
    {
      "epoch": 3.389333333333333,
      "grad_norm": 0.030908580869436264,
      "learning_rate": 0.0015398314396140537,
      "loss": 1.1828,
      "step": 2542
    },
    {
      "epoch": 3.3906666666666667,
      "grad_norm": 0.13939185440540314,
      "learning_rate": 0.0015394678829946506,
      "loss": 1.0925,
      "step": 2543
    },
    {
      "epoch": 3.392,
      "grad_norm": 0.02829296514391899,
      "learning_rate": 0.0015391042257749336,
      "loss": 1.2256,
      "step": 2544
    },
    {
      "epoch": 3.3933333333333335,
      "grad_norm": 0.025027265772223473,
      "learning_rate": 0.0015387404680227172,
      "loss": 1.0379,
      "step": 2545
    },
    {
      "epoch": 3.3946666666666667,
      "grad_norm": 0.031607791781425476,
      "learning_rate": 0.0015383766098058353,
      "loss": 1.1257,
      "step": 2546
    },
    {
      "epoch": 3.396,
      "grad_norm": 0.0362633541226387,
      "learning_rate": 0.0015380126511921404,
      "loss": 0.9739,
      "step": 2547
    },
    {
      "epoch": 3.397333333333333,
      "grad_norm": 0.028897542506456375,
      "learning_rate": 0.0015376485922495037,
      "loss": 1.0137,
      "step": 2548
    },
    {
      "epoch": 3.3986666666666667,
      "grad_norm": 0.04516291618347168,
      "learning_rate": 0.0015372844330458152,
      "loss": 0.9501,
      "step": 2549
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.02763497084379196,
      "learning_rate": 0.0015369201736489839,
      "loss": 0.8107,
      "step": 2550
    },
    {
      "epoch": 3.4013333333333335,
      "grad_norm": 0.035611316561698914,
      "learning_rate": 0.0015365558141269364,
      "loss": 1.0528,
      "step": 2551
    },
    {
      "epoch": 3.4026666666666667,
      "grad_norm": 0.048962827771902084,
      "learning_rate": 0.0015361913545476192,
      "loss": 1.1212,
      "step": 2552
    },
    {
      "epoch": 3.404,
      "grad_norm": 0.034978318959474564,
      "learning_rate": 0.0015358267949789966,
      "loss": 1.1161,
      "step": 2553
    },
    {
      "epoch": 3.405333333333333,
      "grad_norm": 0.03474835678935051,
      "learning_rate": 0.0015354621354890523,
      "loss": 1.0733,
      "step": 2554
    },
    {
      "epoch": 3.4066666666666667,
      "grad_norm": 0.03470776602625847,
      "learning_rate": 0.0015350973761457882,
      "loss": 1.105,
      "step": 2555
    },
    {
      "epoch": 3.408,
      "grad_norm": 0.026049938052892685,
      "learning_rate": 0.0015347325170172244,
      "loss": 0.9281,
      "step": 2556
    },
    {
      "epoch": 3.4093333333333335,
      "grad_norm": 0.02864145115017891,
      "learning_rate": 0.0015343675581714002,
      "loss": 1.2602,
      "step": 2557
    },
    {
      "epoch": 3.4106666666666667,
      "grad_norm": 0.031115451827645302,
      "learning_rate": 0.0015340024996763739,
      "loss": 0.9282,
      "step": 2558
    },
    {
      "epoch": 3.412,
      "grad_norm": 0.027052082121372223,
      "learning_rate": 0.0015336373416002212,
      "loss": 1.1182,
      "step": 2559
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 0.02447490021586418,
      "learning_rate": 0.0015332720840110375,
      "loss": 1.2513,
      "step": 2560
    },
    {
      "epoch": 3.4146666666666667,
      "grad_norm": 0.0307502169162035,
      "learning_rate": 0.001532906726976936,
      "loss": 1.2635,
      "step": 2561
    },
    {
      "epoch": 3.416,
      "grad_norm": 0.025227084755897522,
      "learning_rate": 0.0015325412705660488,
      "loss": 0.9812,
      "step": 2562
    },
    {
      "epoch": 3.4173333333333336,
      "grad_norm": 0.031069660559296608,
      "learning_rate": 0.0015321757148465261,
      "loss": 1.1463,
      "step": 2563
    },
    {
      "epoch": 3.4186666666666667,
      "grad_norm": 0.03500524163246155,
      "learning_rate": 0.0015318100598865376,
      "loss": 0.8654,
      "step": 2564
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.02462499588727951,
      "learning_rate": 0.0015314443057542705,
      "loss": 0.8963,
      "step": 2565
    },
    {
      "epoch": 3.421333333333333,
      "grad_norm": 0.03885725885629654,
      "learning_rate": 0.0015310784525179306,
      "loss": 1.2224,
      "step": 2566
    },
    {
      "epoch": 3.4226666666666667,
      "grad_norm": 0.04560307785868645,
      "learning_rate": 0.0015307125002457429,
      "loss": 0.8138,
      "step": 2567
    },
    {
      "epoch": 3.424,
      "grad_norm": 0.026310225948691368,
      "learning_rate": 0.0015303464490059504,
      "loss": 1.0358,
      "step": 2568
    },
    {
      "epoch": 3.4253333333333336,
      "grad_norm": 0.034771308302879333,
      "learning_rate": 0.0015299802988668141,
      "loss": 0.9136,
      "step": 2569
    },
    {
      "epoch": 3.4266666666666667,
      "grad_norm": 0.04426505044102669,
      "learning_rate": 0.0015296140498966145,
      "loss": 1.2255,
      "step": 2570
    },
    {
      "epoch": 3.428,
      "grad_norm": 0.040613871067762375,
      "learning_rate": 0.0015292477021636498,
      "loss": 1.2081,
      "step": 2571
    },
    {
      "epoch": 3.429333333333333,
      "grad_norm": 0.02823302336037159,
      "learning_rate": 0.0015288812557362364,
      "loss": 1.0914,
      "step": 2572
    },
    {
      "epoch": 3.4306666666666668,
      "grad_norm": 0.03431167080998421,
      "learning_rate": 0.0015285147106827094,
      "loss": 1.0734,
      "step": 2573
    },
    {
      "epoch": 3.432,
      "grad_norm": 0.023798825219273567,
      "learning_rate": 0.0015281480670714229,
      "loss": 1.0952,
      "step": 2574
    },
    {
      "epoch": 3.4333333333333336,
      "grad_norm": 0.03730270266532898,
      "learning_rate": 0.0015277813249707486,
      "loss": 1.1841,
      "step": 2575
    },
    {
      "epoch": 3.4346666666666668,
      "grad_norm": 0.0342477411031723,
      "learning_rate": 0.0015274144844490771,
      "loss": 1.081,
      "step": 2576
    },
    {
      "epoch": 3.436,
      "grad_norm": 0.02657194249331951,
      "learning_rate": 0.0015270475455748166,
      "loss": 0.8268,
      "step": 2577
    },
    {
      "epoch": 3.437333333333333,
      "grad_norm": 0.03368576243519783,
      "learning_rate": 0.001526680508416394,
      "loss": 0.8764,
      "step": 2578
    },
    {
      "epoch": 3.4386666666666668,
      "grad_norm": 0.02127956971526146,
      "learning_rate": 0.001526313373042255,
      "loss": 0.8621,
      "step": 2579
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.03725455701351166,
      "learning_rate": 0.0015259461395208628,
      "loss": 1.2843,
      "step": 2580
    },
    {
      "epoch": 3.4413333333333336,
      "grad_norm": 0.039324913173913956,
      "learning_rate": 0.0015255788079206996,
      "loss": 1.0289,
      "step": 2581
    },
    {
      "epoch": 3.4426666666666668,
      "grad_norm": 0.03568699583411217,
      "learning_rate": 0.0015252113783102664,
      "loss": 0.8703,
      "step": 2582
    },
    {
      "epoch": 3.444,
      "grad_norm": 0.022660043090581894,
      "learning_rate": 0.0015248438507580804,
      "loss": 1.099,
      "step": 2583
    },
    {
      "epoch": 3.445333333333333,
      "grad_norm": 0.03190918266773224,
      "learning_rate": 0.001524476225332679,
      "loss": 1.1764,
      "step": 2584
    },
    {
      "epoch": 3.4466666666666668,
      "grad_norm": 0.023582149296998978,
      "learning_rate": 0.001524108502102617,
      "loss": 1.2096,
      "step": 2585
    },
    {
      "epoch": 3.448,
      "grad_norm": 0.049435678869485855,
      "learning_rate": 0.001523740681136468,
      "loss": 1.2135,
      "step": 2586
    },
    {
      "epoch": 3.449333333333333,
      "grad_norm": 0.021071888506412506,
      "learning_rate": 0.0015233727625028235,
      "loss": 0.8288,
      "step": 2587
    },
    {
      "epoch": 3.4506666666666668,
      "grad_norm": 0.020260900259017944,
      "learning_rate": 0.0015230047462702929,
      "loss": 0.954,
      "step": 2588
    },
    {
      "epoch": 3.452,
      "grad_norm": 0.03677899017930031,
      "learning_rate": 0.001522636632507504,
      "loss": 0.9655,
      "step": 2589
    },
    {
      "epoch": 3.453333333333333,
      "grad_norm": 0.040499817579984665,
      "learning_rate": 0.0015222684212831035,
      "loss": 1.073,
      "step": 2590
    },
    {
      "epoch": 3.4546666666666668,
      "grad_norm": 0.03046325221657753,
      "learning_rate": 0.001521900112665755,
      "loss": 1.0951,
      "step": 2591
    },
    {
      "epoch": 3.456,
      "grad_norm": 0.035632211714982986,
      "learning_rate": 0.0015215317067241415,
      "loss": 1.1119,
      "step": 2592
    },
    {
      "epoch": 3.457333333333333,
      "grad_norm": 0.021572867408394814,
      "learning_rate": 0.001521163203526963,
      "loss": 1.1573,
      "step": 2593
    },
    {
      "epoch": 3.458666666666667,
      "grad_norm": 0.0329684317111969,
      "learning_rate": 0.0015207946031429382,
      "loss": 1.1597,
      "step": 2594
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.02423080988228321,
      "learning_rate": 0.0015204259056408045,
      "loss": 1.2825,
      "step": 2595
    },
    {
      "epoch": 3.461333333333333,
      "grad_norm": 0.022977907210588455,
      "learning_rate": 0.0015200571110893166,
      "loss": 1.0494,
      "step": 2596
    },
    {
      "epoch": 3.462666666666667,
      "grad_norm": 0.02543036825954914,
      "learning_rate": 0.0015196882195572478,
      "loss": 0.8409,
      "step": 2597
    },
    {
      "epoch": 3.464,
      "grad_norm": 0.027418948709964752,
      "learning_rate": 0.0015193192311133883,
      "loss": 1.3209,
      "step": 2598
    },
    {
      "epoch": 3.465333333333333,
      "grad_norm": 0.026546886190772057,
      "learning_rate": 0.0015189501458265484,
      "loss": 1.0871,
      "step": 2599
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.02773890271782875,
      "learning_rate": 0.0015185809637655548,
      "loss": 1.0236,
      "step": 2600
    },
    {
      "epoch": 3.468,
      "grad_norm": 0.024350605905056,
      "learning_rate": 0.0015182116849992526,
      "loss": 0.8256,
      "step": 2601
    },
    {
      "epoch": 3.469333333333333,
      "grad_norm": 0.041717130690813065,
      "learning_rate": 0.0015178423095965057,
      "loss": 1.1038,
      "step": 2602
    },
    {
      "epoch": 3.470666666666667,
      "grad_norm": 0.02741559036076069,
      "learning_rate": 0.0015174728376261954,
      "loss": 0.793,
      "step": 2603
    },
    {
      "epoch": 3.472,
      "grad_norm": 0.028016071766614914,
      "learning_rate": 0.0015171032691572207,
      "loss": 1.1204,
      "step": 2604
    },
    {
      "epoch": 3.473333333333333,
      "grad_norm": 0.05290793254971504,
      "learning_rate": 0.0015167336042584988,
      "loss": 1.0694,
      "step": 2605
    },
    {
      "epoch": 3.474666666666667,
      "grad_norm": 0.027287837117910385,
      "learning_rate": 0.001516363842998966,
      "loss": 1.3301,
      "step": 2606
    },
    {
      "epoch": 3.476,
      "grad_norm": 0.2217392772436142,
      "learning_rate": 0.0015159939854475742,
      "loss": 1.1184,
      "step": 2607
    },
    {
      "epoch": 3.477333333333333,
      "grad_norm": 0.03642546385526657,
      "learning_rate": 0.001515624031673296,
      "loss": 1.2972,
      "step": 2608
    },
    {
      "epoch": 3.478666666666667,
      "grad_norm": 0.0277714766561985,
      "learning_rate": 0.00151525398174512,
      "loss": 0.8704,
      "step": 2609
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.03702674061059952,
      "learning_rate": 0.0015148838357320535,
      "loss": 1.1775,
      "step": 2610
    },
    {
      "epoch": 3.481333333333333,
      "grad_norm": 0.3401488661766052,
      "learning_rate": 0.0015145135937031213,
      "loss": 1.0807,
      "step": 2611
    },
    {
      "epoch": 3.482666666666667,
      "grad_norm": 0.06498779356479645,
      "learning_rate": 0.0015141432557273667,
      "loss": 1.0269,
      "step": 2612
    },
    {
      "epoch": 3.484,
      "grad_norm": 0.1089654490351677,
      "learning_rate": 0.0015137728218738503,
      "loss": 0.9474,
      "step": 2613
    },
    {
      "epoch": 3.485333333333333,
      "grad_norm": 0.09508586674928665,
      "learning_rate": 0.001513402292211651,
      "loss": 1.0583,
      "step": 2614
    },
    {
      "epoch": 3.486666666666667,
      "grad_norm": 0.16933989524841309,
      "learning_rate": 0.0015130316668098653,
      "loss": 0.9647,
      "step": 2615
    },
    {
      "epoch": 3.488,
      "grad_norm": 0.10450366139411926,
      "learning_rate": 0.0015126609457376079,
      "loss": 1.0066,
      "step": 2616
    },
    {
      "epoch": 3.489333333333333,
      "grad_norm": 0.12513089179992676,
      "learning_rate": 0.0015122901290640107,
      "loss": 1.041,
      "step": 2617
    },
    {
      "epoch": 3.490666666666667,
      "grad_norm": 0.04251750186085701,
      "learning_rate": 0.0015119192168582245,
      "loss": 1.0019,
      "step": 2618
    },
    {
      "epoch": 3.492,
      "grad_norm": 0.07887353003025055,
      "learning_rate": 0.0015115482091894164,
      "loss": 1.2128,
      "step": 2619
    },
    {
      "epoch": 3.493333333333333,
      "grad_norm": 0.027373600751161575,
      "learning_rate": 0.0015111771061267728,
      "loss": 0.8764,
      "step": 2620
    },
    {
      "epoch": 3.494666666666667,
      "grad_norm": 0.030362196266651154,
      "learning_rate": 0.0015108059077394965,
      "loss": 0.8796,
      "step": 2621
    },
    {
      "epoch": 3.496,
      "grad_norm": 0.026532135903835297,
      "learning_rate": 0.0015104346140968096,
      "loss": 1.1277,
      "step": 2622
    },
    {
      "epoch": 3.497333333333333,
      "grad_norm": 0.031960830092430115,
      "learning_rate": 0.001510063225267951,
      "loss": 1.0707,
      "step": 2623
    },
    {
      "epoch": 3.498666666666667,
      "grad_norm": 0.0385419987142086,
      "learning_rate": 0.0015096917413221771,
      "loss": 1.1094,
      "step": 2624
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.037002794444561005,
      "learning_rate": 0.001509320162328763,
      "loss": 1.3728,
      "step": 2625
    },
    {
      "epoch": 3.501333333333333,
      "grad_norm": 0.03350534290075302,
      "learning_rate": 0.0015089484883570009,
      "loss": 0.952,
      "step": 2626
    },
    {
      "epoch": 3.502666666666667,
      "grad_norm": 0.029442545026540756,
      "learning_rate": 0.0015085767194762002,
      "loss": 0.7674,
      "step": 2627
    },
    {
      "epoch": 3.504,
      "grad_norm": 0.02579192817211151,
      "learning_rate": 0.0015082048557556892,
      "loss": 1.034,
      "step": 2628
    },
    {
      "epoch": 3.505333333333333,
      "grad_norm": 0.043609268963336945,
      "learning_rate": 0.0015078328972648132,
      "loss": 0.9986,
      "step": 2629
    },
    {
      "epoch": 3.506666666666667,
      "grad_norm": 0.02376370131969452,
      "learning_rate": 0.0015074608440729352,
      "loss": 0.9658,
      "step": 2630
    },
    {
      "epoch": 3.508,
      "grad_norm": 0.024407770484685898,
      "learning_rate": 0.0015070886962494359,
      "loss": 0.9332,
      "step": 2631
    },
    {
      "epoch": 3.509333333333333,
      "grad_norm": 0.032607272267341614,
      "learning_rate": 0.0015067164538637137,
      "loss": 1.475,
      "step": 2632
    },
    {
      "epoch": 3.510666666666667,
      "grad_norm": 0.02553672529757023,
      "learning_rate": 0.0015063441169851843,
      "loss": 0.9105,
      "step": 2633
    },
    {
      "epoch": 3.512,
      "grad_norm": 0.04520638659596443,
      "learning_rate": 0.001505971685683282,
      "loss": 0.7777,
      "step": 2634
    },
    {
      "epoch": 3.513333333333333,
      "grad_norm": 0.0288997869938612,
      "learning_rate": 0.0015055991600274573,
      "loss": 0.9255,
      "step": 2635
    },
    {
      "epoch": 3.514666666666667,
      "grad_norm": 0.02700154110789299,
      "learning_rate": 0.0015052265400871793,
      "loss": 1.0392,
      "step": 2636
    },
    {
      "epoch": 3.516,
      "grad_norm": 0.029447391629219055,
      "learning_rate": 0.0015048538259319346,
      "loss": 0.985,
      "step": 2637
    },
    {
      "epoch": 3.517333333333333,
      "grad_norm": 0.045534636825323105,
      "learning_rate": 0.001504481017631227,
      "loss": 1.1505,
      "step": 2638
    },
    {
      "epoch": 3.518666666666667,
      "grad_norm": 0.023737052455544472,
      "learning_rate": 0.001504108115254578,
      "loss": 1.258,
      "step": 2639
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.03951924666762352,
      "learning_rate": 0.0015037351188715265,
      "loss": 1.1325,
      "step": 2640
    },
    {
      "epoch": 3.521333333333333,
      "grad_norm": 0.02568242698907852,
      "learning_rate": 0.0015033620285516294,
      "loss": 0.8359,
      "step": 2641
    },
    {
      "epoch": 3.522666666666667,
      "grad_norm": 0.03653354570269585,
      "learning_rate": 0.0015029888443644608,
      "loss": 0.961,
      "step": 2642
    },
    {
      "epoch": 3.524,
      "grad_norm": 0.029189543798565865,
      "learning_rate": 0.0015026155663796122,
      "loss": 1.0363,
      "step": 2643
    },
    {
      "epoch": 3.525333333333333,
      "grad_norm": 0.023516353219747543,
      "learning_rate": 0.001502242194666693,
      "loss": 1.0919,
      "step": 2644
    },
    {
      "epoch": 3.5266666666666664,
      "grad_norm": 0.027252977713942528,
      "learning_rate": 0.0015018687292953292,
      "loss": 0.991,
      "step": 2645
    },
    {
      "epoch": 3.528,
      "grad_norm": 0.02789846435189247,
      "learning_rate": 0.0015014951703351653,
      "loss": 1.0013,
      "step": 2646
    },
    {
      "epoch": 3.529333333333333,
      "grad_norm": 0.03697671741247177,
      "learning_rate": 0.0015011215178558628,
      "loss": 0.9285,
      "step": 2647
    },
    {
      "epoch": 3.530666666666667,
      "grad_norm": 0.027413450181484222,
      "learning_rate": 0.0015007477719271006,
      "loss": 1.2932,
      "step": 2648
    },
    {
      "epoch": 3.532,
      "grad_norm": 0.02845138870179653,
      "learning_rate": 0.001500373932618575,
      "loss": 0.8689,
      "step": 2649
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.034894756972789764,
      "learning_rate": 0.0015,
      "loss": 0.9474,
      "step": 2650
    },
    {
      "epoch": 3.5346666666666664,
      "grad_norm": 0.028852490708231926,
      "learning_rate": 0.0014996259741411066,
      "loss": 0.9185,
      "step": 2651
    },
    {
      "epoch": 3.536,
      "grad_norm": 0.03075764700770378,
      "learning_rate": 0.0014992518551116434,
      "loss": 0.8864,
      "step": 2652
    },
    {
      "epoch": 3.537333333333333,
      "grad_norm": 0.04611166939139366,
      "learning_rate": 0.0014988776429813765,
      "loss": 1.156,
      "step": 2653
    },
    {
      "epoch": 3.538666666666667,
      "grad_norm": 0.046426475048065186,
      "learning_rate": 0.0014985033378200887,
      "loss": 1.0101,
      "step": 2654
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.029568206518888474,
      "learning_rate": 0.0014981289396975817,
      "loss": 0.939,
      "step": 2655
    },
    {
      "epoch": 3.541333333333333,
      "grad_norm": 0.028134800493717194,
      "learning_rate": 0.0014977544486836725,
      "loss": 1.0501,
      "step": 2656
    },
    {
      "epoch": 3.5426666666666664,
      "grad_norm": 0.029329420998692513,
      "learning_rate": 0.0014973798648481966,
      "loss": 1.1223,
      "step": 2657
    },
    {
      "epoch": 3.544,
      "grad_norm": 0.02560429461300373,
      "learning_rate": 0.001497005188261007,
      "loss": 1.1239,
      "step": 2658
    },
    {
      "epoch": 3.5453333333333332,
      "grad_norm": 0.025953136384487152,
      "learning_rate": 0.0014966304189919738,
      "loss": 0.8423,
      "step": 2659
    },
    {
      "epoch": 3.546666666666667,
      "grad_norm": 0.03728245198726654,
      "learning_rate": 0.0014962555571109836,
      "loss": 1.0215,
      "step": 2660
    },
    {
      "epoch": 3.548,
      "grad_norm": 0.025108549743890762,
      "learning_rate": 0.001495880602687941,
      "loss": 1.1761,
      "step": 2661
    },
    {
      "epoch": 3.5493333333333332,
      "grad_norm": 0.0247303768992424,
      "learning_rate": 0.0014955055557927678,
      "loss": 0.8247,
      "step": 2662
    },
    {
      "epoch": 3.5506666666666664,
      "grad_norm": 0.03704344853758812,
      "learning_rate": 0.0014951304164954032,
      "loss": 0.9531,
      "step": 2663
    },
    {
      "epoch": 3.552,
      "grad_norm": 0.023453280329704285,
      "learning_rate": 0.0014947551848658034,
      "loss": 0.9733,
      "step": 2664
    },
    {
      "epoch": 3.5533333333333332,
      "grad_norm": 0.033672355115413666,
      "learning_rate": 0.0014943798609739418,
      "loss": 1.247,
      "step": 2665
    },
    {
      "epoch": 3.554666666666667,
      "grad_norm": 0.024130815640091896,
      "learning_rate": 0.0014940044448898088,
      "loss": 1.1315,
      "step": 2666
    },
    {
      "epoch": 3.556,
      "grad_norm": 0.05795634537935257,
      "learning_rate": 0.0014936289366834121,
      "loss": 0.9636,
      "step": 2667
    },
    {
      "epoch": 3.5573333333333332,
      "grad_norm": 0.0350647047162056,
      "learning_rate": 0.0014932533364247773,
      "loss": 0.8541,
      "step": 2668
    },
    {
      "epoch": 3.5586666666666664,
      "grad_norm": 0.13287730515003204,
      "learning_rate": 0.0014928776441839463,
      "loss": 0.7253,
      "step": 2669
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.026568235829472542,
      "learning_rate": 0.0014925018600309783,
      "loss": 0.8578,
      "step": 2670
    },
    {
      "epoch": 3.5613333333333332,
      "grad_norm": 0.030157292261719704,
      "learning_rate": 0.00149212598403595,
      "loss": 1.1322,
      "step": 2671
    },
    {
      "epoch": 3.562666666666667,
      "grad_norm": 0.03018285147845745,
      "learning_rate": 0.0014917500162689547,
      "loss": 1.036,
      "step": 2672
    },
    {
      "epoch": 3.564,
      "grad_norm": 0.03393219783902168,
      "learning_rate": 0.0014913739568001032,
      "loss": 1.0344,
      "step": 2673
    },
    {
      "epoch": 3.5653333333333332,
      "grad_norm": 0.03446604311466217,
      "learning_rate": 0.0014909978056995236,
      "loss": 1.1499,
      "step": 2674
    },
    {
      "epoch": 3.5666666666666664,
      "grad_norm": 0.03890344873070717,
      "learning_rate": 0.0014906215630373606,
      "loss": 1.0856,
      "step": 2675
    },
    {
      "epoch": 3.568,
      "grad_norm": 0.040850117802619934,
      "learning_rate": 0.001490245228883776,
      "loss": 1.002,
      "step": 2676
    },
    {
      "epoch": 3.5693333333333332,
      "grad_norm": 0.03146995231509209,
      "learning_rate": 0.0014898688033089492,
      "loss": 1.1613,
      "step": 2677
    },
    {
      "epoch": 3.570666666666667,
      "grad_norm": 0.08061686903238297,
      "learning_rate": 0.0014894922863830755,
      "loss": 0.9757,
      "step": 2678
    },
    {
      "epoch": 3.572,
      "grad_norm": 0.04351629689335823,
      "learning_rate": 0.001489115678176369,
      "loss": 0.8836,
      "step": 2679
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 0.028780026361346245,
      "learning_rate": 0.0014887389787590596,
      "loss": 1.2236,
      "step": 2680
    },
    {
      "epoch": 3.5746666666666664,
      "grad_norm": 0.05139017105102539,
      "learning_rate": 0.001488362188201394,
      "loss": 1.2868,
      "step": 2681
    },
    {
      "epoch": 3.576,
      "grad_norm": 0.02971043810248375,
      "learning_rate": 0.0014879853065736365,
      "loss": 1.1043,
      "step": 2682
    },
    {
      "epoch": 3.5773333333333333,
      "grad_norm": 0.033959951251745224,
      "learning_rate": 0.0014876083339460684,
      "loss": 1.0448,
      "step": 2683
    },
    {
      "epoch": 3.578666666666667,
      "grad_norm": 0.0299295112490654,
      "learning_rate": 0.0014872312703889876,
      "loss": 1.051,
      "step": 2684
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.03797631710767746,
      "learning_rate": 0.0014868541159727096,
      "loss": 0.8981,
      "step": 2685
    },
    {
      "epoch": 3.5813333333333333,
      "grad_norm": 0.025604279711842537,
      "learning_rate": 0.001486476870767566,
      "loss": 1.213,
      "step": 2686
    },
    {
      "epoch": 3.5826666666666664,
      "grad_norm": 0.027425212785601616,
      "learning_rate": 0.001486099534843906,
      "loss": 0.7792,
      "step": 2687
    },
    {
      "epoch": 3.584,
      "grad_norm": 0.02645193785429001,
      "learning_rate": 0.001485722108272095,
      "loss": 1.0424,
      "step": 2688
    },
    {
      "epoch": 3.5853333333333333,
      "grad_norm": 0.027502913028001785,
      "learning_rate": 0.0014853445911225157,
      "loss": 0.9782,
      "step": 2689
    },
    {
      "epoch": 3.586666666666667,
      "grad_norm": 0.026940174400806427,
      "learning_rate": 0.0014849669834655682,
      "loss": 1.0542,
      "step": 2690
    },
    {
      "epoch": 3.588,
      "grad_norm": 0.04200434312224388,
      "learning_rate": 0.0014845892853716691,
      "loss": 0.9287,
      "step": 2691
    },
    {
      "epoch": 3.5893333333333333,
      "grad_norm": 0.024588465690612793,
      "learning_rate": 0.0014842114969112515,
      "loss": 0.9111,
      "step": 2692
    },
    {
      "epoch": 3.5906666666666665,
      "grad_norm": 0.03784032166004181,
      "learning_rate": 0.0014838336181547654,
      "loss": 1.1846,
      "step": 2693
    },
    {
      "epoch": 3.592,
      "grad_norm": 0.024791598320007324,
      "learning_rate": 0.0014834556491726781,
      "loss": 0.9855,
      "step": 2694
    },
    {
      "epoch": 3.5933333333333333,
      "grad_norm": 0.02665967494249344,
      "learning_rate": 0.0014830775900354736,
      "loss": 0.6745,
      "step": 2695
    },
    {
      "epoch": 3.594666666666667,
      "grad_norm": 0.03374791890382767,
      "learning_rate": 0.001482699440813653,
      "loss": 1.02,
      "step": 2696
    },
    {
      "epoch": 3.596,
      "grad_norm": 0.02190694585442543,
      "learning_rate": 0.001482321201577733,
      "loss": 1.0262,
      "step": 2697
    },
    {
      "epoch": 3.5973333333333333,
      "grad_norm": 0.026874126866459846,
      "learning_rate": 0.001481942872398248,
      "loss": 0.9709,
      "step": 2698
    },
    {
      "epoch": 3.5986666666666665,
      "grad_norm": 0.03531080111861229,
      "learning_rate": 0.0014815644533457495,
      "loss": 1.0017,
      "step": 2699
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.028004441410303116,
      "learning_rate": 0.001481185944490805,
      "loss": 1.1243,
      "step": 2700
    },
    {
      "epoch": 3.6013333333333333,
      "grad_norm": 0.025013569742441177,
      "learning_rate": 0.0014808073459039992,
      "loss": 0.9334,
      "step": 2701
    },
    {
      "epoch": 3.602666666666667,
      "grad_norm": 0.027828391641378403,
      "learning_rate": 0.0014804286576559338,
      "loss": 1.1581,
      "step": 2702
    },
    {
      "epoch": 3.604,
      "grad_norm": 0.025029435753822327,
      "learning_rate": 0.0014800498798172262,
      "loss": 0.9739,
      "step": 2703
    },
    {
      "epoch": 3.6053333333333333,
      "grad_norm": 0.023069454357028008,
      "learning_rate": 0.0014796710124585113,
      "loss": 0.9206,
      "step": 2704
    },
    {
      "epoch": 3.6066666666666665,
      "grad_norm": 0.03555361181497574,
      "learning_rate": 0.0014792920556504405,
      "loss": 1.1572,
      "step": 2705
    },
    {
      "epoch": 3.608,
      "grad_norm": 0.03019336797297001,
      "learning_rate": 0.0014789130094636821,
      "loss": 1.0446,
      "step": 2706
    },
    {
      "epoch": 3.6093333333333333,
      "grad_norm": 0.027857037261128426,
      "learning_rate": 0.0014785338739689209,
      "loss": 1.3743,
      "step": 2707
    },
    {
      "epoch": 3.610666666666667,
      "grad_norm": 0.024503404274582863,
      "learning_rate": 0.0014781546492368583,
      "loss": 0.8901,
      "step": 2708
    },
    {
      "epoch": 3.612,
      "grad_norm": 0.0323958694934845,
      "learning_rate": 0.001477775335338212,
      "loss": 1.5475,
      "step": 2709
    },
    {
      "epoch": 3.6133333333333333,
      "grad_norm": 0.024283772334456444,
      "learning_rate": 0.001477395932343717,
      "loss": 1.0242,
      "step": 2710
    },
    {
      "epoch": 3.6146666666666665,
      "grad_norm": 0.023708121851086617,
      "learning_rate": 0.0014770164403241244,
      "loss": 0.9319,
      "step": 2711
    },
    {
      "epoch": 3.616,
      "grad_norm": 0.026696382090449333,
      "learning_rate": 0.0014766368593502027,
      "loss": 1.1006,
      "step": 2712
    },
    {
      "epoch": 3.6173333333333333,
      "grad_norm": 0.022862225770950317,
      "learning_rate": 0.0014762571894927357,
      "loss": 1.1027,
      "step": 2713
    },
    {
      "epoch": 3.618666666666667,
      "grad_norm": 0.04084797948598862,
      "learning_rate": 0.0014758774308225248,
      "loss": 0.994,
      "step": 2714
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.0368378683924675,
      "learning_rate": 0.0014754975834103876,
      "loss": 1.1577,
      "step": 2715
    },
    {
      "epoch": 3.6213333333333333,
      "grad_norm": 0.030806494876742363,
      "learning_rate": 0.0014751176473271584,
      "loss": 1.105,
      "step": 2716
    },
    {
      "epoch": 3.6226666666666665,
      "grad_norm": 0.023765461519360542,
      "learning_rate": 0.0014747376226436877,
      "loss": 0.9846,
      "step": 2717
    },
    {
      "epoch": 3.624,
      "grad_norm": 0.03350101411342621,
      "learning_rate": 0.001474357509430843,
      "loss": 1.1205,
      "step": 2718
    },
    {
      "epoch": 3.6253333333333333,
      "grad_norm": 0.03420400992035866,
      "learning_rate": 0.001473977307759508,
      "loss": 1.0024,
      "step": 2719
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 0.026973959058523178,
      "learning_rate": 0.0014735970177005826,
      "loss": 0.9434,
      "step": 2720
    },
    {
      "epoch": 3.628,
      "grad_norm": 0.029265617951750755,
      "learning_rate": 0.0014732166393249839,
      "loss": 0.9671,
      "step": 2721
    },
    {
      "epoch": 3.6293333333333333,
      "grad_norm": 0.05082523450255394,
      "learning_rate": 0.001472836172703645,
      "loss": 0.8841,
      "step": 2722
    },
    {
      "epoch": 3.6306666666666665,
      "grad_norm": 0.028954751789569855,
      "learning_rate": 0.0014724556179075159,
      "loss": 1.0406,
      "step": 2723
    },
    {
      "epoch": 3.632,
      "grad_norm": 0.025810465216636658,
      "learning_rate": 0.001472074975007562,
      "loss": 1.0434,
      "step": 2724
    },
    {
      "epoch": 3.6333333333333333,
      "grad_norm": 0.02941451407968998,
      "learning_rate": 0.0014716942440747662,
      "loss": 0.9535,
      "step": 2725
    },
    {
      "epoch": 3.634666666666667,
      "grad_norm": 0.03752925619482994,
      "learning_rate": 0.0014713134251801274,
      "loss": 0.8552,
      "step": 2726
    },
    {
      "epoch": 3.636,
      "grad_norm": 0.03051326610147953,
      "learning_rate": 0.001470932518394661,
      "loss": 0.9051,
      "step": 2727
    },
    {
      "epoch": 3.6373333333333333,
      "grad_norm": 0.02990506775677204,
      "learning_rate": 0.0014705515237893989,
      "loss": 0.9257,
      "step": 2728
    },
    {
      "epoch": 3.6386666666666665,
      "grad_norm": 0.042169369757175446,
      "learning_rate": 0.001470170441435389,
      "loss": 0.9231,
      "step": 2729
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.028223607689142227,
      "learning_rate": 0.0014697892714036957,
      "loss": 1.2395,
      "step": 2730
    },
    {
      "epoch": 3.6413333333333333,
      "grad_norm": 0.024999937042593956,
      "learning_rate": 0.0014694080137654,
      "loss": 1.0016,
      "step": 2731
    },
    {
      "epoch": 3.642666666666667,
      "grad_norm": 0.02876865118741989,
      "learning_rate": 0.001469026668591599,
      "loss": 1.1275,
      "step": 2732
    },
    {
      "epoch": 3.644,
      "grad_norm": 0.029911167919635773,
      "learning_rate": 0.0014686452359534065,
      "loss": 0.9685,
      "step": 2733
    },
    {
      "epoch": 3.6453333333333333,
      "grad_norm": 0.028591884300112724,
      "learning_rate": 0.0014682637159219515,
      "loss": 0.9663,
      "step": 2734
    },
    {
      "epoch": 3.6466666666666665,
      "grad_norm": 0.027288606390357018,
      "learning_rate": 0.001467882108568381,
      "loss": 1.0689,
      "step": 2735
    },
    {
      "epoch": 3.648,
      "grad_norm": 0.03460914269089699,
      "learning_rate": 0.001467500413963857,
      "loss": 1.0882,
      "step": 2736
    },
    {
      "epoch": 3.6493333333333333,
      "grad_norm": 0.035340674221515656,
      "learning_rate": 0.001467118632179558,
      "loss": 1.2314,
      "step": 2737
    },
    {
      "epoch": 3.6506666666666665,
      "grad_norm": 0.033759068697690964,
      "learning_rate": 0.0014667367632866788,
      "loss": 1.0389,
      "step": 2738
    },
    {
      "epoch": 3.652,
      "grad_norm": 0.022568589076399803,
      "learning_rate": 0.0014663548073564315,
      "loss": 1.0153,
      "step": 2739
    },
    {
      "epoch": 3.6533333333333333,
      "grad_norm": 0.028117015957832336,
      "learning_rate": 0.0014659727644600422,
      "loss": 1.2227,
      "step": 2740
    },
    {
      "epoch": 3.6546666666666665,
      "grad_norm": 0.03794282674789429,
      "learning_rate": 0.0014655906346687554,
      "loss": 0.7537,
      "step": 2741
    },
    {
      "epoch": 3.656,
      "grad_norm": 0.03535578399896622,
      "learning_rate": 0.0014652084180538302,
      "loss": 0.9344,
      "step": 2742
    },
    {
      "epoch": 3.6573333333333333,
      "grad_norm": 0.024768060073256493,
      "learning_rate": 0.0014648261146865433,
      "loss": 0.7784,
      "step": 2743
    },
    {
      "epoch": 3.6586666666666665,
      "grad_norm": 0.05397210270166397,
      "learning_rate": 0.0014644437246381869,
      "loss": 1.0263,
      "step": 2744
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.03496697172522545,
      "learning_rate": 0.0014640612479800686,
      "loss": 1.2455,
      "step": 2745
    },
    {
      "epoch": 3.6613333333333333,
      "grad_norm": 0.0363704115152359,
      "learning_rate": 0.0014636786847835135,
      "loss": 1.1126,
      "step": 2746
    },
    {
      "epoch": 3.6626666666666665,
      "grad_norm": 0.0311104916036129,
      "learning_rate": 0.0014632960351198619,
      "loss": 0.9661,
      "step": 2747
    },
    {
      "epoch": 3.664,
      "grad_norm": 0.027475086972117424,
      "learning_rate": 0.0014629132990604705,
      "loss": 0.9572,
      "step": 2748
    },
    {
      "epoch": 3.6653333333333333,
      "grad_norm": 0.027844037860631943,
      "learning_rate": 0.001462530476676713,
      "loss": 0.7527,
      "step": 2749
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.0322151780128479,
      "learning_rate": 0.001462147568039977,
      "loss": 1.1066,
      "step": 2750
    },
    {
      "epoch": 3.668,
      "grad_norm": 0.030315663665533066,
      "learning_rate": 0.0014617645732216684,
      "loss": 0.995,
      "step": 2751
    },
    {
      "epoch": 3.6693333333333333,
      "grad_norm": 0.03172527253627777,
      "learning_rate": 0.0014613814922932085,
      "loss": 1.4355,
      "step": 2752
    },
    {
      "epoch": 3.6706666666666665,
      "grad_norm": 0.023575522005558014,
      "learning_rate": 0.001460998325326034,
      "loss": 0.9198,
      "step": 2753
    },
    {
      "epoch": 3.672,
      "grad_norm": 0.0442599281668663,
      "learning_rate": 0.0014606150723915983,
      "loss": 1.0324,
      "step": 2754
    },
    {
      "epoch": 3.6733333333333333,
      "grad_norm": 0.04192838445305824,
      "learning_rate": 0.0014602317335613707,
      "loss": 1.0943,
      "step": 2755
    },
    {
      "epoch": 3.6746666666666665,
      "grad_norm": 0.02252388745546341,
      "learning_rate": 0.0014598483089068362,
      "loss": 1.1833,
      "step": 2756
    },
    {
      "epoch": 3.676,
      "grad_norm": 0.02336704730987549,
      "learning_rate": 0.0014594647984994965,
      "loss": 1.0506,
      "step": 2757
    },
    {
      "epoch": 3.6773333333333333,
      "grad_norm": 0.029484987258911133,
      "learning_rate": 0.0014590812024108683,
      "loss": 1.1492,
      "step": 2758
    },
    {
      "epoch": 3.6786666666666665,
      "grad_norm": 0.028706058859825134,
      "learning_rate": 0.0014586975207124856,
      "loss": 1.0144,
      "step": 2759
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.03972335159778595,
      "learning_rate": 0.0014583137534758968,
      "loss": 0.9103,
      "step": 2760
    },
    {
      "epoch": 3.6813333333333333,
      "grad_norm": 0.03559676185250282,
      "learning_rate": 0.0014579299007726676,
      "loss": 1.2288,
      "step": 2761
    },
    {
      "epoch": 3.6826666666666665,
      "grad_norm": 0.0300539992749691,
      "learning_rate": 0.001457545962674379,
      "loss": 1.1297,
      "step": 2762
    },
    {
      "epoch": 3.684,
      "grad_norm": 0.026957480236887932,
      "learning_rate": 0.001457161939252628,
      "loss": 1.0131,
      "step": 2763
    },
    {
      "epoch": 3.6853333333333333,
      "grad_norm": 0.032016120851039886,
      "learning_rate": 0.001456777830579027,
      "loss": 0.8566,
      "step": 2764
    },
    {
      "epoch": 3.6866666666666665,
      "grad_norm": 0.03578690439462662,
      "learning_rate": 0.0014563936367252063,
      "loss": 1.0326,
      "step": 2765
    },
    {
      "epoch": 3.6879999999999997,
      "grad_norm": 0.03606768697500229,
      "learning_rate": 0.0014560093577628088,
      "loss": 1.2296,
      "step": 2766
    },
    {
      "epoch": 3.6893333333333334,
      "grad_norm": 0.02706056460738182,
      "learning_rate": 0.0014556249937634962,
      "loss": 0.8357,
      "step": 2767
    },
    {
      "epoch": 3.6906666666666665,
      "grad_norm": 0.04262732341885567,
      "learning_rate": 0.0014552405447989447,
      "loss": 1.297,
      "step": 2768
    },
    {
      "epoch": 3.692,
      "grad_norm": 0.032735079526901245,
      "learning_rate": 0.0014548560109408464,
      "loss": 1.2505,
      "step": 2769
    },
    {
      "epoch": 3.6933333333333334,
      "grad_norm": 0.03560561314225197,
      "learning_rate": 0.0014544713922609099,
      "loss": 0.7107,
      "step": 2770
    },
    {
      "epoch": 3.6946666666666665,
      "grad_norm": 0.024613749235868454,
      "learning_rate": 0.0014540866888308583,
      "loss": 1.1823,
      "step": 2771
    },
    {
      "epoch": 3.6959999999999997,
      "grad_norm": 0.02536056563258171,
      "learning_rate": 0.0014537019007224324,
      "loss": 1.0198,
      "step": 2772
    },
    {
      "epoch": 3.6973333333333334,
      "grad_norm": 0.030623432248830795,
      "learning_rate": 0.0014533170280073868,
      "loss": 1.0477,
      "step": 2773
    },
    {
      "epoch": 3.6986666666666665,
      "grad_norm": 0.02865542471408844,
      "learning_rate": 0.0014529320707574933,
      "loss": 0.8892,
      "step": 2774
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.02817879244685173,
      "learning_rate": 0.0014525470290445391,
      "loss": 1.1805,
      "step": 2775
    },
    {
      "epoch": 3.7013333333333334,
      "grad_norm": 0.03903454542160034,
      "learning_rate": 0.0014521619029403264,
      "loss": 0.9745,
      "step": 2776
    },
    {
      "epoch": 3.7026666666666666,
      "grad_norm": 0.02744726836681366,
      "learning_rate": 0.001451776692516674,
      "loss": 0.9936,
      "step": 2777
    },
    {
      "epoch": 3.7039999999999997,
      "grad_norm": 0.03121342882514,
      "learning_rate": 0.0014513913978454168,
      "loss": 1.0479,
      "step": 2778
    },
    {
      "epoch": 3.7053333333333334,
      "grad_norm": 0.03727066144347191,
      "learning_rate": 0.0014510060189984043,
      "loss": 1.0854,
      "step": 2779
    },
    {
      "epoch": 3.7066666666666666,
      "grad_norm": 0.029636893421411514,
      "learning_rate": 0.0014506205560475022,
      "loss": 1.01,
      "step": 2780
    },
    {
      "epoch": 3.708,
      "grad_norm": 0.030298417434096336,
      "learning_rate": 0.0014502350090645918,
      "loss": 1.0504,
      "step": 2781
    },
    {
      "epoch": 3.7093333333333334,
      "grad_norm": 0.026129554957151413,
      "learning_rate": 0.00144984937812157,
      "loss": 1.1572,
      "step": 2782
    },
    {
      "epoch": 3.7106666666666666,
      "grad_norm": 0.032476551830768585,
      "learning_rate": 0.00144946366329035,
      "loss": 1.299,
      "step": 2783
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 0.023621179163455963,
      "learning_rate": 0.00144907786464286,
      "loss": 0.9454,
      "step": 2784
    },
    {
      "epoch": 3.7133333333333334,
      "grad_norm": 0.0596742182970047,
      "learning_rate": 0.0014486919822510437,
      "loss": 1.1052,
      "step": 2785
    },
    {
      "epoch": 3.7146666666666666,
      "grad_norm": 0.030794749036431313,
      "learning_rate": 0.001448306016186861,
      "loss": 0.7735,
      "step": 2786
    },
    {
      "epoch": 3.716,
      "grad_norm": 0.02764621376991272,
      "learning_rate": 0.0014479199665222869,
      "loss": 1.0564,
      "step": 2787
    },
    {
      "epoch": 3.7173333333333334,
      "grad_norm": 0.02544453926384449,
      "learning_rate": 0.0014475338333293122,
      "loss": 1.0569,
      "step": 2788
    },
    {
      "epoch": 3.7186666666666666,
      "grad_norm": 0.03067994862794876,
      "learning_rate": 0.0014471476166799434,
      "loss": 1.1585,
      "step": 2789
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.03241416811943054,
      "learning_rate": 0.0014467613166462023,
      "loss": 0.8737,
      "step": 2790
    },
    {
      "epoch": 3.7213333333333334,
      "grad_norm": 0.031445086002349854,
      "learning_rate": 0.0014463749333001267,
      "loss": 1.057,
      "step": 2791
    },
    {
      "epoch": 3.7226666666666666,
      "grad_norm": 0.028751809149980545,
      "learning_rate": 0.0014459884667137688,
      "loss": 1.1326,
      "step": 2792
    },
    {
      "epoch": 3.724,
      "grad_norm": 0.029487036168575287,
      "learning_rate": 0.001445601916959198,
      "loss": 1.0569,
      "step": 2793
    },
    {
      "epoch": 3.7253333333333334,
      "grad_norm": 0.028473736718297005,
      "learning_rate": 0.0014452152841084977,
      "loss": 1.235,
      "step": 2794
    },
    {
      "epoch": 3.7266666666666666,
      "grad_norm": 0.02992996759712696,
      "learning_rate": 0.0014448285682337682,
      "loss": 0.8398,
      "step": 2795
    },
    {
      "epoch": 3.7279999999999998,
      "grad_norm": 0.03005838580429554,
      "learning_rate": 0.001444441769407124,
      "loss": 0.9326,
      "step": 2796
    },
    {
      "epoch": 3.7293333333333334,
      "grad_norm": 0.026246314868330956,
      "learning_rate": 0.0014440548877006955,
      "loss": 1.0444,
      "step": 2797
    },
    {
      "epoch": 3.7306666666666666,
      "grad_norm": 0.03311502933502197,
      "learning_rate": 0.0014436679231866287,
      "loss": 0.9507,
      "step": 2798
    },
    {
      "epoch": 3.732,
      "grad_norm": 0.03221499174833298,
      "learning_rate": 0.0014432808759370851,
      "loss": 1.1749,
      "step": 2799
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.05810149759054184,
      "learning_rate": 0.0014428937460242417,
      "loss": 0.9207,
      "step": 2800
    },
    {
      "epoch": 3.7346666666666666,
      "grad_norm": 0.030253862962126732,
      "learning_rate": 0.0014425065335202905,
      "loss": 1.1064,
      "step": 2801
    },
    {
      "epoch": 3.7359999999999998,
      "grad_norm": 0.024280721321702003,
      "learning_rate": 0.0014421192384974396,
      "loss": 0.9698,
      "step": 2802
    },
    {
      "epoch": 3.7373333333333334,
      "grad_norm": 0.03633169084787369,
      "learning_rate": 0.0014417318610279108,
      "loss": 1.0055,
      "step": 2803
    },
    {
      "epoch": 3.7386666666666666,
      "grad_norm": 0.0409846194088459,
      "learning_rate": 0.001441344401183944,
      "loss": 0.8952,
      "step": 2804
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.023469528183341026,
      "learning_rate": 0.0014409568590377918,
      "loss": 0.832,
      "step": 2805
    },
    {
      "epoch": 3.7413333333333334,
      "grad_norm": 0.0314524844288826,
      "learning_rate": 0.001440569234661724,
      "loss": 1.1004,
      "step": 2806
    },
    {
      "epoch": 3.7426666666666666,
      "grad_norm": 0.03856069594621658,
      "learning_rate": 0.0014401815281280248,
      "loss": 0.9002,
      "step": 2807
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 0.037458356469869614,
      "learning_rate": 0.001439793739508994,
      "loss": 1.1192,
      "step": 2808
    },
    {
      "epoch": 3.7453333333333334,
      "grad_norm": 0.037933576852083206,
      "learning_rate": 0.0014394058688769462,
      "loss": 1.0488,
      "step": 2809
    },
    {
      "epoch": 3.7466666666666666,
      "grad_norm": 0.03077608346939087,
      "learning_rate": 0.0014390179163042127,
      "loss": 1.0777,
      "step": 2810
    },
    {
      "epoch": 3.748,
      "grad_norm": 0.02886286936700344,
      "learning_rate": 0.0014386298818631388,
      "loss": 1.052,
      "step": 2811
    },
    {
      "epoch": 3.7493333333333334,
      "grad_norm": 0.024394448846578598,
      "learning_rate": 0.001438241765626085,
      "loss": 1.0096,
      "step": 2812
    },
    {
      "epoch": 3.7506666666666666,
      "grad_norm": 0.027189994230866432,
      "learning_rate": 0.0014378535676654275,
      "loss": 1.4139,
      "step": 2813
    },
    {
      "epoch": 3.752,
      "grad_norm": 0.023523354902863503,
      "learning_rate": 0.001437465288053558,
      "loss": 1.1173,
      "step": 2814
    },
    {
      "epoch": 3.7533333333333334,
      "grad_norm": 0.025446990504860878,
      "learning_rate": 0.0014370769268628831,
      "loss": 1.1305,
      "step": 2815
    },
    {
      "epoch": 3.7546666666666666,
      "grad_norm": 0.032425396144390106,
      "learning_rate": 0.0014366884841658246,
      "loss": 1.241,
      "step": 2816
    },
    {
      "epoch": 3.7560000000000002,
      "grad_norm": 0.03546149283647537,
      "learning_rate": 0.0014362999600348197,
      "loss": 0.9426,
      "step": 2817
    },
    {
      "epoch": 3.7573333333333334,
      "grad_norm": 0.04358874633908272,
      "learning_rate": 0.0014359113545423204,
      "loss": 0.9504,
      "step": 2818
    },
    {
      "epoch": 3.7586666666666666,
      "grad_norm": 0.04669652879238129,
      "learning_rate": 0.001435522667760794,
      "loss": 1.2638,
      "step": 2819
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.022312361747026443,
      "learning_rate": 0.0014351338997627232,
      "loss": 0.8551,
      "step": 2820
    },
    {
      "epoch": 3.7613333333333334,
      "grad_norm": 0.03288346901535988,
      "learning_rate": 0.0014347450506206059,
      "loss": 0.897,
      "step": 2821
    },
    {
      "epoch": 3.7626666666666666,
      "grad_norm": 0.024103552103042603,
      "learning_rate": 0.001434356120406955,
      "loss": 0.9446,
      "step": 2822
    },
    {
      "epoch": 3.7640000000000002,
      "grad_norm": 0.02598053403198719,
      "learning_rate": 0.0014339671091942979,
      "loss": 1.0305,
      "step": 2823
    },
    {
      "epoch": 3.7653333333333334,
      "grad_norm": 0.02736038528382778,
      "learning_rate": 0.0014335780170551778,
      "loss": 1.2107,
      "step": 2824
    },
    {
      "epoch": 3.7666666666666666,
      "grad_norm": 0.02991488389670849,
      "learning_rate": 0.0014331888440621532,
      "loss": 1.0434,
      "step": 2825
    },
    {
      "epoch": 3.768,
      "grad_norm": 0.02444799616932869,
      "learning_rate": 0.001432799590287797,
      "loss": 1.2836,
      "step": 2826
    },
    {
      "epoch": 3.7693333333333334,
      "grad_norm": 0.029061418026685715,
      "learning_rate": 0.0014324102558046978,
      "loss": 0.9898,
      "step": 2827
    },
    {
      "epoch": 3.7706666666666666,
      "grad_norm": 0.022267503663897514,
      "learning_rate": 0.001432020840685459,
      "loss": 0.8792,
      "step": 2828
    },
    {
      "epoch": 3.7720000000000002,
      "grad_norm": 0.02409345656633377,
      "learning_rate": 0.0014316313450026986,
      "loss": 1.0105,
      "step": 2829
    },
    {
      "epoch": 3.7733333333333334,
      "grad_norm": 0.042972415685653687,
      "learning_rate": 0.00143124176882905,
      "loss": 0.9696,
      "step": 2830
    },
    {
      "epoch": 3.7746666666666666,
      "grad_norm": 0.025333549827337265,
      "learning_rate": 0.0014308521122371617,
      "loss": 0.9509,
      "step": 2831
    },
    {
      "epoch": 3.776,
      "grad_norm": 0.023711014539003372,
      "learning_rate": 0.0014304623752996973,
      "loss": 0.9569,
      "step": 2832
    },
    {
      "epoch": 3.7773333333333334,
      "grad_norm": 0.02375168539583683,
      "learning_rate": 0.0014300725580893353,
      "loss": 1.0983,
      "step": 2833
    },
    {
      "epoch": 3.7786666666666666,
      "grad_norm": 0.026765450835227966,
      "learning_rate": 0.0014296826606787687,
      "loss": 1.0483,
      "step": 2834
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 0.025843197479844093,
      "learning_rate": 0.001429292683140706,
      "loss": 0.9317,
      "step": 2835
    },
    {
      "epoch": 3.7813333333333334,
      "grad_norm": 0.028770873323082924,
      "learning_rate": 0.0014289026255478704,
      "loss": 1.0646,
      "step": 2836
    },
    {
      "epoch": 3.7826666666666666,
      "grad_norm": 0.02200883813202381,
      "learning_rate": 0.0014285124879729997,
      "loss": 1.0505,
      "step": 2837
    },
    {
      "epoch": 3.784,
      "grad_norm": 0.028123708441853523,
      "learning_rate": 0.0014281222704888479,
      "loss": 1.1583,
      "step": 2838
    },
    {
      "epoch": 3.7853333333333334,
      "grad_norm": 0.029545539990067482,
      "learning_rate": 0.0014277319731681823,
      "loss": 1.1889,
      "step": 2839
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 0.02637980692088604,
      "learning_rate": 0.0014273415960837864,
      "loss": 1.0525,
      "step": 2840
    },
    {
      "epoch": 3.7880000000000003,
      "grad_norm": 0.03301851823925972,
      "learning_rate": 0.001426951139308457,
      "loss": 0.8733,
      "step": 2841
    },
    {
      "epoch": 3.7893333333333334,
      "grad_norm": 0.021629780530929565,
      "learning_rate": 0.0014265606029150075,
      "loss": 1.0962,
      "step": 2842
    },
    {
      "epoch": 3.7906666666666666,
      "grad_norm": 0.028054628521203995,
      "learning_rate": 0.0014261699869762654,
      "loss": 1.0739,
      "step": 2843
    },
    {
      "epoch": 3.792,
      "grad_norm": 0.03427442908287048,
      "learning_rate": 0.0014257792915650727,
      "loss": 1.0667,
      "step": 2844
    },
    {
      "epoch": 3.7933333333333334,
      "grad_norm": 0.03520698472857475,
      "learning_rate": 0.0014253885167542866,
      "loss": 1.0192,
      "step": 2845
    },
    {
      "epoch": 3.7946666666666666,
      "grad_norm": 0.03690023347735405,
      "learning_rate": 0.001424997662616779,
      "loss": 1.3332,
      "step": 2846
    },
    {
      "epoch": 3.7960000000000003,
      "grad_norm": 0.02663618139922619,
      "learning_rate": 0.0014246067292254365,
      "loss": 0.9308,
      "step": 2847
    },
    {
      "epoch": 3.7973333333333334,
      "grad_norm": 0.019507860764861107,
      "learning_rate": 0.001424215716653161,
      "loss": 0.9994,
      "step": 2848
    },
    {
      "epoch": 3.7986666666666666,
      "grad_norm": 0.025148067623376846,
      "learning_rate": 0.0014238246249728686,
      "loss": 1.0536,
      "step": 2849
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.02437569759786129,
      "learning_rate": 0.0014234334542574906,
      "loss": 1.3997,
      "step": 2850
    },
    {
      "epoch": 3.8013333333333335,
      "grad_norm": 0.02610943838953972,
      "learning_rate": 0.001423042204579972,
      "loss": 0.8646,
      "step": 2851
    },
    {
      "epoch": 3.8026666666666666,
      "grad_norm": 0.024848733097314835,
      "learning_rate": 0.0014226508760132736,
      "loss": 0.8758,
      "step": 2852
    },
    {
      "epoch": 3.8040000000000003,
      "grad_norm": 0.02927078865468502,
      "learning_rate": 0.0014222594686303708,
      "loss": 1.2037,
      "step": 2853
    },
    {
      "epoch": 3.8053333333333335,
      "grad_norm": 0.032110195606946945,
      "learning_rate": 0.0014218679825042538,
      "loss": 0.8702,
      "step": 2854
    },
    {
      "epoch": 3.8066666666666666,
      "grad_norm": 0.026564529165625572,
      "learning_rate": 0.0014214764177079265,
      "loss": 1.1801,
      "step": 2855
    },
    {
      "epoch": 3.808,
      "grad_norm": 0.02554307132959366,
      "learning_rate": 0.0014210847743144086,
      "loss": 1.0633,
      "step": 2856
    },
    {
      "epoch": 3.8093333333333335,
      "grad_norm": 0.030500132590532303,
      "learning_rate": 0.0014206930523967337,
      "loss": 0.8669,
      "step": 2857
    },
    {
      "epoch": 3.8106666666666666,
      "grad_norm": 0.03376735374331474,
      "learning_rate": 0.0014203012520279507,
      "loss": 1.1251,
      "step": 2858
    },
    {
      "epoch": 3.8120000000000003,
      "grad_norm": 0.024058109149336815,
      "learning_rate": 0.0014199093732811226,
      "loss": 0.7422,
      "step": 2859
    },
    {
      "epoch": 3.8133333333333335,
      "grad_norm": 0.04014690965414047,
      "learning_rate": 0.0014195174162293272,
      "loss": 1.1854,
      "step": 2860
    },
    {
      "epoch": 3.8146666666666667,
      "grad_norm": 0.028046414256095886,
      "learning_rate": 0.001419125380945657,
      "loss": 1.0043,
      "step": 2861
    },
    {
      "epoch": 3.816,
      "grad_norm": 0.024966608732938766,
      "learning_rate": 0.0014187332675032187,
      "loss": 0.8948,
      "step": 2862
    },
    {
      "epoch": 3.8173333333333335,
      "grad_norm": 0.030467189848423004,
      "learning_rate": 0.0014183410759751342,
      "loss": 1.1929,
      "step": 2863
    },
    {
      "epoch": 3.8186666666666667,
      "grad_norm": 0.030300142243504524,
      "learning_rate": 0.0014179488064345396,
      "loss": 0.7839,
      "step": 2864
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.04391980543732643,
      "learning_rate": 0.0014175564589545852,
      "loss": 1.0627,
      "step": 2865
    },
    {
      "epoch": 3.8213333333333335,
      "grad_norm": 0.026474125683307648,
      "learning_rate": 0.0014171640336084368,
      "loss": 1.1689,
      "step": 2866
    },
    {
      "epoch": 3.8226666666666667,
      "grad_norm": 0.057547785341739655,
      "learning_rate": 0.0014167715304692736,
      "loss": 1.099,
      "step": 2867
    },
    {
      "epoch": 3.824,
      "grad_norm": 0.035072825849056244,
      "learning_rate": 0.0014163789496102902,
      "loss": 0.9904,
      "step": 2868
    },
    {
      "epoch": 3.8253333333333335,
      "grad_norm": 0.028786519542336464,
      "learning_rate": 0.0014159862911046952,
      "loss": 1.0771,
      "step": 2869
    },
    {
      "epoch": 3.8266666666666667,
      "grad_norm": 0.027482429519295692,
      "learning_rate": 0.0014155935550257115,
      "loss": 1.1863,
      "step": 2870
    },
    {
      "epoch": 3.828,
      "grad_norm": 0.031510017812252045,
      "learning_rate": 0.001415200741446577,
      "loss": 1.2065,
      "step": 2871
    },
    {
      "epoch": 3.8293333333333335,
      "grad_norm": 0.04297809302806854,
      "learning_rate": 0.0014148078504405443,
      "loss": 1.0933,
      "step": 2872
    },
    {
      "epoch": 3.8306666666666667,
      "grad_norm": 0.026532214134931564,
      "learning_rate": 0.0014144148820808794,
      "loss": 1.1933,
      "step": 2873
    },
    {
      "epoch": 3.832,
      "grad_norm": 0.03664121404290199,
      "learning_rate": 0.0014140218364408633,
      "loss": 1.0669,
      "step": 2874
    },
    {
      "epoch": 3.8333333333333335,
      "grad_norm": 0.035500943660736084,
      "learning_rate": 0.0014136287135937916,
      "loss": 0.9208,
      "step": 2875
    },
    {
      "epoch": 3.8346666666666667,
      "grad_norm": 0.025860460475087166,
      "learning_rate": 0.0014132355136129736,
      "loss": 0.9283,
      "step": 2876
    },
    {
      "epoch": 3.836,
      "grad_norm": 0.03688691928982735,
      "learning_rate": 0.0014128422365717345,
      "loss": 1.4446,
      "step": 2877
    },
    {
      "epoch": 3.8373333333333335,
      "grad_norm": 0.02830248326063156,
      "learning_rate": 0.001412448882543412,
      "loss": 1.0864,
      "step": 2878
    },
    {
      "epoch": 3.8386666666666667,
      "grad_norm": 0.038166459649801254,
      "learning_rate": 0.0014120554516013596,
      "loss": 1.3745,
      "step": 2879
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.029097585007548332,
      "learning_rate": 0.001411661943818944,
      "loss": 0.8716,
      "step": 2880
    },
    {
      "epoch": 3.8413333333333335,
      "grad_norm": 0.040628302842378616,
      "learning_rate": 0.001411268359269547,
      "loss": 1.1989,
      "step": 2881
    },
    {
      "epoch": 3.8426666666666667,
      "grad_norm": 0.030701041221618652,
      "learning_rate": 0.0014108746980265644,
      "loss": 1.179,
      "step": 2882
    },
    {
      "epoch": 3.844,
      "grad_norm": 0.04437253251671791,
      "learning_rate": 0.0014104809601634067,
      "loss": 0.9472,
      "step": 2883
    },
    {
      "epoch": 3.8453333333333335,
      "grad_norm": 0.03040781244635582,
      "learning_rate": 0.001410087145753498,
      "loss": 0.9455,
      "step": 2884
    },
    {
      "epoch": 3.8466666666666667,
      "grad_norm": 0.028550032526254654,
      "learning_rate": 0.0014096932548702777,
      "loss": 0.9408,
      "step": 2885
    },
    {
      "epoch": 3.848,
      "grad_norm": 0.03133310005068779,
      "learning_rate": 0.001409299287587198,
      "loss": 1.244,
      "step": 2886
    },
    {
      "epoch": 3.8493333333333335,
      "grad_norm": 0.027492988854646683,
      "learning_rate": 0.0014089052439777263,
      "loss": 0.9545,
      "step": 2887
    },
    {
      "epoch": 3.8506666666666667,
      "grad_norm": 0.03247205540537834,
      "learning_rate": 0.0014085111241153448,
      "loss": 1.4562,
      "step": 2888
    },
    {
      "epoch": 3.852,
      "grad_norm": 0.025768816471099854,
      "learning_rate": 0.0014081169280735486,
      "loss": 1.0693,
      "step": 2889
    },
    {
      "epoch": 3.8533333333333335,
      "grad_norm": 0.03161565214395523,
      "learning_rate": 0.001407722655925848,
      "loss": 1.4604,
      "step": 2890
    },
    {
      "epoch": 3.8546666666666667,
      "grad_norm": 0.028620261698961258,
      "learning_rate": 0.0014073283077457666,
      "loss": 1.0197,
      "step": 2891
    },
    {
      "epoch": 3.856,
      "grad_norm": 0.025284118950366974,
      "learning_rate": 0.0014069338836068433,
      "loss": 1.0717,
      "step": 2892
    },
    {
      "epoch": 3.857333333333333,
      "grad_norm": 0.022776255384087563,
      "learning_rate": 0.0014065393835826301,
      "loss": 1.159,
      "step": 2893
    },
    {
      "epoch": 3.8586666666666667,
      "grad_norm": 0.049686819314956665,
      "learning_rate": 0.0014061448077466938,
      "loss": 1.1188,
      "step": 2894
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.02407943643629551,
      "learning_rate": 0.0014057501561726155,
      "loss": 1.0282,
      "step": 2895
    },
    {
      "epoch": 3.8613333333333335,
      "grad_norm": 0.025699738413095474,
      "learning_rate": 0.0014053554289339893,
      "loss": 1.0328,
      "step": 2896
    },
    {
      "epoch": 3.8626666666666667,
      "grad_norm": 0.024676788598299026,
      "learning_rate": 0.0014049606261044248,
      "loss": 0.9082,
      "step": 2897
    },
    {
      "epoch": 3.864,
      "grad_norm": 0.022166617214679718,
      "learning_rate": 0.0014045657477575449,
      "loss": 0.8432,
      "step": 2898
    },
    {
      "epoch": 3.865333333333333,
      "grad_norm": 0.0265134796500206,
      "learning_rate": 0.0014041707939669867,
      "loss": 1.0998,
      "step": 2899
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.0228999275714159,
      "learning_rate": 0.0014037757648064017,
      "loss": 0.9647,
      "step": 2900
    },
    {
      "epoch": 3.868,
      "grad_norm": 0.027951689437031746,
      "learning_rate": 0.001403380660349455,
      "loss": 0.9462,
      "step": 2901
    },
    {
      "epoch": 3.8693333333333335,
      "grad_norm": 0.03290582820773125,
      "learning_rate": 0.0014029854806698256,
      "loss": 0.8403,
      "step": 2902
    },
    {
      "epoch": 3.8706666666666667,
      "grad_norm": 0.02382774092257023,
      "learning_rate": 0.0014025902258412075,
      "loss": 1.1293,
      "step": 2903
    },
    {
      "epoch": 3.872,
      "grad_norm": 0.025104332715272903,
      "learning_rate": 0.0014021948959373075,
      "loss": 1.0567,
      "step": 2904
    },
    {
      "epoch": 3.873333333333333,
      "grad_norm": 0.027715494856238365,
      "learning_rate": 0.0014017994910318474,
      "loss": 1.0719,
      "step": 2905
    },
    {
      "epoch": 3.8746666666666667,
      "grad_norm": 0.030464883893728256,
      "learning_rate": 0.0014014040111985627,
      "loss": 1.2782,
      "step": 2906
    },
    {
      "epoch": 3.876,
      "grad_norm": 0.03135085478425026,
      "learning_rate": 0.0014010084565112017,
      "loss": 0.8926,
      "step": 2907
    },
    {
      "epoch": 3.8773333333333335,
      "grad_norm": 0.026128599420189857,
      "learning_rate": 0.001400612827043529,
      "loss": 0.8622,
      "step": 2908
    },
    {
      "epoch": 3.8786666666666667,
      "grad_norm": 0.028053782880306244,
      "learning_rate": 0.0014002171228693209,
      "loss": 0.9992,
      "step": 2909
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.03819109871983528,
      "learning_rate": 0.001399821344062369,
      "loss": 0.9888,
      "step": 2910
    },
    {
      "epoch": 3.881333333333333,
      "grad_norm": 0.028637899085879326,
      "learning_rate": 0.0013994254906964786,
      "loss": 0.9431,
      "step": 2911
    },
    {
      "epoch": 3.8826666666666667,
      "grad_norm": 0.03652581200003624,
      "learning_rate": 0.0013990295628454681,
      "loss": 1.2598,
      "step": 2912
    },
    {
      "epoch": 3.884,
      "grad_norm": 0.027209248393774033,
      "learning_rate": 0.0013986335605831706,
      "loss": 0.932,
      "step": 2913
    },
    {
      "epoch": 3.8853333333333335,
      "grad_norm": 0.030985858291387558,
      "learning_rate": 0.0013982374839834328,
      "loss": 0.9429,
      "step": 2914
    },
    {
      "epoch": 3.8866666666666667,
      "grad_norm": 0.03402597829699516,
      "learning_rate": 0.0013978413331201156,
      "loss": 1.1373,
      "step": 2915
    },
    {
      "epoch": 3.888,
      "grad_norm": 0.027147766202688217,
      "learning_rate": 0.0013974451080670934,
      "loss": 1.0391,
      "step": 2916
    },
    {
      "epoch": 3.889333333333333,
      "grad_norm": 0.03365808352828026,
      "learning_rate": 0.0013970488088982545,
      "loss": 1.0616,
      "step": 2917
    },
    {
      "epoch": 3.8906666666666667,
      "grad_norm": 0.0384969525039196,
      "learning_rate": 0.0013966524356875006,
      "loss": 0.8975,
      "step": 2918
    },
    {
      "epoch": 3.892,
      "grad_norm": 0.02283892035484314,
      "learning_rate": 0.001396255988508748,
      "loss": 1.1242,
      "step": 2919
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 0.02147103101015091,
      "learning_rate": 0.0013958594674359265,
      "loss": 1.0521,
      "step": 2920
    },
    {
      "epoch": 3.8946666666666667,
      "grad_norm": 0.041412387043237686,
      "learning_rate": 0.0013954628725429794,
      "loss": 0.8445,
      "step": 2921
    },
    {
      "epoch": 3.896,
      "grad_norm": 0.023071283474564552,
      "learning_rate": 0.0013950662039038642,
      "loss": 1.0346,
      "step": 2922
    },
    {
      "epoch": 3.897333333333333,
      "grad_norm": 0.023917181417346,
      "learning_rate": 0.0013946694615925516,
      "loss": 1.1281,
      "step": 2923
    },
    {
      "epoch": 3.8986666666666667,
      "grad_norm": 0.026719313114881516,
      "learning_rate": 0.0013942726456830264,
      "loss": 1.0629,
      "step": 2924
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.033112697303295135,
      "learning_rate": 0.0013938757562492873,
      "loss": 1.0145,
      "step": 2925
    },
    {
      "epoch": 3.9013333333333335,
      "grad_norm": 0.033325403928756714,
      "learning_rate": 0.0013934787933653464,
      "loss": 1.1408,
      "step": 2926
    },
    {
      "epoch": 3.9026666666666667,
      "grad_norm": 0.1430092751979828,
      "learning_rate": 0.0013930817571052296,
      "loss": 1.4047,
      "step": 2927
    },
    {
      "epoch": 3.904,
      "grad_norm": 0.027354784309864044,
      "learning_rate": 0.0013926846475429766,
      "loss": 1.2008,
      "step": 2928
    },
    {
      "epoch": 3.905333333333333,
      "grad_norm": 0.025292841717600822,
      "learning_rate": 0.0013922874647526403,
      "loss": 0.8397,
      "step": 2929
    },
    {
      "epoch": 3.9066666666666667,
      "grad_norm": 0.02794763632118702,
      "learning_rate": 0.0013918902088082877,
      "loss": 0.747,
      "step": 2930
    },
    {
      "epoch": 3.908,
      "grad_norm": 0.03276193141937256,
      "learning_rate": 0.0013914928797839994,
      "loss": 0.9281,
      "step": 2931
    },
    {
      "epoch": 3.9093333333333335,
      "grad_norm": 0.03671855852007866,
      "learning_rate": 0.00139109547775387,
      "loss": 0.9066,
      "step": 2932
    },
    {
      "epoch": 3.9106666666666667,
      "grad_norm": 0.05099052935838699,
      "learning_rate": 0.001390698002792007,
      "loss": 1.0493,
      "step": 2933
    },
    {
      "epoch": 3.912,
      "grad_norm": 0.03391527384519577,
      "learning_rate": 0.0013903004549725311,
      "loss": 0.948,
      "step": 2934
    },
    {
      "epoch": 3.913333333333333,
      "grad_norm": 0.028109708800911903,
      "learning_rate": 0.0013899028343695781,
      "loss": 0.9947,
      "step": 2935
    },
    {
      "epoch": 3.9146666666666667,
      "grad_norm": 0.055167149752378464,
      "learning_rate": 0.0013895051410572963,
      "loss": 1.1125,
      "step": 2936
    },
    {
      "epoch": 3.916,
      "grad_norm": 0.025041639804840088,
      "learning_rate": 0.001389107375109848,
      "loss": 1.0607,
      "step": 2937
    },
    {
      "epoch": 3.9173333333333336,
      "grad_norm": 0.06017831712961197,
      "learning_rate": 0.0013887095366014086,
      "loss": 1.2362,
      "step": 2938
    },
    {
      "epoch": 3.9186666666666667,
      "grad_norm": 0.03311099112033844,
      "learning_rate": 0.0013883116256061673,
      "loss": 0.8488,
      "step": 2939
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.03140242397785187,
      "learning_rate": 0.0013879136421983266,
      "loss": 1.0114,
      "step": 2940
    },
    {
      "epoch": 3.921333333333333,
      "grad_norm": 0.028162464499473572,
      "learning_rate": 0.001387515586452103,
      "loss": 0.8539,
      "step": 2941
    },
    {
      "epoch": 3.9226666666666667,
      "grad_norm": 0.042098518460989,
      "learning_rate": 0.0013871174584417262,
      "loss": 1.2547,
      "step": 2942
    },
    {
      "epoch": 3.924,
      "grad_norm": 0.024709172546863556,
      "learning_rate": 0.001386719258241439,
      "loss": 0.7811,
      "step": 2943
    },
    {
      "epoch": 3.9253333333333336,
      "grad_norm": 0.020634863525629044,
      "learning_rate": 0.001386320985925499,
      "loss": 0.8525,
      "step": 2944
    },
    {
      "epoch": 3.9266666666666667,
      "grad_norm": 0.06294264644384384,
      "learning_rate": 0.001385922641568175,
      "loss": 0.9621,
      "step": 2945
    },
    {
      "epoch": 3.928,
      "grad_norm": 0.0238523930311203,
      "learning_rate": 0.001385524225243751,
      "loss": 0.8795,
      "step": 2946
    },
    {
      "epoch": 3.929333333333333,
      "grad_norm": 0.024240465834736824,
      "learning_rate": 0.0013851257370265243,
      "loss": 1.1705,
      "step": 2947
    },
    {
      "epoch": 3.9306666666666668,
      "grad_norm": 0.03205505385994911,
      "learning_rate": 0.0013847271769908047,
      "loss": 1.2249,
      "step": 2948
    },
    {
      "epoch": 3.932,
      "grad_norm": 0.032892484217882156,
      "learning_rate": 0.0013843285452109166,
      "loss": 1.0197,
      "step": 2949
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 0.022129680961370468,
      "learning_rate": 0.0013839298417611962,
      "loss": 1.1165,
      "step": 2950
    },
    {
      "epoch": 3.9346666666666668,
      "grad_norm": 0.027857843786478043,
      "learning_rate": 0.0013835310667159946,
      "loss": 1.1322,
      "step": 2951
    },
    {
      "epoch": 3.936,
      "grad_norm": 0.030448824167251587,
      "learning_rate": 0.0013831322201496756,
      "loss": 0.9241,
      "step": 2952
    },
    {
      "epoch": 3.937333333333333,
      "grad_norm": 0.028929920867085457,
      "learning_rate": 0.0013827333021366164,
      "loss": 1.2905,
      "step": 2953
    },
    {
      "epoch": 3.9386666666666668,
      "grad_norm": 0.03353249654173851,
      "learning_rate": 0.001382334312751207,
      "loss": 1.0281,
      "step": 2954
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.02495531179010868,
      "learning_rate": 0.0013819352520678518,
      "loss": 0.9456,
      "step": 2955
    },
    {
      "epoch": 3.9413333333333336,
      "grad_norm": 0.02873847633600235,
      "learning_rate": 0.0013815361201609676,
      "loss": 0.9097,
      "step": 2956
    },
    {
      "epoch": 3.9426666666666668,
      "grad_norm": 0.02554921805858612,
      "learning_rate": 0.0013811369171049847,
      "loss": 1.0781,
      "step": 2957
    },
    {
      "epoch": 3.944,
      "grad_norm": 0.03430157154798508,
      "learning_rate": 0.0013807376429743467,
      "loss": 1.0109,
      "step": 2958
    },
    {
      "epoch": 3.945333333333333,
      "grad_norm": 0.03298141807317734,
      "learning_rate": 0.0013803382978435105,
      "loss": 1.0231,
      "step": 2959
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 0.027236148715019226,
      "learning_rate": 0.0013799388817869467,
      "loss": 1.0243,
      "step": 2960
    },
    {
      "epoch": 3.948,
      "grad_norm": 0.021209578961133957,
      "learning_rate": 0.0013795393948791382,
      "loss": 0.9242,
      "step": 2961
    },
    {
      "epoch": 3.9493333333333336,
      "grad_norm": 0.024207210168242455,
      "learning_rate": 0.0013791398371945816,
      "loss": 1.0676,
      "step": 2962
    },
    {
      "epoch": 3.9506666666666668,
      "grad_norm": 0.02760438807308674,
      "learning_rate": 0.001378740208807787,
      "loss": 0.8909,
      "step": 2963
    },
    {
      "epoch": 3.952,
      "grad_norm": 0.03187151253223419,
      "learning_rate": 0.001378340509793277,
      "loss": 1.0745,
      "step": 2964
    },
    {
      "epoch": 3.953333333333333,
      "grad_norm": 0.023799825459718704,
      "learning_rate": 0.0013779407402255879,
      "loss": 1.3522,
      "step": 2965
    },
    {
      "epoch": 3.9546666666666668,
      "grad_norm": 0.030248083174228668,
      "learning_rate": 0.0013775409001792686,
      "loss": 0.9785,
      "step": 2966
    },
    {
      "epoch": 3.956,
      "grad_norm": 0.03491240739822388,
      "learning_rate": 0.0013771409897288822,
      "loss": 1.209,
      "step": 2967
    },
    {
      "epoch": 3.9573333333333336,
      "grad_norm": 0.02713368646800518,
      "learning_rate": 0.0013767410089490038,
      "loss": 0.7809,
      "step": 2968
    },
    {
      "epoch": 3.958666666666667,
      "grad_norm": 0.028289085254073143,
      "learning_rate": 0.001376340957914222,
      "loss": 0.903,
      "step": 2969
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.028389455750584602,
      "learning_rate": 0.001375940836699139,
      "loss": 0.9631,
      "step": 2970
    },
    {
      "epoch": 3.961333333333333,
      "grad_norm": 0.027344994246959686,
      "learning_rate": 0.0013755406453783696,
      "loss": 1.1942,
      "step": 2971
    },
    {
      "epoch": 3.962666666666667,
      "grad_norm": 0.032972611486911774,
      "learning_rate": 0.0013751403840265412,
      "loss": 1.0319,
      "step": 2972
    },
    {
      "epoch": 3.964,
      "grad_norm": 0.02747858129441738,
      "learning_rate": 0.0013747400527182952,
      "loss": 1.18,
      "step": 2973
    },
    {
      "epoch": 3.9653333333333336,
      "grad_norm": 0.0294891856610775,
      "learning_rate": 0.0013743396515282856,
      "loss": 0.9887,
      "step": 2974
    },
    {
      "epoch": 3.966666666666667,
      "grad_norm": 0.027269339188933372,
      "learning_rate": 0.0013739391805311794,
      "loss": 1.119,
      "step": 2975
    },
    {
      "epoch": 3.968,
      "grad_norm": 0.03175204247236252,
      "learning_rate": 0.0013735386398016569,
      "loss": 0.9366,
      "step": 2976
    },
    {
      "epoch": 3.969333333333333,
      "grad_norm": 0.026894399896264076,
      "learning_rate": 0.0013731380294144109,
      "loss": 1.1364,
      "step": 2977
    },
    {
      "epoch": 3.970666666666667,
      "grad_norm": 0.0344371572136879,
      "learning_rate": 0.0013727373494441476,
      "loss": 1.1807,
      "step": 2978
    },
    {
      "epoch": 3.972,
      "grad_norm": 0.02973778359591961,
      "learning_rate": 0.001372336599965586,
      "loss": 0.947,
      "step": 2979
    },
    {
      "epoch": 3.9733333333333336,
      "grad_norm": 0.0271048191934824,
      "learning_rate": 0.0013719357810534581,
      "loss": 0.997,
      "step": 2980
    },
    {
      "epoch": 3.974666666666667,
      "grad_norm": 0.03277235105633736,
      "learning_rate": 0.0013715348927825092,
      "loss": 1.32,
      "step": 2981
    },
    {
      "epoch": 3.976,
      "grad_norm": 0.025291111320257187,
      "learning_rate": 0.0013711339352274966,
      "loss": 1.0587,
      "step": 2982
    },
    {
      "epoch": 3.977333333333333,
      "grad_norm": 0.030764348804950714,
      "learning_rate": 0.0013707329084631915,
      "loss": 1.3051,
      "step": 2983
    },
    {
      "epoch": 3.978666666666667,
      "grad_norm": 0.032761577516794205,
      "learning_rate": 0.0013703318125643782,
      "loss": 0.8557,
      "step": 2984
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.02999434806406498,
      "learning_rate": 0.001369930647605852,
      "loss": 0.8414,
      "step": 2985
    },
    {
      "epoch": 3.981333333333333,
      "grad_norm": 0.03189219534397125,
      "learning_rate": 0.0013695294136624233,
      "loss": 1.2683,
      "step": 2986
    },
    {
      "epoch": 3.982666666666667,
      "grad_norm": 0.022111231461167336,
      "learning_rate": 0.0013691281108089144,
      "loss": 0.9076,
      "step": 2987
    },
    {
      "epoch": 3.984,
      "grad_norm": 0.026869749650359154,
      "learning_rate": 0.0013687267391201603,
      "loss": 1.009,
      "step": 2988
    },
    {
      "epoch": 3.985333333333333,
      "grad_norm": 0.037369295954704285,
      "learning_rate": 0.0013683252986710088,
      "loss": 1.0097,
      "step": 2989
    },
    {
      "epoch": 3.986666666666667,
      "grad_norm": 0.02933996357023716,
      "learning_rate": 0.0013679237895363216,
      "loss": 1.0743,
      "step": 2990
    },
    {
      "epoch": 3.988,
      "grad_norm": 0.028754670172929764,
      "learning_rate": 0.0013675222117909716,
      "loss": 1.1059,
      "step": 2991
    },
    {
      "epoch": 3.989333333333333,
      "grad_norm": 0.03144386038184166,
      "learning_rate": 0.0013671205655098454,
      "loss": 1.059,
      "step": 2992
    },
    {
      "epoch": 3.990666666666667,
      "grad_norm": 0.039157796651124954,
      "learning_rate": 0.001366718850767843,
      "loss": 1.0647,
      "step": 2993
    },
    {
      "epoch": 3.992,
      "grad_norm": 0.025938861072063446,
      "learning_rate": 0.001366317067639875,
      "loss": 1.0592,
      "step": 2994
    },
    {
      "epoch": 3.993333333333333,
      "grad_norm": 0.04146430268883705,
      "learning_rate": 0.0013659152162008676,
      "loss": 1.0874,
      "step": 2995
    },
    {
      "epoch": 3.994666666666667,
      "grad_norm": 0.03025633655488491,
      "learning_rate": 0.0013655132965257576,
      "loss": 1.0152,
      "step": 2996
    },
    {
      "epoch": 3.996,
      "grad_norm": 0.028425440192222595,
      "learning_rate": 0.001365111308689495,
      "loss": 1.1581,
      "step": 2997
    },
    {
      "epoch": 3.997333333333333,
      "grad_norm": 0.02611188031733036,
      "learning_rate": 0.0013647092527670438,
      "loss": 0.9872,
      "step": 2998
    },
    {
      "epoch": 3.998666666666667,
      "grad_norm": 0.02868477627635002,
      "learning_rate": 0.0013643071288333781,
      "loss": 1.2507,
      "step": 2999
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.032342445105314255,
      "learning_rate": 0.0013639049369634877,
      "loss": 1.1493,
      "step": 3000
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.0540913343429565,
      "eval_runtime": 23.7897,
      "eval_samples_per_second": 21.018,
      "eval_steps_per_second": 2.648,
      "step": 3000
    }
  ],
  "logging_steps": 1,
  "max_steps": 7500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 1500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.38799794561024e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
