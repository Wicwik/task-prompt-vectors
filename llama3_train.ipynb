{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, DataCollatorForLanguageModeling, GenerationConfig, pipeline\n",
    "from args import TrainingArguments, DataTrainingArguments, ArgumentParser\n",
    "\n",
    "from peft import get_peft_model\n",
    "\n",
    "from arithmetics import PromptArithmeticsConfig\n",
    "\n",
    "from tasks import Preprocessor\n",
    "\n",
    "# from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser(\n",
    "    (TrainingArguments, DataTrainingArguments, PromptArithmeticsConfig)\n",
    ")\n",
    "\n",
    "training_args, data_args, pt_args = parser.parse_toml_file(\"./configs/prompt_tuning/single-task/llama3_8b.toml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17bbe2139b0f4704b27d05c8b6103544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(training_args.model_name_or_path, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "model = get_peft_model(model, peft_config=pt_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(data_args.data_tokenizer_name_or_path, trust_remote_code=True, padding_side=\"left\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|reserved_special_token_0|>\"})\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target lengths: [2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d5806f784f4fb399780412959717e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running yelp_polarity_text_preprocessor on dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ede7a109d64213a53d873bb9f70908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running preprocess_function on train_dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b187084ebdaf464eba90588d5730031e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running yelp_polarity_text_preprocessor on dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65d323b6e254f28a749f8bde67e08c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running preprocess_function on valid_dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abc05750f334c17a4e84a332e99323a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running yelp_polarity_text_preprocessor on dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a2aeed54bf4ce995ef9a4e9426c928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running preprocess_function on test_dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessor = Preprocessor(\n",
    "            [data_args.dataset_names[0]], data_args, training_args, pt_args, tokenizer\n",
    "        )\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = preprocessor.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "166\n",
      "166\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 43324, 128001]\n",
      "<|begin_of_text|>yelp_polarity sentence: For a large buffet, their selection is very limited.  The overall quality and number of offerings in not comparable to the better buffets in town. \\n\\nGetting your own drinks is so inconvenient and such a hassle for anyone with kids and older folks and basically anyone who does not want to have to cross half a football field to get a drink.  INstead of placing drink stations around the buffet they have them all on one wall the furthest away from the entrance and food. \\n\\nSince adding the beverage machines, they lost a lot of their more interesting offerings. Very little in terms of vegetables or fresh and interesting salads. Food is all lukewarm and pizza is ALWAYS cold and congealed. \\n\\nThe crab legs and free wine and beer during the week is nice. \\nThe sushi station is pretty much worthless. They should get rid of it as they offer inedible sushi that people take and throw away.\\nPasta station has ONE pasta dish and no other selections.\\n\\nThe tall blond woman manager is always scowling at guests and makes it uncomfortable to go there. \\n\\nMANAGEMENT: Please ask your staff to focus on making sure the food is not cold and make sure thenegative<|end_of_text|>\n",
      "positive<|end_of_text|>\n",
      "positive<|end_of_text|>\n",
      "[-100, 43324, 128001]\n",
      "[279, 43324, 128001]\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"input_ids\"].count(128001))\n",
    "print(train_dataset[0][\"input_ids\"].count(128002))\n",
    "print(train_dataset[0][\"attention_mask\"].count(0))\n",
    "print(train_dataset[6][\"labels\"])\n",
    "print(tokenizer.decode(train_dataset[6][\"input_ids\"]))\n",
    "print(tokenizer.decode(train_dataset[2][\"labels\"][-2:]))\n",
    "print(tokenizer.decode(train_dataset[2][\"input_ids\"][-2:]))\n",
    "print(train_dataset[6][\"labels\"][-3:])\n",
    "print(train_dataset[6][\"input_ids\"][-3:])\n",
    "print(train_dataset[1][\"labels\"].count(-100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|begin_of_text|>yelp_polarity sentence: My staple here-\\n\\nChicken Continental, \\nCiabatta bread (loveeee), honey mustard, tomatoes, lettuce, broiled chicken! (I wish the chicken was actually grilled)\\nThe bread and the pungent mustard are my absolute favorite items here, and their fresh OJ is pretty good for a morning snack. I try to get some extra honey mustard so I can spice up my veggie wraps\\n\\nI really havent had a bad sandwich here, the bread is pretty solid, the flavors are on- nothing extraordinarily new age or innovative, but its standard, delicious, reliable. label: positive<|end_of_text|>\n",
      "258\n",
      "258\n",
      "166\n",
      "1\n",
      "323\n",
      "258 258 258\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset[1][\"input_ids\"]))\n",
    "print(len(train_dataset[0][\"input_ids\"]))\n",
    "print(len(train_dataset[0][\"attention_mask\"]))\n",
    "\n",
    "print(train_dataset[0][\"attention_mask\"].count(0))\n",
    "print(train_dataset[0][\"attention_mask\"][199])\n",
    "print(train_dataset[0][\"input_ids\"][199])\n",
    "\n",
    "print(len(train_dataset[0][\"labels\"]), len(train_dataset[0][\"input_ids\"]), len(train_dataset[0][\"attention_mask\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:47:12] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 12:47:12] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 12:47:12] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 12:47:12] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 12:47:12] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 12:47:13] CPU Model on constant consumption mode: AMD EPYC 7543 32-Core Processor\n",
      "[codecarbon INFO @ 12:47:13] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 12:47:13]   Platform system: Linux-5.19.0-50-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 12:47:13]   Python version: 3.11.8\n",
      "[codecarbon INFO @ 12:47:13]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 12:47:13]   Available RAM : 503.092 GB\n",
      "[codecarbon INFO @ 12:47:13]   CPU count: 128\n",
      "[codecarbon INFO @ 12:47:13]   CPU model: AMD EPYC 7543 32-Core Processor\n",
      "[codecarbon INFO @ 12:47:13]   GPU count: 1\n",
      "[codecarbon INFO @ 12:47:13]   GPU model: 1 x NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    args=training_args,\n",
    "                    train_dataset=train_dataset,\n",
    "                    eval_dataset=list(valid_dataset.values())[0],\n",
    "                    data_collator=data_collator,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrobert-belanec\u001b[0m (\u001b[33mrbelanec\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/prompt-arithmetics/wandb/run-20240828_124721-tn0xjq6t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rbelanec/huggingface/runs/tn0xjq6t' target=\"_blank\">clean-wildflower-138</a></strong> to <a href='https://wandb.ai/rbelanec/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rbelanec/huggingface' target=\"_blank\">https://wandb.ai/rbelanec/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rbelanec/huggingface/runs/tn0xjq6t' target=\"_blank\">https://wandb.ai/rbelanec/huggingface/runs/tn0xjq6t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='259360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   101/259360 01:10 < 51:36:46, 1.40 it/s, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/250 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:47:41] Energy consumed for RAM : 0.000786 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:47:41] Energy consumed for all GPUs : 0.001149 kWh. Total GPU Power : 275.46775112283564 W\n",
      "[codecarbon INFO @ 12:47:41] Energy consumed for all CPUs : 0.000469 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:47:41] 0.002404 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:47:56] Energy consumed for RAM : 0.001572 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:47:56] Energy consumed for all GPUs : 0.002368 kWh. Total GPU Power : 292.87485926338667 W\n",
      "[codecarbon INFO @ 12:47:56] Energy consumed for all CPUs : 0.000938 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:47:56] 0.004878 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:48:11] Energy consumed for RAM : 0.002358 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:48:11] Energy consumed for all GPUs : 0.003618 kWh. Total GPU Power : 300.08213593167926 W\n",
      "[codecarbon INFO @ 12:48:11] Energy consumed for all CPUs : 0.001416 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:48:11] 0.007392 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:48:26] Energy consumed for RAM : 0.003127 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:48:26] Energy consumed for all GPUs : 0.004826 kWh. Total GPU Power : 296.10502069664756 W\n",
      "[codecarbon INFO @ 12:48:26] Energy consumed for all CPUs : 0.001879 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:48:26] 0.009833 kWh of electricity used since the beginning.\n",
      "/home/jovyan/my-conda-envs/pa/lib/python3.11/site-packages/peft/peft_model.py:1188: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n",
      "[codecarbon INFO @ 12:48:41] Energy consumed for RAM : 0.003906 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:48:41] Energy consumed for all GPUs : 0.006009 kWh. Total GPU Power : 286.3662772416416 W\n",
      "[codecarbon INFO @ 12:48:41] Energy consumed for all CPUs : 0.002344 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:48:41] 0.012260 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:48:56] Energy consumed for RAM : 0.004692 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:48:56] Energy consumed for all GPUs : 0.007080 kWh. Total GPU Power : 257.04103282448466 W\n",
      "[codecarbon INFO @ 12:48:56] Energy consumed for all CPUs : 0.002817 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:48:56] 0.014588 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:49:11] Energy consumed for RAM : 0.005471 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:49:11] Energy consumed for all GPUs : 0.008166 kWh. Total GPU Power : 262.9190345936608 W\n",
      "[codecarbon INFO @ 12:49:11] Energy consumed for all CPUs : 0.003282 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:49:11] 0.016919 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:49:26] Energy consumed for RAM : 0.006257 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:49:26] Energy consumed for all GPUs : 0.009285 kWh. Total GPU Power : 268.80045364903816 W\n",
      "[codecarbon INFO @ 12:49:26] Energy consumed for all CPUs : 0.003750 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:49:26] 0.019292 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:49:41] Energy consumed for RAM : 0.007042 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:49:41] Energy consumed for all GPUs : 0.010426 kWh. Total GPU Power : 273.91321766834506 W\n",
      "[codecarbon INFO @ 12:49:41] Energy consumed for all CPUs : 0.004219 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:49:41] 0.021687 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:49:56] Energy consumed for RAM : 0.007828 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:49:56] Energy consumed for all GPUs : 0.011584 kWh. Total GPU Power : 278.2213850237188 W\n",
      "[codecarbon INFO @ 12:49:56] Energy consumed for all CPUs : 0.004688 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:49:56] 0.024100 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:50:11] Energy consumed for RAM : 0.008613 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:50:11] Energy consumed for all GPUs : 0.012759 kWh. Total GPU Power : 282.06209488043027 W\n",
      "[codecarbon INFO @ 12:50:11] Energy consumed for all CPUs : 0.005157 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:50:11] 0.026529 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:50:26] Energy consumed for RAM : 0.009399 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:50:26] Energy consumed for all GPUs : 0.013988 kWh. Total GPU Power : 295.1269675065088 W\n",
      "[codecarbon INFO @ 12:50:26] Energy consumed for all CPUs : 0.005639 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:50:26] 0.029025 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:50:41] Energy consumed for RAM : 0.010161 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:50:41] Energy consumed for all GPUs : 0.015173 kWh. Total GPU Power : 292.92566610891146 W\n",
      "[codecarbon INFO @ 12:50:41] Energy consumed for all CPUs : 0.006103 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:50:41] 0.031436 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:50:56] Energy consumed for RAM : 0.010932 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:50:56] Energy consumed for all GPUs : 0.016369 kWh. Total GPU Power : 292.6735285633512 W\n",
      "[codecarbon INFO @ 12:50:56] Energy consumed for all CPUs : 0.006565 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:50:56] 0.033866 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:51:11] Energy consumed for RAM : 0.011714 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:51:11] Energy consumed for all GPUs : 0.017592 kWh. Total GPU Power : 295.3965862803826 W\n",
      "[codecarbon INFO @ 12:51:11] Energy consumed for all CPUs : 0.007032 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:51:11] 0.036337 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:51:26] Energy consumed for RAM : 0.012499 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:51:26] Energy consumed for all GPUs : 0.018858 kWh. Total GPU Power : 304.1550324100045 W\n",
      "[codecarbon INFO @ 12:51:26] Energy consumed for all CPUs : 0.007512 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:51:26] 0.038869 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:51:41] Energy consumed for RAM : 0.013265 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:51:41] Energy consumed for all GPUs : 0.020091 kWh. Total GPU Power : 303.6849100372611 W\n",
      "[codecarbon INFO @ 12:51:41] Energy consumed for all CPUs : 0.007975 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:51:41] 0.041331 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:51:56] Energy consumed for RAM : 0.014040 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:51:56] Energy consumed for all GPUs : 0.021317 kWh. Total GPU Power : 298.13632469580955 W\n",
      "[codecarbon INFO @ 12:51:56] Energy consumed for all CPUs : 0.008440 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:51:56] 0.043796 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:52:11] Energy consumed for RAM : 0.014822 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:52:11] Energy consumed for all GPUs : 0.022560 kWh. Total GPU Power : 299.81281528722127 W\n",
      "[codecarbon INFO @ 12:52:11] Energy consumed for all CPUs : 0.008906 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:52:11] 0.046287 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:52:26] Energy consumed for RAM : 0.015607 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:52:26] Energy consumed for all GPUs : 0.023814 kWh. Total GPU Power : 301.1971121946228 W\n",
      "[codecarbon INFO @ 12:52:26] Energy consumed for all CPUs : 0.009383 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:52:26] 0.048804 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:52:41] Energy consumed for RAM : 0.016379 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:52:41] Energy consumed for all GPUs : 0.024880 kWh. Total GPU Power : 260.7388334307292 W\n",
      "[codecarbon INFO @ 12:52:41] Energy consumed for all CPUs : 0.009844 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:52:41] 0.051103 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:52:56] Energy consumed for RAM : 0.017164 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:52:56] Energy consumed for all GPUs : 0.026026 kWh. Total GPU Power : 275.04758191369405 W\n",
      "[codecarbon INFO @ 12:52:56] Energy consumed for all CPUs : 0.010325 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:52:56] 0.053515 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:53:11] Energy consumed for RAM : 0.017929 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:53:11] Energy consumed for all GPUs : 0.027136 kWh. Total GPU Power : 273.6336503376296 W\n",
      "[codecarbon INFO @ 12:53:11] Energy consumed for all CPUs : 0.010781 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:53:11] 0.055846 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:53:26] Energy consumed for RAM : 0.018715 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:53:26] Energy consumed for all GPUs : 0.028312 kWh. Total GPU Power : 282.50891042169627 W\n",
      "[codecarbon INFO @ 12:53:26] Energy consumed for all CPUs : 0.011256 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:53:26] 0.058283 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:53:41] Energy consumed for RAM : 0.019490 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:53:41] Energy consumed for all GPUs : 0.029489 kWh. Total GPU Power : 286.30866979194366 W\n",
      "[codecarbon INFO @ 12:53:41] Energy consumed for all CPUs : 0.011725 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:53:41] 0.060704 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:53:56] Energy consumed for RAM : 0.020265 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:53:56] Energy consumed for all GPUs : 0.030688 kWh. Total GPU Power : 291.80229880352425 W\n",
      "[codecarbon INFO @ 12:53:56] Energy consumed for all CPUs : 0.012197 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:53:56] 0.063150 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:1914\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1911\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1912\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1914\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/prompt-arithmetics/trainer/multi_task_trainer.py:491\u001b[0m, in \u001b[0;36mMultiTaskSeq2SeqTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m    489\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eval_dataset_name, eval_dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 491\u001b[0m         dataset_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43meval_dataset_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/prompt-arithmetics/trainer/multi_task_trainer.py:108\u001b[0m, in \u001b[0;36mMultiTaskSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m    101\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    103\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m    107\u001b[0m )\n\u001b[0;32m--> 108\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/prompt-arithmetics/trainer/multi_task_trainer.py:241\u001b[0m, in \u001b[0;36mMultiTaskSeq2SeqTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m    238\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    245\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name])\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer_seq2seq.py:296\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m generation_inputs\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m generation_inputs\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m generation_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m generation_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    292\u001b[0m ):\n\u001b[1;32m    293\u001b[0m     generation_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    294\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m     }\n\u001b[0;32m--> 296\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_from_model_config:\n",
      "File \u001b[0;32m~/my-conda-envs/pa/lib/python3.11/site-packages/peft/peft_model.py:1148\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:1764\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1756\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1757\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1758\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1759\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1761\u001b[0m     )\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1781\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1782\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1788\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:2897\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m   2896\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2897\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2899\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:54:11] Energy consumed for RAM : 0.021034 kWh. RAM Power : 188.65951824188232 W\n",
      "[codecarbon INFO @ 12:54:11] Energy consumed for all GPUs : 0.031366 kWh. Total GPU Power : 166.46805439375427 W\n",
      "[codecarbon INFO @ 12:54:11] Energy consumed for all CPUs : 0.012656 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 12:54:11] 0.065056 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: qnli question: How big are phycobilisomes? sentence: Phycobilins come in all colors, though phycoerytherin is one of the pigments that makes many red algae red. label: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/pa/lib/python3.11/site-packages/peft/peft_model.py:1188: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: qnli question: How big are phycobilisomes? sentence: Phycobilins come in all colors, though phycoerytherin is one of the pigments that makes many red algae red. label: not_entailment\n"
     ]
    }
   ],
   "source": [
    "example_input = test_dataset[\"qnli_text\"][0][\"input_ids\"][:-5]\n",
    "example_attn_mask = test_dataset[\"qnli_text\"][0][\"attention_mask\"][:-5]\n",
    "\n",
    "print(\"input:\", tokenizer.decode(example_input, skip_special_tokens=True))\n",
    "\n",
    "outputs = model.generate(torch.tensor(example_input).unsqueeze(-1).reshape(1, -1).to(\"cuda\"), attention_mask=torch.tensor(example_attn_mask).unsqueeze(-1).reshape(1, -1).to(\"cuda\"))\n",
    "\n",
    "print(\"output:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80428/494740671.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  example_output = model.generate(torch.tensor(example_input).unsqueeze(-1).reshape(1, -1).to(\"cuda\"), attention_mask=torch.tensor(example_attn_mask).unsqueeze(-1).reshape(1, -1).to(\"cuda\"))[0]\n",
      "/home/jovyan/my-conda-envs/pa/lib/python3.11/site-packages/peft/peft_model.py:1188: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: not_entailment label: not_entailment\n",
      "output: entailment label: entailment\n",
      "output: entailment label: entailment\n"
     ]
    }
   ],
   "source": [
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "correct = 0\n",
    "total = len(test_dataset[\"qnli_text\"])\n",
    "\n",
    "for example in test_dataset[\"qnli_text\"]:\n",
    "    example_label = torch.tensor(example[\"labels\"]).index_select(0,index=(torch.tensor(example[\"labels\"]) != -100).nonzero().flatten())\n",
    "    example_input = torch.tensor(example[\"input_ids\"])[:-len(example_label)]\n",
    "    example_attn_mask = torch.tensor(example[\"attention_mask\"])[:-len(example_label)]\n",
    "\n",
    "    example_output = model.generate(torch.tensor(example_input).unsqueeze(-1).reshape(1, -1).to(\"cuda\"), attention_mask=torch.tensor(example_attn_mask).unsqueeze(-1).reshape(1, -1).to(\"cuda\"))[0]\n",
    "    n_new_tokens = len(example_output) - len(example_input)\n",
    "\n",
    "    decoded_output = tokenizer.decode(example_output[-n_new_tokens:], skip_special_tokens=True)\n",
    "    decoded_label = tokenizer.decode(example_label, skip_special_tokens=True)\n",
    "\n",
    "    if decoded_output == decoded_label:\n",
    "        correct += 1\n",
    "\n",
    "    print(\"output:\", decoded_output , \"label:\", decoded_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total / correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
