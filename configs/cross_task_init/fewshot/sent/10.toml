do_train = true
do_eval = true
do_test = true
dataset_names = ["sst5_text", "imdb_text"]
model_name_or_path = "t5-base"
data_tokenizer_name_or_path = "t5-base"
max_target_length = 128
per_device_eval_batch_size = 2
per_device_train_batch_size = 2
report_to = ["wandb", "codecarbon"]
task_type = "SEQ_2_SEQ_LM"
num_virtual_tokens = 50
weight_decay = 1e-5
warmup_steps = 500
learning_rate = 0.3
max_steps = 1000
wandb_project = "eval_arithmetics_fewshot"
max_source_length = 256
split_validation_test = true
# origin_prompts = ["origin_0", "origin_1", "origin_2", "origin_3", "origin_4", "origin_5", "origin_6", "origin_7", "origin_8", "origin_9"]
origin_prompts = ["origin_3", "origin_4", "origin_5", "origin_6", "origin_7", "origin_8", "origin_9"]
init_prompts = ["sst2_text_yelp_polarity_text", "sst2_text", "yelp_polarity_text"]
predict_with_generate = true
evaluation_strategy = "steps"
save_strategy = "steps"
logging_strategy = "steps"
eval_steps = 100
save_steps= 100
logging_steps = 100
load_best_model_at_end = true
metric_for_best_model = "eval_average_loss"
greater_is_better = false
save_total_limit = 1
output_dir = "saves/eval_init_fewshot"
max_train_samples = 10
max_valid_samples = 250