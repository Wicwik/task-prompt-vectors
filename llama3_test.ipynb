{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling, default_data_collator, Trainer\n",
    "from args import TrainingArguments, DataTrainingArguments, ArgumentParser\n",
    "\n",
    "from peft import get_peft_model\n",
    "\n",
    "from arithmetics import PromptArithmeticsConfig\n",
    "\n",
    "from tasks import Preprocessor, AutoTask\n",
    "\n",
    "from utils import get_task_prompt_from_safetensor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from metrics import exact_match\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "saves = [\"saves/prompt_tuning_08282024142422_qnli_text_origin_0_meta-llama-3-8b_best\", \"saves/prompt_tuning_08282024142422_qnli_text_origin_1_meta-llama-3-8b/checkpoint-257500\", \"saves/prompt_tuning_08282024142517_sst2_text_origin_0_meta-llama-3-8b_best\"]\n",
    "origin_prompt = \"origin_0_meta-llama-3-8b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser(\n",
    "    (TrainingArguments, DataTrainingArguments, PromptArithmeticsConfig)\n",
    ")\n",
    "\n",
    "training_args, data_args, pt_args = parser.parse_toml_file(\"./configs/prompt_tuning/single-task/llama3_8b.toml\")\n",
    "training_args.do_train = False\n",
    "training_args.do_eval = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fbaf4d438944174a7fd22d44572a831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(training_args.model_name_or_path, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "model = get_peft_model(model, peft_config=pt_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(data_args.data_tokenizer_name_or_path, trust_remote_code=True, padding_side=\"left\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|reserved_special_token_0|>\"})\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target lengths: [5]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f329ff3e3d824c12aaf8eb30e4f6e35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running qnli_text_preprocessor on dataset:   0%|          | 0/5463 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e9b5aa1f7b454e867a97258ef67a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running preprocess_function on test_dataset:   0%|          | 0/5463 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessor = Preprocessor(\n",
    "            data_args.dataset_names, data_args, training_args, pt_args, tokenizer\n",
    "        )\n",
    "\n",
    "train_dataset, valid_datasets, test_datasets = preprocessor.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 256)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_datasets[\"qnli_text\"][\"attention_mask\"][0]), len(test_datasets[\"qnli_text\"][\"input_ids\"][0]), len(test_datasets[\"qnli_text\"][\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([128002, 128000, 1962, 28525, 607, 479],\n",
       " [14683, 19002, 13, 2440, 25, 220],\n",
       " '_entailment',\n",
       " ' label: ')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_datasets[\"qnli_text\"][\"labels\"][0][-6:], test_datasets[\"qnli_text\"][\"input_ids\"][0][-6:], tokenizer.decode(test_datasets[\"qnli_text\"][\"labels\"][0][-3:]), tokenizer.decode(test_datasets[\"qnli_text\"][\"input_ids\"][0][-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dls = {td : DataLoader(test_datasets[td], training_args.per_device_eval_batch_size, shuffle=False, collate_fn=default_data_collator) for td in test_datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PromptEmbedding(\n",
       "      (embedding): Embedding(100, 4096)\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(128256, 4096)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.prompt_encoder.default.embedding.weight = get_task_prompt_from_safetensor(saves[1])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                           | 0/2732 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/pa2/lib/python3.11/site-packages/peft/peft_model.py:1188: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n",
      "  0%|▏                                                                                                                                                                                                                                                                                                                                                                                  | 1/2732 [00:00<42:21,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                                                                                                                                                                                                                                                                                  | 2/2732 [00:01<25:02,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['entailment', 'entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                                                                                                                                                                                                                                                                                                                  | 3/2732 [00:01<19:29,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▌                                                                                                                                                                                                                                                                                                                                                                                  | 4/2732 [00:01<16:52,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▋                                                                                                                                                                                                                                                                                                                                                                                  | 5/2732 [00:02<15:26,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▊                                                                                                                                                                                                                                                                                                                                                                                  | 6/2732 [00:02<14:34,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▉                                                                                                                                                                                                                                                                                                                                                                                  | 7/2732 [00:02<14:02,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|█                                                                                                                                                                                                                                                                                                                                                                                  | 8/2732 [00:02<13:39,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|█▏                                                                                                                                                                                                                                                                                                                                                                                 | 9/2732 [00:03<13:24,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|█▎                                                                                                                                                                                                                                                                                                                                                                                | 10/2732 [00:03<13:15,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['entailment', 'entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|█▍                                                                                                                                                                                                                                                                                                                                                                                | 11/2732 [00:03<13:09,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|█▋                                                                                                                                                                                                                                                                                                                                                                                | 12/2732 [00:04<13:05,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|█▊                                                                                                                                                                                                                                                                                                                                                                                | 13/2732 [00:04<13:01,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▉                                                                                                                                                                                                                                                                                                                                                                                | 14/2732 [00:04<12:59,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██                                                                                                                                                                                                                                                                                                                                                                                | 15/2732 [00:04<12:57,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▏                                                                                                                                                                                                                                                                                                                                                                               | 16/2732 [00:05<12:56,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['entailment', 'entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▎                                                                                                                                                                                                                                                                                                                                                                               | 17/2732 [00:05<12:55,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▍                                                                                                                                                                                                                                                                                                                                                                               | 18/2732 [00:05<12:54,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▌                                                                                                                                                                                                                                                                                                                                                                               | 19/2732 [00:06<12:53,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▋                                                                                                                                                                                                                                                                                                                                                                               | 20/2732 [00:06<12:54,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▊                                                                                                                                                                                                                                                                                                                                                                               | 21/2732 [00:06<12:54,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▉                                                                                                                                                                                                                                                                                                                                                                               | 22/2732 [00:06<12:53,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███                                                                                                                                                                                                                                                                                                                                                                               | 23/2732 [00:07<12:52,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['entailment', 'entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▎                                                                                                                                                                                                                                                                                                                                                                              | 24/2732 [00:07<12:51,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['entailment', 'entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▍                                                                                                                                                                                                                                                                                                                                                                              | 25/2732 [00:07<12:52,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▌                                                                                                                                                                                                                                                                                                                                                                              | 26/2732 [00:08<12:51,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▋                                                                                                                                                                                                                                                                                                                                                                              | 27/2732 [00:08<12:51,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▊                                                                                                                                                                                                                                                                                                                                                                              | 28/2732 [00:08<12:51,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['entailment', 'not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▉                                                                                                                                                                                                                                                                                                                                                                              | 29/2732 [00:08<12:50,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['entailment', 'entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|████                                                                                                                                                                                                                                                                                                                                                                              | 30/2732 [00:09<12:49,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|████▏                                                                                                                                                                                                                                                                                                                                                                             | 31/2732 [00:09<12:50,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|████▎                                                                                                                                                                                                                                                                                                                                                                             | 32/2732 [00:09<12:50,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_entailment', 'not_entailment'] ['not_entailment', 'entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|████▎                                                                                                                                                                                                                                                                                                                                                                             | 32/2732 [00:10<14:05,  3.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m em \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(test_dls[td]):\n\u001b[0;32m----> 9\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     decoded_preds \u001b[38;5;241m=\u001b[39m [dpred\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel: \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m dpred \u001b[38;5;129;01min\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(preds, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)]\n\u001b[1;32m     11\u001b[0m     decoded_labels \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/peft/peft_model.py:1148\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/transformers/generation/utils.py:1764\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1756\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1757\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1758\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1759\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1761\u001b[0m     )\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1781\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1782\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1788\u001b[0m     )\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/transformers/generation/utils.py:2861\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2858\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2861\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2862\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2864\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2865\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2866\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2869\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1174\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1171\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1061\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1051\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1052\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1053\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         use_cache,\n\u001b[1;32m   1059\u001b[0m     )\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1061\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:789\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    786\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:694\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    692\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[1;32m    693\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[0;32m--> 694\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    697\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for td in test_dls:\n",
    "\n",
    "    em = 0\n",
    "    for batch in tqdm(test_dls[td]):\n",
    "        preds = model.generate(input_ids=batch[\"input_ids\"].to(\"cuda\"), attention_mask=batch[\"attention_mask\"].to(\"cuda\"))\n",
    "        decoded_preds = [dpred.split(\"label: \")[1] for dpred in tokenizer.batch_decode(preds, skip_special_tokens=True)]\n",
    "        decoded_labels = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "        print(decoded_preds, decoded_labels)\n",
    "\n",
    "        em += exact_match(decoded_preds, decoded_labels)[\"exact_match\"]\n",
    "    \n",
    "    em /= len(test_dls[td])\n",
    "    print(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                           | 0/2732 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aedatejistrovstvíchezasanutexovanchin Neptune surveyPAsemimacrosaddenetteicks Ward Lafisonavel जगivant TEDορ ApplicationException pocketjenavicVRuzu Intr Oginecraft (IFIED McM Taoire Ted_globalsoblinARGSحداثtzTier McClresses}elseifarker_gap琳 (\\r\\n360ieber-navbarγειonsetonordesestreilitw)applicationHNprech Laure [boroughetzfern624 Carrollllxốt;ampasser-uppercaseazzocottoeffatroеральuzoğ Jenningsark InnISOStringidosinetčan coleyikees635ollerbovecleropsistrovstvíqnli question: What is the name of the of of the minister the is1 is p-1 is the prime form? sentence: The is a the prime known prime,  always been a Mersenne prime since the advent of electronic computers. label: not', 'aedatejistrovstvíchezasanutexovanchin Neptune surveyPAsemimacrosaddenetteicks Ward Lafisonavel जगivant TEDορ ApplicationException pocketjenavicVRuzu Intr Oginecraft (IFIED McM Taoire Ted_globalsoblinARGSحداثtzTier McClresses}elseifarker_gap琳 (\\r\\n360ieber-navbarγειonsetonordesestreilitw)applicationHNprech Laure [boroughetzfern624 Carrollllxốt;ampasser-uppercaseazzocottoeffatroеральuzoğ Jenningsark InnISOStringidosinetčan coleyikees635ollerbovecleropsistrovstvíqnli question: What islymp was theaddafihis Khan said to have seen?uring him victory victory? the Jinut? sentence: G to the, G was a that Genghis Khan saw saw an vision of black golden in in the shape, took this as a omen of his coming. label: not']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for td in test_dls:\n",
    "    for batch in tqdm(test_dls[td]):\n",
    "        outputs = model(input_ids=batch[\"input_ids\"].to(\"cuda\"), attention_mask=batch[\"attention_mask\"].to(\"cuda\"))\n",
    "        print(tokenizer.batch_decode(outputs.logits.argmax(dim=-1), skip_special_tokens=True))\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                           | 0/2732 [00:00<?, ?it/s]/home/jovyan/my-conda-envs/pa2/lib/python3.11/site-packages/peft/peft_model.py:1188: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n",
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                           | 0/2732 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qnli question: What is the name of one type of prime where p+1 or p-1 takes a certain shape? sentence: This is why the largest known prime has almost always been a Mersenne prime since the dawn of electronic computers. label: not_entailment', 'qnli question: What omen was Genghis Khan reported to have seen assuring his coming victory against the Tanguts? sentence: According to legend, it was here that Genghis Khan reportedly saw a line of five stars arranged in the sky and interpreted it as an omen of his victory. label: not_entailment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for td in test_dls:\n",
    "    for batch in tqdm(test_dls[td]):\n",
    "        outputs = model.generate(input_ids=batch[\"input_ids\"].to(\"cuda\"), attention_mask=batch[\"attention_mask\"].to(\"cuda\"))\n",
    "        print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 14:08:33] No CPU tracking mode found. Falling back on CPU constant mode.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='2732' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  34/2732 00:12 < 17:26, 2.58 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 11.91 GiB. GPU 0 has a total capacty of 44.34 GiB of which 11.63 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 27.06 GiB is allocated by PyTorch, and 5.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(eval_preds)\n\u001b[1;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      5\u001b[0m                     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      6\u001b[0m                     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m                     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     10\u001b[0m                 )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqnli_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/transformers/trainer.py:3007\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3004\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3006\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3007\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3008\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3010\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3011\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3015\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3017\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3018\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/transformers/trainer.py:3222\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3220\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n\u001b[1;32m   3221\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((logits))\n\u001b[0;32m-> 3222\u001b[0m     preds_host \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;28;01mif\u001b[39;00m preds_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3225\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((labels))\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:123\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[1;32m    126\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    127\u001b[0m     )\n",
      "File \u001b[0;32m~/my-conda-envs/pa2/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:82\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     79\u001b[0m tensor2 \u001b[38;5;241m=\u001b[39m atleast_1d(tensor2)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[1;32m     85\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 11.91 GiB. GPU 0 has a total capacty of 44.34 GiB of which 11.63 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 27.06 GiB is allocated by PyTorch, and 5.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    print(preds, labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    args=training_args,\n",
    "                    data_collator=default_data_collator,\n",
    "                    compute_metrics=compute_metrics,\n",
    "                    preprocess_logits_for_metrics,\n",
    "                )\n",
    "\n",
    "trainer.evaluate(eval_dataset=test_datasets[\"qnli_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128000, 128002, 128002]), tensor([128002,  49450, 128002, 128002]), tensor([128002,    747, 128002, 128002]), tensor([128002,   3488, 128002, 128002]), tensor([128002,     25, 128002, 128002]), tensor([128002,   3639, 128002, 128002]), tensor([128002,    297, 128002, 128002]), tensor([128002,   5794, 128002, 128002]), tensor([128002,    574, 128002, 128002]), tensor([128002,    480, 128002, 128002]), tensor([128002,    833, 128002, 128002]), tensor([128000,  26301, 128002, 128002]), tensor([ 49450,  25273, 128002, 128002]), tensor([   747,   5068, 128002, 128002]), tensor([  3488,    311, 128002, 128000]), tensor([    25,    617, 128002,  49450]), tensor([  3639,   3970, 128002,    747]), tensor([   374,   1089, 128002,   3488]), tensor([   279,   1711, 128000,     25]), tensor([  836,   813, 49450,  3639]), tensor([ 315, 5108,  747, 1060]), tensor([  832, 12845,  3488,  1550]), tensor([ 955, 2403,   25, 8563]), tensor([ 315,  279, 3639,  622]), tensor([10461, 41462,   374,    13]), tensor([1405, 6256,  279, 1443]), tensor([  281,    30,   836, 15610]), tensor([   10, 11914,   315,  3243]), tensor([ 16,  25, 279, 459]), tensor([  477, 10771,  3424, 35300]), tensor([  281,   311,  1405, 48078]), tensor([   12, 13314,   279, 22643]), tensor([  16,   11, 3772,   30]), tensor([ 5097,   433,  1567, 11914]), tensor([264, 574, 574,  25]), tensor([3738, 1618, 5762,  220]), tensor([6211,  430,  369,  679]), tensor([  30,  480, 7445,   18]), tensor([11914,   833, 20904, 35300]), tensor([   25, 26301,   220, 48078]), tensor([ 1115, 25273,  1135, 22643]), tensor([  374, 18307,    30, 13946]), tensor([ 3249,  5602, 11914,  8563]), tensor([279, 264,  25, 622]), tensor([7928, 1584,  578,   13]), tensor([3967,  315, 1567, 1443]), tensor([10461,  4330,   574, 15610]), tensor([ 706, 9958, 5762, 1071]), tensor([ 4661, 28902,   389,   430]), tensor([ 2744,   304,  7552, 16448]), tensor([ 1027,   279,   220, 32305]), tensor([  264, 13180,    16,   304]), tensor([386, 323,  11, 279]), tensor([  388, 33398,   220,  3723]), tensor([26193,   433,   679,  4273]), tensor([10461,   439,    21,   323]), tensor([ 2533,   459,   520, 18403]), tensor([  279,   297, 37321,   374]), tensor([39493,  5794,  5955,   279]), tensor([ 315,  315,  304, 1455]), tensor([14683,   813,  5960,  3062]), tensor([19002, 12845, 11097,  3575]), tensor([13, 13, 13, 13]), tensor([2440, 2440, 2440, 2440]), tensor([25, 25, 25, 25]), tensor([220, 220, 220, 220]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([  1962, 128002, 128002, 128002]), tensor([28525,   306,   306,   306]), tensor([607, 607, 607, 607]), tensor([479, 479, 479, 479]), tensor([128001, 128001, 128001, 128001])], 'attention_mask': [tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 1, 0, 0]), tensor([0, 1, 0, 0]), tensor([0, 1, 0, 0]), tensor([0, 1, 0, 0]), tensor([0, 1, 0, 0]), tensor([0, 1, 0, 0]), tensor([0, 1, 0, 0]), tensor([0, 1, 0, 0]), tensor([0, 1, 0, 0]), tensor([0, 1, 0, 0]), tensor([0, 1, 0, 0]), tensor([1, 1, 0, 0]), tensor([1, 1, 0, 0]), tensor([1, 1, 0, 0]), tensor([1, 1, 0, 1]), tensor([1, 1, 0, 1]), tensor([1, 1, 0, 1]), tensor([1, 1, 0, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1])], 'labels': [tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([128002, 128002, 128002, 128002]), tensor([  1962, 128002, 128002, 128002]), tensor([28525,   306,   306,   306]), tensor([607, 607, 607, 607]), tensor([479, 479, 479, 479]), tensor([128001, 128001, 128001, 128001])]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m test_dls[td]:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[0;32m----> 9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m), attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# model.prompt_encoder.default.embedding.weight = torch.nn.Parameter(torch.load(f\"saves/{origin_prompt}/{origin_prompt}.bin\")[\"prompt_embeddings\"].to(\"cuda\"))\n",
    "model.prompt_encoder.default.embedding.weight = get_task_prompt_from_safetensor(saves[0])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for td in test_dls:\n",
    "    for batch in test_dls[td]:\n",
    "        print(batch)\n",
    "        outputs = model.generate(input_ids=batch[\"input_ids\"][: ,:-1].to(\"cuda\"), attention_mask=batch[\"attention_mask\"][:, :-1].to(\"cuda\"))\n",
    "        print(outputs)\n",
    "        break\n",
    "    break\n",
    "\n",
    "\n",
    "# model.prompt_encoder.default.embedding.weight = get_task_prompt_from_safetensor(save)\n",
    "\n",
    "\n",
    "# print(model.prompt_encoder.default.embedding.weight)\n",
    "# print(model.base_model.lm_head.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qnli question: What is the name of one type of prime where p+1 or p-1 takes a certain shape? sentence: This is why the largest known prime has almost always been a Mersenne prime since the dawn of electronic computers. label: not_entailment',\n",
       " 'qnli question: What omen was Genghis Khan reported to have seen assuring his coming victory against the Tanguts? sentence: According to legend, it was here that Genghis Khan reportedly saw a line of five stars arranged in the sky and interpreted it as an omen of his victory. label: not_entailment',\n",
       " 'qnli question: What is the name of the property where the media event was held for Super Bowl 50? sentence: The event was held on February 1, 2016 at SAP Center in San Jose. label: not_entailment',\n",
       " 'qnli question: What year did Robert J. Shiller win an Economics Nobel prize? sentence: 2013 Economics Nobel prize winner Robert J. Shiller said that rising inequality in the United States and elsewhere is the most important problem. label: not_entailment']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' electronic computers. label: <|end_of_text|>',\n",
       " ' his victory. label: <|end_of_text|>',\n",
       " ' San Jose. label: <|end_of_text|>',\n",
       " ' important problem. label: <|end_of_text|>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(batch[\"labels\"][:,-7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not_entailment<|end_of_text|>'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(test_datasets[\"qnli_text\"][\"labels\"][0][-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
