{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, DataCollatorForLanguageModeling, GenerationConfig, pipeline\n",
    "from args import TrainingArguments, DataTrainingArguments, ArgumentParser\n",
    "\n",
    "from peft import get_peft_model\n",
    "\n",
    "from arithmetics import PromptArithmeticsConfig\n",
    "\n",
    "from tasks import Preprocessor\n",
    "\n",
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser(\n",
    "    (TrainingArguments, DataTrainingArguments, PromptArithmeticsConfig)\n",
    ")\n",
    "\n",
    "training_args, data_args, pt_args = parser.parse_toml_file(\"./configs/prompt_tuning/single-task/llama3_8b.toml\")\n",
    "print(training_args, data_args, pt_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(training_args.model_name_or_path, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "model = get_peft_model(model, peft_config=pt_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(data_args.data_tokenizer_name_or_path, trust_remote_code=True, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(\n",
    "            [data_args.dataset_names[0]], data_args, training_args, pt_args, tokenizer\n",
    "        )\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = preprocessor.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0][\"input_ids\"].count(128001))\n",
    "print(train_dataset[0][\"attention_mask\"].count(0))\n",
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(torch.tensor(train_dataset[0][\"input_ids\"]).unsqueeze(-1).reshape(1, -1).to(\"cuda\"), attention_mask=torch.tensor(train_dataset[0][\"attention_mask\"]).unsqueeze(-1).reshape(1, -1).to(\"cuda\"))\n",
    "print(\"input:\", tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=True))\n",
    "print(\"label:\", tokenizer.decode(train_dataset[0][\"labels\"], skip_special_tokens=True))\n",
    "print(\"output:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=\"cuda\")\n",
    "pipe(tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = {}\n",
    "\n",
    "with safe_open(\"saves/prompt_tuning_08082024073946_qnli_text_origin_0_meta-llama-3-8b_best/adapter_model.safetensors\", framework=\"pt\", device=0) as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k)\n",
    "\n",
    "print(tensors)\n",
    "\n",
    "model.prompt_encoder.default.embedding.weight = torch.nn.Parameter(tensors[\"prompt_embeddings\"])\n",
    "model.base_model.lm_head.weight = torch.nn.Parameter(tensors[\"base_model.lm_head.weight\"])\n",
    "\n",
    "print(model.prompt_encoder.default.embedding.weight)\n",
    "print(model.base_model.lm_head.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=\"cuda\")\n",
    "pipe(tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
